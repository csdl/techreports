%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ReviewerResponse.tex --     Automated Software Engineering submission
%% Author          : Philip Johnson 
%% Created On      : Tue Jan 06 10:41:51 2009
%% Last Modified By: Philip Johnson
%% Last Modified On: Mon Jul 13 11:18:28 2009
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 2009  Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 

\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{url}             


\begin{document}
\title{Responses to Reviewer Comments for \\
Operational Definition and Automated Inference of Test-Driven Development with Zorro}
\author{Hongbing Kou \and Philip M. Johnson \and Hakan Erdogmus}
\date{July, 2009}

\maketitle

\section{Editor's comment and response}


\bigskip {\noindent \em The single most important issue is touched on by both Reviewer \#1 and Reviewer \#2 (\#1's first comment
and \#2's triple-starred (***) comment). Please make certain you address that one well.  Both reviewers
have asked for more detail on the JESS encoding of your rules (in different ways). Please consider
spending a few paragraphs on this issue as well as consider whether adding an appendix with the
rule base is feasible. Finally, you will see that the reviewers' advice with respect to the unsuccessful industrial 
case study are not the same. I would strongly prefer that you adhere to reviewer \#2's advice in this regard,
as I feel a slightly deeper explanation of it could provide useful information to the reader and any researchers
wishing to follow up on your work.}

\noindent Response: (forthcoming)


\section{Reviewer 1 comments and responses}


{\noindent \em My main problem with the paper is that it seems there is a consistency issue with the description of episode types (Figure 3). Specifically, TF-1/TF-2/TF-3/TF-4 which are the only TDD conformant episodes include in their definition RF-3/CP-1/CP-2/CP-3 which are context sensitive or even non conformant.  I refer to the sub-pattern 'code editing -$>$ test pass' that is at the end of TF-1/TF-2/TF-3/TF-4. This sub-pattern is actually RF-3/CP-1/CP-2/CP-3 with different explanations for 'code editing' that cover the code editing possibilities. The implication is that you do not have a way to determine a TDD conformant episode. Please explain.}

\noindent Response: Instead of calling RF-3/CP-1/CP-2/CP-3 sub-patterns of
TF-1/TF-2/TF-3/TF-4, you could say that TF-1/TF-2/TF-3/TF-4 are more  deterministic development behaviors as they follow a more rigid workflow.
If there weren't test case creation activities (determined by increase of test methods and/or assertion statements), an episode once classified as TF-X would be RF-3 or CP-X. To make things easier, we could simply classify all development
behavior as production. But doing so would not help us understand how Test-Driven Development stands out and is actually different from other development processes such as Test-Last Development and ad-hoc. In summary,  rules of TF-X episodes are more specific than rules of RF-3 and CP-X episodes. The JESS rule engine would apply the most specific rules applicable on an episode in the recognition process, in turn we could differentiate TF-X episodes from RF-3 and CP-X episodes.

\noindent Hakan:  The reviewer is righ in being confused here. I think we should explain this differently (I don't think the engine matches the most specific rule. It's effectively the opposite.).First, we state in the paper that "The definition column only gives a typical instance for each episode category. The implementation allows repetitions of certain sub-patterns inside." So the ambiguity is understandable. The episodes are delimited by "Test pass" subpatters. The "Test pass" subpattern is matched initially to split a development data into disjoint episodes. Then each episode is assigned to a category. To assign an episode into a category, every part of the episode must match a subpattern, starting from the beginning of an episode. Therefore, for an episode to be matched to a category, the first subpattern of the episode must be matched to the first subpattern specified by that category's definition. For example, an episode beginning with a "Code editing" would not be assigned to category TF-1, since the first subpattern in TF-1 is "Test creation". But if the subpattern satisfies an additional constraint (a decrease in number of statements or methods in the associated source file), and is immediately followed by a "Test pass" subpattern that terminates the episode, the episode is assigned to category RF-3. In terms of regular expression matching, this stragegy corresponds to having an explicit episode-begin token that must be matched for every category pattern. 

each episode category as prescribed by that category

\bigskip {\noindent \em In the introduction: It is suggested to add references when first mentioning Zorro.}

\noindent Response: \cite{csdl2-07-04}

\bigskip {\noindent \em ".. once installed, there is no overhead on the developer with respect to data collection" seems too strong. The process that is activated by the developer is surely affected even if minimally.}

\noindent Response: Hakan: There is a bit of a concern here. Burak had it installed at a company in Turkey. The devs claimed it slowed down Eclipse and they removed it later. This was not my experience with it in the older version though. I barely noticed it. I have addressed this in the text by the explanation "Automatic collection
and analysis of data make Zorro practical for use in both laboratory and
real-world settings: once installed, overhead on the developer
with respect to data collection is minimal. Zorro is unobtrusive:  
it works in the background listening on the events of interest in the environment 
in which it is installed. It does not require input from the developer. 
However it incurs a small performance penalty, which depends on the environment." 


\bigskip {\noindent  \em Section 2 is named 'related work' but starts with 2.1 which is the practice of TDD which is not related work. It is suggested to present TDD in a separate section leaving the last two paragraphs of 2.1 in the related work.}

\noindent Response: Hakan: Done. Section 2.1 has been moved to a separate section (except the last two paragraphs). The last two paragraphs of the former Section 2.1 becomes the first subsection of "Related Work" titled "Claims about TDD".


\bigskip {\noindent \em  Section 2.4 starts with the noun "Automated software process research systems" - it is suggested to rephrase for clarity.}

\noindent Response: Hakan: Done. Changed to "Research on automated inference of software process"

\bigskip {\noindent \em  Also in 2.4 -ISPW 6/7 is not referenced or explained. Hackystat is not referenced.   Please reference these.}

\noindent Response: \cite{ispw6} and \cite{Hackystat:06,csdl2-04-11,csdl2-04-22,csdl2-03-12} Detailed description to ISPW-6 can be found at http://www.idi.ntnu.no/~epos/ispw6.html. Hakan: Done. Added citations to the manuscript. 

\bigskip {\noindent \em  In Section 3.2: It seems that you assume a 'nice work' of the user as presented in Figure 2. Taking real data it is not always so clear. It is expected to see also an unclear example in order to get the feeling of the problem and its translating. You write about the need to do it but then delve immediately to the episodes.}

\noindent Response: Hakan/Hongbing: What we meant here was that if the development scenario is neat and clean, it's easy to identify TDD behaviors. However they are not, and of course we do specifically handle the cases in which they are not. The main purpose of this section is to describe the mechanism behind Zorro on inferring development behavior of TDD if a developer is actually doing it. Recognition of more realistic, diverse, and complex episodes are explained in section 3.3. The complexity of handling realistic behaviors is in the diversity of the 20+ episode types and in the ability to classify ambiguous episodes using context-sensitivity. This paragraph was rephrased to clarify: "In the case where a developer follows the
canonical TDD approach involving a few episode types, identifying TDD behavior is relatively easy.  The more important issue is how to deal with the
complexities of real world software development behaviors.  An example of variant TDD behavior that is difficult to recognize is test addition followed by successful test invocation, but with no code editing. This behavior may happen both in the context of applying TDD and outside TDD. To be able to recognize it as TDD behavior, Zorro looks for neigboring episodes that are easily identified as typical TDD. If such an ambiguous episode occurs in the context of other episodes easily identified as TDD, they are considered to be part of TDD behavior. Otherwise, they are classified as non-TDD-conformant. Thus Zorro is capable of context-sensitive episode classification. In the next section, we explain how Zorro
handles more diverse and ambiguous episode types to recognize a wide range of behaviors."


\bigskip {\noindent \em  Relating to Figure 2, how do we know that for example step 1 is test creation. It is not shown in the stream and not further explained.}

\noindent Response: Hakan/Hongbing The figure does not show the metric data associated with development activities. Inferring Each activity is an aggregration of a series of snapshot of file metrics. Metrics of test code include number of test cases and assertations in test cases. if these two metrics increase as a result of an activity, we can infer whether the activity is about test creation. 

\bigskip {\noindent \em  In section 3.2 the data should be better explained so the analysis you present will be clearer. }

\noindent Response: Hakan: more explanation provided in Section 3.2. Philip to check. 

\bigskip {\noindent \em  The example in Figure 2 shows only one TDD iteration. Readers can benefit from a complete TDD example that shows few test-code steps and refactoring (as the practice states).}

\noindent Response: Hongbing: The following section(3.3) addresses more variation of TDD iterations including TF-1/TF-2/TF-3/TF-4/RF/CP etc. 
\noindent Response: Hakan: putting on my editor hat, I don't think this would satisfy the reviewer entirely. It sounds like dodging his suggestion, and reviewers often get upset by this. Can we include another example and figure?

\bigskip {\noindent \em   Do we have any indication that the test is not empty (trivial pass) in Figure 2 step 5? If yes, please add it in text.}

\noindent Response: Hakan/Hongbing: Yes. As mentioned before we know number of test cases and assertations. Explaination added to Section 3.2 by Hakan.

\bigskip {\noindent \em  The TDD iteration you selected for Figure 2 does not include a test-fail event but rather a compilation-error event. Is there a reason for that? Please add in text.}

\noindent Response: Hongbing/Hakan: This example is from the begining of a development session in TDD. Following principals of TDD this developer made the production method return a hard-coded constant to make the test pass. He would then refactor the production code with real behavior. The compilation error is automatically reported by the IDE because the production code under test had not been developed yet.

\noindent Response: Hakan: can we not change the figure to have one of the episodes contain a test-fail action? It seems easy to do. adding a test fail action in the top stream in Fig 2 between type 3 and type 4 actions would do it. it can be done manually. Can Hongbing do this?


\bigskip {\noindent \em  In Section 3.3: Test creation is not explained. Is this for a class or a method? Both? }

\noindent Response: Hongbing/Hakan: Not class but test methods and assertions. Now explained in Section 3.2. 

\bigskip {\noindent \em  With respect to the TF-X definitions, can you elaborate on why you searched patterns that end with 'test pass' vs patterns that end with 'test fail'? This is another way to look at these rules and it can be of interest to the readers.}

\noindent Response: Hongbing: Completely agree. We plan to add more ways to tokenize development streams to other types of episodes to study development process 
other than TDD, one idea is to study when and how deveopers change from one artifact to another in development. The reason that we choose successful test run as end activity of an episode is that TDD advocates that developers should frequently run test suites to get rapid feedback and then make failing test work quickly (the stop-light metaphor). 

\noindent Response: Hakan: Agree with Hongbing if the purpose is not to analyze TDD but other processes (like what?), but Zorro's purpose is to analyze TDD and its variants that operate on the principle of incremental developments. In TDD test-passes are landmark events. They indicate the end of an incremental development cycle. Test fails represent transient states and happen often. Episodes delimited by test-fail events are not in themselves meaningful.  

\bigskip {\noindent \em  Code editing as appears in the different episodes is not clear. Does it mean editing in a specific file? Specific method? Why 100 bytes is the number to differ between CP-2 and CP-3? How do you match test editing with the appropriate code editing? According to method? Class file? Please elaborate.}

\noindent Response: Hongbing: 100 is a magic number here and changing it to 200 or other values would not change its nature. This number tells whether there is significant amount of work in an episode. If there is significant amount of work produced in a very short time period, a developer is likely not following TDD and likely copies and pastes code from elsewhere. 

\noindent Response: Hakan: Why would copy-paste be counter to TDD? I don't understand this. Mine would be: 100 is just a threshold we use in the heuristic to signify significant work. Minor code editing is permitted, but editing significant amount of new production code without running tests (e.g. either adding a large chunk of functionality, or a big refactoring) in one increment, i.e., in a single episode, would be counter to TDD. I explained it in the text as: "Zorro uses several heuristics based on the file metrics assoicated with elementary actions. For example, in episode types CP-2 ad CP-3, the underlying heuristic checks whether the code editing action increases the size of the source file by more or less than a preset, threshold (currently set to 100 bytes). The threshold determines whether the editing action involves significant amount of production code activity in a single chunk without running the tests. If so, the behavior is counter to TDD and classified as non-conformant. A small amount of production code editing would be indicative of minor tweaks or the addition of a small amount of functionality, such as the addition of getter and setter methods, that do not always warrant the addition of a corresponding test depending on the developer's personal style or the development team's standard practice. This latter behavior is permissible and expected in TDD. Similarly, the long-episode thresholds used in the episode types LN-1 and LN-2 are  indicative of behaviors that are too coarse grained, or not sufficiently incremental, for TDD. TDD, being an inherently fine-grained incremental process, discourages large amounts of source or test code editing without running and passing tests.   The significant activity threshold and the long-episode thresholds used in episode types LN-1 and LN-2 are arbitrary, but can be changed depending on the expected granularity of the applied process and the tolerance level for deviating from the expected granularity."

\bigskip {\noindent \em  It is not clear why 'test editing -$>$ test pass' is necessarily refactoring (RF-1).}

\noindent Response: Hongbing/Hakan: Because if there is no test creation (and we have the metrics
on test methods and assertions to infer that) or no production editing, then the episode is about refactoring existing tests. Test code is code and may be subject to refactoring. Hakan: Now explained in the text. 

\bigskip {\noindent \em  What between episodes and rules? Are these the same?}

\noindent Response: Hakan: Rules are used to infer the action types, episode types, and episode conformance. Now explained in beginning of section 3.3 as "The heart of Zorro is its episode classification algorithm, implemented as
a set of JESS rules. JESS rules are applied on a raw development stream recorded by the Hackystat sensor
installed in the developer environment to infer elementary action types first. Once action types are
assigned, higher level rules are used to infer episode types from streams of actions and classify episodes that can be identified as TDD-conformant or not in isolation. Finally, additional rules are applied to classify uncategorized episodes from the episode stream in which they occur using contextual information." // Philip to check this explanation. 

\bigskip {\noindent \em  For refactoring you may also check cyclomatic complexity so to not lean only on number of methods and statements.}

\noindent Response: Hongbing/Hakan: We agree. It would be interesting to include it. We may consider it in the next release. 

\bigskip {\noindent \em  Can you explain why the TF-X episodes do not include 'test editing' or why all start with 'test creation'? It seems odd for TDD conformant episodes.}

\noindent Response: Test editing means that test code has been changed but we
did not observe new test cases. If there were, we would claim the editing activity as test creation.  That said, new production code should be driven by failing unit tests in TDD so TF-X must have test creation(s). Developer can edit test code although the development process is Test-Last. 

\bigskip {\noindent \em  What is activity in LN-1? It seems everything is included in LN-1.}

\noindent Response: Hongbing: Short-iteration is a characteristic of TDD and an iteration should last a few seconds to several minutes only. And developers can only have limited number of activities in such a short period of time. The development process would most likely be ad-hoc not TDD if hundred development activities are contributed. LN-1 is defined so that TDD recognition rules won't be evaluated on these kinds of software development process. 

\noindent Response: Hakan: The answer is related to the comment about the 100 bytes threshold above. TDD is an incremental process and as such a large amount of work is not performed without running and passing tests. These episodes are detected by a preset threshold. Explained in the text now.  


\bigskip {\noindent \em  In Section 3.4: The terminology is changed in Figure 4: in the first case shown in the figure? does production edit is code edit? If so it is CP-2 and it is unambiguous. }

\noindent Response: Hongbing: Yes. Agree.

\noindent Response: Hakan: Don't understand Hongbing's response. What action is he suggesting? I think all references to Code editing should be changed to production editing. I did a search/replace in the text. figure 2 should be changed as well. Regarding Cp-2 being ambigious, I think the reviewer means: why do we say "it looks like" in the figure? Because nothing is sure, and we always present the episode type assignments with some degree of uncertainty. I added to the text "Under the column titled Zorro's Inference, the episode classifications are always 
presented with uncertainty (with wording such as "this portion of developmetn appears to be..." and "this episode looks like...") because no absolute correct classification exists. Zorro's episode classification, like in other similar tools, is heuristic-based and captures our our understanding of the TDD process."

\bigskip {\noindent \em  The second case in Figure 2 //Hakan: Figure 4? // seems as RF-1 and CP-2 so why TDD conformant?}

\noindent Response: Hakan: This case is TF-1. The explanation in pink under the "Zorro's Inference" column explains why it's TF-1. Explained in the text: "For example, the second case in the figure represents a TF-1 type episode. The summary enclosed in 
the dashed box lists a sequence of activities that match the TF-1 pattern in \ref{fig:Categories}."

\bigskip {\noindent \em  How come that the associated file is not applicable? (p. 17 line 29-30)}

\noindent Response: This is due to the fact that the IDE can fire a build event without associating this activity with a file. 

\noindent Response: hakan: can't find the item on p. 17????

\bigskip {\noindent \em  Does event type in the UI is activity? Also, in Figure 5 does PR is CP?}

\noindent Response: Yes, it is episode type. 

\bigskip {\noindent \em  You refer to real world data in p. 18 line 47. Please elaborate if this is from one person? A team? With pair programming? Without? Which kind of a project? Skills and experience? Etc.}

\noindent Response: This data is from a single developer without pair-programming. We use this chart to illustrate the capability of analysis one can do with Zorro. We did not use data from a case study or experiment. 

\bigskip {\noindent \em  How do you explain Figure 7? You write it is interesting still do not give any further discussion. }

\noindent Response: "This seems to resonate the statement that TDD helps to improve code coverage." (Philip, can you word it better)

\bigskip {\noindent \em  In Section 4:  In the introduction you write that you have industrial-based case studies. It was expected to see them here (Section 4). Then I saw you mentioned them in Section 5 mainly to say it didn't go well yet. I suggest omitting all sentences with this respect (from the introduction and section 5). Of course it can be mentioned as future direction without all the explanations why it didn't go well so far).}

\noindent Response: Do we want to massage our experiemnt to make this paper look better?  Guess we wouldn't from science point of view. The lessons learned should benefit others who might consider doing the same.

\section{Reviewer 2 comments and responses}


This is a very well written paper, describing interesting work on a significant issue, namely measuring and
reporting on compliance with a defined process.  I have a few questions/comments below, but basically
feel it is a strong manuscript.

\bigskip {\noindent \em  Please provide a reference for Hackystat in sec 3.1}

\noindent Response: (forthcoming)

\bigskip {\noindent \em  [sec 3.1, "SDSA" subsection] What if a developer works on
  two or more independent problems (e.g. failing tests) concurrently?
  Can your tool find two episodes whose events overlap in time?
  E.g., Create test1. Create test2. Test1 fail. Test2 fail. Edit sourcefile 1.
  Edit sourcefile 2. Compile. Test1 pass. Test2 pass. 
  If so, please say this and explain briefly how it does this.
  If not, please discuss this possibility and explain if it is a limitation and
  whether it arises in practice.}

\noindent Response: Yes in most case. The metrics we collect are file-based. That said, if you can differentiate files of project A from file of projects B based on their  locations, then you could be abke to analyze them separately.
But if they happen to sit in the same folder, then it would be a problem.

\bigskip {\noindent \em  [sec 3.3] Are the JESS rules 1-for-1 with the lines of Figure 3, or are they
  more involved?  Please discuss this encoding and any issues it raises.
  Also, please provide the JESS rule base as an appendix if this is feasible.
  (I.e. if this is, say, $<$= 5 pages) Otherwise, is the rulebase available on the web?
  In any case, we need some discussion of the JESS-encoding and statistics about how
  many rules, possibly a brief discussion of run-time complexity of JESS on this problem.}

\noindent Response: (forthcoming)


\bigskip {\noindent \em  *** [Fig 3 and text surrounding] Some of your definitions appear to intersect, such as one
  is a superset (prefix, suffix, etc) of another. Does your rule matching always choose
  the longest possible match or the most specific match, both of which are pretty common
  in Rule Based Systems applications? Or do you have some other parsing strategy? 
  Please discuss/clarify.  E.g. RF-3 is a suffix of TF-1.}

\noindent Response: The most specific rules are matched in episode recognition.
Philip, can you add a paragraph to describe this. The first reviewer has this question as well.

\bigskip {\noindent \em  Please provide a reference for JESS the first time it is mentioned.}

\noindent Response: \cite{Friedman-Hill:03}

\bigskip {\noindent \em  [sec 5] Please move the discussion of the failed industrial case study from here
  to sec 4 (e.g. a new sec 4.3). Please explain the circumstances you feel led to failure,
  to the extent possible without revealing proprietary information or being gratuitously
  ad hominem.  (E.g. I don't want a full explanation of whose fault anything was, just
  a high level understanding of the main issues: did the prototype not work in the
  industrial IT architecture, time or cost constraints intervened, or what?)}

\noindent Response: (forthcoming) Philippe, can you address this?

\bibliographystyle{spbasic}      % basic style, author-year citations
\bibliography{tdd,zorro,csdl-trs,hackystat,psp}
\end{document}
