Votta's paper:

Central claim:
  Inspection meetings are bad because:
   * development interval increase occurs due to scheduling delay
   * 'synergy' was not observed to occur.
   * 'filtering' was not observed to occur.
   * 'education' is a false benefit of inspection.

Issues with the paper:

 * 'education' issue is debatable:
     * empirical data from Tektronix suggests that education can and does
       occur _about domain_ during inspection. Votta limits the scope of
       'education' to the inspection process itself, and suggests training
       classes are more effective, but does not factor in the cost of such
       a program to his recommendations. Even if it were cost-effective,
       such a training program would not be able to cover domain-specific
       issues.
     * Votta provides no empirical evidence for his assertion that
       education is not a useful aspect of review, but refers to a comment
       by Deming that in-process training is like the blind leading the
       blind. (Can educational theorists respond to this challenge?)

 * developmental interval increase is quite strange.  My experience is that
   people tend to put off their preparation until shortly before the review
   meeting, partly so that the issues will be 'fresh' (and partly due to
   simple procrastination).  Why are AT&T developers consistently finishing
   preparation two weeks before it is required? Do they wait until
   reviewing is done to begin scheduling the next meeting?  This seems
   weird.

Relationship to CSRS experiment:

 Basic question is the same: what are the advantages of synchronous
 meetings, if any?  Votta answers that there are none, and that they can be
 reduced or eliminated without cost.  However, there is no testing of this
 hypothesis in the paper; no evaluation of a deposition-based method was
 actually performed. 

 In our paper, we also question whether a synchronous meeting has value,
 and devised an experiment that allowed actual comparison of a synchronous
 meeting-based method with an asynchronous, nominal group approach.  We
 found that the synchronous method was significantly more costly that the 
 nominal group, but that we could not detect a significant difference
 in effectiveness between the two methods. However, we found a significant
 difference in reviewer _perception_ of the two methods: reviewers
 overwhelmingly preferred the synchronous group method, and believed that
 the resulting review was more accurate and of higher quality. 


------------------------------------------------------------------------

Porter and Votta:

Claim: choice of analysis method significantly influences the outcome of
review.

Hypothesis: nonsystematic procedures lead to reviewer overlap and gaps, 
while systematic procedures with separated responsibilities lead to 
increased effectiveness. 

Outline:
Introduction
 Inspection literature
 Detection methods
 Hypothesis

The experiment
 Experimental design
  variables
  design
  threats to internal validity
  threats to external validity
  analysis strategy
 Experimental instrumentation
  SRS
  defect detection methods
  defect report forms
 Experiment preparation
 Conducting the experiment
  Training
  First trial
  second trial   

Data and analysis
 data
 analysis

Summary and conclusions

Future work


Issues:

  * They claim that a partitioned approach in which each reviewer searches
    for a different class of errors is more effective. Our study also
    supports this view: the nominal groups did find many more defects,
    although replication of defects resulted in no greater ultimate
    effectiveness (in terms of unique/defects per person hour) 








