<HTML>
<HEAD>

<TITLE>Measurement Dysfunction in Formal Technical Review</TITLE>
<!-- Created by: Philip Johnson,  10-Nov-1996 -->

</HEAD>
<BODY>

<CENTER>
<H1>Measurement Dysfunction in Formal Technical Review</H1>
<A HREF="http://www.ics.hawaii.edu/~johnson/">Philip Johnson</A> <BR>
Associate Professor<BR>
<A HREF="http://www.ics.hawaii.edu/~csdl/">Collaborative Software Development Laboratory</A> <BR>
<A HREF="http://www.ics.hawaii.edu/">Department of Information and Computer Sciences</A><BR>
<A HREF="http://www.hawaii.edu/">University of Hawaii</A><BR>
2565 The Mall<BR>
Honolulu, HI 96822<BR>
Voice: (808) 956-3489<BR>
Fax: (808) 956-3548<BR>
<A HREF="mailto:johnson@hawaii.edu">johnson@hawaii.edu</A> 
<p>
ICS Technical Report 96-16<p>
<A HREF="http://www.ics.hawaii.edu/~csdl/techreports/96-16/96-16.html">http://www.ics.hawaii.edu/~csdl/techreports/96-16/96-16.html</A> <BR>
<p>
<hr>
<p>
<center>
  Last Modified On: Fri Mar  5 09:44:28 1999
</center>
<p>
<hr>
</CENTER>

<h2> Introduction</h2>

Empirical measurement of the process and products of software
review has become a foundation for  evaluation and process
improvement. For example, by measuring the time spent on review
and the number of defects discovered, one can derive a simple measure of 
review efficiency.  Having obtained this measure of efficiency,
a natural process improvement goal might be to improve the
numerical value of review efficiency over time. 
<p>
This paper explores some of the issues that arise in effective
use of measures to monitor and improve formal technical review
practice in industrial settings. It focuses on <em>measurement
dysfunction:</em> a situation in which the act of measurement 
affects the organization in a counter-productive fashion, 
which leads to results directly counter to those intended
by the organization for the measurement.
<p>
The next section discusses several general concepts in measurement and
measurement dysfunction. The following section applies these
concepts to formal technical review and catalogs some of the 
potential measurement dysfunctions that might occur in FTR.
The final section proposes some strategies for minimizing 
the threat of measurement dysfunction in FTR.

<h2>Measurement and measurement dysfunction</h2>

A recent book by Robert Austin entitled "Measuring and Managing 
Performance in Organizations" (Dorset House, 1996) explores the 
notion of measurement and measurement dysfunction, and the concepts
described in this section are taken directly from that work.

<p>

According to Austin, whenever you measure an attribute of an organization
with the goal of improving the organization's performance, you run the risk
of worsening the organization's performance as a direct result of the
measurement.  As a simple illustration, consider the apocryphal story of
the Soviet Boot factory.  Faced with high production quotas, they chose to
produce only size-7-left boots. The desired productivity was achieved, as
defined by the measurement, though the basic goals of the organization were
sacrificed in the process.

<p>

For the purposes of exploring measurement dysfunction in formal technical
review, a very simple analysis of measurement practice can suffice. Using
Austin's terminology, there are at least two uses to which a given measurement
can be applied: for 
<em>information</em> and for <em>motivation</em>. 

<p>
Informational measurement "tells about 
an organizational process... It is used to learn from and to plan." In 
more FTR-specific terms, informational measures support process
improvement. 

<p>
Motivational measurement, on the other hand, "is used to quantify the 
value of compensation for compliance with objectively verifiable standards
of work." In other words, motivational measurement is used to evaluate
the performance of individuals.
<p>

One tenet of this book is that an individual measure is "value-free" 
with respect to its application: it can be used for informational
purposes, motivational purposes, or both.  Importantly, it is impossible
for an organization to guarantee that a measure, once taken, will never
be used for motivational purposes.  Thus, individuals in an organization may
tend to operate under the assumption that any measures of individual
performance can be used for motivational purposes, regardless of the
stated intention of the organization with respect to that measure at
the time it is taken.

<p>
Measurement dysfunction occurs when individuals behave in a manner that
produces good values of the measure for the purposes of motivation
(i.e. performance improvement), although this behavior simultaneously
undermines the value of the measure for the purposes of information
(process improvement). 

<p>

Austin does not claim that measurement dysfunction is <em>guaranteed</em>
to occur when a measurement can be applied for both information and
motivation.  However, he does provide a compelling argument that
dysfunction is a quite natural outcome and, unfortunately, that it is
typically quite difficult to detect the presence and degree of measurement
dysfunction.

<p>

Austin's book provides much more detail on the general issues and remedies for
measurement dysfunction. The next section turns to the specific case
of dysfunction in formal technical review. 


<h2>Measurement dysfunction in FTR</h2>

Most training materials for software review indicate that review measures
should be used for informational, not motivational purposes. For example,
in Tom Gilb's "Software Inspection" (Addison-Wesley, 1993), he states that
one organizational cause of Inspection failure is the "use [of] defect
metrics for individual personnel appraisal."
<p>

What Austin's work reveals, however, is that it is not enough for the
organization to claim that it will not use a review metric for personnel
appraisal. As long as the review metric is available to management for use
in that manner, reviewers may act as if it will be used in that way
(either now or in the future).  The problem with software review processes
such as Gilb's Inspection (as well as by review processes of my own
design such as CSRS/FTArm) is that they are designed in such a way that
virtually all measures of review are available to management for use as
motivational measures. Measurement dysfunction is a potential outcome of
such a review process, <em>regardless of whether or not the organization
ever actually uses the measures for such purposes.</em>

<p>

To detail this problem, this section provides explicit descriptions of
eight different forms of measurement dysfunction possible in FTR.  


<p>

Measurement occurs during FTR at both the individual and the group level.
Individuals generate measurable data during planning, review preparation,
and rework. Groups generate measurable data during the review
meeting. Dysfunction can occur in both individual and group measures.  For
each type of dysfunction described below, its occurrence at either or both
of the individual or group level is noted.


<h3>Defect severity inflation</h3>

Most FTR methods include some sort of measurement of defect severity, 
such as a set of categories like: "critical", "severe", "moderate", 
and "minor".  The placement of a defect into this category is a 
subjective process. 
<p>

Measurement dysfunction can occur when an organizational goal is to
increase the number of "important" defects found during FTR, as measured by
metrics such as the percentage or frequency of non-minor errors found
during FTR.  Groups and/or individuals can artificially inflate the defect
severity level accorded to defects to achieve the desired measurement
improvement, even if the "real" measure is stable or falling.  Such
inflation may be done subconsciously, without any direct intent to deceive
the organization by individuals or groups.



<h3>Defect density inflation</h3>

Defect density is a measure of how many defects have been discovered
in a given volume of work product. A typical defect density measure for
source code is defects per thousand lines of code; for requirements
documents, defects per page. 
<p>
Measurement dysfunction can occur when an organizational goal is to improve
FTR defect detection, as measured by an increase in the defect density
measure over time. In this case, groups and/or individuals can improve this
number over time by starting to classify certain items as defects that were
previously classified into some non-defect category (such as "minor
issues", "syntax", "formatting", or "author question").  Again, such 
change might be entirely subconscious, and could result in an 
improvement in the reported measure though the "real" measure
might be stable or falling. 

<h3>Artificial defect closure</h3>

Defect closure is a measure of the number of defects that have been
not only identified, but physically removed from the work product. 
<p>
Measurement dysfunction can occur when an organizational goal is 
to improve defect closure rates or efficiency, as measured by
metrics such as the number of open defects or the average 
time-to-close a defect. An organization might also weight this
measure by the defect severity in order to encourage more
effort into closing the more severe defects. 
<p>
In this case, groups and/or individuals can improve this number over time
in at least two ways. First, they can begin classifying certain
difficult-to-fix issues as "enhancements" rather than "defects", which
lowers the measured number of hard defects in the work product and
thus improves the measure.  Second, in the case of weighted measures,
they can gradually lower the severity classification provided to a
defect of a given type, which will improve the value of the weighted
measure.

<h3>Reviewer preparation time inflation</h3>

Reviewer preparation time is a measure of the effort allocated
by review team members in analyzing the review document
prior to the meeting.  Review preparation time has been shown
to be a key indicator of overall review quality.  Most 
review training materials emphasize the importance of 
allocating substantial time to preparation, and provide
example values ranging from 90 minutes to two hours or more.
<p>
Measurement dysfunction can occur when an organizational goal
is to improve or assess review (or reviewer) quality as measured
by review preparation time. Measurement dysfunction
can also occur as a result of the way in which reviewer
preparation time data is collected.  Typically, preparation
time is publically presented by each reviewer to the moderator
at the beginning of the review meeting.  In this latter case,
measurement dysfunction can result from pressure to 
be viewed as having worked as hard on the review preparation
as the other participants, and thus to not be viewed as a "slacker".

<p>
Since reviewer preparation is a private activity, individuals are free to
report a greater than actual preparation time value to the moderator
without fear of being found out. Furthermore, there is substantial pressure on
participants to never report "no preparation", since this
could result in re-scheduling of the meeting.  Low preparation effort
could also show up (either implicitly or explicitly) as a negative
factor  on a future individual performance evaluation.  
<p>

Preparation time inflation can also occur subconsciously, as when a
reviewer simply "rounds off" a preparation time of a little over an hour
and a half to two hours. Even such rounding results in a 30 percent error
in the measure.


<h3>Defect discovery rate inflation</h3>

Defect discovery rate is a measure of how much time, on average, 
it takes for an individual or group to discover an error. It
is a measure of "efficiency" in the review process.

<p>
Measurement dysfunction can occur when an organizational goal
is to improve individual or group efficiency as measured by
defect discovery rate. In this case, reviewers may feel pressured
to adjust their reported preparation times downward in order
to move their efficiency upward. As noted above, reviewer
preparation times are particularly vulnerable to alteration, since 
they are provided publically and because they are essentially
unverifiable.  

<p>
For example, a reviewer who spends three hours
preparing (rather than the recommended two hours) would only 
benefit from reporting a preparation time closer to the 
recommended value: her efficiency would appear to be greater, 
and the disparity in preparation effort between her and her
colleagues would probably be reduced.

<h3>Defect duplication inflation</h3>

Defect duplication is a measure of how many issues were 
noted by more than one reviewer during preparation. Although defect
duplication occurs, it is not typically tracked by organizations.
<p>

Measurement dysfunction can occur when an organizational goal is 
to improve either defect density or defect detection rates for
individuals.  In this case, individuals will feel pressure
to improve the number of defects they can report as found
during preparation.  One way, noted previously, is to 
inflate severity.  Another way is to talk with other reviewers
about the document and find out what errors they've discovered.
The reviewer can then report these errors on his preparation
defect report.  The result is an increase in the personal 
measures for the individual, although these duplicate issues
are simply collated during the meeting and do not result in
any net quality improvement.


<h3>Defect severity deflation</h3>

Defect severity was defined above in the context of inflationary
measurement dysfunction.  The opposite dysfunctional pressure can also
exist when an organizational goal is to decrease the number of non-minor
defects surviving past a particular phase. In this case, reviewers and/or
groups may feel pressure to decrease the severity attached to a defect (or
reclassify it as an enhancement) in cases where it is anticipated that the
defect might not be removed immediately.

<h3>Review participation/coverage  inflation</h3>

Review participation is a measure of the frequency with which 
an individual participates in reviews.  Review coverage is a
measure of the percentage of work products that have undergone
review.
<p>
Measurement dysfunction can occur when an organizational goal
is to increase review usage, as measured by either or both of
review participation or review coverage.  In either of these
cases, there is a pressure to simply maximize participation and/or
coverage while minimizing actual effort allocated to the review.
The result is high frequency but low quality review.



<h2>Preventative measures for  measurement dysfunction in formal technical review</h2>

Before introducing some suggestions for preventative measures, it is 
useful to reiterate the central themes of measurement dysfunction
that apply to formal technical review:
<ol>
<li> <em>Dysfunction occurs when a measurement designed for informational
purposes is perceived as having the potential to be used for 
motivational purposes. </em>
<p>
Dysfunction can occur as soon as one or more individuals recognize that 
a measure might be used either now or in the future for performance
evaluation.  Thus, promises by current management to use the measures
"appropriately" are largely ineffectual in preventing measurement
dysfunction. As everyone knows, managers and management policies are
subject to periodic and unpredictable change. 

<p><li> <em>The occurrence of dysfunction is difficult to observe.</em>  
<p>
As shown in several of the examples above, dysfunction can occur without
any explicit, conscious, malicious attempts on the part of the developers
to subvert the data. In many cases, it manifests itself gradually over an
extended period of time as a "false trend".  Determining its existence
would take extraordinary efforts for most organizations, such as comparing
similar defect types over a period of a year or more to determine if
inflation in severity types is occurring.  Other forms of dysfunction, such
as in preparation time reporting, may be impossible to detect.
<p> 

</ol>

So, how does one address a problem in which merely the 
perception of its possibility 
can be sufficient to produce behavior that is difficult to detect?
One simple, yet extreme answer is the following:
<em> Eliminate the possibility that measures of FTR could be used,
either now or in the future, for motivational purposes.</em>

<P>

First, if individuals and groups were provided with a measurement system that 
was impossible to use for performance evaluation, then there would
be little motivation to record measurements inaccurately. Second, if the 
measurement system was clearly useful for informational purposes, 
then there would be clear motivation to record the measures
consistently and reliably.  Obtaining FTR measurement with minimal
dysfunction appears to require both of these conditions to be met. 
<p>

Let's assume that the measures list above in the previous section have
value as informational measures, and thus the goal is to determine how to
obtain accurate values for them for use in process improvement while
minimizing the of possibility of measurement dysfunction. What would
a formal technical review process look like under these circumstances?
Here are some design principles for formal technical review processes
based upon the goal of minimal measurement dysfunction:
<p>


<h3>Personal review data is private.</h3>

The fundamental design principle is that review data generated by a single
individual must be under the direct control of that individual and not
accessible in its raw form to the organization. This is because any
measurement of an individual available to the organization can be perceived
by the individual to be used in the evaluation of them. Once that
perception exists, dysfunctional measurement can follow regardless
of whether or not the organization actually uses the measure for evaluation.

<p>
The degree to which a review method minimizes the possibility of 
dysfunction is directly correlated with the degree to which it protects
the privacy of individual data. Any individual data not protected by a 
review method is subject to potential dysfunction and may not truly 
represent the process.
<p>
Some of the more important implications of this principle are:
<ul>
<li> Individual preparation times are private.
<li> Individual defect lists generated during preparation are private.
<li> Rework times are private. 
</ul>

<h3>Personal data is always reported in aggregate and anonymously.</h3>

The problem with keeping personal data totally private is, for example,
that defects generated by individuals during preparation won't do anyone
any good if the individual never shares them with anyone. Thus, although
personal data must be private, it must also somehow be publicized. 
<p>
A solution to this problem is to ensure that any data generated by
individuals is always presented in aggregate form along with data from
other individuals, and with any ownership information missing. 
<p>
Some implications of this principle are:
<ul>
<li> Preparation times are recorded as an aggregate value over all
     reviewers. No data accessible to the organization records individual
     preparation times.
<li> Participation in reviews is private data.  No data accessible to the
     organization records which individuals participated in which reviews. 
<li> Defect lists are recorded as an aggregate of all individual lists and
     without author attribution. No data accessible to the organization
     records which individuals uncovered which defects. 
</ul>

<h3>Individual process improvement is an individual matter.</h3>

Recall that obtaining accurate measurements requires more than
simply removing the possibility that they might be used for motivational
purposes. It also requires that they be useful for informational purposes
(i.e. process improvement). Otherwise, there's little motivation for
an individual to allocate the effort to collect them. 
<p>
To satisfy both requirements, there must be some way for individuals to
personally benefit from the effort invested into collecting review
measures. A design requirement for a minimally-dysfunctional review process
is to provide mechanisms for analysis of individual review data that
supports individual process improvement. Fortunately, there exists a rich
source of insight into how to do empirically-driven personal software
process improvement in the form of Watts Humphrey's "A Discipline for
Software Engineering" (Addison-Wesley, 1995).

<h3>Development group review data is private.</h3>

While many measures of review are generated directly by individuals, many
others, particularly those related to review outcome, are attributes of the
development group as a whole. For example, a measure like review efficiency
(which could be calculated as the total number of defects detected via
review divided by the total time spent by all members on review, from
planning through postmortem) is an intrinsically a measurement of the group
as a whole. If the group perceives that this measure may be accessible to 
management for use in evaluation of the group, then it is susceptible to 
measurement dysfunction.  Once again, a solution is privacy.
<p>

Here are some example review measures of group activity that, in order to minimize
dysfunctionality, should be kept privately by the group and not made 
available to management:
<ul>
<li> Total effort spent on review
<li> Total review coverage of the product
<li> Percentage of successful (passing) reviews
<li> Review participation
<li> Review yield
<li> Total defects removed through review
</ul>


<h3>Group review data is always reported qualitatively.</h3>

Unfortunately, it appears more difficult to aggregate group review data together
in a way that preserves privacy and prevents dysfunction. At the group
level, an alternative approach seems more feasible: the results of review 
procedures are reported qualitatively rather than quantitatively. For
example, review groups could report the following information about their
process and products to management:
<p>
<ul>
<li> Use of the enclosed checklist for C++ has improved our review outcomes.
<li> We modified the review process in the following way, with good results.
<li> We find that the cost of review groups of a size greater than 5 do not
     justify the benefits. 
<li> We have found that the following criterion for re-review has good results.
</ul>

<h3>Group process improvement is a group matter.</h3>

Just as in the case of individuals, group measures must have some direct
utility to the group without their being made public for external analysis.
Otherwise, there is little motivation for the group to collect these
measures.
<p>
This implies that groups must be trained and provided with the tools
necessary to do analysis and process improvement of their own data. 




<h2>Implications</h2>

<h3>For FTR</h3>

The implications of measurement dysfunction for the theory and practice of
Formal Technical Review are potentially profound. No metrics-based method
(from the seminal Fagan Inspection method to recent computer-mediated
approaches such as CSRS) is designed to minimize dysfunction. In fact,
as the preceding section indicates, the kinds of FTR measures produced
and the manner in which they are typically collected makes them 
particularly prone to dysfunction.  
<p>

As a worst case scenario, if measurement dysfunction is found to be a
substantial and widespread problem in Formal Technical Review practice, then:
<ol>
<li> The published (case-study) data showing positive outcomes for
software review is suspect. The findings may be a result of participants
producing measurement data dysfunctionally in an effort to provide the
results they know management wants to see, rather than the data reflecting
the true underlying impact upon software quality.
<li> Current best practices in formal technical review must be changed to 
reduce their susceptibility to measurement dysfunction.  The design
guidelines above provide examples of what changes might be necessary.
</ol>

<p>
It is important to note that susceptibility to measurement dysfunction does
not necessarily translate into actual dysfunction.  Dysfunction measurement
susceptibility must be viewed as a continuum.  Whether an organization
needs to change some, most, or all of its FTR measurement mechanisms
depends upon its culture and history. 

<p>
Finally, there is also a silver lining.  Formal technical review, while
being shown over and over again to provide positive benefits, still suffers
from a low adoption rate. Perhaps at least some of these adoption problems
stem from the perceived invasiveness of FTR measures.  As new FTR methods 
appear that attempt to actively prevent measurement dysfunction, at least
one of the current obstacles to FTR adoption might be ameliorated. 

<h3>For performance evaluation</h3>

The goal of this paper is not to advocate that organizations abandon
performance measurement and evaluation of individuals and development
groups.  The goal of this paper is to suggest that if an organization
desires accurate measurements of their formal technical review process,
then these measures should be separate from the measures used to evaluate
individuals and groups. If this separation is not made, and if active steps
are not taken to guarantee, as much as possible, that it will be
<em>impossible</em> for the organization to apply measures of FTR for
performance purposes, then dysfunction in these measures is a possibility.


</BODY> 
</HTML>

