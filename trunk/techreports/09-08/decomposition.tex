\chapter{Dimensionality reduction, indexing and bounding.}
As we saw with the application of LCS to images in \cite{citeulike:4367061}, Yazdani and Ozsoyoglu characterized each of the images by a set of Fourier coefficients. By employing this approach they managed to resolve the ``dimensionality curse'' in the time series-similarity problem which essentially decreases the performance of any similarity algorithm to the one of the sequential search with the growth of dimensionality above 16 \cite{citeulike:4408223} \cite{citeulike:4384496} \cite{citeulike:2843857} \cite{citeulike:4384489} \cite{citeulike:343069}.

The problem addressed in this chapter is the approximation (lossy compression) of a time series data in a way which maintains fast random access, comparison and indexing of the time-series with minimal errors. There are many approximations proposed in the research literature and Figure \ref{fig:approximations} from \cite{citeulike:2821475} illustrates them in a hierarchy. All of the methods essentially consist of two steps: at first they transform a given set of time series into some low-dimensional representation and secondly index them in order to utilize data-mining algorithms later. Important issues to understand include the sensitivity and selectivity of algorithms, their complexity and performance.

\begin{figure}[tbp]
   \centering
   \includegraphics[height=45mm]{ts_representations.eps}
   %%{seriesheatmap}
   \caption{The figure from \cite{citeulike:2821475} illustrating a hierarchy of all of the flavors of time series representation (decomposition).}
   \label{fig:approximations}
\end{figure} 
