\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.75\textwidth]{sdsa.eps}
  \caption{SDSA classifies this episode as ``(TDD, 2)''. This represents a
normal TDD cycle.  The two shaded ``COMPILE'' cells are yellow, and the
last two ``UNIT TEST'' cells are red and green, respectively.}
  \label{fig:sdsa}
\end{figure*}

Figure \ref{fig:sdsa} illustrates the use of SDSA to classify developer
behavior as belonging to the TDD best practice. This screen image shows the
first episode of a programming session on March 15, 2005, whose total
duration was a little over three minutes.  In this episode, the developer
began by implementing a class named TestStack. He then attempted to compile
the class, but it failed to compile since the class it was testing (Stack)
was not implemented.  SDSA indicates this with a (yellow) Compile token.
The developer implements the Stack class, but forgets the isEmpty() method,
yielding another yellow Compile error. Finally, the Stack class stub is
implemented, producing a (red) Unit Test Failure token.  A short time
later, the remainder of the Stack class is implemented, producing a (green)
Unit Test Pass token, which completes the TDD episode.

Our research on SDSA is less mature than our research on Software Project
Telemetry, and we are only beginning the process of evaluating its
generality and utility.  Our initial pre-pilot experiments in the domain of
Test Driven Design appear promising, and the system has been able to
correctly classify TDD episodes with over 80\% accuracy in a small sample
of developer data.   Currently, Eclipse is the only development
environment for which we have developed a Hackystat sensor that captures
the kinds of raw sensor data required for TDD recognition. SDSA is currently 
focused on representation of a single developer's behavior, although we have
begun preliminary design work on representations of group practices. 




In this research, we propose to investigate the potential synergy between
Software Project Telemetry and Software Development Stream Analysis.
Software Project Telemetry provides an efficient and effective mechanism
for detecting changes in the measures describing a project.  With
telemetry, we can easily detect that test case coverage is going down, or
build failures are going up, or inter-package coupling is increasing, or
the variance in developer active time is decreasing. Telemetry can tell us
that project characteristics are changing, but offers no insights into the
behaviors that led to these changes.  SDSA provides the opposite: it gives
us insight into the behaviors that are occurring during development, but
does not tell us what the impact of these behaviors might be on the
project.

The integration of these two analysis mechanisms provides a powerful
vehicle for evidence-based investigation of software development best
practices.  In the case of test-driven development, its proponents have
made radical claims for the technique: that it produces 100\% test case
coverage, that it produces higher quality software, more maintainable
software, and improves programmer productivity.  Initial empirical studies
of TDD have not provided strong evidence for or against these claims
\cite{George:03,George:04,Geras:04,Muller:02,Matjaz:03}.  Using Hackystat,
Software Project Telemetry, and SDSA, we can provide a replicable, standard
infrastructure that automatically (a) gathers outcome measures, such as
coverage, active time, and unit test invocations; and (b) assesses
compliance with the definition of TDD, so that we know whether the
experimental subjects were actually using the technique without requiring
physical observation. We will discuss this research effort further in
Section \ref{sec:research-plan}.

\subsection{Toward evidence-based software engineering}

Our proposed research is designed to provide technology, data, and
methodology to support evidence-based software engineering.  The Hackystat
framework, along with Software Project Telemetry and Software Development
Stream Analysis provides new and useful
infrastructure for the creation of evidence regarding a given software
engineering practice. Our case studies in the classroom setting will create
new, replicable data that can be used in evidence-based evaluation of the
test-driven design, and our industrial case studies will provide similar
evidence for high performance computing productivity.  Finally, our
experiences applying the tools and analyzing the data will result in our
recommendations for effective ways to utilize the technologies and assess
the data that results.

