\chapter{Final Remarks}  \label{Chapter:Conclusion}

This chapter presents some final remarks about this thesis research. 
It begins with a summary of the research in Section \ref{Conclusion:ResearchSummary}.
It then lists main contributions in Section \ref{Conclusion:Contribution}.
Finally, it discusses future directions in Section \ref{Conclusion:FutureDirections}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                       %
%                   S E C T I O N                       %
%                                                       %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Summary} \label{Conclusion:ResearchSummary}

This research was inspired by many existing software measurement approaches, such as PSP \cite{Humphrey:1995, Humphrey:1996}, COCOMO \cite{Cocomo:1981, Cocomo:2000}, and CMMI \cite{Royce:2002}. These approaches aim to systematically improve software development processes in order to enable a software organization to produce software products in a controllable and repeatable manner. But the \textit{``metrics collection cost problem''} and \textit{``metrics decision-making problem''} make effective application of these approaches far from mainstream in practice. The ``metrics collection problem'' refers to the high cost associated with metrics collection; while the ``metrics decision-making problem'' refers to the question how to make project management and process improvement decisions based on the information in the metrics. They are the motivation behind this thesis research.

My solution is a novel approach to software measurement called ``software project telemetry,'' which enables (1) automated metrics collection and analysis, and (2) in-process, empirically-guided software development process problem detection and diagnosis. 
Software project telemetry addresses the ``metrics collection cost problem'' through highly automated measurement machinery: software sensors are written to collect metrics automatically and unobtrusively. Sensors keep metrics collection cost low by eliminating the chronic ``context-switch'' overhead inherent in both manual approaches, such as PSP, and tool-assisted approaches, such as LEAP \cite{Moore:1999}.
Software project telemetry addresses the ``metrics decision-making problem'' through a domain-specific language designed for the representation of telemetry trends for different aspects of software development process. Project management and process improvement decisions are made by detecting changes in telemetry trends, and comparing trends in two different periods of the same project, instead of between two completed projects found in model-based approaches such as COCOMO. The advantage is that it not only eliminates the need to build statistical models that require frequent calibration, but also enables in-process control for a project that is still being developed.

In order to evaluate software project telemetry, I conducted two empirical studies: one in the classroom, and the other in CSDL. The research followed, for the most part, the constructivist paradigm.
The classroom study was relatively simple. The students were divided into groups of 2 - 4 members collaborating on group projects. They were introduced software project telemetry to collect metrics and perform analyses on their own data. At the end of the study, I distributed a questionnaire to collect the students' opinions about software project telemetry. I also analyzed their telemetry analysis invocation pattern to assess the extent to which their opinions were based on the actual system usage.
On the other hand, the CSDL study was much more comprehensive. I introduced software project telemetry as a metrics-based process improvement program. I pursued an in-depth data collection and analysis strategy over a much longer period of time. It involved many methods from the constructivist paradigm: it was a case study, in which I collected data through observations and interviews and generated hypotheses from the data. It also involved a limited form of hypothesis testing in the post-positivist tradition.

The results from the two studies suggested that software project telemetry had acceptably-low metrics collection and analysis cost, and that it was able to provide project management and process improvement decision-making values. They also suggested that software project telemetry would deliver best value when metrics collection and telemetry analysis could be customized to the specific need of a software organization. Top-down telemetry design and bottom-up metrics collection are best practices. However, data privacy concerns among developers and lack of telemetry expertise in organizations outside CSDL might become technology adoption barriers. 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                       %
%                   S E C T I O N                       %
%                                                       %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dissertation Contribution} \label{Conclusion:Contribution}

There are three main contributions from this research:
\begin{enumerate}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
	\item the idea of software project telemetry itself,
	\item the system implementation,
	\item and, finally, the insights gained from the two studies.
\end{enumerate}




\subsection{Concept of Software Project Telemetry}

First, the idea of software project telemetry itself is a significant contribution.
It uses sensors to collect metrics, which represents a leap from traditional manual and tool based approaches. These traditional approaches require human intervention or developer effort to collect metrics. The developers are constantly distracted from their main work (i.e., developing software), because they have to switch back and forth between doing the work and recording what work is being done. Several studies \cite{Borstler:2002, Humphrey:1995, Johnson:2003} have identified the high metrics collect cost as a major adoption problem. On the contrary, in a sensor-based approach, sensors collect metrics automatically and unobtrusively. It significantly lowers the metrics collection cost by eliminating the chronic ``context-switch'' overhead inherent in manual and tool based approaches. It also has an added advantage: metrics collection no longer suffers from bias (either deliberate or unconscious), error, omission, or delay, and we don't have to worry about the types of data quality problems reported in \cite{Johnson:1998}.

In metrics decision-making, software project telemetry follows a light-weight approach. It represents a deviation from traditional model-based approaches such as COCOMO. Since software project telemetry does not compare the current project with the projects in historical database, there is no need to build formal models based upon statistics over previously completed projects. This avoids many problems inherent in traditional metrics models, such as the need to accumulate a historical project database and ensure that the historical data remain comparable to current and future projects. Instead, software project telemetry makes comparisons between two different periods of the same project, and the comparison involves much smaller time scale than the whole lifecycle of a project. The metrics from the initial period of the project are used to establish a baseline and bootstrap the process. Project management and process improvement decisions are made by detecting changes in telemetry trends and comparing trends in two periods of the same project. In-process control for a project that is still being developed is made possible exactly because comparisons are made within the same project. It also solves a dilemma for many software organizations with low process maturity: these organizations simply have no basis to perform model-based cross-project comparison because their software development processes change as work changes. 

The classroom and CSDL studies showed that software project telemetry had sufficiently low metrics collection and analysis cost, and that it was able to deliver project management and process improvement decision-making values at least within the exploratory context of the two studies.
In the classroom study, most students responded that they felt their software development processes had improved. Though I was unable to ascertain the degree to which the students' self-claimed improvement was caused by the use of software project telemetry, it was certain that software project telemetry made their software development processes transparent. 
In the CSDL study, by introducing software project telemetry as a metrics-based process improvement program, I was able to help the project manager institute changes to improve project management practices, and help the developers gain insights into their software development processes.







\subsection{Implementation of Software Project Telemetry}

Second, the implementation of software project telemetry is also a significant contribution. Two pieces of software are the direct result of this thesis research. 
%They make it possible to continuously monitor software project status, and then generate and validate process improvement hypothesis.
One of them is a server-side component of about 28,000 lines of code (the Core\_Telemetry module plus the custom implementation of telemetry reducers and functions). It includes the software code to interpret the telemetry language, and the code to perform telemetry analyses and generate telemetry charts. It also includes a web-based management console for telemetry construct definitions. A user can perform three types of analysis using a web browser: the expert analysis, the chart analysis, and the report analysis.
The other piece of software is the telemetry control center (Figure \ref{fig:TelemetryWall}) of about 3,000 lines of code. It is a client-side application that can be configured to automatically retrieve telemetry charts from the server and display them. Though it was deployed on the telemetry wall during the CSDL study, it can also run on a standard personal computer.

The two pieces of software enable two complementary modes of operation. In the first case, a user logs on to the server using a web browser to \textit{``actively''} explore relationships between different software metrics. In the second case, the telemetry control center makes a sequence of telemetry charts continuously available to the user, providing \textit{``passive''} awareness of the project status.

The performance of the server component has been profiled and fine-tuned. It is capable of handling a large amount of sensor data generated by enterprise-level large-scale software projects and multiple concurrent requests smoothly. For example, it is being used by CSDL to analyze Hackystat product and process metrics, and Hackystat had nearly 300,000 lines of code in total.

The source code is GPL licensed. It is freely available. The GPL license ensures that any third-party improvement will be contributed back to the community. An added advantage is that the code has become a standard component of the much wider scoped Hackystat project, which means it will be actively maintained for a long time. The system has already been adopted by several external sites, such as Sun Microsystem and the University of Maryland.








\subsection{Insights from Empirical Studies} \label{Conclusion:Contribution:insights}

Lastly, there are a number of valuable insights by comparing and contrasting the results from the classroom and CSDL studies. I grouped my insights into five categories:

\begin{itemize}
	\item \textbf{Metrics Collection} --- Sensor-based metrics collection appears to have eliminated the long term ``context-switch'' overhead inherent in manual-based approaches, such as PSP, and tool-based approaches, such as LEAP, PSP Studio, and Software Process Dashboard. The installer appears to have reduced the one-time sensor setup cost considerably. However, due to the fact that sensor collects metrics automatically in the background, broken sensor data might go unnoticed for a long time. The good news is that it is possible to design special-purpose telemetry charts to help developers make quick assessment whether the underlying sensor data are likely broken or not.
	
	\item \textbf{Analysis Invocation} --- The telemetry language appears to be quite complex for a normal user. It seems a good idea to have a telemetry expert pre-define telemetry charts and reports before-hand, so that a normal user could select from a list of pre-defined definitions to invoke the analysis. This is exactly the idea behind the telemetry chart/report analyses in the current implementation. However, there was a usability issue with respect to the input of parameter values. Fortunately, the issue appears resolvable by switching to a richer Web UI framework.
	
	\item \textbf{Decision Making} --- There is clear evidence that telemetry analysis makes software development process transparent. However, whether a team or a developer can get decision-making values out of it seems to depend on a number of factors, which include the level of process maturity, the ability to understand and interpret telemetry data, and the ability to customize telemetry analysis to the specific need of an organization.
	
	\item \textbf{Best Practice} ---  ``Top-down telemetry design'' and ``bottom-up metrics collection'' appears to be best practices of software project telemetry. Top-down telemetry design refers to the idea that each telemetry chart should have a clear purpose, such as to help the development team meet a specific process improvement goal. Bottom-up metrics collection refers to the recommendation to collect whatever metrics a software organization can.
	
	\item \textbf{Adoption Barrier} --- One technology adoption barrier involves data privacy concerns, which seem most severe with effort-related personal process metrics, such as \textit{``ActiveTime''}. Though the current implement of software project telemetry has a mechanism to limit access to metrics, overcoming this issue largely depends on what the metrics are used for in an organization (i.e., process improvement vs. performance evaluation). Lack of telemetry expertise within an organization might be another technology adoption barrier.	Software project telemetry will not likely deliver the best value if used straight ``out of the box.'' Effective use requires customization, which includes both setting up the sensors to collect metrics and designing telemetry charts and reports.
	
	
\end{itemize}

%Please refer to Section \ref{EvaluationConclusion:Results} for details.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                       %
%                   S E C T I O N                       %
%                                                       %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Directions} \label{Conclusion:FutureDirections}


%\subsection{Industrial Studies and Comparative Studies}
%
%Due to the limitation of the studies, to some degree, my knowledge about the use of software project telemetry is tied to the academic environments in which the technology was deployed. There are questions that remain to be answered. For example, to what extent will it be useful in industrial settings? Will we be able to reach the same findings? How does it compare to other competing metrics-based approaches? Only continued evaluation of software project telemetry can provide the answers. 
%
%One of the future direction is to conduct more case studies in industrial settings. 
%After more experiences have been accumulated with industrial use of software project telemetry, more controlled experiments could be set up. For example, different metrics-based project management and process improvement approaches use different methods for metrics collection and analysis. A controlled experiment would allow me to compare the decision-making value of software project telemetry to that of other competing approaches. Section \ref{EvaluationConclusion:FutureEvaluation} contained a more detailed discussion of this topic.


%\subsection{Data Mining, Pattern Discovery, and Statistical Process Control}

Software engineering is highly contextual. Two recurring questions in the classroom and CSDL study were: what is good telemetry, and how do we recognize bad trends? The answers are most likely dependent on the type and the goal of the project. Currently, software project telemetry requires human judgment to make project management and process improvement decisions. Is it possible to provide some degree of automated decision-making support? To what extent can such support be automated? Data mining provides an interesting direction for future research.

Data mining is the process of extracting valid, authentic, and actionable information from large databases. In other words, it derives patterns and trends that exist in data. There are many commercial softwares that offer sophisticated data mining support. Most of them are based on OLAP (On Line Analytical Processing). The most fundamental data structure in OLAP is multi-dimensional cube. A cube is a set of measures, which are facts, and dimensions, which are areas of interest. Measures are data, and dimensions define the ways data can be summarized. A dimension can have multiple hierarchies. The analogy is Google map: when viewed from a distance, you see the entire city or the entire country; when zoomed in, you see houses and streets. 

There is a high degree of similarity between Hackystat and OLAP: a cube in OLAP corresponds to a project in Hackystat. Measures are equivalent to sensor data (a.k.a. software metrics); dimensions are members, workspaces, and time intervals for a project. The difference is that a cube operates at a higher level of abstraction than a Hackystat project: it treats members, workspaces, and time intervals in a uniform way. This similarity makes it easy to take advantage of wide range of data mining algorithms in existing commercial software packages. For example, Microsoft SQL Server Analysis Services 2005 (SSAS) has several classes of data mining algorithms:

\begin{itemize}
	\item \textbf{Classification Algorithms} --- They predict one or more discrete variables, based on the other attributes in the dataset. 

  \item \textbf{Regression Algorithms} --- They predict one or more continuous variables, based on the other attributes in the dataset.

  \item \textbf{Segmentation Algorithms} --- They divide data into groups, or clusters, of items that have similar properties.

  \item \textbf{Association Algorithms} --- They find correlations between different attributes in a dataset.

  \item \textbf{Sequence Analysis Algorithms} --- They summarize frequent sequences or episodes in data.
\end{itemize}

The introduction of data mining to software project telemetry could open some very interesting new possibilities for metrics analysis.
In the past, the focus of software project telemetry analysis has been on \textit{manually} detecting covariance between software process metrics and product metrics. The idea is that if we can identify the relationship between them, then we can increase product quality by controlling the process. This seems to be a special case of an association algorithm: identifying correlations between different attributes in a dataset. Therefore, one possibility with data mining would be to investigate the extent to which the manual process of identifying covariance in telemetry streams could be automated by using association algorithms.

Another possibility would be to apply sequence analysis algorithms to recognize software engineering best practices. For example, we may wish to identify process sequences in telemetry streams that correspond to decreasing number of reported bugs in a project. Sequence analysis algorithms find the most common sequences by grouping identical sequences together. They have long been used by online retailers to recommend related purchases. A related research currently supported by Hackystat is SDSA \cite{Kou:2006}, which analyzes software process metrics to classify them as indicating the use of a best practice. The difference is that SDSA requires \textit{a priori} knowledge of what constitutes software development best practices, but sequence analysis algorithms have no such requirement. 
%Another benefit with ``sequence analysis algorithms'' is that the best practices are backed by empirical data and can be continuously validated.

Yet another possibility would be to apply regression algorithms for statistical process control. Existing telemetry streams establish baseline values for various software development measures in a software organization. Assuming the process is stable, then regression algorithms could be used to find statistical control bounds to send alert if new metrics values fall outside the bounds. The idea is similar to six sigma, which is a project management methodology that uses data and statistical analysis to measure and improve a company's operational performance. 
%Since \textit{``Six Sigma''} is manufacturing oriented, much research needs to be done to apply the concept to software development.

An important note is that using of data mining approaches to automate the discovery of ``interesting'' telemetry trend relationships is bottom-up in nature, and that one of the lessons learned from the CSDL study was that telemetry charts generated in a bottom-up fashion were generally of little value because they were not designed for any purpose (see Section \ref{EvaluationInCSDL:EventsDescription:TopDownDesign}). Therefore, much research is needed in the future to control the false positive rate in data mining. 






	 
	

