\chapter{Introduction}
\label{chap:Intro}

\section{Motivation}
\label{sec:motivation}
Test-Driven Development(TDD), a newly emerged software development method,
has been practiced by more and more software developers and intensive
research focus has been paid to it by researchers. Many experience reports
and research work concluded that TDD helps to have better requirement
understanding, reduce debugging effort, improve productivity, yield high
quality code and promote simple
design\cite{Beck:03,George:03,Maximilien:03}; however, negative results
were also reported by reseachers and professional developers.

This phenonymon is rather intricate than interesting because test-driven
development is not a very difficult practice. It has two simple rules only
and is very well articulated and demonstrated by Kent Beck in his
book\cite{Beck:03}. The thing of test-driven development is that it is easy
to do it for a while but it is not easy to do it consistently over a long
time period. TDD developers may do it sometimes but not all the time in the
empirical test-driven development study. This is not a hard science problem
but a plain fact.  Test-driven development radically changes development
``upside down''\cite{Pipka:03} , which is very counter-intuitive to
software developers. This story repeats many times already in the practice
of TDD: a developer tries TDD on a simple problem, he/she thinks TDD is
good, and then he/she forgets it in the future software development and
returns to the old way to develop software.

Disciplined test-driven development process is desired in order to have
more substantial and conclusive empirical research, butthere is no better
way than verbal confirmation and participation observation to ensure this.
It will be very interesting and valuable if we can know how well developers
do test-driven development automatically and unbiasedly. To take it
further, this knowledge will also help us understand TDD better and use it
smartly in software development.

\begin{comment}
Historically software process research has focused on high-level, long 
duration phases in software development. Recently increasing attention
has been paid to low-level, short duration activities as well. While a
high-level activity such as ``requirement analysis'' might take weeks
to months to complete, a low-level activity such as ``refactor class
Foo to have interface IFoo'' might only take several seconds with modern
IDE. 

Software organizations and developers are continuously confront the needs
to improve their software processes: the software product should meet
customers' requirements better; it should have less bugs; development
process needs to be more stable and predictable; productivity should be
improved to kick the software out of door as early as possible and so on.
Unlike other disciplines such as civil engineering and mechanic
engineering, software engineering is still too young to have mature
software process and quality control process. Software is very cognitive
complicated and software development is a very creative process, thus,
each software is unique and there are no two projects that are exactly the
same. It's challenging to improve software development process.

To a software development organization there are two ways to improve its
development process: one is to identify problems in current process and
solve the problems internally; another one is to adopt successful practices
from others. Internal software process improvement program is often through
process quality control and improvement program. It is heavily emphasized
by SEI's capability maturity model (CMM) and standard ISO9001. Personal
Software Process (PSP) and Team Software Process (TSP) are two processes
proposed by SEI to achieve internal process improvement continuously.
Adopting best practices from others is another way to improve software
process by borrowing others' successful experiences. Usually it comes with
the introduction of new technologies in current development process. For
example, another development tool can be used because it has elegant
features and powerful capabilities; continuous integration can be adopted
to maintain a working version and detect problems early before they sneak
into the final product; software review can be adopted to improve software
quality by peer inspection. Of the two improvement programs, internal
process improvement is gradual and steady, while adopting external best
practice may bring dramatic changes to the development process.

Best practices are from prior successful experiences and they provide a set
of guidelines and recommendations to improve software development. Although
they are proved to be successful elsewhere, they may or may not be
appropriate in a new organization. Managers in the new organizations are
either persuaded by consultants or inspired by other projects' successful
experiences to adopt a new best practice. Introducing best practice is more
or less a trial-and-error approach, and usually the execution discipline is
enforced by management and developers' self-discipline. To software
developers, adopting new best practice is a passive process, and they may
have different opinions as managers do. Everett Rogers modeled new
technology adoption process as adoption curve \cite{Potter:02}, in which
there are five kinds of adopters when a new technology is introduced:
innovators, early adopters, early majority, late majority and laggards.
When a new technology is introduced there is always the learning process
too. Developers may have slow start at the beginning before they understand
the best practice well. Adding these factors together, there will have a
lot uncertainess when a new best practice is exercised such that it is not
possible to tell whether a best practice is appropriate or not in the new
organizations without thorough evaluation. It is fairly reasonable to
conclude that the evaluation process will be more error-prone if the
practice itself is hard to be understood or it has high discipline
requirements.

One may come up with the possible solution to add more management resources
to ensure that the best practices are understood and executed well by
developers, or the development process can be monitored and recorded for
review by process experts.  Although these solutions are viable aeffective in manufacture industry, it could be very different to software
organizations. One apparent reason is that software is unique and cannot be
massive produced such that it is very costly to invest management and
monitoring resources on software projects as manufacture industries do.
This puts software organizations in a very awkward situation. On one hand,
it is not easy to employ best practices well to improve software development
process, on another hand, software organizations confront the needs to
improve development process continuously to meet the ever-increasing
demands and dependences on software in our society. 

To lower the management overhead of software development, specifically best
practice implementation, we propose to manage and control best practice
execution automatically with micro-process analysis technique in our
research. Micro-process is an innovative approach to keep track of
individual software developer's development process. A framework, software
development stream analysis (SDSA), is used to study micro-processes in
software development.  On the contrary to traditional managerial topdown
approach to enforce process disciplines, SDSA framework takes the bottom-up
step instead. It instruments development process, constructs development
stream out of process data, tokenizes software development stream into
episodes or micro-processes and evaluates development process with
micro-processes. In our research we employ this framework on Test-Driven
Development, a very popular best practice that was formalized and
evangelized by Kent Beck, pioneer of Extreme Programming (XP). Test-Driven
Development is very well accepted by software developers' community but the
researches of Test-Driven Development do not always agree with each other.
Because Test-Driven Development emphesizes on how developers do their work,
it is perfect to apply our framework on TDD to study its micro-processes.
We introduce two terms with our work.
\begin{description}
\item \textit{Software Development Stream} is a series of software
  development activities in order that are conducted by software developers
  over a period.
\item \textit{Micro-Process} is a small set of development activities to
  accomplish one certain programming task. In my proposal I will use both
  episode and micro-process interchangeably.
\end{description}
\end{comment}



\section{Test-Driven Development}
\subsection{Introduction}
\begin{quote}
\textit{Test-driven development (TDD) is emerging as one of the most successful
developer productivity enhancing techniques to be recently discovered. The
three-step: write test, write code, refactor - is a dance many of us are
enjoying. \\
  - Eric Vautier, David Vydra from testdriven.com}
\end{quote}

Test-driven development(TDD), also known as Test-first design(TFD), is a
new way to develop software in which development is driven by automated
unit tests. As its name suggests, TDD radically changes software
development from top-down, design documentation oriented approach to
bottom-up, requirement oriented approach. It has two fundamental
principles\cite{Beck:03}: {\it
\begin{itemize}
\item Write new code only if an automated test has failed.
\item Eliminate duplication
\end{itemize}
} Test-driven development is an incremental and iterative development (IID)
method. Each iteration lasts several minutes and it is rarely longer than
10 minutes. In the beginning of an iteration, developer writes the
automated unit test based on requirement. This newly created unit test may
not be able to compile or its execution will fail because it tests a
feature that is not implemented yet. It drives the implementation of
production code to make it pass. With TDD, a feature is added with unit
test to guard implementation correctness. The hidden philosophy behind TDD
priciples is that developer only writes enough code to make test pass. If
new code adds redundance developer should refactor it to remove duplication
and keep all tests pass. Wake\cite{StopLight} abstracted TDD style software
development as stop-light pattern because the unit testing framework xUnit
uses red bar to indicate test failure or error and green bar to indicate
test success and the iterative development manner of TDD works similar as
traffic light.

\subsection{Related work}
``Clean code that works'' is Ron Jeffries's pithy phrase to the goal of
Test-Driven Development\cite{Beck:03}. Since code is driven by automated
tests, system developed in this manner will have 100\% coverage and
developers will have incredible courage to refactor it fearlessly. This
reward is so great that ``most people learn TDD find that their programming
practice changed for good''\cite{Beck:03}. This shift is coined as ``test
infected'' by Erich Gamma. The test in TDD is unit test, a.k.a component
test. Early practice of TDD helped to create xUnit framework, the de facto
standard of unit testing. It was originally designed for SmallTalk and is
already expanded to 35 languages support according to the list on
Wikipedia\cite{xUnit}.

Many TDD advocators appraise TDD because they think TDD helps them improve
productivity and code quality; however, the research conclusions on TDD are
very divided. George and Williams's study got positive results on TDD. They
conducted a structured TDD experiment and compared TDD group who developed
in test-driven development and controlled group who developed in
waterfall-like process \cite{George:03}. Their study found that TDD group
yielded superior external code quality compared to controlled group, but
TDD group took 16\% more time on the development. It is quite interesting
that most TDD developers thought that TDD is effective and improves their
productivity (80\% and 78\% respectively) in the follow-up survey.  Another
study conducted by Maximilien and Williams also reported code quality
increase. Developers reimplemeted a legacy software system with TDD in this
study. They found that the new system reduced defect density by 50\% in
functional verification test(FVT) compared to the legacy system that was
developed in ad-hoc manner\cite{Maximilien:03}. There was somewhat
productivity decrease in the experiment but developers inclined to continue
using TDD in their future software development. The study conducted by
Geras et. al. did not see significant increases on either productivity and
code quality.  They designed an experiment to ask participants work on two
projects with Test-First and Test-Last in different order in two groups
\cite{Geras:04}.  Development effort, tests per KLOC, customer test
invocations and unplanned failures after delivery were compared between two
groups and there was only slight difference between Test-First and
Test-Last processes in both groups. Counter result was reported by M\"uller
and Hagner\cite{Muller:02}.  In their study, TDD group and controlled group
worked on a same graphic library and they verbally confirmed that
Test-First subjects got along with Test-First process. They found that TDD
does not accelerate the implementation and the resulting programs are not
more reliable except that it supports better program understanding.

\subsection{Problem statement}
Researchers realized that TDD is hard to be done consistently by test
subjects and they tried all they can do to guard TDD discipline in their
experiments. Pair-programming, management, script guideline and verbal
confirmation were all ever used in the research to help test subjects stick
to TDD in the reported empirical studies. However, these measures are not
only time-consuming but also very inefficient. Navigator in
pair-programming can review the code constantly and insist driver staying
on the track of test-driven development, but navigator is not always
available and many developers still cannot accept pair-programming.
Test-driven development is iterative and is constructured of many
short-duration activities such that there is no effective way to manage
them, except for watching developers over shoulders all the time.  Script
guideline and verbal confirmation are not reliable at all because
developers can go other way around such as adding unit tests after
production code and neglecting unit tests sometimes.

\section{Research statement}
On one hand, TDD is getting more popular among software developers and
intensive efforts have been taken to improve practice of TDD by
professionals and researchers\cite{Beck:03,TestDrivenWeb,TddYahooGroup}. On
another hand, TDD changes software development ``upside down''
\cite{Pipka:03} and is very counter-intuitive to software developers'
problem solving mind-set nature. The result is that it is hard for
developers to do TDD style development consistently and researchers have
hard time to conduct conclusive empirical evaluation on TDD, not even to
mention process improvement.

Test-driven development is a low-level software process that consists of
many short duration activities. This nature determines that traditional
software process management technique can not be applied directly.  The
discipline of low-level software process in general and test-driven
development in particular will largely depend on developer's self control.

Measuring and assessing low-level software process become possible with the
emergence of Hackystat\cite{Hackystat}, a framework for collecting various
kinds of software metrics and developer behaviors. My research attempts to
solve the discipline problems exist in empirical research and practice of
test-driven development with the help from Hackystat. Meanwhile, I
generalize this approach to Software Development Stream Analysis (SDSA)
framework to assit empirical research and practice of other low-level
software processes.

\section{Proposed Solution: Zorro Software System}
The system I develop to assess TDD and improve empirical use of TDD is
called Zorro, which is a three-layer architectured software system
including Hackystat, Software Development Stream Analysis(SDSA) and TDD
specific rules (Figure \ref{fig:ThreeLayer}). 
\begin{figure}[htbp] 
  \centering
  \includegraphics{figs/Three-Layer.eps}
  \caption{Three-layer architecture of Zorro}\label{fig:ThreeLayer}
\end{figure} 
Layer 1 is Hackystat that collects product metric and developer behavior
data to be used by upper layers. It also deals with data sending, storage
and encapsulation as data provider. Layer 2 is SDSA framework, which
implements software development stream and tokenizes development stream
into small episodes. Episode is the smallest unit in iterative software
development process that has a clear goal and activites to accomplish it.
Although there are many stochastic methods such as Bayesian and Monte Carlo
methods, SDSA implements JESS\cite{Friedman-Hill:03} rule-based system to
recognize episodes derived from development stream. TDD specific rules are
supplied in layer 3 to recognize and classify episodes tokenized by
test-pass tokenizer.

\subsection{Hackystat}
Hackystat\cite{Hackystat} is an open-source framework developed in
Collaborative Software Development Lab(CSDL) at Univeristy of Hawaii for
automated collection and analysis of software product and process metrics
and empirical software engineering experimentation. Hackystat sensors are
small programs plugged into the development environment to collect software
product and process metrics unobtrusively. Hackystat also supplies seamless
data sending, saving and retrieving. A set of analyses on product and
process metrics of software are defined to support evalutions on various
aspects of software engineering. Zorro analysis on test-driven development
compliance extends Hackystat process metric analysis into low-level
software process domain.

\subsection{Software development stream analysis framework}
Software development stream analysis (SDSA) is a generic framework to
analyze low-level software process that are realized by development
activities such as file editing, compilation, testing and debugging. SDSA
retrieves low-level development activities collected by Hackystat sensors,
constructs development stream by serializing these activities, defines a
set of tokenizers to organize related activities into episodes and
implements an interface to recognize episodes with rule-based system
support.

SDSA is a component-based system and is highly configurable. End users of
SDSA can selectively add sub stream, the unified development stream with
one single type of activities, into the main development stream. It defines
interface to plug in different episode tokenizing mechanisms and allows end
users define episode recognition rules by themselves.

\subsection{Recognition rules of Test-Driven Development}
Test-driven development is an iterative and incremental method, each
iteration consists of three portions red/green/refactor. Zorro instantiates
SDSA framework and selects test-pass tokenizer to have software development
episodes that end with successful unit test execution(s). A TDD iteration
will be separated into two portions red/green and refactoring, both of
which end with successful unit tests invocation. An episode is refactoring
if there is no new test and it is red/green if there is new test added. A
red/green episode is test-driven if and only if development is driven by
unit test.  So it can be further divided as test-driven in which unit test
is created to drive product code implementation, and test-last in which
unit test is created after the production code implementation. Complete
classification tree of test-pass episodes is depicted in Figure
\ref{fig:EpisodeTree}.
\begin{figure}[htbp] 
  \centering
  \includegraphics{figs/EpisodeClassficationTree.eps}
  \caption{Zorro test-pass episode classification tree}\label{fig:EpisodeTree}
\end{figure} 


\subsection{Contribution of Zorro}
Test-Driven Development has been claimed to naturally generate 100\%
coverage, improve refactoring, provide useful executable documentation,
produce higher code quality and reduce defect
rates\cite{Beck:03,George:03,Maximilien:03}. The three-layer structure I
propose can automate collection of developer behavior data and test-driven
development recognition. If it recognizes TDD correctly, then we would have
a powerful mechanism for exploring how test-driven development is used in
practice and its effect on software quality and productivity. The
non-intrusive nature of data collection provided by Hackystat will make it
very easy to deploy Zorro in both classroom and industrial settings to
study TDD compliance (management), potential discovery of alternative
processes and investigation of impacts of TDD.

\section{Zorro validation studies}
Before Zorro can be used in the practice and research of TDD, it must be
validated to sustain that (1) it can collect the software metrics and
dveloper behavior data necessary to determine TDD, and (2) it can recognize
TDD correctly with the collected data. A pilot validation study on Zorro
was already conducted in Univeristy of Hawaii in spring 2006. Further
validation study with professional TDD developers and extended validation
study in classroom setting are scheduled in fall 2006.

\subsection{Pilot validation study}
A pilot Zorro validation study of Zorro was conducted in Univeristy of
Hawaii in spring 2006 to validate two aspects of Zorro. Seven experienced
java developers were recruited to implement strack data structure in java
with test-driven development method following the provided TDD tutorial and
implementation guidelines. The development environment is limited to
Eclipse IDE and development process was instrumented by Zorro with
Hackystat Eclipse sensor.

We realized that it is necessary to have another independent data source to
compare data collected and analyzed by Zorro. An Eclipse plugin, Eclipse
Screen Recorder(ESR), was designed and implemented to record changes of
Eclipse window caused by development activities into a Quicktime
movie\cite{csdl2-06-02}. Because ESR can sincerely record development
activities, we will have an independent high fidelity data source to
validate developer behavior data collected by Zorro. Also we can use the
recorded video to validate Zorro episode recognition results. Figure
\ref{fig:PilotSummary} presents this pilot validation results in a table.
\begin{figure}[htbp] 
  \centering
  \includegraphics{figs/Pilot-Summary.eps}
  \caption{Summary of pilot validation study}\label{fig:PilotSummary}
\end{figure}
 
The pilot study demonstrated that Zorro can collect complete enough
developer behavior data to recognize test-driven development and the
recognition accuracy is close to 90\% percent (figure
\ref{fig:PilotSummary}). However, 10\% episodes were wrongly classified due
to incomplete data and insufficient TDD recognition rules. Causes were
identified and improvements were made after the pilot study.

This study also reveals that developers spent quite some time doing
test-last development although they knew that it was a study on test-driven
developmnent and guidelines were provided for reference. Maybe the stack
data structure is too simple or this is the nature of test-driven
development. Further studies on more complicated problems with experienced
TDD developers must be conducted to validate Zorro and find the cause of
high percentage of test-last episodes discovered in the pilot study.

\subsection{Case study within TDD community}
Pilot study demonstrates that the automatic data collection and recognition
of test-driven development are feasible with Zorro software system. Zorro
can recognize 90\% episodes and technically it can achieve much higher
accuracy with the new improvements made after the pilot study. Extended
study is desired in order to persuade people that Zorro really works on
both developer behavior data collection and TDD recognition in larger
scales. A good case study is to validate Zorro with experienced TDD
developers.

\subsubsection{Test subjects}
Test-driven development is a new practice and many people still do not know
it yet. It is impossible to recruit enough experienced TDD developers
locally to conduct Zorro validation study. We must turn to other sources to
look for experienced TDD developers. Web site \textit{testdriven.com} and
TDD user group \cite{TddYahooGroup} are dedicated to disseminate
information on TDD and discuss issues related to TDD. Members of TDD user
group and visitors of \textit{testdriven.com} are interested in test-driven
development so they might like to have a tool like Zorro to assess their
TDD development. We can recruit professional TDD developers from these two
communities to participate in Zorro validation study.

\subsubsection{Data collection}
The easy installation of Hackystat Eclipse sensor and unobtrusive manner of
data collection of Zorro make it easy for test subjects to conduct this
study by themselves. Eclipse Screen Recorder(ESR) is a Eclipse plugin with
very simple user interface. Developers only need to start the recorder at
the beginning and stop it after the experiment. Product metrics and
developer behavior data are automatically sent to Hackystat server for
assessment by Zorro. They can send us the video files and inform us the
development period to help compare two data sources to validate Zorro.

To persuade developers participating in this study and help them configure
test environment we wrote two Hackystat DocBook chapters Zorro user guide
and administrator guide, they are available on Hackystat web
server\cite{Hacksytat}.

\subsubsection{Validation analysis}
Developer behavior data and ESR video are two data sources for analysis. A
Zorro project can be configured to every test subject to run Zorro
analysis. Similar as pilot study we will use the recorded video to validate
the collected behavior data and the recognition results. Zorro
administrator guide has one section on how to conduct this validation.

\subsection{Case study in Classroom}
We will conduct an expanded Zorro study in classroom in fall 2006 to
validate Zorro's data collection and recognition of test-driven
development. All students in the advanced software engineering class are
incouraged to learn test-driven development and develop their projects with
this new technique. The development process will be instrumented by
Hackystat Eclipse sensor to collect developer behavior data for Zorro
assessment. ESR will be used to record students' development process while
they are practicing TDD. 

\subsubsection{Materials}
Unit testing framework junit will be used as the testing technique and
reading on test-driven development will be assigned to students. I chose
three well-known problems and wrote the guidelines to help students practice
test-driven development.
\begin{enumerate}
\item{Stack is a classical data structure that works in Last-In-First-Out
    (LIFO) principle}
\item{Roman Numeral problem asks developers to write a program that can
    convert any integer number between 0 and 50 into Roman numerals.}
\item{Bowling Game problem asks developer write a software program to
    calculate bowling game score.}
\end{enumerate}

\subsubsection{Experiment procedure}
The classroom case study of Zorro consists of four steps skill building,
test-driven development practice, project development in TDD and project
development.
\paragraph{Skill Building}
Considering that students are new to test-driven development and they are
lack of unit testing skills, we allow a skill building process at the
begining of the semester. Students learn development tools such as Eclipse,
version control, ANT build tool and coding standard etc. and development
skills such as MVC pattern, component-based design, unit testing and
test-driven development etc.
\paragraph{Test-Driven Development}
All students are asked to choose 2 of the three problems to implement them
in test-driven development method. Meanwhile, the development process will
be instrumented by Hackystat Eclipse sensor and ESR.

\paragraph{Project development in TDD}
Students will be asked to work on their own projects in test-driven
development in the first three weeks. The development process will be
instrumented by Hackystat Eclipse sensor.

\paragraph{Project development}
After the first three weeks, students can selectively opt out test-driven
development in their project development. Whether or not use test-driven
development is left as a personal choice. Again, the development process
will be instrumeted by Hackystat Eclipse sensor.

\section{Contribution}
Zorro software system is the contribution this research made to the toolset
of software engineering. It can automatically collect developer behavior
data and assess test-driven development compliance. It helps researcher to
conduct qualitative research on test-driven development and improves
practice of test-driven development. Also it can be used in the education
and training of test-driven development to improve its discipline. 

The generic software development stream analysis framework is significant
contribution to practice and research of low-level software process with
rule-based system. 

Eclipse screen recorder (ESR) is very light-weight and it can compress
video screen effective with help from Quicktime. It solves a long living
problem on data validation in software engineering discipline. It is
simple, light-weight and works. 

\section{Software Development Stream Analysis Framework}
At the very beginning software development was ad-hoc and software
projects' successes largely depended on the talents and capabilities of
individual programmers. The introduction of software process such as
waterfall model changed the route of software development from chaos into
discipline. In definition, software development stream is a series of
software development activities in chronological order that are conducted
by developers over a period. Since these activities track what programmers
do, we can say that software development stream reflects how developers
execute software process. It is great to have software development stream
but we do not end up with it because one development stream can be very
long and has too many activities. The massive information it contains
and analysis complexity of software development stream inspired us to
develop micro-process, a small portion of the software development stream
to accomplish a programming task. Like microscope to magnify objects,
micro-process provides a very detailed view on software development
process. It introduces a very adequate way to analyze software process as
well as best practice in software development.  Software development stream
analysis framework binds development stream and micro-process together. It
includes activity data collection, development stream construction,
episode recognition and evaluation as well.

\subsection{Software Development Stream Construction}
In the course of software development, developers interact with tools to
produce software artifacts. The data collection system will have to collect
both development activity data and software metrics to keep track of the
software product changes. Process data are either developers' direct
interactions with tools or the program's changes committed by developers.

Our data collection is supported by Hackystat, an in-process unobtrusive
metric collection system. Hackystat has more than 10 sensors to collect
development activities, software metrics, unit test, build, file commits
and so on. Hackystat stores process metric data in a centralized server in
XML format. We reduce raw software metric data into activities directly or
indirectly by checking continuous software metric variations. For
convenience reason each activity type is processed separately to form
development sub steam, which has homogeneous activities only. We merge sub
streams together to create software development stream.  Table
\ref{tab:stackstream} is part of the software development stream whilst I
worked on a stack data structure implementation.
\begin{table}[!h]
\centering
  \begin{tabular}{|llll|}
  \hline
    23:43:57 & TestStack.java & ADD CLASS & package org.sci \\
    23:43:57 & TestStack.java & ADD IMPORT & import junit.framework.TestCase \\
    23:43:57 & TestStack.java & MOVE CLASS & org.sci --\textgreater TestStack.java \\
    23:44:19 & TestStack.java & ADD METHOD & void testEmptyStack() \\
    23:44:38 & TestStack.java & TEST EDIT & 6sec \\
    23:44:39 & TestStack.java & COMPILE & Stack cannot be resolved \\
    23:45:07 & Stack.java     & ADD CLASS & Stack.java \\
    23:45:07 & Stack.java     & BUFFTRANS & FROM TestStack.java \\
    23:45:34 & Stack.java     & ADD METHOD & Stack() \\
    23:45:56 & Stack.java     & PRODUCTION EDIT & 18sec \\
    23:46:02 & TestStack.java & BUFFTRANS FROM & Stack.java \\
  \hline
  \end{tabular}
  \caption{Development Stream Example}\label{tab:stackstream}  
\end{table}
Stack works on the principle of Last-In-Last-Out(LILO). I implemented stack
in Java with best practice Test-Driven Development (TDD). The
implementation starts with test object \textsf{\textbf{TestStack.java}} to
drive the implementation of stack. Test case \textsf{\textbf{void
    testEmptyStack()}} is added right after since it is an easy task to
test and implement. The compilation of the test class failed because object
\textsf{\textbf{Stack}} does not exist yet. The rest of the work is stack
emptiness check. This is a typical TDD-style development scenario that can
be replayed by software development stream.

\subsection{Best Practice Tokenization and Identification}
Development stream has rich information on software process. It's easy to
analyze a small portion of the development stream, but it is not feasible
to look up the real development stream because it may have thousands of
activities. The identification and evaluation of best practices with
development stream must be done automatically. There are some literature
work on automatic software development process discovery and modeling. Cook
et al \cite{Cook:95} implemented a system called Balboa to discover and
validate formal software process with finite state machine.  Chris Jen et
al automate discovery and modeling of open source project development by
studying web activities with email, message board and instant messaging
etc\cite{Jensen:04}.

Software development best practice contains a set of rules and
recommendations to regulate development process. It either defines the
order of development activities or adds new pieces to the development
process. For example, incremental build is the best practice to audit
developers' edit work and build system once there are changes being made;
unit test best practice emphasizes that developers should write their unit
tests to improve the quality of their programs. Because best practices are
repeated in development process, the software development stream should
contain many best practice iterations when it is deployed. Our strategy is
to tokenize development stream into iterations, which can also be called
micro-process or episode.

An episode contains a series of activities following the best practice if
developers comply the rules and recommendations of it. Rule-based system is a
natural choice to identify the execution of best practice. In a rule-based
system knowledge is represented in the form of \textit{if...then...}
rules\cite{Luger:02}. The inference engine in rule-based system can apply
best practice rules on development episode to identify best practice.
Figure \ref{fig:concept} illustrates the concept and infrastructure of this
recognition system.
\begin{figure}[h]
  \centering
  \includegraphics{figs/ConceptStructure.eps}
  \caption{Episode Recognition System}\label{fig:concept}
\end{figure} 
It provides a powerful tool to identify and evaluate the execution of best
practice without human being's intervention.

Software Development Stream Analysis (SDSA) framework unites data collection, 
development stream, episode tokenization and episode identification together 
to study best practice in software development. In my thesis work I apply this 
framework on best practice Test-Driven Development, a practice of Extreme 
Programming (XP).

\begin{comment}
\subsection{Test-Driven Development Process Measurement}
\subsubsection{TDD Episode Tokenization}
Red/Green bar is the pattern of Test-Driven Development. It implies that an
episode ends with successful unit test execution, the green bars are tokens
of Test-Driven Development. We define {\it Test Pass Tokenizer} to partition
development stream in the context of Test-Driven Development. It ends an
episode when there is a successful unit test execution, figure
\ref{fig:testpass} is one test-pass episode example.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{figs/TestPassEpisode.eps}
  \caption{Test Pass Episode}\label{fig:testpass}
\end{figure} 

Technically test-pass tokenizer is enough to detect iterations of
Test-Driven Development. However, in team software development, developers
also deploy their work with the whole system to make sure new code will not
break it before integration. To accommodate team software development we
defined \textit{Commit} and \textit{Command} tokenizers for integration and
deployment activities. On the other side, developers may not do TDD or
occasionally step away from TDD without frequent unit test invocations. We
defined \textit{Buffer Transition} tokenizer to divide this kind of
micro-processes into smaller buffer transition micro-processes for further
analysis.

\begin{itemize}
\item \textit{Commit tokenizer} ends an episode when it encounters a bunch
  of file commit activities. It can be used to inspect what developers do
  before integration.
\item \textit{Command tokenizer} ends an episode when there are some
  consecutive command build activities to deploy system in local
  environment.
\item \textit{Test Pass tokenizer} ends an episode when there are
  successful unit test invocations. We implemented it to find the iterations
  in Test-Driven Development.
\item \textit{Buffer Transition tokenizer} starts an episode when it
  encounters consecutive buffer transition activities. It sums what
  developers did to the working buffer.
\end{itemize}
\end{comment}

\subsubsection{TDD Episode Identification and Classification}
Test-pass tokenizer partitions development stream into many iterations that
end up with successful unit test execution. In term of TDD, a test-pass
episode can be either ``test-driven'' or ``refactoring''. The significant
property of refactoring is that there is no new test case in refactoring 
episode. With this character we can classify test-pass episodes of TDD into 
two categories, ``test-driven'' and ``refactoring''.  Moreover, a test-pass 
episode with test creation may or may not be ``test-driven''.  Test-Driven 
Development implies the order of development is to ``test a little, code a 
little and repeat."\cite{Beck:03} such that it will be ``test-last'' if test 
is not created before code implementation. Figures \ref{fig:tdd} and
\ref{fig:refactoring} illustrate different kinds of ``test-driven'' and 
``refactoring'' episodes respectively.

\begin{figure}[h] 
  \centering
  \includegraphics[width=0.8\textwidth]{figs/TDD.eps}
  \caption{Test-Driven Episode Classification}\label{fig:tdd}
\end{figure} 

\pagebreak
\begin{figure}[h] 
  \centering
  \includegraphics[width=0.8\textwidth]{figs/Refactoring.eps}
  \caption{Refactoring Episode Classification}\label{fig:refactoring}
\end{figure} 

\section{Roadmap}
This thesis work is to evaluate best practice execution in software development 
with development stream. Chapter \ref{chap:relatedwork} is the related
literature work on Hackystat, software process automation, event stream and
Test-Driven Development. The system design structure and implementation details
are described in chapter \ref{chap:implementation}. The framework is tested
against Test-Driven Development with a series of experiments in 
chapter \ref{chap:evaluation}.

\begin{comment}

 as in Figure \ref{fig:EpisodeTree}.
\begin{figure}[htbp] 
  \centering
  \includegraphics[width=0.8\textwidth]{figs/EpisodeTree.eps}
  \caption{Episode Tree}\label{fig:EpisodeTree}
\end{figure} 

Some empirical case studies \cite{George:03}, \cite{Maximilien:03}
reported successes on TDD practice. According to these studies TDD group
passed more black box tests than TLD group and they spent less time on
projects than TLD group. Even though most empirical studies drew positive
conclusion on Test-Driven Development there are still some neutral or
negative reports on TDD. Geras etc. \cite{Geras:04} found that there is
little or no difference in developer productivity in TDD and TLD processes.
Another study \cite{Muller:02} concluded that TDD does not accelerate the
implementation and the resulting programs are not more reliable than TLD.

\section{Validation}
Unlike many other cumbersome software processes such as Spiral model, PSP or
RUP Test-Driven Development is very lightweight. It contains two simple
rules only. In practice TDD is hard to follow compared to other processes
because there is no management involved in the development. In situations
that pair programming is not involved developers are fully responsible for
TDD execution by themselves without monitoring. This weakness will bring
many questions to TDD process. Do developers do Test-Driven Development
when they are told to do so? And if they do how well they do TDD in their
development? Do developers always follow two TDD rules?

In my thesis study I will build a system on top of Hackystat to answer
these questions as well as provide a tool to discipline TDD process.  There
are two goals to pursue in my work. One is to study how the development is
being done, especially how the unit testing is conducted in Test-Driven
Development. Another goal is to study properties of TDD. Will developers
spend more time on development and yield high quality code or not? Will the
test coverage be naturally 100\%?

Scott Ambler's UML diagram (Figure \ref{fig:TDDSteps}) depicts TDD development
iterations.

\begin{figure}[htbp] 
  \centering
  \includegraphics{figs/TDDSteps.eps}
  \caption{The Steps of TDD}\label{fig:TDDSteps}
\end{figure} 

\begin{itemize}
\item The first step is to add a test quickly. Basically it just fails the
  code.
\item Next run your test or test suite to make sure the new test does fail.
\item Make changes to the function code to make test pass. As long as it
  can make test pass you even can fake the implementation, for instance,
  return a constant number.
\item Run your tests again. If it still fails you go ahead to update your
  function code. Once all tests pass you can start over with a new test.
\end{itemize}



 
In the early era of software development there is no software process.
Software systems are developed in a chaotic style and their successes
depended largely on individual's skills and capabilities. Water-fall model
is the first and most popular software development process and it still
exists in many organizations. In water-fall model software development is
divided into stages from requirement analysis to operation and testing is
conducted after project is implemented. Bugs and defects found in testing
will be fed back to the development team or maintenance team. Since bugs
and defects are revealed at the late stage of software development
water-fall model is reluctant to meet customer's requirements and defect
fix. If defects are rooted from design it will be extremely hard to adapt
changes according to water-fall model.

Modern iterative incremental development(IID) was developed to fix this
problem by dividing a large project into many parts.  They are implemented
iteratively. Each iteration can be a mini-waterfall process so that defects
can be fixed and changes can be made very quickly.  Spiral model is the
first process definition to formulate iterative incremental development
practice. Other variations like prototyping, RAD, CleanRoom, Scrum, RUP,
FDD, Extreme Programming are used in many projects successfully.  Latest
development of continuous integration builds system once a day. In these
processes testing is done after some amount of work is finished to improve
project quality.
  
\section{Personal Software Process}
\label{sec:psp}

   Introduce LEAP and Hackystat.

\section{Test-Driven Development}
\label{sec:tdd}

\subsection{Unit Testing}
\label{sec:UnitTest}
Object-oriented technique treats abstract concepts as entities such that
each of them is independent and can exist without relying on other code.
Unit testing was invented to test a component before it is integrated to
interact with other components. Unit testing originates from
pre-object-oriented era and an individual test concentrates on a single
unit of the system rather than on the entire system. At present when we say
unit testing it refers to component testing. In object-oriented world a
component test usually tests an object or class which is the smallest component
of a system. ``In computer programming, a unit test is a method of testing the
correctness of a particular module of source code.'' \cite{UnitTestWiki}

Unit testing is becoming more and more popular in modern software
development.  The de facto unit testing standard, ``xUnit'' framework, is
already ported to more than 30 language support. It makes test automation
be feasible and test cases created with XUnit framework can server as
regression test suite too. Except for verifying correctness of each class
unit testing is also the executing documentation. In recent software
project development unit testing is integrated into development process in
many organizations. Test cases are written by developers who are also
responsible for programs. Even though source code is visible to developers
but unit testing is still thought as black-box testing because it simulates
calls from other components.  

JUnit is the most well-known XUnit framework implementation in software
development and other dialects NUnit, CPPUnit, PyUnit so on and so forth
are created to make writing unit tests be very simple in different
programming languages. JUnit and NUnit also have IDE plug-in supports so
that a unit test case can be exercised by just a single click. For
continuous integration unit test cases can also be batched together to run
as ANT task. Since writing unit tests is not a cumbersome job any more it
is recommended to write unit tests in the development process instead of
allocating extra resource to let quality assurance testers to create unit
tests separately. It improves the effectiveness of testing such that
software systems are in high quality before they are delivered to quality
assurance department or customers if there is no quality assurance process.
Since less time is needed to do quality assurance it will save overall
development time conceptually.


In TDD unit testing is crucial because it drives the implementation and provides
instant feedback of the developing code to the developers. The XUnit
framework family make it easy to write tests and execute tests often in
software development.

\section{Thesis Work}


\section{Road Map}

\end{comment}
\begin{comment}
\label{sec:ResearchObjective}
Test-Driven Development is being accepted by more and more developers and
organizations. On the other side there are still many developers and
researchers resist Test-Driven Development or doubt its effectiveness in
software development. Kent Beck argued that TDD will not increase
development cost but can actually improve productivity as well as software
quality. Ron Jeffries's pithy word ``Clean code that works'' is the goal
of Test-Driven Development.


\section{Extreme Programming(XP) and Test-Driven Development(TDD)}
[How good is unit test? and why TDD]

  
This testing pyramid corresponds to software system development process.
In general a large system is divided into many components to be implemented
independently. Unit testing is created to verify correctness of each
component before it is integrated into the system.
  
Usually unit testing is done by programmers themselves to verify the
correctness or by quality assurance specialists to find errors in the
existing code.  According to the cost model of removing bugs in different
software development stages removing bugs in development phase is the most
cost-effective way.  Because unit testing is good from many aspects Extreme
Programming, an agile software development process, advocates Test-Driven
Development (TDD) as its core component. In TDD unit tests are
incrementally written prior to code implementation \cite{George:03} thus
it drives software specification design in addition to validating and
verifying program correctness. Because of this capability TDD is also
called Test-First Design (TFD). As comparison the old way writing unit
tests after code is ready is called Test-Last Development(TLD).
  
  
  The controversial conclusions can be either from TDD process itself or
  from the poorly executed experiments. There is one thing in common that
  all these studies failed to address how they managed experiments to
  guarantee TDD developers do Test-Driven Development while TLD developers
  do Test-Last Development in their studies. There is no process data
  available to crossly validate process disciplines. The lack of
  fine-grained process data and software metrics makes it impossible to
  conclude whether developers were doing TFD, ad-hoc or TLD in the previous
  experiments; thus, conclusions drawn from these experiments are weakened
  and become questionable. In my thesis I will add in-process metric
  support provided by Hackystat to study how unit tests are created and
  exercised in the development process to regulate software development.
\end{comment}

\begin{comment}  
\section{Testing}
\label{sec:Testing}
Testing is very important to software project success because developers
can not always produce perfect code that works well at the first time. Many
reasons determine that we need testing in software development. First of
all, many software systems deal with large number of states with complex
algorithms, also it is hard and impossible to address all system
requirements at the beginning of software projects development. Developers
always have to deal with new and changing requirements during the
development.  Another important factor is that software projects are
written by developers.  As human beings developers are not good at
repeative programming work without committing mistakes.  Since software
development is error-prone many design and development support tools such
as flowchart, dash board, UML , compilers, debuggers, version control
system, project management software, bug tracking system and software
review etc are brought up to support qualitative software development.
Moreover, there are rare software projects developed and maintained by an
individual programmer nowadays. The ineffective collaboration and
cooperation in the developing team will make software volatile to defects
and bugs too \cite{Pfleeger:01}.  To ensure software quality there are
quality assurance departments in many software institutes and many testing
techniques appeared in the development of software engineering discipline.
From visibility of source code there are white-box testing and black-box
testing. Depending on when testing happens there are unit testing,
regression testing, integration testing, system testing and acceptance
testing which can be done by programmers, quality assurance testers or
customers manually or automatically.

\begin{figure}[ht] 
  \centering
  \includegraphics[width=0.5\textwidth]{figs/TestLayerPyramid.eps}
  \caption{Software Testing Pyramid}\label{fig:TestLayer}
\end{figure} 

In tradition, software testing is thought as quality assurance testers' or
customers' job in water-fall software process model. Even in recent
iterative incremental development (IID) models quality assurance
specialties and customers still play important roles on testing.


\section{Software Processes}
\label{sec:SoftwareProcess}
In the early era of software development there is no software process.
Software systems are developed in a chaotic style and their successes
depended largely on individual's skills and capabilities. Water-fall model
is the first and most popular software development process and it still
exists in many organizations. In water-fall model software development is
divided into stages from requirement analysis to operation and testing is
conducted after project is implemented. Bugs and defects found in testing
will be fed back to the development team or maintenance team. Since bugs
and defects are revealed at the late stage of software development
water-fall model is reluctant to meet customer's requirements and defect
fix. If defects are rooted from design it will be extremely hard to adapt
changes according to water-fall model.

Modern iterative incremental development(IID) was developed to fix this
problem by dividing a large project into many parts.  They are implemented
iteratively. Each iteration can be a mini-waterfall process so that defects
can be fixed and changes can be made very quickly.  Spiral model is the
first process definition to formulate iterative incremental development
practice. Other variations like prototyping, RAD, CleanRoom, Scrum, RUP,
FDD, Extreme Programming are used in many projects successfully.  Latest
development of continuous integration builds system once a day. In these
processes testing is done after some amount of work is finished to improve
project quality.



\section{Extreme Programming}
\label{sec:XP}
Extreme programming (XP) grows very fast recently and many organizations
are using it or considering to adopt it in their developments. Extreme
Programming is also one kind of iterative incremental development (IID)
process.  It begins with collecting ``user stories'', which are brief
description of tasks to be accomplished. Developers can discuss with on-site
customers to make release plan based on user stories. After each release
customer can test the sub system and give feedback to developers quickly.
XP can not only meet customers' changing requirements well but also provide
high quality code because it stresses heavily on unit tests in the
development. Four rules are enforced in extreme programming.

\begin{itemize}
\item All code must have unit tests.
\item All code must pass all unit tests before it can be released.
\item When a bug is found tests are created.
\item Acceptance tests are run often and the score is published.
\end{itemize}

To enact these rules XP projects should be developed in Test-Driven
Development process, in which unit tests are created before code
implementation.  Developers write unit tests based on user stories first
and then generate code to make test pass.
\end{comment}
