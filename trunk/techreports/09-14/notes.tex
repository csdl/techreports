\chapter{notes}

This thesis presents a broadly applicable methodology for classification observed 
behaviors based on algorithmic information theory and algorithmic statistics. 

The proposed methodology generates a concise and meaningful summary of the software 
processes performed in time. Basing on these captured summarization is captured by using an algorithmic information theoretic
distance measure in conjunction with the gap statistic to estimate the randomness
deficiency from algorithmic statistics for a clustering model. This is the first known
technique for approximating the randomness deficiency; the lack of practical
approximations for randomness deficiency have to date prevented any practical
applications of algorithmic statistics. 

The methodology assumes the availability of object extraction and tracking algorithms 
for a given application. Beyond these requirements, it
is parameter free and requires no training data. The gap statistic based approximation of
the randomness deficiency is proposed, and used as the criterion for feature subset
selection, and to ensure that the clustering maximally captures the meaningful
information in the image data. The clustering result corresponding to the maximum
magnitude of the gap statistic is the proposed summary. Implementing our methodology
is straightforward, and relies on common data compression algorithms such as bzip.
Our formulation is novel in its use of the relative Kolmogorov complexity in the form of
an algorithmic information theoretic distance capable of quantifying any and all
differences between digital objects. The proposed Normalized Adaptive Information
Distance (NAID) measure, based on the normalized information distance proposed by
Vitanyi et al., is unique in its ability to compare vector or time-series data, including
data of different dimensions. The image data is analyzed at multiple levels of precision
using the proposed closed-form automatic multi-dimensional quantization algorithm.
The optimal quantization level is selected jointly with other problem parameters such as
the number of groups in the summary and the relevant feature subset. We also propose a
closed-form method for multidimensional quantization of numeric data capable of
assigning any number of symbols to data of any dimension, such that the symbols are
equiprobable under the assumption of normally distributed data. This dramatically


According to Chester Barnard \cite{citeulike:1414171} intuition is the most appropriate 
cognitive tool when used in working with short time horizons and with data which is either 
poor quality or very limited. He is advocating, that intuition is not only an efficient 
tool but it provides sufficient support in the situations where the logical reasoning 
cannot be applied - ``... to material that is so insecure that it cannot bear the weight 
of ponderous logic... '' 
While he is being overoptimistic in the saying that ``Our logical methods and our endless 
analysis of things has often blinded us to an appreciation of structure and organization...''
And finally he says: ``To understand the society you live in, you must feel organization - 
which is exactly what you do with your non-logical minds...'' (\cite{citeulike:1414171}, p. 317)

Barnard's work was succeeded by Herbert Simon, who being a scientists took a somewhat 
different approach - he worked on understanding of the intuition phenomena. At the beginning 
he followed Barnard's mysterious intuition concept, but after all the advances in artificial
intelligence and cognitive sciences, he conducted that intuition is a subconscious pattern 
recognition. Thus, it is a rational but not concious analytical method appropriate for decision 
making \cite{citeulike:6708618}. 

Maybe in the software development process (or software process enactment) it is impossible 
to apply sort of \textsl{satisficing} \cite{citeulike:10055914} - ... maybe?



citeulike:10055684

The Institute of Electrical and Electronics Engineers defines software engineering as 
“the application of a systematic, disciplined, quantifiable approach to development, 
operation, and maintenance of software; that is, the application of engineering software”
IEEE Standard Computer Dictionary, Institute
of Electrical and Electronics Engineers, New
York, 1990.

"There are two ways of constructing a software design. One way is to make it so simple 
that there are obviously no deficiencies. And the other way is to make it so complicated that
there are no obvious deficiencies."
- C.A.R. Hoare

``Based on our PSP and TSP experience, software engineers can be taught new methods and
convinced to use them. The courses are challenging and require a lot of work but the 
resulting benefits are substantial. The graduate or senior-level undergraduate PSP 
course must be approached more as an experience than an intellectual exercise. 
Merely explaining the methods will produce little or no meaningful benefits. 
The students must work through the course exercises, measure their work, and 
analyze their data to see improved performance. When they do this, the course works. 
If they just read the text or listen to lectures, they are generally wasting their 
own and the professor's time.''
Why Don't They Practice What We Preach?
Watts S. Humphrey

This approach to explaining things around us dates back at least to Epicurus
(342?-270?BC) (Li 1993, p. 274). Let’s consider theory formulation in science as the
process of obtaining a compact description of past observations together with future ones.
Let us suggest that the preliminary data of an investigator, the hypothesis proposed, the
experimental design and setups, the trials performed, the outcomes obtained, the new
hypothesis formulated, etc., can be encoded as an initial segment of an infinite binary
sequence. The investigator obtains increasingly longer initial segments of an infinite
binary sequence by performing more and more experiments. To describe the underlying
regularity in the sequence, the investigator tries to formulate a theory that governs the
sequence based on the outcome of past experiments. Candidate theories or hypotheses
are identified from the sequences starting with the observation of the initial segment.
There are many different possible infinite sequences or histories on which the
investigator can embark. The phenomenon the investigator is trying to understand or the
strategy used can be stochastic. In this type of view, a phenomenon can be identified
with a measure, i.e. probability distribution, on a continuous sample space.
This research attempts to express the task of learning a certain concept in terms of
sequences over a basic alphabet. We express what we know as a finite sequence over the
alphabet. An experiment to acquire more knowledge is encoded as a sequence over the
alphabet, the outcome is encoded over the alphabet, new experiments are encoded over
the alphabet, and so on. This way we can view a concept as a probability distribution
(measure) over a sample space of all one way infinite binary sequences. Each sequence
corresponds to one never ending sequential history of conjectures, refutations, and
confirmations. The distribution can be said to be the concept of phenomenon involved.
We can predict what is likely to turn up next with an initial segment. Using Bayesian
analysis (Bayes 1763) to compute the conditional probability, we can predict and
extrapolate future outcomes. This is the general thrust of this research.

There are many studies about the proper formulation for learning curves for
different problem sets. The majority of the learning curve models indicate that the time
to perform a task decreases with the number of times a task has been performed. This is
covered extensively in the literature. A review of the relevant historical studies is shown
in Chapter II (in 10. Learning Curves, p90). Chapter III develops the learning curve
formulations used in this research (Appendix G Learning Curve, p443).

===

Software is usually coded by teams. Members of these teams have a variety of skills, experiences, 
believes and motivations. While they agree on use of particular technologies, development tools, 
and a development process which also imposes a timeline and a budget, the software development
activity is highly creative and non-recurring. 

. These are necessary constraints placed in order to to keep work organized, 
however there is a great freedom in what they actually do in every single moment of time in 
order to progress towards lines of code which eventually will result in software. 

There are "pathological" software/business processes that are resistant to systematic (re)engineering, and thus should be avoided.

    * These processes are characterized by:
          o lack of frequent enactment repetition
          o ad-hoc process structure or flow
          o highly creative, non-recurring activities
          o long-duration process enactment cycle times
          o processes whose modeling or formalization overwhelms their simplicity.

For example one developer may follow test first process while
another writes tests at last.  This freedom of choice in ordering of development activities 
while being much appreciated by talented and creative individuals creates an impression 
of chaotic and unordered activities for random observers, newbies and people in 
charge - so there we have all the attempts of imposing an order 
(or control) on all of the development activities. Metrics and models of processes