%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% project-related.tex -- 
%% Author          : Philip Johnson
%% Created On      : Thu Aug 12 11:04:09 1993
%% Last Modified By: Philip Johnson
%% Last Modified On: Thu Nov  1 10:19:56 2001
%% Status          : Unknown
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1993 University of Hawaii
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Related work}

This section describes relationships to work by other researchers and how
it influences the proposed project.

\vspace*{-.15in} \paragraph{Measurement theory and data validity in empirical software
  engineering.}  

Research by Fenton, Bush, Hall, Pfleeger, Kitchenham, and others has
improved understanding of effective measurement collection and analysis in
empirical software engineering
\cite{Bush90,Fenton97,Hall97,Kitchenham01,Kitchenham95,Pfleeger97}. One
influence of this work on the design of Hackystat involves data
comparability.  For example, Hackystat requires that all sensors for active
development time collect data in the same way, although they may be written
in different languages for different tools.

Research by Basili, Fuggetta, Rombach, and others on data validation and
the Goal-Question-Metric (GQM) paradigm stresses the need for validation
procedures and explicit ties between low-level measurements and high-level
organizational goals \cite{Basili88,Basili94,Birk98,Fuggetta98,Solingen99}.
In Hackystat, validation procedures are required to ensure that the model
of user behavior as represented by the sensor data is sufficiently accurate
for the purposes of the analyses.  In addition, users need to know why the
measures are being collected in order to feel comfortable allowing the
sensors to be active.

Finally, Fenton and Neil present a critique of traditional applications of
software measurement which provide simplistic correlations between such
measures as size and effort, as well as pre-release and
post-release quality \cite{Fenton00}. The Hackystat project is designed to
provide the kinds of data appropriate for either simplistic correlations
(when such correlations are actually helpful in the context of
developer-centric measurement), as well as the kinds of more sophisticated
causal analysis mechanisms they advocate.

\vspace*{-.15in} \paragraph{Statistical process control in empirical software engineering.}

Another influential branch of research involves the use of statistical
process control techniques to assess and improve the stability and
predictability of software development procedures
\cite{Florac97,Wheeler92}. The normal application of statistical process
control techniques is to drive the software development process toward a
predictable state, where future values of measures can be reliably
estimated and historical variations are within ``natural'' limits.  In
Hackystat, statistical process control techniques (such as control charts)
will be applied to support developers in understanding whether or not the
measures under collection are ``in-control'' or not, and thus whether it
would be appropriate to use them in estimation.  However, Hackystat does
not require processes to be ``in-control'' in order for the system to be
useful.


\vspace*{-.15in} \paragraph{Agile software development methods.}  

Newly emerging software development methods such as Extreme Programming,
Scrum, Crystal methods, adaptive software development, and others are
critical of traditional approaches to project measurement as requiring too
much developer overhead with little immediate benefit
\cite{Highsmith01,Jeffries01}. Hackystat is designed to appeal to this
community by providing extremely low cost measurements oriented around
the needs of developers, and by providing feedback within a development
iteration. For example, Hackystat already provides a sensor for JUnit that
collects data on test case invocation and its results.

\vspace*{-.15in} \paragraph{The Personal Software Process (PSP) and Team Software Process
(TSP).}  

Research on the PSP and TSP strongly influence the design of this research.
First, the PSP and TSP demonstrate the importance and potential utility of
collecting size, time, and defect data.  With sufficiently accurate and
complete historical data concerning these three primary measurements, the
PSP and TSP show how developers can generate a variety of analyses that can
lead to substantial improvements in both their project planning
capabilities and in the ultimate quality of their software.  

Despite these promising results, case studies of the PSP suggest that the
level of in-process interruption required by the methods creates
fundamental adoptability problems \cite{csdl-98-13,Webb99}.  Hackystat will
explore whether automated data collection and analysis can be used to
provide most of the utility of the PSP in a manner suited to a broad
spectrum to developers and organizational contexts.

\vspace*{-.15in} \paragraph{Measurement toolkits.} 

Other toolkits have been created for empirical software project data
collection and analysis, though we know of no system satisfying our
developer-centric, in-process, and non-disruptive requirements.

A variety of toolkits collect PSP-inspired data from developers through
in-process disruption.  Examples of these systems include PAMPA
\cite{Simmons98}, the PSP Process Dashboard \cite{PSPDashboard}, the PSP
Studio \cite{PSPStudio}, and the Excel spreadsheet distributed with the Team
Software Process \cite{Humphrey00}.

The Balboa system supports in-process collection of 
software engineering data \cite{Cook98}.  These streams of event 
data are analyzed to infer and/or validate process models via
grammar-based rules. Thus, Balboa can aid developers
in answering questions like, ``Was every minor release preceded by a formal
code inspection?'', though developers must manually decide when to ask
these questions and what questions to ask. 

Research on the Amadeus system explored a set of architectural principles
and abstract interfaces for designing metric-driven analysis and feedback
systems \cite{Selby91}. Amadeus was intended to include
active agents, a custom scripting language, and custom client-server
components, but little experience with a working implementation has been
reported.

The Ginger2 system provides sensors for detecting keystrokes and mouse
gestures, eye traces, three dimensional movement, skin resistance levels,
and video recording of the developer \cite{Torii99}. Unlike Hackystat,
whose sensors are designed for deployment ``in the field'', Ginger is a
laboratory-based environment.

Finally, MetricCenter \cite{MetricCenter} is a commercial measurement
toolkit provided by Distributive Software. Like Hackystat, MetricCenter
provides sensors that send data to a central repository for analysis.
Unlike Hackystat, the sensors and analyses performed by MetricCenter are
management-centric, not developer-centric.

\vspace*{-.15in} \paragraph{Software project metrics repositories.}

The Hackystat server functions as a metrics repository, and benefits from
prior research in this area.  The International Software Benchmarking
Standards Group maintains a public repository with data on over 1,200
projects from over 20 countries, including information on project context
such as the business area, product characteristics such as the user base,
development characteristics such as the programming language, size
characteristics such as function points, as well as others \cite{Lokan01}.
Hackystat leverages insights from ISBSG such as the importance of anonymity
in gaining industrial cooperation, though Hackystat data is
developer-centric, not management-centric.

A less successful metrics repository initiative is the National Software
Data and Information Repository (NSDIR). While a variety of factors appear
to have contributed to the failure of this initiative, one issue was ``that
program managers, under pressure to complete projects on time and on
budget, did not perceive participation in NSDIR would yield benefits
equaling the effort.\cite{Goth01}.''  In the Hackystat project, in
contrast, the overhead of participation is designed to be extremely low, so
that a positive cost-benefit analysis is easier to obtain for individuals.











