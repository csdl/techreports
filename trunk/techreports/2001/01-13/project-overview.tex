%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% project-overview.tex -- 
%% Author          : Philip Johnson
%% Created On      : Thu Oct  4 08:05:31 2001
%% Last Modified By: Philip Johnson
%% Last Modified On: Mon Nov  5 16:43:53 2001
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 2001 Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 

\subsection{Overview}

Collection and analysis of empirical software project data is central to
modern techniques for improving software quality, programmer productivity,
and the economics of software project development.  

For example, Cocomo \cite{Boehm81} and Cocomo II \cite{Boehm00} provide
models of software development that enable organizations to predict the
cost in resources and schedule time given characteristics of the proposed
project.  An essential phase of Cocomo model development is
calibration, where empirical software project data from organizations is
collected and used to determine internal model parameters.  The Personal
Software Process (PSP) \cite{Humphrey95} is a method for improving software
project planning and software quality assurance for individual programmers
by collection and analysis of empirical software project data.  The Team
Software Process (TSP) \cite{Humphrey00} builds upon the data collected by
the PSP with additional analyses to support group-based software
development. The Goal-Question-Metric paradigm (GQM) \cite{Basili88}
provides a methodology for collection and analysis of software project data
with explicit traceability to organizational goals, from which an
Experience Factory \cite{Basili94} can be built.  Both the Fagan
\cite{Fagan76} and Gilb \cite{Gilb93} Inspection methods use collection and
analysis of software review data to improve the efficiency and
effectiveness of future reviews. The Capability Maturity Model for Software
(SW-CMM) \cite{Paulk95} requires organizations to track project cost,
schedule, and functionality to reach Maturity Level 2, and to provide
quantitative measures of process and quality at Maturity Level 4. 

While these approaches and others explore the potential
benefits of empirically guided approaches to software development,
effective collection and analysis of software project data is rare in
mainstream software development. While a wide variety of factors contribute
to this situation \cite{Pfleeger97}, studies suggest that three
primary barriers are: (1) {\em cost}: gathering empirical software
engineering project data is frequently expensive in resources and time; (2)
{\em quality}: it is often difficult to validate the accuracy of the data;
and (3) {\em utility}: many metrics programs succeed in collecting data but
fail to make that data useful to developers.

For example, in the development of the Cocomo II model, over 2000 datasets
were gathered from industrial organizations for the purposes of calibrating
the model, yet only 161 of them were sufficiently accurate and complete
enough to be used for calibration \cite{Boehm01}.  A case study of the PSP
uncovered over 1500 errors made by 19 developers during data collection and
analysis \cite{csdl-98-13}.  One study found that the total average effort
to introduce GQM-based measurement is approximately one person-year
\cite{Fuggetta98}.  Extreme Programming (XP) eschews most traditional
project and process measures as requiring too much developer effort and
leading to insufficiently timely feedback \cite{Jeffries01}. A case study
of industrial metrics programs revealed very low developer confidence in
the accuracy of software project metrics \cite{Hall97}. One reason cited
for the failure of the National Software Data and Information Repository
was the perceived high cost of data submission and low value of benefits
returned \cite{Goth01}.  A 1990 survey of 300 major US information
technology companies implementing measurement programs determined that only
60 could be viewed as ``successful''. Reasons for the failure of the
remaining 240 programs included: the measures were viewed as irrelevant,
developers thought measures might be used against them, and data collection
was too time-consuming \cite{Rubin90}.

The problems associated with the cost, quality, and utility of empirical
software project  data creates problems for research, because it raises
the expense of empirical research and the time required for study results
to feed back into practice.  It also create problems for industry, because
many organizations do not believe they can afford the resources required to
collect and analyze empirical software project data or believe the effort
will be cost-effective.

This report describes Hackystat, a technology initiative and research
project that explores the strengths and weaknesses of a {\em
  developer-centric}, {\em in-process}, and {\em non-disruptive} approach
to empirical software project data collection and analysis. In essence,
Hackystat makes available to developers a set of custom sensors that they
voluntarily attach to such development tools as their editor, source code
control system, unit testing framework, and so forth.  Once installed,
these sensors automatically monitor characteristics of the developer's
process and products and send data using SOAP and XML to a centralized web
service.  The web service maintains a repository of process and product
data for each developer, performs analyses on the repository, and
automatically sends the developer an email when new, unexpected, and/or
potentially interesting analysis results become available.

Hackystat is {\em developer-centric} because all data is collected directly
from developer activities (such as writing code, running test cases,
checking in documents to a repository, executing the system, encountering
defects, and so forth).  Analyses are also developer-centric, in that they
are oriented toward the immediate interests and needs of developers. For
example, Hackystat might email the developer that test case coverage of a
particular module has decreased significantly due to recent enhancements.
Finally, data access is limited to the developer who generated it.  Other
measurement approaches often require the involvement of non-developers to
collect and/or analyze the data, or result in data that is of only marginal
use to developers.

Hackystat is {\em in-process} because data is collected regularly
throughout project development.  Analysis results are also in-process, in
that they are provided to developers throughout project development and are
meant to help guide in-process changes.  In contrast, many other
measurement approaches are effectively ``between-process'', in that they
collect data after completion of a project and analyze it in order to
improve development on a future project.

Hackystat is {\em non-disruptive} because developers do not have to stop
what they are doing in order to tell a measurement tool what data should be
collected about what they are currently doing.  Hackystat sensors do not
interrupt developers or required them to shift their focus of attention
from their primary task.  Analysis results from Hackystat are also
non-disruptive; they arrive as email so that developers have control over
when and whether to respond to them.  This contrasts with other
``disruptive'' in-process measurement approaches such as the Personal
Software Process \cite{Humphrey95}.

The overall goal of the Hackystat project is to accelerate adoption of
empirically guided software project measurement by providing a new approach
to addressing the barriers of cost, quality, and utility identified above.
Hackystat addresses cost by requiring that all empirical software project
data be collected and analyzed automatically through sensors and without
any developer or manager involvement.  Eliminating human involvement in
data collection and analysis also addresses several documented problems
with quality, such as when the high overhead of manual collection and
analysis leads to incomplete data \cite{csdl-98-13} or to the ``massaging''
of data \cite{Hall97}.  By making the system responsible for detecting
interesting or anomalous conditions in the data and by notifying the developer
by email when this occurs, the approach ensures that the design of data
collection and analysis is tied directly to developer utility.  

This project goal and approach creates a new set of research questions and
risks.  Clearly, automated measurement lowers the cost of data collection,
but is the data collected automatically sufficiently accurate for these
purposes?  Certain kinds of project data can't be sensed automatically;
what limitations does this place on the utility of the analyses? Will
commercial tools expose the appropriate API for Hackystat sensors?
Finally, will the privacy and security mechanisms suffice to prevent
adoption problems due to the specter of ``Big Brother''?  The Hackystat
project is designed to answer these and other questions by gathering
metrics of the system's accuracy, utility, and adoptability in academic and
industrial settings.

The basic components of Hackystat are available. Over the past six
months, we have implemented the web service and an initial set of sensors
and associated analysis mechanisms.  The system is in daily use and the
sensors and web service are available for public download \cite{Hackystat}.

\begin{figure}[t]
 {\centerline {\psfig{figure=screenshots.eps}}}
 \caption{{\em Sample screenshots illustrating the current Hackystat
 implementation. Screen A shows the web server home page with links to
 download sensors. Screen B illustrates an email to a developers. Screen C 
shows what data has been collected on any particular day, with green button links 
indicating ``normal'' and red button links indicating ``interesting''
data. Screen D shows a specific log file obtained by clicking on a button.}}
 \label{fig:screenshots}
\end{figure}

Figure \ref{fig:screenshots} illustrates some of the initial features of
the Hackystat implementation.  Developers use the web server to download
and install sensors for a variety of tools, as shown in Screen A.  Once
installed, the sensors send data gathered from tool usage to the server,
which analyzes it and sends email as shown in Screen B back to the
developer whenever analyses indicate important or anomalous trends, but no
more than once a day.  The email contains URLs which can be used to ``drill
down'' into the data repository if the developer so desires. Screen C
provides an overview of the collected data, with button links to individual
log files such as the one shown in Screen D.  A button link is green if the
data inside appears ``normal'', and red if the data inside contains
anomalies or other information that should be brought to the attention of
the developer.

The approach embodied in Hackystat has both evolutionary and revolutionary
aspects. It is evolutionary because its design reflects
our research results in empirical software engineering over the past
ten years.  It is revolutionary because various empirical software
engineering research has concluded that totally automated metrics collection
and analysis can't and/or shouldn't be expected or attempted
\cite{Humphrey95,Fuggetta98,Zelkowitz99}.

We will pursue the goals of the Hackystat project through the following
research components:

\begin{smallenum2}
  
\item 
{\em  Bootstrap and ongoing technology development.}  The
  ``bootstrap'' phase is focused on creating a critical mass of sensors
  and analysis mechanisms to initiate verification, validation, and
  experimentation.  Additional development and refinement of sensors and
  analyses will occur continuously throughout the project.
  
\item 
{\em Verification and validation.}  Verification focuses on assessing
the fidelity of the sensors; in other words, does a sensor that is intended 
to detect ``idle time'' actually detect it with sufficient accuracy to
support related analyses? Validation focuses on assessing the utility of
the analyses: do developers find the analyses to be useful, and do they
actually make changes based upon the feedback they receive?
  
\item 
{\em A comparative study of data collection and analysis in Hackystat
and the PSP}.  The Personal Software Process (PSP) is
a developer-centric, in-process, {\em disruptive} approach to software 
project data collection and analysis.  This case study will explore the 
strengths and weaknesses of disruptive vs. non-disruptive approaches. 

\item 
{\em A case study of automated data collection and analysis for
Extreme Programming.} This case study will explore whether the Hackystat
approach can add value and provide new insight into ``agile''
development methods such as XP.

\item 
{\em A longitudinal study of software development skill maturation.}
  We will deploy Hackystat into both introductory and advanced computer
  science courses at the University of Hawaii.  By the end of the three
  years of the study, we will have gathered in-process software development
  data from students as they progress from introductory to advanced
  programming. This data will be analyzed to provide insights into the 
  development of advanced programmers, with the goal of improving
  educational practice.

\end{smallenum2}  

These components will be described in detail in the research plan, Section
\ref{sec:project-plan}.  The intervening sections describe results from our
previous NSF-funded research and how they motivate the Hackystat project;
discuss influences of other research on this project; and document the
current status of the project.




