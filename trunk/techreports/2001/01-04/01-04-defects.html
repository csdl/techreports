

<head>
<title>Defect Collection and Analysis</title>
</head>

<h2 align="center"><font size="3">Hackystat Design Notes Series<br>
</font><br>
Defect Collection and Analysis in Hackystat</h2>

<p align="center"><a href="http://csdl.ics.hawaii.edu/~cmoore/">Philip Johnson</a><br>

<a href="http://csdl.ics.hawaii.edu/">Collaborative Software Development
Laboratory</a><br>

<a href="http://www.ics.hawaii.edu/">Department of Information and Computer 
Sciences </a><br>

<a href="http://www.hawaii.edu/">
University of Hawaii</a></p>

<p align="center">June 16, 2001<br>
</p>

<p>I've been invited to participate in an online conference of software
engineering researchers to discuss defect analysis. As part of the preparation,
they would like people to answer the following questionnaire. I'd like show you
the questionnaire, and then talk a little bit about my perspective on defect
collection and analysis in Hackystat in order to stimulate your own thinking on
the topic. Please think critically about the questionnaire and my response and
formulate your own opinion and share it with the rest of us.</p>
<p><font face="Courier New" size="2">&gt;
----------------------------------------------------------<br>
&gt; Statement 4: About 80% of the defects come from 20% of the modules<br>
&gt; and about half the modules are defect free.<br>
&gt;<br>
&gt; a. Do you have data that confirms or refutes this statement?<br>
&gt; (Briefly explain)<br>
&gt;<br>
&gt; b. Can you refine the numbers given in this question (e.g., only<br>
&gt; 50% of the defects, 75% defect free)?<br>
&gt;<br>
&gt; c. Can you cite any published papers that directly address this<br>
&gt; question? (Give citations)<br>
&gt;<br>
&gt; d. What are the implications if this statement is true? How should<br>
&gt; we change development practices to address this problem?<br>
&gt; ----------------------------------------------------------<br>
&gt; Statement 5: About 90% of the downtime comes from at most 10% of<br>
&gt; the defects.<br>
&gt;<br>
&gt; a. Do you have data that confirms or refutes this statement?<br>
&gt; (Briefly explain)<br>
&gt;<br>
&gt; b. Can you refine the numbers given in this question (e.g., only<br>
&gt; 50% of the downtime comes from 20% of the defects)?<br>
&gt;<br>
&gt; c. Can you cite any published papers that directly address this<br>
&gt; question? (Give citations)<br>
&gt;<br>
&gt; d. What are the implications if this statement is true? How should<br>
&gt; we change development practices to address this problem?<br>
&gt;<br>
&gt; ----------------------------------------------------------<br>
&gt; Statement 9: All other things being equal, it costs 50% more per<br>
&gt; source instruction to develop high-dependability software products<br>
&gt; than to develop low-dependability software products. However, the<br>
&gt; investment is more than worth it if significant operations and<br>
&gt; maintenance costs are involved.<br>
&gt;<br>
&gt; a. Do you have data that confirms or refutes this statement?<br>
&gt; (Briefly explain)<br>
&gt;<br>
&gt; b. Can you refine the numbers given in this question (e.g., 25%<br>
&gt; more expensive for high-dependability software)?<br>
&gt;<br>
&gt; c. Can you cite any published papers that directly address this<br>
&gt; question? (Give citations)<br>
&gt;<br>
&gt; d. What are the implications if this statement is true? How should<br>
&gt; we change development practices to address this problem?<br>
&gt; ----------------------------------------------------------<br>
&gt; Statement 10: About 40-50% of user programs enter use with<br>
&gt; nontrivial defects.<br>
&gt;<br>
&gt; a. Do you have data that confirms or refutes this statement?<br>
&gt; (Briefly explain)<br>
&gt;<br>
&gt; b. Can you refine the numbers given in this question (e.g., only<br>
&gt; 10% come with non trivial defects)?<br>
&gt;<br>
&gt; c. Can you cite any published papers that directly address this<br>
&gt; question? (Give citations)<br>
&gt;<br>
&gt; d. What are the implications if this statement is true? How should<br>
&gt; we change development practices to address this problem?<br>
&gt; ----------------------------------------------------------</font></p>
<p>OK, to begin with, let me show you some of my reply, which will get at least
one of my biases out on the table :-)</p>
<p>Marv,</p>
<p>I don't have really any interesting data-based comments to make on the
questions (although I have my own strongly held religious feelings like everyone
else which I will spare you).</p>
<p>I do have a huge amount of PSP-based data on defects, but frankly I think PSP-based
defect data (at least when collected from students) is of very suspect quality.
Well, perhaps I should rephrase that as I think _my_ PSP-based defect data is of
very suspect quality. I know from post-class interviews and informal
observations that most students found defect collection the most time-consuming
and disruptive of all the PSP practices, and defect analysis to be the least
insightful and productive of all the PSP practices. (Time/size analysis and
estimation frequently surprised them with counter-intuitive, but relatively
accurate and useful results.) Defect analysis never seemed to reveal anything
new, interesting, or useful to them. When you have something that is both (a)
difficult to do, and (b) not particularly useful, you tend to either (1) not do
it at all, or (2) fake it if the instructor makes you do it anyway.</p>
<p>OK, I guess I didn't spare you from my religious feelings. :-)</p>
<p>Cheers,<br>
Philip<br>
----------------------------------------------------------------</p>
<p>So, the basic question in my mind is, how can we use Hackystat to make an
impact on (a) the quality of defect data that's collected, and (b) the kind of
defect data that's collected? Can we use Hackystat to provide better numbers to
the community? If so, how would those numbers actually be of any use?</p>
<p>Let's start by thinking about the kinds of defect data we could potentially
collect in a totally automated fashion from Hackystat. Here's my initial list:</p>
<p>1. Compiler warnings and errors<br>
2. Run-time errors<br>
3. JUnit testing failures<br>
4. Javadoc warnings and errors<br>
5. Hackytask Change Requests (that are classified as a defect fix)</p>
<p>Can you guys think of any others?</p>
<p>Now let's think about what we could do with each of these kinds of data.</p>
<p><b>1. Compiler warnings and errors.</b></p>
<p>At first glance, it doesn't seem like there's that much we can do. All of us
know that one single defect in the code can spawn a zillion compiler errors, so
I don't think recording each one or even the total number automatically provides
anything useful. It might perhaps be somewhat interesting to record the _first_
compiler error in any run of the compiler, but I think we'd have to do some kind
of experiment to see what this correlates with, and the results might be very
programmer specific. What do you all think?</p>
<p>I had another idea. Instead of recording the actual defects, just record a
_timestamp_ indicating &quot;compiler invoked; errors reported&quot;. Then, when
the compiler runs and there are no errors, record a time stamp indicating
&quot;compiler invoked; no errors&quot;. More specifically, I'm thinking of a
notification type called &quot;compilation&quot; that is sent off each time the
user invokes the compiler. This notification includes a timestamp about when the
compiler was invoked, the set of classes compiled, a boolean indicating whether
or not the compilation was successful, and a field indicating the tool being
used (i.e. JBuilder, Emacs, Ant, etc.)</p>
<p>What you could infer from this data is the _length of time spent removing
compiler defects_. (This assumes that compiler error removal is not interrupted
by other software development activity (of course, it can be interrupted by idle
time). At least in my own experience, I think that once I start running the
compiler and getting compiler defects, that's pretty much all I work on until
the compiler errors are removed.</p>
<p>We can even be a little bit more sophisticated. Once we see that at least one
compiler error has occurred, we could track which files are modified. I think it
is reasonable to infer that in the vast majority of cases the modification of
these files is generally a result of fixing the compiler errors. That can allow
us to infer the _classes_ that contained the compiler errors. Again, we don't
really know what specific errors occurred, just that there appeared to be a
compiler problem in a specific file and therefore a specific class. (Let's not
worry about files with multiple classes in them for now.)</p>
<p>What could we do with this data? Well, for one thing, there might be some
interesting demographic analyses that could be done with respect to student
programmers. We can easily compute the time spent removing compiler errors as a
total of all development time (per day, per week, per project, per file/class).
We can watch how this varies over time. It might be that as students get better,
the time they spend eliminating compiler defects reduces. We could do a study
that compares different development environments---is it the case that (given a
randomized assortment of students) those using JBuilder get rid of their
compiler defects faster than those using Together/J? We could look at the
variation: how much faster do &quot;good&quot; students get rid of compiler
errors than &quot;bad&quot; students on the same assignment?</p>
<p>Looking at professional software development (i.e. a long running development
project of months or years), this data could potentially yield interesting
patterns. I conjecture that large periods of time spent focussing on compiler
defects in a given file reveal the presence of &quot;fresh&quot; code, which
might be more prone to run-time or requirements level errors. (Of course, large
periods of time spent focussing on compiler defects could also reveal the
presence of &quot;fresh&quot; programmers! Measurement dysfunction alert!)</p>
<p><b>2. Run-time errors.</b></p>
<p>Run-time errors are even more problematic than compiler errors, and I'm not
sure what to do with them. We could potentially collect (a) the fact that a
run-time error occurred, (b) the specific top-level error that occurred, and (c)
the stack trace which gives us a set of method invocations, one of which is at
least somewhat likely to be the location of the error (but not necessarily).</p>
<p>First, I don't think we can get a measure of &quot;time spent eliminating the
run-time error&quot; like we can for compiler errors, because (a) it's not clear
that people spend all their development time eliminating a run-time error once
they notice it (some errors occur only rarely and people might decide to just
let them be for a while), and also it's going to be much more difficult to infer
from user actions when the error was removed. (Could we install a microphone
that senses when the user bellows out, &quot;YES!!!&quot; :-)</p>
<p>Similarly, I'm not confident that we can infer that time spent editing a file
following the occurrence of a run-time error gives us much of a clue about where
the run-time error actually is.</p>
<p>However, I do think that tracking the simple occurrence of run-time errors
could be pretty useful when combined with information about invocations of the
system. At the professional development level, let's say a system is invoked 100
times a week without problems, and then all of a sudden it's got all of these
run-time errors. That would indicate that it's become a great deal more brittle.
At the student level, it might be interesting to simply gain insight into the
relative frequency of occurrence of different types of run-time errors. Does
&quot;Null pointer exception&quot; occur more or less frequently depending upon
the project? Depending upon the student? Can we use this data to help teach them
more effectively? Could the TA come to class and say, &quot;Today, we are going
to talk about Null Pointer Exceptions and how to deal with them, because 85% of
you have encountered one for the first time in the past 24 hours.&quot;)</p>
<p>So, to support this, I propose a two new notification types.
&quot;invocation&quot;, which is sent off each time the user invokes the system.
This notification includes the following: (a) the top-level class invoked to get
the system started, (b) the date/time when invocation occurred, (c) the tool
being used (note that I am now realizing that this should be a standard field of
every notification.) We will also need a &quot;run time error&quot;
notification, which consists of (a) top-level class that was invoked, (b)
date/time when error occurred, (c) tool-used, (d) the top-level error name (i.e.
Null Pointer Exception, etc.)<br>
Perhaps these two could be combined, but then I worry about dealing with systems
that run for a long time before crashing. I'd rather send off the
&quot;invocation&quot; notification immediately and get that recorded, then
follow up with the &quot;run time error&quot; one when/if it occurs.</p>
<p><b>3. JUnit testing failures</b></p>
<p>These are relatively easy. We can define a new notification type called
&quot;JUnit invocation&quot;, that includes the time invoked, the name of the
test, the class invoked, whether or not it passed, and if it didn't pass, what
failed.</p>
<p>How do we use these? Here are interesting potential things to try to look at:</p>
<ul>
  <li>The relative frequency of invocation. Are the JUnit tests being invoked
    regularly? Are some developers invoking them more than others? Are there
    periods of time when they're being invoked a lot, followed by a precipitous
    decline?</li>
  <li>The relative frequency of failure. Say that one or more JUnit tests
    suddenly start to fail more frequently than they did in the past. Does that
    reveal brittleness?</li>
  <li>The percentage change in the number of different JUnit tests. Are new
    JUnit tests being added at a regular rate? Does the percentage of JUnit
    tests with respect to the total number of classes stay constant, or is it
    declining, or is it increasing?</li>
</ul>
<p>I just realized that it is not necessarily trivial to detect JUnit invocation
automatically. Either we have to instrument the JUnit Ant task, or instrument
JUnit itself. There are pros and cons to both techniques. Should we do both?</p>
<p><b>4. JavaDoc warnings and errors.</b></p>
<p>The notification here would be &quot;JavaDoc invocation&quot;, and would
specify the timestamp, the tool, the classes that were JavaDoc'd, and the
warnings and errors.</p>
<p>Note that there is a package called DocLint (http://www.znerd.demon.nl/doclint/)
that detects missing JavaDoc comments. We should definitely provide a &quot;doclint&quot;
notification type that allows us to track the number of missing comments, as
well as the percentage of missing comments.</p>
<p>We can do analyses over time to see if (a) JavaDoc is being run regularly,
and (b) that the number of warnings/errors/doclint problems are
increasing/decreasing etc.</p>
<p><b>5. Hackytask Change Requests (that are classified as a defect fix)</b></p>
<p>If the Change Request supports &quot;probable location&quot;, then we can do
some analyses on the brittleness/volatility of different code regions, as well
as the normal longitudinal stuff.</p>
<p><b>SUMMARY</b></p>
<p>Let's look at some general implications of the ideas above:</p>
<p>We've got five distinct &quot;flavors&quot; of defect data (so far): Compiler
defects, Run-time defects, Unit test defects, Documentation defects, and (for
lack of a better word) Requirements defects.</p>
<p>There appears to be real differences between the kinds of analyses that would
be interesting and relevent for students learning to program as opposed to
professionals engaged in longer term development projects. However, both kinds
of analyses are supported by the same kinds of data.</p>
<p>Apart from Hackytask data (which is, interestingly, essentially manual in
nature), defect data that we can collect automatically tells us relatively
little about the _specific_ defect and its _specific_ location.</p>
<p>If we instead think of defect data as a more binary present/absent kind of
information, then Hackystat provides a much richer, fine-grained level of
information than can be provided manually. For example, &quot;percentage time
spent compiling&quot; is completely unfeasible to collect manually but
straightforward with Hackystat.</p>
<p>Next, let's take another look at the questions for the online conference and
what the Hackystat perspective might be:</p>
<p><font face="Courier New" size="2">&gt; Statement 4: About 80% of the defects
come from 20% of the modules<br>
&gt; and about half the modules are defect free.<br>
&gt;<br>
&gt; a. Do you have data that confirms or refutes this statement?</font></p>
<p>First, the question seems ambiguous: what is meant by &quot;defect&quot;?
We've identified five flavors---which do they mean here? Are they saying that
all five flavors occur in an 80/20 ratio?</p>
<p>Second, I would claim that hackystat data can't really be used effectively to
answer this question, because it's just too hard to get at the precise location
reliably and completely for _any_ of the five types of data. (Well, maybe we can
for documentation defects but I don't think that's very interesting.)</p>
<p><font face="Courier New" size="2">&gt; Statement 5: About 90% of the downtime
comes from at most 10% of<br>
&gt; the defects.</font></p>
<p>Hmm. I wonder what they mean by &quot;downtime&quot;? Could we automatically
infer &quot;uptime&quot; as the interval during which the system is passing all
JUnit tests, and &quot;downtime&quot; as the interval between a JUnit test
failing and all tests passing again?</p>
<p>Assuming we could come up with a way to infer that the system is effectively
&quot;up&quot; vs &quot;down&quot;, could we account for the number or
percentage of defects during the down periods? Seems difficult. At least in
Hackystat right now, lots of downtime is due to enhancement, which seems
different in some sense from a defect.</p>
<p><font face="Courier New" size="2">&gt; Statement 9: All other things being
equal, it costs 50% more per<br>
&gt; source instruction to develop high-dependability software products<br>
&gt; than to develop low-dependability software products. However, the<br>
&gt; investment is more than worth it if significant operations and<br>
&gt; maintenance costs are involved.</font></p>
<p>This is something we might be able to actually get some kind of handle on.
First, we can use our time data as a _proxy_ for cost (let's assume there is
some function that can extrapolate from time spent in the IDE to overall project
cost (which includes meetings and other non-IDE activities).</p>
<p>Second, we could either consult an oracle to classify a system as high
dependability, or else we could even infer it ourselves (by recourse to such
things as the number/density of JUnit tests and the trace of defect data over
time).</p>
<p><font face="Courier New" size="2">&gt; Statement 10: About 40-50% of user
programs enter use with<br>
&gt; nontrivial defects.</font></p>
<p>This is another interesting one. Of course, the HackyTask Change request data
gives us some kind of handle on this, though I inherently distrust manual
developer or user-reported data. On the other hand, if we're not talking about
requirements level defects, then all we need to do is somehow indicate/infer
that a particular system is now &quot;in use&quot; and then look at
post-&quot;in use&quot; data.</p>
<p>Going through this exercise leaves me with the following nagging feeling: are
these the right questions to be asking? Maybe the presence of Hackystat will
lead to new and better questions to ask about defects. What would those
questions be?</p>
<p>Let me know what you think about this.<br>
<br>
<br>
</p>
