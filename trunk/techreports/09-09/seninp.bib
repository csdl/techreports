@incollection{citeulike:5128110,
	abstract = {The topic of process mining has attracted the attention of both researchers and tool vendors in the Business Process Management (BPM) space. The goal of process mining is to discover process models from event logs, i.e., events logged by some information system are used to extract information about activities and their causal relations. Several algorithms have been proposed for process mining. Many of these algorithms cannot deal with concurrency. Other typical problems are the presence of duplicate activities, hidden activities, non-free-choice constructs, etc. In addition, real-life logs contain noise (e.g., exceptions or incorrectly logged events) and are typically incomplete (i.e., the event logs contain only a fragment of all possible behaviors). To tackle these problems we propose a completely new approach based on genetic algorithms. As can be expected, a genetic approach is able to deal with noise and incompleteness. However, it is not easy to represent processes properly in a genetic setting. In this paper, we show a genetic process mining approach using the so-called causal matrix as a representation for individuals. We elaborate on the relation between Petri nets and this representation and show that genetic algorithms can be used to discover Petri net models from event logs. Keywords: Process Mining, Petri Nets, Genetic Algorithms, Process Discovery, Business Process Intelligence, Business Activity Monitoring.},
	author = {van der Aalst, W. M. P. and de Medeiros, Alves K. A. and Weijters, A. J. M. M.},
	citeulike-article-id = {5128110},
	citeulike-linkout-0 = {http://dx.doi.org/10.1007/11494744_5},
	citeulike-linkout-1 = {http://www.springerlink.com/content/kue571vhwbm6x05n},
	doi = {10.1007/11494744_5},
	journal = {Applications and Theory of Petri Nets 2005},
	keywords = {proposal},
	pages = {48--69},
	posted-at = {2009-07-12 17:40:46},
	priority = {2},
	title = {Genetic Process Mining},
	url = {http://dx.doi.org/10.1007/11494744_5},
	year = {2005}
}
	

@article{citeulike:5128101,
	abstract = {Contemporary workflow management systems are driven by explicit process models, i.e., a completely specified workflow design is required in order to enact a given workflow process. Creating a workflow design is a complicated time-consuming process and typically, there are discrepancies between the actual workflow processes and the processes as perceived by the management. Therefore, we propose a technique for rediscovering workflow models. This technique uses workflow logs to discover the workflow process as it is actually being executed. The workflow log contains information about events taking place. We assume that these events are totally ordered and each event refers to one task being executed for a single case. This information can easily be extracted from transactional information systems (e.g., Enterprise Resource Planning systems such as SAP and Baan). The rediscovering technique proposed in this paper can deal with noise and can also be used to validate workflow processes by uncovering and measuring the discrepancies between prescriptive models and actual process executions.},
	address = {Amsterdam, The Netherlands, The Netherlands},
	author = {Weijters, A. J. M. M. and van der Aalst, W. M. P.},
	citeulike-article-id = {5128101},
	citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1273325},
	issn = {1069-2509},
	journal = {Integr. Comput.-Aided Eng.},
	keywords = {proposal},
	number = {2},
	pages = {151--162},
	posted-at = {2009-07-12 17:36:56},
	priority = {2},
	publisher = {IOS Press},
	title = {Rediscovering workflow models from event-based data using little thumb},
	url = {http://portal.acm.org/citation.cfm?id=1273325},
	volume = {10},
	year = {2003}
}
	


@incollection{citeulike:5044991,
	abstract = {Modern enterprises increasingly use the workflow paradigm to prescribe how business processes should be performed. Processes are typically modeled as annotated activity graphs. We present an approach for a system that constructs process models from logs of past, unstructured executions of the given process. The graph so produced conforms to the dependencies and past executions present in the log. By providing models that capture the previous executions of the process, this technique allows easier introduction of a workflow system and evaluation and evolution of existing process models. We also present results from applying the algorithm to synthetic data sets as well as process logs obtained from an IBM Flowmark installation.},
	author = {Agrawal, Rakesh and Gunopulos, Dimitrios and Leymann, Frank},
	citeulike-article-id = {5044991},
	citeulike-linkout-0 = {http://dx.doi.org/10.1007/BFb0101003},
	citeulike-linkout-1 = {http://www.springerlink.com/content/0836165x01666l40},
	doi = {10.1007/BFb0101003},
	journal = {Advances in Database Technology — EDBT'98},
	keywords = {white-paper},
	pages = {467--483},
	posted-at = {2009-07-03 12:55:20},
	priority = {2},
	title = {Mining process models from workflow logs},
	url = {http://dx.doi.org/10.1007/BFb0101003},
	year = {1998}
}

	

@article{citeulike:3718014,
	abstract = {Abstract  Process mining includes the automated discovery of processes from event logs. Based on observed events (e.g., activities being executed or messages being exchanged) a process model is constructed. One of the essential problems in process mining is that one cannot assume to have seen all possible behavior. At best, one has seen a representative subset. Therefore, classical synthesis techniques are not suitable as they aim at finding a model that is able to exactly reproduce the log. Existing process mining techniques try to avoid such  ” overfitting” by generalizing the model to allow for more behavior. This generalization is often driven by the representation language and very crude assumptions about completeness. As a result, parts of the model are  ” overfitting” (allow only for what has actually been observed) while other parts may be  ” underfitting” (allow for much more behavior without strong support for it). None of the existing techniques enables the user to control the balance between  ” overfitting” and  ” underfitting”. To address this, we propose a two-step approach. First, using a configurable approach, a transition system is constructed. Then, using the  ” theory of regions”, the model is synthesized. The approach has been implemented in the context of ProM and overcomes many of the limitations of traditional approaches.},
	author = {van der Aalst, W. and Rubin, V. and Verbeek, H. and van Dongen, B. and Kindler, E. and G\"{u}nther, C.},
	citeulike-article-id = {3718014},
	citeulike-linkout-0 = {http://dx.doi.org/10.1007/s10270-008-0106-z},
	citeulike-linkout-1 = {http://www.springerlink.com/content/u43v780550278h4l},
	doi = {10.1007/s10270-008-0106-z},
	journal = {Software and Systems Modeling},
	keywords = {proposal},
	posted-at = {2009-07-11 23:17:06},
	priority = {2},
	title = {Process mining: a two-step approach to balance between underfitting and overfitting},
	url = {http://dx.doi.org/10.1007/s10270-008-0106-z},
	year = {2009}
}


@phdthesis{citeulike:5120757,
	address = {Boulder, CO, USA},
	author = {Cook, Jonathan E.},
	citeulike-article-id = {5120757},
	citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=924798},
	isbn = {0-591-25740-8},
	keywords = {proposal},
	posted-at = {2009-07-11 22:03:30},
	priority = {2},
	publisher = {University of Colorado at Boulder},
	title = {Process discovery and validation through event-data analysis},
	url = {http://portal.acm.org/citation.cfm?id=924798},
	year = {1996}
}

	

@article{citeulike:5120603,
	author = {Biermann, A. W. and Feldman, J.},
	citeulike-article-id = {5120603},
	journal = {IEEE Trans. Computers},
	keywords = {proposal},
	pages = {592--597},
	posted-at = {2009-07-11 21:35:17},
	priority = {2},
	title = {On the Synthesis of Finite-State Machines from Samples of Their Behavior},
	year = {1972}
}

	

@article{citeulike:328044,
	abstract = {Many software process methods and tools presuppose the existence of a formal model of a process. Unfortunately, developing a formal model for an on-going, complex process can be difficult, costly, and error prone. This presents a practical barrier to the adoption of process technologies, which would be lowered by automated assistance in creating formal models. To this end, we have developed a data analysis technique that we term  process discovery.  Under this technique, data describing process events are first captured from an on-going process and then used to generate a formal model of the behavior of that process. In this article we describe a Markov method that we developed specifically for process discovery, as well as describe two additional methods that we   adopted from other domains and augmented for our purposes. The three methods range from the purely algorithmic to the purely statistical. We compare the methods and discuss their application in an industrial case study.},
	address = {New York, NY, USA},
	author = {Cook, Jonathan E. and Wolf, Alexander L.},
	citeulike-article-id = {328044},
	citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=287001},
	citeulike-linkout-1 = {http://dx.doi.org/10.1145/287000.287001},
	doi = {10.1145/287000.287001},
	issn = {1049-331X},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	keywords = {white-paper},
	month = {July},
	number = {3},
	pages = {215--249},
	posted-at = {2009-07-03 13:04:14},
	priority = {2},
	publisher = {ACM},
	title = {Discovering models of software processes from event-based data},
	url = {http://dx.doi.org/10.1145/287000.287001},
	volume = {7},
	year = {1998}
}

	

@inproceedings{citeulike:5112229,
	abstract = {Software is a ubiquitous component of our daily life. We often depend on the correct working of software systems. Due to the difficulty and complexity of software systems, bugs and anomalies are prevalent. Bugs have caused billions of dollars loss, in addition to privacy and security threats. In this work, we address software reliability issues by proposing a novel method to classify software behaviors based on past history or runs. With the technique, it is possible to generalize past known errors and mistakes to capture failures and anomalies. Our technique first mines a set of discriminative features capturing repetitive series of events from program execution traces. It then performs feature selection to select the best features for classification. These features are then used to train a classifier to detect failures. Experiments and case studies on traces of several benchmark software systems and a real-life concurrency bug from MySQL server show the utility of the technique in capturing failures and anomalies. On average, our pattern-based classification technique outperforms the baseline approach by 24.68\% in accuracy.},
	address = {New York, NY, USA},
	author = {Lo, David and Cheng, Hong and Han, Jiawei and Khoo, Siau C. and Sun, Chengnian},
	booktitle = {KDD '09: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining},
	citeulike-article-id = {5112229},
	citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1557019.1557083},
	citeulike-linkout-1 = {http://dx.doi.org/10.1145/1557019.1557083},
	doi = {10.1145/1557019.1557083},
	isbn = {978-1-60558-495-9},
	keywords = {proposal},
	location = {Paris, France},
	pages = {557--566},
	posted-at = {2009-07-11 00:34:50},
	priority = {2},
	publisher = {ACM},
	title = {Classification of software behaviors for failure detection: a discriminative pattern mining approach},
	url = {http://dx.doi.org/10.1145/1557019.1557083},
	year = {2009}
}

	
@techreport{citeulike:5090131,
	address = {Pittsburgh, PA 15213},
	author = {Pomeroy-Huff, Marsha and Mullaney, Julia and Cannon, Robert and Seburn, Mark},
	citeulike-article-id = {5090131},
	editor = {Humphrey, Watts S.},
	institution = {Software Engineering Institute},
	keywords = {proposal},
	organization = {Carnegie Mellon University},
	posted-at = {2009-07-07 22:57:28},
	priority = {2},
	title = {The Personal Software Process (PSP)
Body of Knowledge, Version 1.0},
	year = {2008}
}

	
@misc{Sayyad-Shirabad+Menzies:2005,
	author = {Shirabad, J. Sayyad and Menzies, T. J.},
	citeulike-article-id = {5070504},
	howpublished = {School of Information Technology and Engineering, University of Ottawa, Canada},
	keywords = {proposal, white-paper},
	posted-at = {2009-07-06 03:15:39},
	priority = {2},
	title = {{The {PROMISE} Repository of Software Engineering Databases.}},
	url = {http://promise.site.uottawa.ca/SERepository},
	year = {2005}
}

	

@inproceedings{citeulike:5043676,
	abstract = {Software repositories such as source control systems, defect tracking systems, and archived communications between project personnel are used to help manage the progress of software projects. Software practitioners and researchers more and more recognize the potential benefit of mining this information to support the maintenance of software systems, improve software design/reuse, and empirically validate novel ideas and techniques. Research is now proceeding to uncover the ways in which mining these repositories can help to understand software development, to support predictions about software development, and to plan various evolutionary aspects of software projects.},
	author = {Gall, Harald and Lanza, Michele and Zimmermann, Thomas},
	booktitle = {Software Engineering - Companion, 2007. ICSE 2007 Companion. 29th International Conference on},
	citeulike-article-id = {5043676},
	doi = {10.1109/ICSECOMPANION.2007.8},
	journal = {Software Engineering - Companion, 2007. ICSE 2007 Companion. 29th International Conference on},
	keywords = {white-paper},
	pages = {107--108},
	posted-at = {2009-07-03 02:09:42},
	priority = {2},
	title = {4th International Workshop on Mining Software Repositories ({MSR} 2007)},
	url = {http://dx.doi.org/10.1109/ICSECOMPANION.2007.8},
	year = {2007}
}

	

@incollection{citeulike:5043673,
	abstract = {Under the umbrella of buzzwords such as  ” Business Activity Monitoring” (BAM) and  ” Business Process Intelligence” (BPI) both academic (e.g., EMiT, Little Thumb, InWoLvE, Process Miner, and MinSoN) and commercial tools (e.g., ARIS PPM, HP BPI, and ILOG JViews) have been developed. The goal of these tools is to extract knowledge from event logs (e.g., transaction logs in an ERP system or audit trails in a WFM system), i.e., to do process mining. Unfortunately, tools use different formats for reading/storing log files and present their results in different ways. This makes it difficult to use different tools on the same data set and to compare the mining results. Furthermore, some of these tools implement concepts that can be very useful in the other tools but it is often difficult to combine tools. As a result, researchers working on new process mining techniques are forced to build a mining infrastructure from scratch or test their techniques in an isolated way, disconnected from any practical applications. To overcome these kind of problems, we have developed the ProM framework, i.e., an  ” pluggable” environment for process mining. The framework is flexible with respect to the input and output format, and is also open enough to allow for the easy reuse of code during the implementation of new process mining ideas. This paper introduces the ProM framework and gives an overview of the plug-ins that have been developed.},
	author = {van Dongen, B. F. and de Medeiros, A. K. A. and Verbeek, H. M. W. and Weijters, A. J. M. M. and van der Aalst, W. M. P.},
	citeulike-article-id = {5043673},
	doi = {10.1007/11494744\_25},
	journal = {Applications and Theory of Petri Nets 2005},
	keywords = {white-paper},
	pages = {444--454},
	posted-at = {2009-07-03 01:44:31},
	priority = {2},
	title = {The {ProM} Framework: A New Era in Process Mining Tool Support},
	url = {http://dx.doi.org/10.1007/11494744\_25},
	year = {2005}
}



@incollection{citeulike:1885717,
	abstract = {Software development processes are often not explicitly modelled and sometimes even chaotic. In order to keep track of the involved documents and files, engineers use Software Configuration Management (SCM) systems. Along the way, those systems collect and store information on the software process itself. Thus, SCM information can be used for constructing explicit process models, which is called software process mining. In this paper we show that (1) a Process Mining Framework can be used for obtaining software process models as well as for analysing and optimising them; (2) an algorithmic approach, which arose from our research on software processes, is integrated in the framework.},
	author = {Rubin, Vladimir and G\"{u}nther, Christian and van der Aalst, Wil and Kindler, Ekkart and van Dongen, Boudewijn and Sch\"{a}fer, Wilhelm},
	citeulike-article-id = {1885717},
	doi = {10.1007/978-3-540-72426-1\_15},
	journal = {Software Process Dynamics and Agility},
	keywords = {white-paper},
	pages = {169--181},
	posted-at = {2009-07-03 01:42:24},
	priority = {2},
	title = {Process Mining Framework for Software Processes},
	url = {http://dx.doi.org/10.1007/978-3-540-72426-1\_15},
	year = {2007}
}



@article{citeulike:5043672,
	abstract = {The paper discusses the problems that a software development organization must address in order to assess and improve its software processes. In particular, the authors are involved in a project aiming at assessing and improving the current practice and the quality manual of the Business Unit Telecommunications for Defense (BUTD) of a large telecommunications company. The paper reports on the usage of formal process modeling languages to detect inconsistencies, ambiguities, incompleteness, and opportunities for improvement of both the software process and its documentation},
	author = {Bandinelli, S. and Fuggetta, A. and Lavazza, L. and Loi, M. and Picco, G. P.},
	booktitle = {Software Engineering, IEEE Transactions on},
	citeulike-article-id = {5043672},
	doi = {10.1109/32.387473},
	journal = {Software Engineering, IEEE Transactions on},
	keywords = {white-paper},
	number = {5},
	pages = {440--454},
	posted-at = {2009-07-03 01:38:26},
	priority = {2},
	title = {Modeling and improving an industrial software process},
	url = {http://dx.doi.org/10.1109/32.387473},
	volume = {21},
	year = {1995}
}



@incollection{citeulike:5043670,
	abstract = {Capturing a process as it is being executed in a descriptive process model is a key activity in process improvement. Performing descriptive process modeling in industry environments is hindered by factors such as dispersed process knowledge or inconsistent understanding of the process among different project members. A systematic approach can alleviate some of the problems. This paper sketches fundamental difficulties in gaining process knowledge and describes a systematic approach to process elicitation. The approach employs techniques from other domains like social sciences that have been tailored to the process elicitation context and places them in a decision framework that gives guidance on selecting appropriate techniques in specific modeling situations. Initial experience with the approach is reported.},
	author = {Becker-Kornstaedt, Ulrike},
	citeulike-article-id = {5043670},
	doi = {10.1007/3-540-44813-6\_27},
	journal = {Product Focused Software Process Improvement},
	keywords = {white-paper},
	pages = {312--325},
	posted-at = {2009-07-03 01:36:12},
	priority = {2},
	title = {Towards Systematic Knowledge Elicitation for Descriptive Software Process Modeling},
	url = {http://dx.doi.org/10.1007/3-540-44813-6\_27},
	year = {2001}
}



@incollection{citeulike:5043664,
	abstract = {This paper describes a reference model for open source software (OSS) processes and its application towards discovering such processes from OSS project artifacts. This reference model is the means to map evidence of an enacted process to a classification of agents, resources, tools, and activities that characterize the process.},
	author = {Jensen, Chris and Scacchi, Walt},
	citeulike-article-id = {5043664},
	doi = {10.1007/978-0-387-72486-7\_26},
	journal = {Open Source Development, Adoption and Innovation},
	keywords = {white-paper},
	pages = {265--270},
	posted-at = {2009-07-03 01:18:35},
	priority = {2},
	title = {Guiding the Discovery of Open Source Software Processes with a Reference Model},
	url = {http://dx.doi.org/10.1007/978-0-387-72486-7\_26},
	year = {2007}
}



@misc{citeulike:5043104,
	abstract = {Contents 1 Motivation and Basic Ideas 7  1.1 Software Market Sizes, Software Crisis ....... 8  1.2 Where are we in Software Process Improvement (SPI)? . . . ....................... 10  1.3 Six Approaches to SPI ................. 11  1.4 Some reflections on SPI................ 12 2 Total Quality Management (TQM) 13 3 Capability Maturity Model (CMM) 16  3.1 CMM survey ...................... 16  3.2 Extended CMM levels ................. 17  3.3 CMM, level details . .................. 18  3.4 CMM Process Assessments . ............. 27  3.5 Software Engineering Process Groups, SEPGs . . . 28  3.6 The Personal Software Process, PSP ........ 30  3.7 Further work around CMM at SEI . . . ........ 32 4 Quality Improvement Paradigm (QIP) 33  4.1 QIP survey ....................... 33  4.2 Typical goals of process improvement ........ 35  4.3 NASA Software Engineering Laboratory, NASA--SEL 37  4.4 Software Experience Factory, consolidated NASA-SEL 38 5 SPICE, ISO-standardization 40 6 European BOOTS},
	author = {Conradi, Reidar},
	citeulike-article-id = {5043104},
	keywords = {white-paper},
	posted-at = {2009-07-02 18:41:32},
	priority = {2},
	title = {{SPI} frameworks: {TQM}, {CMM}, {SPICE}, {ISO} 9001, {QIP} Experiences and trends - Norwegian {SPIQ} project},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.12.3010}
}


@article{citeulike:5043101,
	abstract = {Mining knowledge about ordering from sequence data is an important problem with many applications, such as bioinformatics, Web mining, network management, and intrusion detection. For example, if many customers follow a partial order in their purchases of a series of products, the partial order can be used to predict other related customers' future purchases and develop marketing campaigns. Moreover, some biological sequences (e.g., microarray data) can be clustered based on the partial orders shared by the sequences. Given a set of items, a total order of a subset of items can be represented as a string. A string database is a multiset of strings. In this paper, we identify a novel problem of mining frequent closed partial orders from strings. Frequent closed partial orders capture the nonredundant and interesting ordering information from string databases. Importantly, mining frequent closed partial orders can discover meaningful knowledge that cannot be disclosed by previous data mining techniques. However, the problem of mining frequent closed partial orders is challenging. To tackle the problem, we develop Frecpo (for frequent closed partial order), a practically efficient algorithm for mining the complete set of frequent closed partial orders from large string databases. Several interesting pruning techniques are devised to speed up the search. We report an extensive performance study on both real data sets and synthetic data sets to illustrate the effectiveness and the efficiency of our approach},
	author = {Pei, J. and Wang, H. and Liu, J. and Wang, K. and Wang, Jianyong and Yu, P. S.},
	booktitle = {Knowledge and Data Engineering, IEEE Transactions on},
	citeulike-article-id = {5043101},
	doi = {10.1109/TKDE.2006.172},
	journal = {Knowledge and Data Engineering, IEEE Transactions on},
	keywords = {white-paper},
	number = {11},
	pages = {1467--1481},
	posted-at = {2009-07-02 18:36:57},
	priority = {2},
	title = {Discovering Frequent Closed Partial Orders from Strings},
	url = {http://dx.doi.org/10.1109/TKDE.2006.172},
	volume = {18},
	year = {2006}
}



@misc{citeulike:5043099,
	abstract = {Sequences of events describing the behavior and actions of users or systems can be collected in several domains. An episode is a collection of events that occur relatively close to each other in a given partial order. We consider the problem of discovering frequently occurring episodes in a sequence. Once such episodes are known, one can produce rules for describing or predicting the behavior of the sequence. We give efficient algorithms for the discovery of all frequent episodes from a given class of episodes, and present detailed experimental results. The methods are in use in telecommunication alarm management.},
	author = {Mannila, Heikki and Toivonen, Hannu and Verkamo, A. Inkeri},
	citeulike-article-id = {5043099},
	keywords = {white-paper},
	posted-at = {2009-07-02 18:34:20},
	priority = {2},
	title = {Discovery of Frequent Episodes in Event Sequences},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.3051},
	year = {1997}
}



@incollection{citeulike:5043097,
	abstract = {Sequential pattern mining is an important data mining problem with broad applications. However, it is also a challenging problem since the mining may have to generate or examine a combinatorially explosive number of intermediate subsequences. Recent studies have developed two major classes of sequential pattern mining methods: (1) a candidate generation-and-test approach, represented by (i)GSP [30], a horizontal format-based sequential pattern mining method, and (ii) SPADE [36], a vertical format-based method; and (2) a sequential pattern growth method, represented by PrefixSpan [26] and its further extensions, such as CloSpan for mining closed sequential patterns [35].},
	author = {Han, J. and Pei, J. and Yan, X.},
	citeulike-article-id = {5043097},
	doi = {10.1007/11362197\_8},
	journal = {Foundations and Advances in Data Mining},
	keywords = {white-paper},
	pages = {183--220},
	posted-at = {2009-07-02 18:32:10},
	priority = {2},
	title = {Sequential Pattern Mining by Pattern-Growth: Principles and Extensions*},
	url = {http://dx.doi.org/10.1007/11362197\_8},
	year = {2005}
}



@inproceedings{citeulike:5043094,
	abstract = {Abstract. The Unification-based Temporal Grammar is a temporal extension of static unification-based grammars. It defines a hierarchical temporal rule language to express complex patterns present in multivariate time series. The Temporal Data Mining Method is the accompanying framework to discover temporal knowledge based on this rule language. A semiotic hierarchy of temporal patterns, which are not a priori given, is build in a bottom up manner from static logical descriptions of multivariate time instants. We demonstrate the methods using music data, extracting typical parts of songs. 1},
	author = {M\"{o}rchen, Fabian and Ultsch, Alfred},
	booktitle = {Proceedings of the 27th Annual German Conference in Artificial Intelligence (KI'04},
	citeulike-article-id = {5043094},
	keywords = {white-paper},
	pages = {127--140},
	posted-at = {2009-07-02 18:24:06},
	priority = {2},
	title = {Mining hierarchical temporal patterns in multivariate time series},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.88.603},
	year = {2004}
}



@misc{citeulike:5043086,
	abstract = {Rule mining is the practice of discovering interesting and unexpected rules from large data sets. Depending on the exact problem formulation, this may be a very complicated problem. Existing methods typically make strong simplifying assumptions about the form of the rules, and limit the measure of rule quality to simple properties, such as confidence. Because confidence in itself is not a good indicator of how interesting a rule is to the user, the mined rules are typically sorted according to some secondary interestingness measure. In this paper we present a rule mining method that is based on genetic programming. Because we use specialized pattern matching hardware to evaluate each rule, our method supports a very wide range of rule formats, and can use any reasonable fitness measure. We develop a fitness measure that is well-suited for our method, and give empirical results of applying the method to synthetic and real-world data sets.},
	author = {S{\ae}trom, P\r{a}l and Hetland, Magnus L.},
	citeulike-article-id = {5043086},
	keywords = {white-paper},
	posted-at = {2009-07-02 18:16:42},
	priority = {2},
	title = {Unsupervised Temporal Rule Mining with Genetic Programming and Specialized Hardware},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.3417},
	year = {2003}
}



@article{citeulike:5042856,
	abstract = {Most of the software in regular use in businesses and organisations all over the world cannot be completely specified. It cannot be implemented, once and for all. Both the original implementation and the inevitable subsequent evolution (maintenance) are a continual learning experience driven, inter alia, by feedback from the results of the behaviour under execution of the software, as perceived by various stakeholders, by advances and growth in the user organisations and by adaptation to changes in the external world, both independent and as a result of installation and use of the software. Real world, termed type-E, software is essentially evolutionary in nature. The study of the processes of evolution of such software is of considerable interest, as is that of the domains that co-evolve with the software. After briefly discussing the meaning of the term evolution in the context of software, its technology, the software process and related domains, this paper describes some of the facets of the evolution phenomenon and implications to the evolution process as identified during many years of active interest in the topic.},
	author = {Lehman, Meir M. and Ramil, Juan F.},
	citeulike-article-id = {5042856},
	doi = {10.1023/A:1020557525901},
	journal = {Annals of Software Engineering},
	keywords = {white-paper},
	month = {December},
	number = {1},
	pages = {275--309},
	posted-at = {2009-07-02 15:36:19},
	priority = {2},
	title = {Software Evolution and Software Evolution Processes},
	url = {http://dx.doi.org/10.1023/A:1020557525901},
	volume = {14},
	year = {2002}
}



@inproceedings{citeulike:5029482,
	abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article.  ACM has opted to expose the complete List rather than only correct and linked references.},
	address = {New York, NY, USA},
	author = {Humphrey, Watts S.},
	booktitle = {ESP '97: Papers presented at the seventh workshop on Empirical studies of programmers},
	citeulike-article-id = {5029482},
	doi = {10.1145/266399.266418},
	isbn = {0-89791-992-0},
	keywords = {white-paper},
	location = {Alexandria, Virginia, United States},
	pages = {224--232},
	posted-at = {2009-07-01 05:18:32},
	priority = {2},
	publisher = {ACM},
	title = {What do we know about programming?},
	url = {http://dx.doi.org/10.1145/266399.266418},
	year = {1997}
}



@techreport{citeulike:5012661,
	address = {Finland},
	author = {Vilo, J.},
	citeulike-article-id = {5012661},
	institution = {University of Helsinki},
	keywords = {proposal},
	posted-at = {2009-06-29 14:40:15},
	priority = {2},
	school = {Department of Computer Science},
	title = {Discovering frequent patterns from strings.},
	year = {1998}
}

	

@inproceedings{citeulike:775528,
	abstract = {We are given a large database of customer transactions, where each transaction consists of customer-id, transaction time, and the items bought in the transaction. We introduce the problem of mining sequential patterns over such databases. We present three algorithms to solve this problem, and empirically evaluate their performance using synthetic data. Two of the proposed algorithms, AprioriSome and AprioriAll, have comparable performance, albeit AprioriSome performs a little better when the minimum number of customers that must support a sequential pattern is low. Scale-up experiments show that both AprioriSome and AprioriAll scale linearly with the number of customer transactions. They also have excellent scale-up properties with respect to the number of transactions per customer and the number of items in a transaction},
	author = {Agrawal, R. and Srikant, R.},
	citeulike-article-id = {775528},
	doi = {10.1109/ICDE.1995.380415},
	journal = {Data Engineering, 1995. Proceedings of the Eleventh International Conference on},
	pages = {3--14},
	posted-at = {2009-06-29 13:48:02},
	priority = {2},
	title = {Mining sequential patterns},
	url = {http://dx.doi.org/10.1109/ICDE.1995.380415},
	year = {1995}
}

	

@article{citeulike:707616,
	abstract = {An on-line algorithm is presented for constructing the suffix tree for a given string in time linear in the length of the string. The new algorithm has the desirable property of processing the string symbol by symbol from left to right. It has always the suffix tree for the scanned part of the string ready. The method is developed as a linear-time version of a very simple algorithm for (quadratic size) suffix tries. Regardless of its quadratic worst-case this latter algorithm can be a good...},
	author = {Ukkonen, Esko},
	citeulike-article-id = {707616},
	journal = {Algorithmica},
	keywords = {proposal},
	number = {3},
	pages = {249--260},
	posted-at = {2009-06-29 12:35:04},
	priority = {2},
	title = {On-Line Construction of Suffix Trees},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.751},
	volume = {14},
	year = {1995}
}


	
@inproceedings{citeulike:3978085,
	abstract = {Time series motifs are approximately repeated patterns foundwithin the data. Such motifs have utility for many data mining algorithms, including rule-discovery,novelty-detection, summarization and clustering. Since the formalization of the problem and the introduction of efficient linear time algorithms, motif discovery has been successfully applied tomany domains, including medicine, motion capture, robotics and meteorology.},
	address = {New York, NY, USA},
	author = {Yankov, Dragomir and Keogh, Eamonn and Medina, Jose and Chiu, Bill and Zordan, Victor},
	booktitle = {KDD '07: Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining},
	citeulike-article-id = {3978085},
	doi = {10.1145/1281192.1281282},
	isbn = {978-1-59593-609-7},
	keywords = {proposal},
	location = {San Jose, California, USA},
	pages = {844--853},
	posted-at = {2009-06-29 02:13:44},
	priority = {2},
	publisher = {ACM},
	title = {Detecting time series motifs under uniform scaling},
	url = {http://dx.doi.org/10.1145/1281192.1281282},
	year = {2007}
}



@inproceedings{citeulike:3025877,
	address = {New York, NY, USA},
	author = {Keogh, Eamonn and Lonardi, Stefano and Chiu, Bill '.},
	booktitle = {KDD '02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
	citeulike-article-id = {3025877},
	doi = {10.1145/775047.775128},
	isbn = {158113567X},
	keywords = {proposal},
	location = {Edmonton, Alberta, Canada},
	pages = {550--556},
	posted-at = {2009-06-28 23:06:23},
	priority = {2},
	publisher = {ACM},
	title = {Finding surprising patterns in a time series database in linear time and space},
	url = {http://dx.doi.org/10.1145/775047.775128},
	year = {2002}
}



@incollection{citeulike:5003404,
	abstract = {We consider the problem of finding frequent subsequences in sequential data. We examine three algorithms using a trie with K levels. The O(K 2 n) breadth-first (BF) algorithm inserts a pattern into the trie at level k only if level k-1 has been completed. The O(Kn) depth-first (DF) algorithm inserts a pattern and all its prefixes into the trie before examining another pattern. A threshold is used to store only frequent subsequences. Since DF cannot apply the threshold until the trie is complete, it makes poor use of memory. The heuristic depth-first (HDF) algorithm, a variant of DF, uses the threshold in the same manner as BF. HDF gains efficiency but loses a predictable amount of accuracy.},
	author = {Jiang, Linhui and Hamilton, Howard},
	citeulike-article-id = {5003404},
	doi = {10.1007/3-540-44886-1\_38},
	journal = {Advances in Artificial Intelligence},
	keywords = {proposal},
	pages = {992},
	posted-at = {2009-06-28 23:01:07},
	priority = {2},
	title = {Methods for Mining Frequent Sequential Patterns},
	url = {http://dx.doi.org/10.1007/3-540-44886-1\_38},
	year = {2003}
}



@incollection{citeulike:5003338,
	abstract = {In the last years, the completion of the human genome sequencing showed up a wide range of new challenging issues involving raw data analysis. In particular, the discovery of information implicitly encoded in biological sequences is assuming a prominent role in identifying genetic diseases and in deciphering biological mechanisms. This information is usually represented by patterns frequently occurring in the sequences. Because of biological observations, a specific class of patterns is becoming particularly interesting: frequent structured patterns. In this respect, it is biologically meaningful to look at both  ” exact” and  ” approximate” repetitions of the patterns within the available sequences. This paper gives a contribution in this setting by providing some algorithms which allow to discover frequent structured patterns, either in  ” exact” or  ” approximate” form, present in a collection of input biological sequences.},
	author = {Palopoli, Luigi and Terracina, Giorgio},
	citeulike-article-id = {5003338},
	doi = {10.1007/3-540-36182-0\_6},
	journal = {Discovery Science},
	keywords = {proposal},
	pages = {283--296},
	posted-at = {2009-06-28 22:52:07},
	priority = {2},
	title = {Discovering Frequent Structured Patterns from String Databases: An Application to Biological Sequences},
	url = {http://dx.doi.org/10.1007/3-540-36182-0\_6},
	year = {2008}
}


@book{citeulike:465665,
	abstract = {{This introductory text offers a clear exposition of the algorithmic principles driving advances in bioinformatics. Accessible to students in both biology and computer science, it strikes a unique balance between rigorous mathematics and practical techniques, emphasizing the ideas underlying algorithms rather than offering a collection of apparently unrelated problems.<br /> <br /> The book introduces biological and algorithmic ideas together, linking issues in computer science to biology and thus capturing the interest of students in both subjects. It demonstrates that relatively few design techniques can be used to solve a large number of practical problems in biology, and presents this material intuitively.<br /> <br /> <i>An Introduction to Bioinformatics Algorithms</i> is one of the first books on bioinformatics that can be used by students at an undergraduate level. It includes a dual table of contents, organized by algorithmic idea and biological idea; discussions of biologically relevant problems, including a detailed problem formulation and one or more solutions for each; and brief biographical sketches of leading figures in the field. These interesting vignettes offer students a glimpse of the inspirations and motivations for real work in bioinformatics, making the concepts presented in the text more concrete and the techniques more approachable.<br /> <br /> PowerPoint presentations, practical bioinformatics problems, sample code, diagrams, demonstrations, and other materials can be found at the Author's website.}},
	author = {Jones, Neil C. and Pevzner, Pavel A.},
	citeulike-article-id = {465665},
	howpublished = {Hardcover},
	isbn = {0262101068},
	month = {August},
	posted-at = {2009-06-28 22:34:44},
	priority = {2},
	publisher = {{The MIT Press}},
	title = {An Introduction to Bioinformatics Algorithms (Computational Molecular Biology)},
	url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20&path=ASIN/0262101068},
	year = {2004}
}


@incollection{citeulike:3978065,
	abstract = {The Unification-Based Temporal Grammar is a temporal extension of static unification-based grammars. It defines a hierarchical temporal rule language to express complex patterns present in multivariate time series. The Temporal Data Mining Method is the accompanying framework to discover temporal knowledge based on this rule language. A semiotic hierarchy of temporal patterns, which are not a priori given, is built in a bottom up manner from static logical descriptions of multivariate time instants. We demonstrate the methods using music data, extracting typical parts of songs.},
	address = {Berlin / Heidelberg, Germany},
	author = {M\"{o}rchen, Fabian and Ultsch, Alfred},
	booktitle = {KI 2004: Advances in Artificial Intelligence},
	citeulike-article-id = {3978065},
	doi = {10.1007/b100351},
	editor = {Biundo, Susanne and Fr\"{u}hwirth, Thom and Palm, G\"{u}nther},
	isbn = {978-3-540-23166-0},
	issn = {0302-9743 (Print) 1611-3349 (Online)},
	journal = {KI 2004: Advances in Artificial Intelligence},
	keywords = {proposal},
	pages = {127--140},
	posted-at = {2009-06-28 17:36:05},
	priority = {2},
	publisher = {Springer},
	series = {Lecture Notes in Artificial Intelligence},
	title = {Mining Hierarchical Temporal Patterns in Multivariate Time Series},
	url = {http://dx.doi.org/10.1007/b100351},
	volume = {3238},
	year = {2004}
}


@article{citeulike:3978076,
	abstract = {Abstract\&nbsp;\&nbsp;We present a new method for the understandable description of local temporal relationships in multivariate data, called Time Series Knowledge Mining (TSKM). We define the Time Series Knowledge Representation (TSKR) as a new language for expressing temporal knowledge in time interval data. The patterns have a hierarchical structure, with levels corresponding to the temporal concepts duration, coincidence, and partial order. The patterns are very compact, but offer details for each element on demand. In comparison with related approaches, the TSKR is shown to have advantages in robustness, expressivity, and comprehensibility. The search for coincidence and partial order in interval data can be formulated as instances of the well known frequent itemset problem. Efficient algorithms for the discovery of the patterns are adapted accordingly. A novel form of search space pruning effectively reduces the size of the mining result to ease interpretation and speed up the algorithms. Human interaction is used during the mining to analyze and validate partial results as early as possible and guide further processing steps. The efficacy of the methods is demonstrated using two real life data sets. In an application to sports medicine the results were recognized as valid and useful by an expert of the field.},
	author = {M\"{o}rchen, Fabian and Ultsch, Alfred},
	citeulike-article-id = {3978076},
	doi = {10.1007/s10618-007-0070-1},
	journal = {Data Mining and Knowledge Discovery},
	keywords = {proposal},
	month = {October},
	number = {2},
	pages = {181--215},
	posted-at = {2009-06-28 17:34:38},
	priority = {2},
	title = {Efficient mining of understandable patterns from multivariate interval time series},
	url = {http://dx.doi.org/10.1007/s10618-007-0070-1},
	volume = {15},
	year = {2007}
}



@inproceedings{citeulike:5000906,
	author = {Vilain, M.},
	booktitle = {2nd National (US) Conference on Artificial Intelligence},
	citeulike-article-id = {5000906},
	keywords = {proposal},
	location = {Pittsburgh, Pa.},
	pages = {197--201},
	posted-at = {2009-06-28 17:13:46},
	priority = {2},
	publisher = {AAAI Press},
	title = {A system for reasoning about time.},
	year = {1982}
}



@article{citeulike:5000870,
	abstract = {The temporal interval relationships formalized by Allen, and later extended to accommodate semiintervals by Freksa, have been widely utilized in both data modeling and artificial intelligence research to facilitate reasoning between the relative temporal ordering of events. In practice, however, some modifications to the relationships are necessary when linear temporal sequences are provided, when event times are aggregated, or when data is supplied to a granularity which is larger than required. This paper discusses these modifications and outlines a solution to this problem which accommodates any available knowledge of interval midpoints.},
	author = {Roddick, J. F. and Mooney, C. H.},
	booktitle = {Knowledge and Data Engineering, IEEE Transactions on},
	citeulike-article-id = {5000870},
	doi = {10.1109/TKDE.2005.12},
	journal = {Knowledge and Data Engineering, IEEE Transactions on},
	keywords = {proposal},
	number = {1},
	pages = {133--135},
	posted-at = {2009-06-28 17:08:08},
	priority = {2},
	title = {Linear temporal sequences and their interpretation using midpoint relationships},
	url = {http://dx.doi.org/10.1109/TKDE.2005.12},
	volume = {17},
	year = {2005}
}



@inproceedings{citeulike:5000685,
	address = {London, UK},
	author = {Rainsford, Chris P. and Roddick, John F.},
	booktitle = {PKDD '99: Proceedings of the Third European Conference on Principles of Data Mining and Knowledge Discovery},
	citeulike-article-id = {5000685},
	isbn = {3-540-66490-4},
	pages = {504--509},
	posted-at = {2009-06-28 16:37:29},
	priority = {2},
	publisher = {Springer-Verlag},
	title = {Adding Temporal Semantics to Association Rules},
	url = {http://portal.acm.org/citation.cfm?id=669526},
	year = {1999}
}



@incollection{citeulike:4994404,
	abstract = {The class hierarchy is an important aspect of object-oriented software development. Design and maintenance of such a hierarchy is a difficult task that is often accomplished without any clear guidance or tool support. Formal concept analysis provides a natural theoretical framework for this problem because it can guarantee maximal factorization while preserving specialization relationships. The framework can be useful for several software development scenarios within the class hierarchy life-cycle such as design from scratch using a set of class specifications, or a set of object examples, refactoring/reengineering from existing object code or from the observation of the actual use of the classes in applications and hierarchy evolution by incrementally adding new classes. The framework can take into account different levels of specification details and suggests a number of well-defined alternative designs. These alternatives can be viewed as normal forms for class hierarchies where each normal form addresses particular design goals. An overview of work in the area is presented by highlighting the formal concept analysis notions that are involved. One particularly difficult problem arises when taking associations between classes into account. Basic scaling has to be extended because the scales used for building the concept lattice are dependent on it. An approach is needed to treat this circularity in a well-defined manner. Possible solutions are discussed.},
	author = {Godin, Robert   and Valtchev, Petko  },
	citeulike-article-id = {4994404},
	doi = {10.1007/11528784\_16},
	journal = {Formal Concept Analysis},
	keywords = {proposal},
	pages = {304--323},
	posted-at = {2009-06-28 04:38:03},
	priority = {2},
	title = {Formal Concept Analysis-Based Class Hierarchy Design in Object-Oriented Software Development},
	url = {http://dx.doi.org/10.1007/11528784\_16},
	year = {2005}
}



@book{citeulike:257416,
	author = {Hesse, Wolfgang   and Tilley, Thomas  },
	citeulike-article-id = {257416},
	doi = {10.1007/11528784\_15},
	journal = {Lecture Notes in Computer Science},
	keywords = {proposal},
	month = {July},
	pages = {288--303},
	posted-at = {2009-06-28 04:35:44},
	priority = {2},
	title = {Formal Concept Analysis Used for Software Analysis and Modelling},
	url = {http://dx.doi.org/10.1007/11528784\_15},
	volume = {3626},
	year = {2005}
}



@incollection{citeulike:1378634,
	abstract = {Formal Concept Analysis (FCA) has typically been applied in the field of software engineering to support software maintenance and object-oriented class identification tasks. This paper presents a broader overview by describing and classifying academic papers that report the application of FCA to software engineering. The papers are classified using a framework based on the activities defined in the ISO12207 Software Engineering standard. Two alternate classification schemes based on the programming language under analysis and target application size are also discussed. In addition, the authors work to support agile methods and formal specification via FCA is introduced.},
	author = {Tilley, Thomas   and Cole, Richard   and Becker, Peter   and Eklund, Peter  },
	citeulike-article-id = {1378634},
	doi = {10.1007/11528784\_13},
	journal = {Formal Concept Analysis},
	keywords = {proposal},
	pages = {250--271},
	posted-at = {2009-06-28 04:34:12},
	priority = {2},
	title = {A Survey of Formal Concept Analysis Support for Software Engineering Activities},
	url = {http://dx.doi.org/10.1007/11528784\_13},
	year = {2005}
}



@article{citeulike:272197,
	abstract = {We propose the time interval multimedia event (TIME) framework as a robust approach for classification of semantic events in multimodal video documents. The representation used in TIME extends the Allen temporal interval relations and allows for proper inclusion of context and synchronization of the heterogeneous information sources involved in multimodal video analysis. To demonstrate the viability of our approach, it was evaluated on the domains of soccer and news broadcasts. For automatic classification of semantic events, we compare three different machine learning techniques, i.c. C4.5 decision tree, maximum entropy, and support vector machine. The results show that semantic video indexing results significantly benefit from using the TIME framework.},
	author = {Snoek, C. G. M.  and Worring, M. },
	citeulike-article-id = {272197},
	doi = {10.1109/TMM.2005.850966},
	journal = {Multimedia, IEEE Transactions on},
	keywords = {proposal},
	number = {4},
	pages = {638--647},
	posted-at = {2009-06-28 04:04:05},
	priority = {2},
	title = {Multimedia Event-Based Video Indexing Using Time Intervals},
	url = {http://dx.doi.org/10.1109/TMM.2005.850966},
	volume = {7},
	year = {2005}
}


@misc{citeulike:4072008,
	author = {Alspaugh, Thomas  A. },
	citeulike-article-id = {4072008},
	howpublished = {webpage},
	institution = {Department of Informatics, Bren School of Information and Computer Sciences},
	location = {Irvine},
	organization = {University of California},
	posted-at = {2009-06-28 03:09:03},
	priority = {2},
	title = {Allen's interval algebra},
	url = {http://www.ics.uci.edu/\~{}alspaugh/foundations/allen.html}
}



@article{citeulike:4991400,
	abstract = {Without Abstract},
	author = {Chittaro, Luca   and Montanari, Angelo  },
	citeulike-article-id = {4991400},
	doi = {10.1023/A:1018933906603},
	journal = {Annals of Mathematics and Artificial Intelligence},
	keywords = {proposal},
	month = {February},
	number = {1},
	pages = {1--4},
	posted-at = {2009-06-27 20:12:54},
	priority = {2},
	title = {Editorial: Temporal representation and reasoning},
	url = {http://dx.doi.org/10.1023/A:1018933906603},
	volume = {22},
	year = {1998}
}



@article{citeulike:4991332,
	address = {Essex, UK},
	author = {Freksa, Christian  },
	citeulike-article-id = {4991332},
	doi = {10.1016/0004-3702(92)90090-K},
	issn = {0004-3702},
	journal = {Artif. Intell.},
	keywords = {proposal},
	number = {1-2},
	pages = {199--227},
	posted-at = {2009-06-27 19:59:03},
	priority = {2},
	publisher = {Elsevier Science Publishers Ltd.},
	title = {Temporal reasoning based on semi-intervals},
	url = {http://dx.doi.org/10.1016/0004-3702(92)90090-K},
	volume = {54},
	year = {1992}
}



@article{citeulike:191348,
	address = {New York, NY, USA},
	author = {Allen, James  F. },
	citeulike-article-id = {191348},
	doi = {10.1145/182.358434},
	issn = {0001-0782},
	journal = {Commun. ACM},
	keywords = {proposal},
	month = {November},
	number = {11},
	pages = {832--843},
	posted-at = {2009-06-27 19:36:52},
	priority = {2},
	publisher = {ACM Press},
	title = {Maintaining knowledge about temporal intervals},
	url = {http://dx.doi.org/10.1145/182.358434},
	volume = {26},
	year = {1983}
}



@misc{citeulike:1277366,
	abstract = {this paper, we surveyed the list of existing association rule mining techniques.
This investigation is prepared to our new project titled mining historical changes
to web delta},
	author = {Zhao, Qiankun   and Bhowmick, Sourav  S. },
	citeulike-article-id = {1277366},
	keywords = {proposal},
	posted-at = {2009-06-24 15:29:52},
	priority = {2},
	title = {Sequential Pattern Matching: A Survey},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.59.8692}
}



@article{citeulike:1748833,
	address = {New York, NY, USA},
	author = {M\"orchen, Fabian  },
	citeulike-article-id = {1748833},
	doi = {10.1145/1294301.1294302},
	issn = {1931-0145},
	journal = {SIGKDD Explor. Newsl.},
	keywords = {proposal},
	month = {June},
	number = {1},
	pages = {41--55},
	posted-at = {2009-06-22 19:01:45},
	priority = {2},
	publisher = {ACM Press},
	title = {Unsupervised pattern mining from symbolic temporal data},
	url = {http://dx.doi.org/10.1145/1294301.1294302},
	volume = {9},
	year = {2007}
}



@article{citeulike:819708,
	address = {Piscataway, NJ, USA},
	author = {Yang, Jiong   and Wang, Wei   and Yu, Philip  S. },
	citeulike-article-id = {819708},
	doi = {10.1109/TKDE.2003.1198394},
	issn = {1041-4347},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	keywords = {proposal},
	month = {March},
	number = {3},
	pages = {613--628},
	posted-at = {2009-06-21 17:30:08},
	priority = {2},
	publisher = {IEEE Educational Activities Department},
	title = {Mining Asynchronous Periodic Patterns in Time Series Data},
	url = {http://dx.doi.org/10.1109/TKDE.2003.1198394},
	volume = {15},
	year = {2003}
}



@book{citeulike:143101,
	abstract = {{"People sometimes ask me what they should read to find out about artificial intelligence. Herbert Simon's book The Sciences of the Artificial is always on the list I give them. Every page issues a challenge to conventional thinking, and the layman who digests it well will certainly understand what the field of artificial intelligence hopes to accomplish. I recommend it in the same spirit that I recommend Freud to people who ask about psychoanalysis, or Piaget to those who ask about child psychology: If you want to learn about a subject, start by reading its founding fathers." -- George A. Miller, <i>Complex Information Processing</i>  <P>Continuing his exploration of the organization of complexity and the science of design, this new edition of Herbert Simon's classic work on artificial intelligence adds a chapter that sorts out the current themes and tools -- chaos, adaptive systems, genetic algorithms -- for analyzing complexity and complex systems. There are updates throughout the book as well. These take into account important advances in cognitive psychology and the science of design while confirming and extending the book's basic thesis: that a physical symbol system has the necessary and sufficient means for intelligent action. The chapter "Economic Reality" has also been revised to reflect a change in emphasis in Simon's thinking about the respective roles of organizations and markets in economic systems.}},
	author = {Simon, Herbert  A. },
	citeulike-article-id = {143101},
	howpublished = {Paperback},
	isbn = {0262691914},
	keywords = {proposal},
	month = {October},
	posted-at = {2009-06-20 02:10:19},
	priority = {2},
	publisher = {{The MIT Press}},
	title = {The Sciences of the Artificial - 3rd Edition},
	url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0262691914},
	year = {1996}
}



@book{citeulike:606469,
	abstract = {{Richard Dawkins is not a shy man. Edward Larson's research shows that most scientists today are not formally religious, but Dawkins is an in-your-face atheist in the witty British style:<p> <blockquote>I want to persuade the reader, not just that the Darwinian world-view happens to be true, but that it is the only known theory that could, in principle, solve the mystery of our existence.</blockquote><p> The title of this 1986 work, Dawkins's second book, refers to the Rev.  William Paley's 1802 work, <I>Natural Theology</I>, which argued that just as finding a watch would lead you to conclude that a watchmaker must exist, the complexity of living organisms proves that a Creator exists. Not so, says Dawkins: "All appearances to the contrary, the only watchmaker in nature is the blind forces of physics, albeit deployed in a very special way... it is the <I>blind</I> watchmaker."<p> Dawkins is a hard-core scientist: he doesn't just tell you what is so, he shows you how to find out for yourself. For this book, he wrote <I>Biomorph</I>, one of the first artificial life programs. You can check Dawkins's results on your own Mac or PC.} {<B>"The best general account of evolution I have read in recent years."\&\#151;E. O. Wilson. With a new introduction.</B><BR><BR>Twenty years after its original publication, <I>The Blind Watchmaker</I>, framed with a new introduction by the author, is as prescient and timely a book as ever. The watchmaker belongs to the eighteenth-century theologian William Paley, who argued that just as a watch is too complicated and functional to have sprung into existence by accident, so too must all living things, with their far greater complexity, be purposefully designed. Charles Darwin's brilliant discovery challenged the creationist arguments; but only Richard Dawkins could have written this elegant riposte. Natural selection\&\#151;the unconscious, automatic, blind, yet essentially nonrandom process Darwin discovered\&\#151;is the blind watchmaker in nature.}},
	author = {Dawkins, Richard  },
	citeulike-article-id = {606469},
	howpublished = {Paperback},
	isbn = {0393315703},
	keywords = {proposal},
	month = {September},
	posted-at = {2009-06-20 02:07:35},
	priority = {2},
	publisher = {{W. W. Norton}},
	title = {The Blind Watchmaker: Why the Evidence of Evolution Reveals a Universe Without Design},
	url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0393315703},
	year = {1996}
}



@article{citeulike:4913213,
	author = {Fischer, Gerhard  },
	citeulike-article-id = {4913213},
	doi = {10.1023/A:1022972113929},
	journal = {Automated Software Engineering},
	keywords = {proposal},
	month = {April},
	number = {2},
	pages = {233--237},
	posted-at = {2009-06-20 01:34:55},
	priority = {2},
	title = {Desert Island: Software Engineering—A Human Activity},
	url = {http://dx.doi.org/10.1023/A:1022972113929},
	volume = {10},
	year = {2003}
}



@inproceedings{citeulike:2826276,
	abstract = {Efficient and accurate similarity searching on a large time series data set is an important but non- trivial problem. In this work, we propose a new approach to improve the quality of similarity search on time series data by combining symbolic aggregate approximation (SAX) and piecewise linear approximation. The approach consists of three steps: transforming real valued time series sequences to symbolic strings via SAX, pattern matching on the symbolic strings and a post-processing via Piecewise Linear Approximation.},
	author = {Nguyen and Anh, Duong  T. },
	booktitle = {Information Technology Convergence, 2007. ISITC 2007. International Symposium on},
	citeulike-article-id = {2826276},
	doi = {10.1109/ISITC.2007.24},
	journal = {Information Technology Convergence, 2007. ISITC 2007. International Symposium on},
	keywords = {sax, search, similarity},
	pages = {58--62},
	posted-at = {2009-05-27 18:26:19},
	priority = {2},
	title = {Combining SAX and Piecewise Linear Approximation to Improve Similarity Search on Financial Time Series},
	url = {http://dx.doi.org/10.1109/ISITC.2007.24},
	year = {2007}
}



@article{citeulike:4518026,
	abstract = {Abstract\&nbsp;\&nbsp;Process mining aims at extracting information from event logs to capture the business process as it is being executed. Process mining is particularly useful in situations where events are recorded but there is no system enforcing people to work in a particular way. Consider for example a hospital where the diagnosis and treatment activities are recorded in the hospital information system, but where health-care professionals determine the  ” careflow.” Many process mining approaches have been proposed in recent years. However, in spite of many researchers' persistent efforts, there are still several challenging problems to be solved. In this paper, we focus on mining non-free-choice constructs, i.e., situations where there is a mixture of choice and synchronization. Although most real-life processes exhibit non-free-choice behavior, existing algorithms are unable to adequately deal with such constructs. Using a Petri-net-based representation, we will show that there are two kinds of causal dependencies between tasks, i.e., explicit and implicit ones. We propose an algorithm that is able to deal with both kinds of dependencies. The algorithm has been implemented in the ProM framework and experimental results shows that the algorithm indeed significantly improves existing process mining techniques.},
	author = {Wen, Lijie   and van der Aalst, Wil   and Wang, Jianmin   and Sun, Jiaguang  },
	citeulike-article-id = {4518026},
	doi = {10.1007/s10618-007-0065-y},
	journal = {Data Mining and Knowledge Discovery},
	keywords = {mining, process},
	month = {October},
	number = {2},
	pages = {145--180},
	posted-at = {2009-05-14 13:33:00},
	priority = {2},
	title = {Mining process models with non-free-choice constructs},
	url = {http://dx.doi.org/10.1007/s10618-007-0065-y},
	volume = {15},
	year = {2007}
}


@article{citeulike:4501572,
	abstract = {The last decade has witnessed a tremendous growths of interests in applications that deal with querying and mining of time series data. Numerous representation methods for dimensionality reduction and similarity measures geared towards time series have been introduced. Each individual work introducing a particular method has made specific claims and, aside from the occasional theoretical justifications, provided quantitative experimental observations. However, for the most part, the comparative aspects of these experiments were too narrowly focused on demonstrating the benefits of the proposed methods over some of the previously introduced ones. In order to provide a comprehensive validation, we conducted an extensive set of time series experiments re-implementing 8 different representation methods and 9 similarity measures and their variants, and testing their effectiveness on 38 time series data sets from a wide variety of application domains. In this paper, we give an overview of these different techniques and present our comparative experimental findings regarding their effectiveness. Our experiments have provided both a unified validation of some of the existing achievements, and in some cases, suggested that certain claims in the literature may be unduly optimistic.},
	author = {Ding, Hui   and Trajcevski, Goce   and Scheuermann, Peter   and Wang, Xiaoyue   and Keogh, Eamonn  },
	citeulike-article-id = {4501572},
	doi = {10.1145/1454159.1454226},
	journal = {Proc. VLDB Endow.},
	number = {2},
	pages = {1542--1552},
	posted-at = {2009-05-10 22:19:43},
	priority = {2},
	publisher = {VLDB Endowment},
	title = {Querying and mining of time series data: experimental comparison of representations and distance measures},
	url = {http://dx.doi.org/10.1145/1454159.1454226},
	volume = {1},
	year = {2008}
}



@inproceedings{citeulike:4446167,
	abstract = {The last decade has seen a huge interest in classification of time series. Most of this work assumes that the data resides in main memory and is processed offline. However, recent advances in sensor technologies require resource-efficient algorithms that can be implemented directly on the sensors as real-time algorithms. We show how a recently introduced framework for time series classification, time series bitmaps, can be implemented as efficient classifiers which can be updated in constant time and space in the face of very high data arrival rates. We describe results from a case study of an important entomological problem, and further demonstrate the generality of our ideas with an example from robotics.},
	address = {Washington, DC, USA},
	author = {Kasetty, Shashwati   and Stafford, Candice   and Walker, Gregory  P.  and Wang, Xiaoyue   and Keogh, Eamonn  },
	booktitle = {ICTAI '08: Proceedings of the 2008 20th IEEE International Conference on Tools with Artificial Intelligence},
	citeulike-article-id = {4446167},
	doi = {10.1109/ICTAI.2008.143},
	isbn = {978-0-7695-3440-4},
	keywords = {litreview, sax, similarity},
	pages = {149--156},
	posted-at = {2009-04-30 19:36:30},
	priority = {2},
	publisher = {IEEE Computer Society},
	title = {Real-Time Classification of Streaming Sensor Data},
	url = {http://dx.doi.org/10.1109/ICTAI.2008.143},
	year = {2008}
}



@inproceedings{citeulike:3175770,
	author = {Wei, Li   and Keogh, Eamonn  J.  and Xi, Xiaopeng  },
	booktitle = {ICDM},
	citeulike-article-id = {3175770},
	doi = {10.1109/ICDM.2006.138},
	keywords = {litreview, sax, search, similarity},
	pages = {711--720},
	posted-at = {2009-04-30 19:33:04},
	priority = {2},
	publisher = {IEEE Computer Society},
	title = {SAXually Explicit Images: Finding Unusual Shapes},
	url = {http://dx.doi.org/10.1109/ICDM.2006.138},
	year = {2006}
}



@book{citeulike:4434481,
	abstract = {Using high-quality, real-world case studies and examples, this introduction to
mathematical statistics shows how to use statistical methods and when to use
them. This book can be used as a brief introduction to design of experiments.
This successful, calculus-based book of probability and statistics, was one of
the first to make real-world applications an integral part of motivating
discussion. The number of problem sets has increased in all sections. Some
sections include almost 50\% new problems, while the most popular case studies
remain. For anyone needing to develop proficiency with Mathematical
Statistics.},
	author = {Larsen, Richard  J.  and Marx, Morris  L. },
	citeulike-article-id = {4434481},
	edition = {3rd},
	howpublished = {Hardcover},
	isbn = {0139223037},
	keywords = {litreview, similarity},
	month = {January},
	posted-at = {2009-04-29 14:54:10},
	priority = {2},
	publisher = {Prentice Hall},
	title = {An Introduction to Mathematical Statistics and Its Applications (3rd Edition)},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0139223037},
	year = {2000}
}



@inproceedings{citeulike:227029,
	address = {Washington, DC, USA},
	author = {Keogh, Eamonn   and Lin, Jessica   and Truppel, Wagner  },
	booktitle = {ICDM '03: Proceedings of the Third IEEE International Conference on Data Mining},
	citeulike-article-id = {227029},
	isbn = {0769519784},
	keywords = {litreview, sax},
	posted-at = {2009-04-28 14:29:55},
	priority = {2},
	publisher = {IEEE Computer Society},
	title = {Clustering of Time Series Subsequences is Meaningless: Implications for Previous and Future Research},
	url = {http://portal.acm.org/citation.cfm?id=952156},
	year = {2003}
}



@article{citeulike:4412621,
	abstract = {We develop an event detection framework that has two significant  advantages over past work. First, we introduce an  extended set of time-wise and object-wise statistical features  including not only the trajectory coordinates but also  the histograms and HMM based representations of object's  speed, orientation, location, size, and aspect ratio. These  features enable detection of events that cannot be detected  with the existing trajectory features reported so far. Second,  we introduce a spectral clustering algorithm that can automatically  estimate the optimal number of clusters. First,  we construct feature-wise affinity matrices from the pair-wise  similarity scores of objects using the extended set of  features. To determine the usual events, we apply eigen-vector  decomposition and obtain object clusters. We show  that the number of eigenvectors used in the decomposition is  proportional to the optimal number of clusters. Unlike the  conventional approaches that try to fit predefined models to  events, we analyze the conformity of objects using affinity  matrices to find the unusual events. We improve the feature  selection process by incorporating feature variances.  We prove that the clustering stage is not adversely affected  by high dimensionality of data space. Our simulations with  synthetic and real data reveal that the proposed detection  methods accurately detect usual and unusual events.},
	address = {Los Alamitos, CA, USA},
	author = {Porikli, Fatih   and Haga, Tetsuji  },
	citeulike-article-id = {4412621},
	doi = {10.1109/CVPR.2004.335},
	issn = {1063-6919},
	journal = {Computer Vision and Pattern Recognition Workshop},
	keywords = {litreview, similarity},
	pages = {114+},
	posted-at = {2009-04-27 22:01:38},
	priority = {2},
	publisher = {IEEE Computer Society},
	title = {Event Detection by Eigenvector Decomposition Using Object and Frame Features},
	url = {http://dx.doi.org/10.1109/CVPR.2004.335},
	volume = {7},
	year = {2004}
}



@inproceedings{citeulike:4412617,
	abstract = {Consider the problem of monitoring tens of thousands of time series data streams in an online fashion and making decisions based on them. In addition to single stream statistics such as average and standard deviation, we also want to find high correlations among all pairs of streams. A stock market trader might use such a tool to spot arbitrage opportunities. This paper proposes efficient methods for solving this problem based on Discrete Fourier Transforms and a three level time interval hierarchy. Extensive experiments on synthetic data and real world financial trading data show that our algorithm beats the direct computation approach by several orders of magnitude. It also improves on previous Fourier Transform approaches by allowing the efficient computation of time-delayed correlation over any size sliding window and any time delay. Correlation also lends itself to an efficient grid-based data structure. The result is the first algorithm that we know of to compute correlations over thousands of data streams in real time. The algorithm is incremental, has fixed response time, and can monitor the pairwise correlations of 10,000 streams on a single PC. The algorithm is embarrassingly parallelizable.},
	author = {Zhu, Yunyue   and Shasha, Dennis  },
	booktitle = {VLDB '02: Proceedings of the 28th international conference on Very Large Data Bases},
	citeulike-article-id = {4412617},
	keywords = {litreview, similarity},
	location = {Hong Kong, China},
	pages = {358--369},
	posted-at = {2009-04-27 21:58:48},
	priority = {2},
	publisher = {VLDB Endowment},
	title = {StatStream: statistical monitoring of thousands of data streams in real time},
	url = {http://portal.acm.org/citation.cfm?id=1287401},
	year = {2002}
}



@techreport{citeulike:4041809,
	abstract = {Hackystat is an open source framework for automated collection and analysis of software engineering process and product data. Hackystat has been in development since 2001, and has gone through eight major architectural revisions during that time. In 2007, we performed the latest architectural revision, whose primary goal was to reimplement Hackystat as a service-oriented architecture (SOA). This version has now been in public release for a year, and this paper reports on our experiences: the motivations that led us to reimplement the system as a SOA, the costs and benefits of that conversion, and our lessons learned.},
	address = {Los Angeles, California},
	author = {Johnson, Philip  M.  and Zhang, Shaoxuan   and Senin, Pavel  },
	booktitle = {Submitted to the 2009 {IEEE} Service Cup Conference},
	citeulike-article-id = {4041809},
	institution = {Department of Information and Computer Sciences, University of Hawaii, Honolulu, Hawaii 96822},
	keywords = {publication},
	month = {February},
	number = {{CSDL}-09-07},
	posted-at = {2009-04-27 13:00:38},
	priority = {2},
	title = {Experiences with Hackystat as a service-oriented architecture},
	url = {http://csdl.ics.hawaii.edu/techreports/09-07/09-07.pdf},
	year = {2009}
}



@book{citeulike:167581,
	author = {Duda, Richard  O.  and Hart, Peter  E.  and Stork, David  G. },
	citeulike-article-id = {167581},
	howpublished = {Hardcover},
	isbn = {0471056693},
	keywords = {litreview},
	month = {November},
	posted-at = {2009-04-26 20:58:15},
	priority = {2},
	publisher = {Wiley-Interscience},
	title = {Pattern Classification (2nd Edition)},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0471056693},
	year = {2000}
}



@inproceedings{citeulike:4408223,
	abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article.  ACM has opted to expose the complete List rather than only correct and linked references.},
	address = {New York, NY, USA},
	author = {Hellerstein, Joseph  M.  and Koutsoupias, Elias   and Papadimitriou, Christos  H. },
	booktitle = {PODS '97: Proceedings of the sixteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems},
	citeulike-article-id = {4408223},
	doi = {10.1145/263661.263688},
	isbn = {0-89791-910-6},
	keywords = {litreview, sax, search, similarity},
	location = {Tucson, Arizona, United States},
	pages = {249--256},
	posted-at = {2009-04-26 19:53:17},
	priority = {2},
	publisher = {ACM},
	title = {On the analysis of indexing schemes},
	url = {http://dx.doi.org/10.1145/263661.263688},
	year = {1997}
}



@misc{citeulike:4406444,
	abstract = {We introduce an extended representation of time series that allows fast, accurate classification and clustering in addition to the ability to explore time series data in a relevance feedback framework. The representation consists of piecewise linear segments to represent shape and a weight vector that contains the relative importance of each individual linear segment. In the classification context, the weights are learned automatically as part of the training cycle. In the relevance feedback context, the weights are determined by an interactive and iterative process in which users rate various choices presented to them. Our representation allows a user to define a variety of similarity measures that can be tailored to specific domains. We demonstrate our approach on space telemetry, medical and synthetic data. 1.0 Introduction Time series account for much of the data stored in business, medical, engineering and social science databases. Much of the utility of collecting this data com...},
	author = {Keogh, Eamonn  J.  and Pazzani, Michael  J. },
	citeulike-article-id = {4406444},
	keywords = {litreview, paa, similarity},
	posted-at = {2009-04-26 16:51:34},
	priority = {2},
	title = {An Enhanced Representation of Time Series Which Allows Fast and Accurate Classification, Clustering and Relevance Feedback},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.9288},
	year = {1998}
}



@book{citeulike:499150,
	abstract = {{Wavelets are a mathematical development that may revolutionize the world of information storage and retrieval according to many experts. They are a fairly simple mathematical tool now being applied to the compression of data--such as fingerprints, weather satellite photographs, and medical x-rays--that were previously thought to be impossible to condense without losing crucial details.  <P>This monograph contains 10 lectures presented by Dr. Daubechies as the principal speaker at the 1990 CBMS-NSF Conference on Wavelets and Applications. The author has worked on several aspects of the wavelet transform and has developed a collection of wavelets that are remarkably efficient.  <P>The opening chapter provides an overview of the main problems presented in the book. Following chapters discuss the theoretical and practical aspects of wavelet theory, including wavelet transforms, orthonormal bases of wavelets, and characterization of functional spaces by means of wavelets. The last chapter presents several topics under active research, as multidimensional wavelets, wavelet packet bases, and a construction of wavelets tailored to decompose functions defined in a finite interval. Because of their interdisciplinary origins, wavelets appeal to scientists and engineers of many different backgrounds.}},
	author = {Daubechies, Ingrid  },
	citeulike-article-id = {499150},
	howpublished = {Paperback},
	isbn = {0898712742},
	keywords = {litreview, wavelet},
	month = {December},
	posted-at = {2009-04-26 13:49:40},
	priority = {2},
	publisher = {{Soc for Industrial \& Applied Math}},
	title = {Ten Lectures on Wavelets (C B M S - N S F Regional Conference Series in Applied Mathematics)},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0898712742},
	year = {1992}
}



@article{citeulike:4384535,
	abstract = {Time series stored as feature vectors can be indexed by multi-dimensional index trees like R-Tree for fast retrieval. Due to the dimensionality curse problem, transformations are applied to time series to reduce the number of dimensions of the feature vectors. Different transformations like Discrete Fourier Transform (DFT), Discrete Wavelet Transform (DWT), Karhunen-Loeve (K-L) transform or Singular Value Decomposition (SVD) can be applied. While the use of DFT and K-L transform or SVD have been studied in the literature, to our knowledge, there is no in-depth study on the application of DWT. In this paper, we propose to use Haar Wavelet Transform for time series indexing. The major contributions are: (1) we show that Euclidean distance is preserved in the Haar transformed domain and no false dismissal will occur in range query, (2) we show that Haar transform can outperform DFT through experiments, (3) a new similarity model is suggested to accommodate vertical shift of time series, and (4) a two-phase method is proposed for efficient n-nearest neighbor query in time series databases.},
	address = {Los Alamitos, CA, USA},
	author = {Chan, Kin  P.  and Fu, Wai  C. },
	citeulike-article-id = {4384535},
	doi = {10.1109/ICDE.1999.754915},
	issn = {1063-6382},
	journal = {Data Engineering, International Conference on},
	keywords = {litreview, similarity, wavelet},
	pages = {126+},
	posted-at = {2009-04-23 11:27:16},
	priority = {2},
	publisher = {IEEE Computer Society},
	title = {Efficient Time Series Matching by Wavelets},
	url = {http://dx.doi.org/10.1109/ICDE.1999.754915},
	volume = {0},
	year = {1999}
}



@incollection{citeulike:4384496,
	abstract = {The existing multi-dimensional index structures are not adequate for indexing higher-dimensional data sets. Although conceptually they can be extended to higher dimensionalities, they usually require time and space that grow exponentially with the dimensionality. In this paper, we analyze the existing index structures and derive some requirements of an index structure for content-based image retrieval. We also propose a new structure, called CIR(Content-based Image Retrieval)-tree, for indexing large amounts of point data in high dimensional space that satisfies the requirements. In order to justify the performance of the proposed structure, we compare the proposed structure with the existing index structures in the various environments. We show through experiments that our proposed structure outperforms the existing structures in terms of retrieval time and storage overhead.},
	author = {Yoo, Jae   and Shin, Myung   and Lee, Seok   and Choi, Kil   and Cho, Ki   and Hur, Dae  },
	citeulike-article-id = {4384496},
	doi = {10.1007/3-540-48962-2\_10},
	journal = {Advanced Multimedia Content Processing},
	keywords = {litreview, search, similarity, tree},
	pages = {131--144},
	posted-at = {2009-04-23 11:07:15},
	priority = {2},
	title = {An Efficient Index Structure for High Dimensional Image Data},
	url = {http://dx.doi.org/10.1007/3-540-48962-2\_10},
	year = {1999}
}



@inproceedings{citeulike:2843857,
	address = {San Francisco, CA, USA},
	author = {Berchtold, Stefan   and Keim, Daniel  A.  and Kriegel, Hans-Peter  },
	booktitle = {VLDB '96: Proceedings of the 22th International Conference on Very Large Data Bases},
	citeulike-article-id = {2843857},
	isbn = {1558603824},
	keywords = {litreview, similarity, tree},
	pages = {28--39},
	posted-at = {2009-04-23 11:06:15},
	priority = {2},
	publisher = {Morgan Kaufmann Publishers Inc.},
	title = {The {X}-tree: An Index Structure for High-Dimensional Data},
	url = {http://portal.acm.org/citation.cfm?id=645922.673502},
	year = {1996}
}



@inproceedings{citeulike:4384489,
	abstract = {Feature-based similarity searching is emerging as an important search paradigm in database systems. The technique used is to map the data items as points into a high-dimensional feature space which is indexed using a multidimensional data structure. Similarity searching then corresponds to a range search over the data structure. Although several data structures have been proposed for feature indexing, none of them is known to scale beyond 10-15 dimensional spaces. This paper introduces the hybrid tree-a multidimensional data structure for indexing high-dimensional feature spaces. Unlike other multidimensional data structures, the hybrid tree cannot be classified as either a pure data partitioning (DP) index structure (such as the R-tree, SS-tree or SR-tree) or a pure space partitioning (SP) one (such as the KDB-tree or hB-tree); rather it combines the positive aspects of the two types of index structures into a single data structure to achieve a search performance which is more scalable to high dimensionalities than either of the above techniques. Furthermore, unlike many data structures (e.g. distance-based index structures like the SS-tree and SR-tree), the hybrid tree can support queries based on arbitrary distance functions. Our experiments on \&ldquo;real\&rdquo; high-dimensional large-size feature databases demonstrate that the hybrid tree scales well to high dimensionality and large database sizes. It significantly outperforms both purely DP-based and SP-based index mechanisms as well as linear scans at all dimensionalities for large-sized databases},
	author = {Chakrabarti, K.  and Mehrotra, S. },
	booktitle = {Data Engineering, 1999. Proceedings., 15th International Conference on},
	citeulike-article-id = {4384489},
	doi = {10.1109/ICDE.1999.754960},
	journal = {Data Engineering, 1999. Proceedings., 15th International Conference on},
	keywords = {litreview, search, similarity, tree},
	pages = {440--447},
	posted-at = {2009-04-23 11:03:52},
	priority = {2},
	title = {The hybrid tree: an index structure for high dimensional feature spaces},
	url = {http://dx.doi.org/10.1109/ICDE.1999.754960},
	year = {1999}
}



@inproceedings{citeulike:4373408,
	abstract = {Abstract: The problem of finding patterns of interest in time series databases (query by content) is an important one, with applications in virtually every field of science. A variety of approaches have been suggested. These approaches are robust to noise, offset translation, and amplitude scaling to varying degrees. However, they are all extremely sensitive to scaling in the time axis (longitudinal scaling). We present a method for similarity search that is robust to scaling in the time axis, in addition to noise, offset translation, and amplitude scaling. The method has been tested on medical, financial, space telemetry and artificial data. Furthermore the method is exceptionally fast, with the predicted 2 to 4 orders of magnitude speedup actually observed. The method uses a piecewise linear representation of the original data. We also introduce a new algorithm which both decides the optimal number of linear segments to use, and produces the actual linear representation.},
	address = {Washington, DC, USA},
	author = {Keogh, E. },
	booktitle = {ICTAI '97: Proceedings of the 9th International Conference on Tools with Artificial Intelligence},
	citeulike-article-id = {4373408},
	keywords = {litreview, similarity},
	pages = {578+},
	posted-at = {2009-04-22 04:51:15},
	priority = {2},
	publisher = {IEEE Computer Society},
	title = {Fast similarity search in the presence of longitudinal scaling in time series databases},
	url = {http://portal.acm.org/citation.cfm?id=880107},
	year = {1997}
}



@inproceedings{citeulike:4373332,
	abstract = {We examine the problem of finding similar tumor  shapes. Starting from a natural similarity  function (the so-called `max morphological  distance\&\#039;), we show how to lower-bound it and  how to search for nearest neighbors in large  collections of tumor-like shapes.  Specifically, we use state-of-the-art concepts  from morphology, namely the `pattern spectrum  \&\#039; of a shape, to map each shape to a point  in n-dimensional space. Following [16, 30], we  organize the n-d points in an R-tree. We show  that the L1 (= max) norm in the n-d space  lower-bounds the actual distance. This guarantees  no false dismissals for range queries. In  addition, we present a nearest neighbor algorithm  that also guarantees no false dismissals.  Finally, we implemented the method and  tested it against a testbed of realistic tumor  shapes, using an established tumor-growth  model of Murray Eden[13]. The experiments  Permission to copy without fee all or part of this material is granted provided that the copies ...},
	author = {Korn, Flip   and Sidiropoulos, Nikolaos   and Faloutsos, Christos   and Siegel, Eliot   and Protopapas, Zenon  },
	booktitle = {In Proceedings of the Int. Conf. on Very Large Data Bases},
	citeulike-article-id = {4373332},
	keywords = {litreview, similarity},
	pages = {215--226},
	posted-at = {2009-04-22 02:57:59},
	priority = {2},
	title = {Fast Nearest Neighbor Search in Medical Image Databases},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.9993},
	year = {1996}
}



@article{citeulike:4373331,
	author = {Heck, A. },
	citeulike-article-id = {4373331},
	doi = {10.1023/A:1005078508777},
	journal = {Space Science Reviews},
	editor = {D. Maoz, A. Sternberg, E. M. Leibowitz},
	month = {August},
	number = {3},
	pages = {549},
	posted-at = {2009-04-22 02:54:00},
	priority = {2},
	title = {{A}stronomical {T}ime {S}eries, {P}roceedings of {T}he {F}lorence and {G}eorge {W}ise {O}bservatory 25th {A}nniversary {S}ymposium},
	url = {http://dx.doi.org/10.1023/A:1005078508777},
	volume = {85},
	year = {1998}
}



@article{citeulike:4367455,
	author = {Kruskal, Joseph  B. },
	citeulike-article-id = {4367455},
	journal = {SIAM Review},
	keywords = {litreview, similarity},
	number = {2},
	pages = {201--237},
	posted-at = {2009-04-20 04:17:44},
	priority = {2},
	publisher = {SIAM},
	title = {An Overview of Sequence Comparison: Time Warps, String Edits, and Macromolecules},
	url = {http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal\&id=SIREAD000025000002000201000001\&idtype=cvips\&gifs=yes},
	volume = {25},
	year = {1983}
}



@article{citeulike:3980022,
	abstract = {The Dynamic Time Warping (DTW) is a popular similarity measure between time
series. The DTW fails to satisfy the triangle inequality and its computation
requires quadratic time. Hence, to find closest neighbors quickly, we use
bounding techniques. We can avoid most DTW computations with an inexpensive
lower bound (LB Keogh). We compare LB Keogh with a tighter lower bound (LB
Improved). We find that LB Improved-based search is faster. As an example, our
approach is 2-3 times faster over random-walk and shape time series.},
	author = {Lemire, Daniel  },
	citeulike-article-id = {3980022},
	eprint = {0811.3301},
	keywords = {dtw, litreview, similarity},
	month = {Nov},
	posted-at = {2009-04-20 03:46:18},
	priority = {2},
	title = {Faster Retrieval with a Two-Pass Dynamic-Time-Warping Lower Bound},
	url = {http://arxiv.org/abs/0811.3301},
	year = {2008}
}



@book{citeulike:180287,
	abstract = {{Aimed at any serious programmer or computer science student,  the new second edition of <I>Introduction to Algorithms</I> builds on  the tradition of the original with a truly magisterial guide to the  world of algorithms. Clearly presented, mathematically rigorous, and  yet approachable even for the math-averse, this title sets a high  standard for a textbook and reference to the best algorithms for  solving a wide range of computing problems.<p>With sample problems and  mathematical proofs demonstrating the correctness of each algorithm,  this book is ideal as a textbook for classroom study, but its reach  doesn't end there. The authors do a fine job of explaining each  algorithm. (Reference sections on basic mathematical notation will help  readers bridge the gap, but it will help to have some math background  to appreciate the full achievement of this handsome hardcover volume.)  Every algorithm is presented in pseudo-code, which can be implemented  in any computer language, including C/C++ and Java. This ecumenical  approach is one of the book's strengths. When it comes to sorting and  common data structures, from basic linked lists to trees (including  binary trees, red-black, and B-trees), this title really shines, with  clear diagrams that show algorithms in operation. Even if you just  glance over the mathematical notation here, you can definitely benefit  from this text in other ways.<p>The book moves forward with more  advanced algorithms that implement strategies for solving more  complicated problems (including dynamic programming techniques, greedy  algorithms, and amortized analysis). Algorithms for graphing problems  (used in such real-world business problems as optimizing flight  schedules or flow through pipelines) come next. In each case, the  authors provide the best from current research in each topic, along  with sample solutions.<p>This text closes with a grab bag of useful  algorithms including matrix operations and linear programming,  evaluating polynomials, and the well-known Fast Fourier Transformation  (FFT) (useful in signal processing and engineering). Final sections on  "NP-complete" problems, like the well-known traveling salesman problem,  show off that while not all problems have a demonstrably final and best  answer, algorithms that generate acceptable approximate solutions can  still be used to generate useful, real-world answers.<p>Throughout this  text, the authors anchor their discussion of algorithms with current  examples drawn from molecular biology (like the Human Genome Project),  business, and engineering. Each section ends with short discussions of  related historical material, often discussing original research in each  area of algorithms. On the whole, they argue successfully that  algorithms are a "technology" just like hardware and software that can  be used to write better software that does more, with better  performance. Along with classic books on algorithms (like Donald  Knuth's three-volume set, <I>The Art of Computer  Programming</I>), this title sets a new standard for compiling the  best research in algorithms. For any experienced developer, regardless  of their chosen language, this text deserves a close look for extending  the range and performance of real-world software. <I>--Richard  Dragan</I> <p> <B>Topics covered:</B> Overview of algorithms (including algorithms as  a technology); designing and analyzing algorithms; asymptotic notation;  recurrences and recursion; probabilistic analysis and randomized  algorithms; heapsort algorithms; priority queues; quicksort algorithms;  linear time sorting (including radix and bucket sort); medians and  order statistics (including minimum and maximum); introduction to data  structures (stacks, queues, linked lists, and rooted trees); hash  tables (including hash functions); binary search trees; red-black  trees; augmenting data structures for custom applications; dynamic  programming explained (including assembly-line scheduling, matrix-chain  multiplication, and optimal binary search trees); greedy algorithms  (including Huffman codes and task-scheduling problems); amortized  analysis (the accounting and potential methods); advanced data  structures (including B-trees, binomial and Fibonacci heaps,  representing disjoint sets in data structures); graph algorithms  (representing graphs, minimum spanning trees, single-source shortest  paths, all-pairs shortest paths, and maximum flow algorithms); sorting  networks; matrix operations; linear programming (standard and slack  forms); polynomials and the Fast Fourier Transformation (FFT); number  theoretic algorithms (including greatest common divisor, modular  arithmetic, the Chinese remainder theorem, RSA public-key encryption,  primality testing, integer factorization); string matching;  computational geometry (including finding the convex hull);  NP-completeness (including sample real-world NP-complete problems and  their insolvability); approximation algorithms for NP-complete problems  (including the traveling salesman problem); reference sections for  summations and other mathematical notation, sets, relations, functions,  graphs and trees, as well as counting and probability backgrounder  (plus geometric and binomial distributions).}},
	author = {Cormen, Thomas  H.  and Leiserson, Charles  E.  and Rivest, Ronald  L.  and Stein, Clifford  },
	citeulike-article-id = {180287},
	howpublished = {Hardcover},
	isbn = {0262032937},
	keywords = {litreview},
	month = {September},
	posted-at = {2009-04-20 00:51:56},
	priority = {2},
	publisher = {{The MIT Press}},
	title = {Introduction to Algorithms, Second Edition},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0262032937},
	year = {2001}
}



@inproceedings{citeulike:343069,
	address = {New York, NY, USA},
	author = {Beckmann, Norbert   and Kriegel, Hans-Peter   and Schneider, Ralf   and Seeger, Bernhard  },
	booktitle = {SIGMOD '90: Proceedings of the 1990 ACM SIGMOD international conference on Management of data},
	citeulike-article-id = {343069},
	doi = {10.1145/93597.98741},
	issn = {0163-5808},
	keywords = {lcs, litreview},
	month = {June},
	number = {2},
	pages = {322--331},
	posted-at = {2009-04-20 00:31:55},
	priority = {2},
	publisher = {ACM Press},
	title = {The {R}*-tree: an efficient and robust access method for points and rectangles},
	url = {http://dx.doi.org/10.1145/93597.98741},
	volume = {19},
	year = {1990}
}



@inproceedings{citeulike:4367061,
	abstract = {We propose an inter-sequence matching method for exact and similarity matching of image sequences. Our method transforms the image sequence matching problem into matching sequences of real numbers. The method does not require sequences to be of the same length. It uses a modified version of the Longest Common Subsequence (LCS) method for actually matching two sequences. We also propose a feature-based indexing mechanism to filter out those sequences which are matching candidates with a given query sequence from a large data set. Like all other feature-based indexing methods, our method maps each sequence into a point in K dimensional space, where K is the number of extracted features for the sequence. It operates in two phases, hypothesizing and verification. Lengths and moments (mean and variance) of sequences are used as features. Experimental results indicate that the features and proposed method for query processing do well as a filter.},
	address = {Washington, DC, USA},
	author = {Yazdani, Nasser   and \"{O}zsoyoglu, Meral  Z. },
	booktitle = {SSDBM '96: Proceedings of the Eighth International Conference on Scientific and Statistical Database Management},
	citeulike-article-id = {4367061},
	doi = {10.1109/SSDM.1996.505915},
	isbn = {0-8186-7264-1},
	keywords = {lcs, litreview},
	pages = {53--62},
	posted-at = {2009-04-20 00:24:26},
	priority = {2},
	publisher = {IEEE Computer Society},
	title = {Sequence Matching of Images},
	url = {http://dx.doi.org/10.1109/SSDM.1996.505915},
	year = {1996}
}



@incollection{citeulike:4367057,
	abstract = {The purpose of subsequence matching is to find a query sequence from a long data sequence. Due to the abundance of applications, many solutions have been proposed. Virtually all previous solutions use the Euclidean measure as the basis for measuring distance between sequences. Recent studies, however, suggest that the Euclidean distance often fails to produce proper results due to the irregularity in the data, which is not so uncommon in our problem domain. Addressing this problem, some non-Euclidean measures, such as Dynamic Time Warping (DTW) and Longest Common Subsequence (LCS), have been proposed. However, most of the previous work in this direction focused on the whole sequence matching problem where query and data sequences are the same length. In this paper, we propose a novel subsequence matching framework using a non-Euclidean measure, in particular, LCS, and a new index query scheme. The proposed framework is based on the Dual Match framework where data sequences are divided into a series of disjoint equi-length subsequences and then indexed in an R-tree. We introduced similarity bound for index matching with LCS. The proposed query matching scheme reduces significant numbers of false positives in the match result. Furthermore, we developed an algorithm to skip expensive LCS computations through observing the warping paths. We validated our framework through extensive experiments using 48 different time series datasets. The results of the experiments suggest that our approach significantly improves the subsequence matching performance in various metrics.},
	author = {Han, Tae   and Ko, Seung-Kyu   and Kang, Jaewoo  },
	citeulike-article-id = {4367057},
	doi = {10.1007/978-3-540-73499-4\_44},
	journal = {Machine Learning and Data Mining in Pattern Recognition},
	keywords = {lcs, litreview},
	pages = {585--600},
	posted-at = {2009-04-20 00:20:00},
	priority = {2},
	title = {Efficient Subsequence Matching Using the Longest Common Subsequence with a Dual Match Index},
	url = {http://dx.doi.org/10.1007/978-3-540-73499-4\_44},
	year = {2007}
}



@inproceedings{citeulike:4344279,
	abstract = {Jagadish et al. (see Proc. ACM SIGACT-SIGMOD-SIGART PODS, p.36-45, 1995) developed a general framework for posing queries based on similarity. The framework enables a formal definition of the notion of similarity for an application domain of choice, and then its use in queries to perform similarity-based search. We adapt this framework to the specialized domain of real-valued sequences. (Although some of the ideas we present are applicable to other types of data as well). In particular we focus on whole-match queries. By whole-match query we mean the case where the user has to specify the whole sequence. Similarity-based search can be computationally very expensive. The computation cost depends heavily on the length of sequences being compared. To make such similarity testing feasible on large data sets, we propose the use of a signature based technique. In a nutshell, our approach is to \&ldquo;shrink\&rdquo; the data sequences into signatures, and search the signatures instead of the real sequences, with further comparison being required only when a possible match is indicated. Being shorter, signatures can usually be compared much faster than the original sequences. In addition, signatures are usually easier to index. For such a signature-based technique to be effective one has to assure that (1) the signature comparison is fast, and (2) the signature comparison gives few false alarms, and no false dismissals. We obtain measures of goodness for our technique. The technique is illustrated with a couple of very different examples},
	author = {Faloutsos, C.  and Jagadish, H. V.  and Mendelzon, A. O.  and Milo, T. },
	booktitle = {Compression and Complexity of Sequences 1997. Proceedings},
	citeulike-article-id = {4344279},
	doi = {10.1109/SEQUEN.1997.666899},
	journal = {Compression and Complexity of Sequences 1997. Proceedings},
	keywords = {litreview, similarity},
	pages = {2--20},
	posted-at = {2009-04-17 16:18:29},
	priority = {2},
	title = {A signature technique for similarity-based queries},
	url = {http://dx.doi.org/10.1109/SEQUEN.1997.666899},
	year = {1997}
}



@inproceedings{citeulike:3168542,
	address = {New York, NY, USA},
	author = {Rafiei, Davood   and Mendelzon, Alberto  },
	booktitle = {SIGMOD '97: Proceedings of the 1997 ACM SIGMOD international conference on Management of data},
	citeulike-article-id = {3168542},
	doi = {10.1145/253260.253264},
	isbn = {0897919114},
	keywords = {litreview},
	location = {Tucson, Arizona, United States},
	pages = {13--25},
	posted-at = {2009-04-17 16:13:48},
	priority = {2},
	publisher = {ACM},
	title = {Similarity-based queries for time series data},
	url = {http://dx.doi.org/10.1145/253260.253264},
	year = {1997}
}



@article{citeulike:4343933,
	abstract = {Recently some fast methods ( LAESA  and  TLAESA ) have been proposed to find nearest neighbours in metric spaces. The average number of distances computed by these algorithms does not depend on the number of prototypes and they show linear space complexity. These results where obtained through vast experimentation using only artificial data. In this paper, we corroborate this behaviour when applied to handwritten character recognition tasks. Moreover, we compare  LAESA  and  TLAESA  with some classical algorithms also working in metric spaces.},
	author = {Mic\'{o}, L. },
	citeulike-article-id = {4343933},
	doi = {10.1016/S0167-8655(98)00007-5},
	issn = {01678655},
	journal = {Pattern Recognition Letters},
	keywords = {litreview},
	month = {March},
	number = {3-4},
	pages = {351--356},
	posted-at = {2009-04-17 14:31:47},
	priority = {2},
	title = {Comparison of fast nearest neighbour classifiers for handwritten character recognition},
	url = {http://dx.doi.org/10.1016/S0167-8655(98)00007-5},
	volume = {19},
	year = {1998}
}



@article{citeulike:4343286,
	address = {Amsterdam, The Netherlands, The Netherlands},
	author = {Vidal, Enrique   and Casacuberta, Francisco  },
	citeulike-article-id = {4343286},
	doi = {10.1016/0167-6393(88)90022-2},
	issn = {0167-6393},
	journal = {Speech Commun.},
	keywords = {litreview},
	number = {1},
	pages = {67--79},
	posted-at = {2009-04-17 14:31:35},
	priority = {2},
	publisher = {Elsevier Science Publishers B. V.},
	title = {On the verification of triangle inequality by dynamic time-warping dissimilarity measures},
	url = {http://dx.doi.org/10.1016/0167-6393(88)90022-2},
	volume = {7},
	year = {1988}
}



@incollection{citeulike:4342003,
	abstract = {Time series are comprehensively appeared and developed in many applications. Similarity search under time warping has attracted much interest between the time series in the large databases. DTW (Dynamic Time Warping) is a robust distance measure and is superior to Euclidean distance. Nevertheless, it is more unfortunate that DTW has a quadratic time and the false dismissals are come forth since DTW distance does not satisfy the triangular inequality. In this paper, we propose an efficient range query algorithm based on a new similarity search method under time warping. When our range query applies for this method, it can remove the significant non-qualify time series as early as possible. Hence, it speeds up the calculation time and reduces the number of scanning the time series. Guaranteeing no false dismissals the lower bounding function is advised that consistently underestimate the DTW distance and satisfy the triangular inequality. Through the experimental results, our range query algorithm outperforms the existing others.},
	author = {Li, Chuyu   and Jin, Long   and Seo, Sungbo   and Ryu, Keun  },
	citeulike-article-id = {4342003},
	doi = {10.1007/11596448\_106},
	journal = {Computational Intelligence and Security},
	keywords = {litreview},
	pages = {721--728},
	posted-at = {2009-04-17 10:28:40},
	priority = {2},
	title = {An Efficient Range Query Under the Time Warping Distance},
	url = {http://dx.doi.org/10.1007/11596448\_106},
	year = {2005}
}



@incollection{citeulike:4326324,
	abstract = {In retail industry, it is very important to understand seasonal sales pattern, because this knowledge can assist decision makers in managing inventory and formulating marketing strategies. Self-Organizing Map (SOM) is suitable for extracting and illustrating essential structures because SOM has unsupervised learning and topology preserving properties, and prominent visualization techniques. In this experiment, we propose a method for seasonal pattern analysis using Self-Organizing Map. Performance test with real-world data from stationery stores in Indonesia shows that the method is effective for seasonal pattern analysis. The results are used to formulate several marketing and inventory management strategies. Keywords: Visualization, Clustering, Temporal Data, Self-Organizing Maps.},
	author = {Denny and Lee, Vincent  C. },
	citeulike-article-id = {4326324},
	journal = {Advances in Knowledge Discovery and Data Mining},
	keywords = {litreview},
	pages = {424--430},
	posted-at = {2009-04-16 13:56:51},
	priority = {2},
	title = {An Alternative Methodology for Mining Seasonal Pattern Using Self-Organizing Map},
	url = {http://www.springerlink.com/content/k3q3tpd3acaf2c0b
},
	year = {2004}
}



@article{citeulike:3000416,
	abstract = {Abstract.\&nbsp;\&nbsp; The problem of similarity search in large time series databases has attracted much attention recently. It is a non-trivial problem because of the inherent high dimensionality of the data. The most promising solutions involve first performing dimensionality reduction on the data, and then indexing the reduced data with a spatial access method. Three major dimensionality reduction techniques have been proposed: Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and more recently the Discrete Wavelet Transform (DWT). In this work we introduce a new dimensionality reduction technique which we call Piecewise Aggregate Approximation (PAA). We theoretically and empirically compare it to the other techniques and demonstrate its superiority. In addition to being competitive with or faster than the other methods, our approach has numerous other advantages. It is simple to understand and to implement, it allows more flexible distance measures, including weighted Euclidean queries, and the index can be built in linear time.},
	author = {Keogh, Eamonn   and Chakrabarti, Kaushik   and Pazzani, Michael   and Mehrotra, Sharad  },
	citeulike-article-id = {3000416},
	doi = {10.1007/PL00011669},
	journal = {Knowledge and Information Systems},
	keywords = {litreview, sax},
	number = {3},
	pages = {263--286},
	posted-at = {2009-04-15 03:52:46},
	priority = {2},
	title = {Dimensionality Reduction for Fast Similarity Search in Large Time Series Databases},
	url = {http://dx.doi.org/10.1007/PL00011669},
	volume = {3},
	year = {2001}
}



@article{citeulike:2821475,
	abstract = {Abstract\&nbsp;\&nbsp;Many high level representations of time series have been proposed for data mining, including Fourier transforms, wavelets, eigenwaves, piecewise polynomial models, etc. Many researchers have also considered symbolic representations of time series, noting that such representations would potentiality allow researchers to avail of the wealth of data structures and algorithms from the text processing and bioinformatics communities. While many symbolic representations of time series have been introduced over the past decades, they all suffer from two fatal flaws. First, the dimensionality of the symbolic representation is the same as the original data, and virtually all data mining algorithms scale poorly with dimensionality. Second, although distance measures can be defined on the symbolic approaches, these distance measures have little correlation with distance measures defined on the original time series. In this work we formulate a new symbolic representation of time series. Our representation is unique in that it allows dimensionality/numerosity reduction, and it also allows distance measures to be defined on the symbolic approach that lower bound corresponding distance measures defined on the original series. As we shall demonstrate, this latter feature is particularly exciting because it allows one to run certain data mining algorithms on the efficiently manipulated symbolic representation, while producing identical results to the algorithms that operate on the original data. In particular, we will demonstrate the utility of our representation on various data mining tasks of clustering, classification, query by content, anomaly detection, motif discovery, and visualization.},
	author = {Lin, Jessica   and Keogh, Eamonn   and Wei, Li   and Lonardi, Stefano  },
	citeulike-article-id = {2821475},
	doi = {10.1007/s10618-007-0064-z},
	journal = {Data Mining and Knowledge Discovery},
	keywords = {litreview, sax},
	month = {October},
	number = {2},
	pages = {107--144},
	posted-at = {2009-04-15 03:51:17},
	priority = {2},
	title = {Experiencing {SAX}: a novel symbolic representation of time series},
	url = {http://dx.doi.org/10.1007/s10618-007-0064-z},
	volume = {15},
	year = {2007}
}



@inproceedings{citeulike:4303331,
	abstract = {Ad hoc querying is difficult on very large datasets, since it is usually not possible to have the entire dataset on disk. While compression can be used to decrease the size of the dataset, compressed data is notoriously difficult to index or access.   In this paper we consider a very large dataset comprising multiple distinct time sequences. Each point in the sequence is a numerical value. We show how to compress such a dataset into a format that supports ad hoc querying, provided that a small error can be tolerated when the data is uncompressed. Experiments on large, real world datasets ( AT\&amp;T  customer calling patterns) show that the proposed method achieves an average of less than 5\% error in any data value after compressing to a mere 2.5\% of the original space ( i.e. , a 40:1 compression ratio), with these numbers not very sensitive to dataset size. Experiments on aggregate queries achieved a 0.5\% reconstruction error with a space requirement under 2\%.},
	address = {New York, NY, USA},
	author = {Korn, Flip   and Jagadish, H. V.  and Faloutsos, Christos  },
	booktitle = {SIGMOD '97: Proceedings of the 1997 ACM SIGMOD international conference on Management of data},
	citeulike-article-id = {4303331},
	doi = {10.1145/253260.253332},
	isbn = {0-89791-911-4},
	keywords = {litreview},
	location = {Tucson, Arizona, United States},
	pages = {289--300},
	posted-at = {2009-04-12 22:00:21},
	priority = {2},
	publisher = {ACM},
	title = {Efficiently supporting ad hoc queries in large datasets of time sequences},
	url = {http://dx.doi.org/10.1145/253260.253332},
	year = {1997}
}



@inproceedings{citeulike:4295248,
	abstract = {Key words numerical time sequence, subsequence matching, indexing structure, a sequence of linear segments 1},
	author = {Morinaka, Y.  and Yoshikawa, M.  and Amagasa, T.  and Uemura, S. },
	citeulike-article-id = {4295248},
	journal = {PAKDD},
	keywords = {litreview, similarity},
	posted-at = {2009-04-09 19:11:07},
	priority = {2},
	title = {The {L}-index: An Indexing Structure for Efficient Subsequence Matching in TimeSequence Databases},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.128.4751},
	year = {2001}
}



@inproceedings{citeulike:4165220,
	abstract = {Similarity-based search over time-series databases has been a hot research topic for a long history, which is widely used in many applications, including multimedia retrieval, data mining, web search and retrieval, and so on. However, due to high dimensionality (i.e. length) of the time series, the similarity search over directly indexed time series usually encounters a serious problem, known as the "dimensionality curse". Thus, many dimensionality reduction techniques are proposed to break such curse by reducing the dimensionality of time series. Among all the proposed methods, only  Piecewise Linear Approximation  (PLA) does not have indexing mechanisms to support similarity queries, which prevents it from efficiently searching over very large time-series databases. Our initial studies on the effectiveness of different reduction methods, however, show that PLA performs no worse than others. Motivated by this, in this paper, we re-investigate PLA for approximating and indexing time series. Specifically, we propose a novel distance function in the reduced PLA-space, and prove that this function indeed results in a lower bound of the Euclidean distance between the original time series, which can lead to no  false dismissals  during the similarity search. As a second step, we develop an effective approach to index these lower bounds to improve the search efficiency. Our extensive experiments over a wide spectrum of real and synthetic data sets have demonstrated the efficiency and effectiveness of PLA together with the newly proposed lower bound distance, in terms of both  pruning power  and  wall clock time , compared with two state-of-the-art reduction methods,  Adaptive Piecewise Constant Approximation  (APCA) and  Chebyshev Polynomials  (CP).},
	author = {Chen, Qiuxia   and Chen, Lei   and Lian, Xiang   and Liu, Yunhao   and Yu, Jeffrey  X. },
	booktitle = {VLDB '07: Proceedings of the 33rd international conference on Very large data bases},
	citeulike-article-id = {4165220},
	isbn = {978-1-59593-649-3},
	keywords = {litreview, pla},
	location = {Vienna, Austria},
	pages = {435--446},
	posted-at = {2009-04-09 19:10:15},
	priority = {2},
	publisher = {VLDB Endowment},
	title = {Indexable {PLA} for efficient similarity search},
	url = {http://portal.acm.org/citation.cfm?id=1325851.1325903},
	year = {2007}
}



@inproceedings{citeulike:2753031,
	address = {New York, NY, USA},
	author = {Cai, Yuhan   and Ng, Raymond  },
	booktitle = {SIGMOD '04: Proceedings of the 2004 ACM SIGMOD international conference on Management of data},
	citeulike-article-id = {2753031},
	doi = {10.1145/1007568.1007636},
	isbn = {1581138598},
	keywords = {chebyshev, litreview},
	pages = {599--610},
	posted-at = {2009-04-09 19:08:19},
	priority = {2},
	publisher = {ACM},
	title = {Indexing spatio-temporal trajectories with Chebyshev polynomials},
	url = {http://dx.doi.org/10.1145/1007568.1007636},
	year = {2004}
}



@inproceedings{citeulike:3734066,
	abstract = {Considers the use of wavelet transformations as a dimensionality reduction technique to permit efficient similarity searching over high-dimensional time-series data. While numerous transformations have been proposed and studied, the only wavelet that has been shown to be effective for this application is the Haar wavelet. In this work, we observe that a large class of wavelet transformations (not only orthonormal wavelets but also bi-orthonormal wavelets) can be used to support similarity searching. This class includes the most popular and most effective wavelets being used in image compression. We present a detailed performance study of the effects of using different wavelets on the performance of similarity searching for time-series data. We include several wavelets that outperform both the Haar wavelet and the best-known non-wavelet transformations for this application. To ensure our results are usable by an application engineer, we also show how to configure an indexing strategy for the best-performing transformations. Finally, we identify classes of data that can be indexed efficiently using these wavelet transformations},
	author = {Popivanov, I.  and Miller, R. J. },
	booktitle = {Data Engineering, 2002. Proceedings. 18th International Conference on},
	citeulike-article-id = {3734066},
	doi = {10.1109/ICDE.2002.994711},
	journal = {Data Engineering, 2002. Proceedings. 18th International Conference on},
	keywords = {dwt, litreview},
	pages = {212--221},
	posted-at = {2009-04-09 19:07:19},
	priority = {2},
	title = {Similarity search over time-series data using wavelets},
	url = {http://dx.doi.org/10.1109/ICDE.2002.994711},
	year = {2002}
}



@misc{citeulike:4295242,
	abstract = {Time series data are of growing importance in many newdatabase applications, such as data warehousing and data mining [3, 8, 2, 12]. A time series (or time sequence) is a sequence ofreal numbers, each number representing a value at a time point. Typical examples include stock prices or currency exchange rates,biomedical measurements, weather data, etc... collected over time. Therefore, time series databases supporting fast retrieval oftime series data and similarity queries are desired.},
	citeulike-article-id = {4295242},
	keywords = {dwt, litreview},
	posted-at = {2009-04-09 19:05:40},
	priority = {2},
	title = {Efficient Time Series Matching by Wavelets},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.113.8628}
}



@incollection{citeulike:3973409,
	abstract = {We propose an indexing method for time sequences for processing similarity queries. We use the Discrete Fourier Transform (DFT) to map time sequences to the frequency domain, the crucial observation being that, for most sequences of practical interest, only the first few frequencies are strong. Another important observation is Parseval's theorem, which specifies that the Fourier transform preserves the Euclidean distance in the time or frequency domain. Having thus mapped sequences to a lower-dimensionality space by using only the first few Fourier coefficients, we use R * -trees to index the sequences and efficiently answer similarity queries. We provide experimental results which show that our method is superior to search based on sequential scanning. Our experiments show that a few coefficients (1–3) are adequate to provide good performance. The performance gain of our method increases with the number and length of sequences.},
	author = {Agrawal, Rakesh   and Faloutsos, Christos   and Swami, Arun  },
	citeulike-article-id = {3973409},
	doi = {10.1007/3-540-57301-1\_5},
	journal = {Foundations of Data Organization and Algorithms},
	keywords = {litreview},
	pages = {69--84},
	posted-at = {2009-04-09 19:04:28},
	priority = {2},
	title = {Efficient similarity search in sequence databases},
	url = {http://dx.doi.org/10.1007/3-540-57301-1\_5},
	year = {1993}
}



@inproceedings{citeulike:3978002,
	abstract = {The problem of efficiently locating previously known patterns in a time series database (i.e., query by content) has received much attention and may now largely be regarded as a solved problem. However, from a knowledge discovery viewpoint, a more interesting problem is the enumeration of previously unknown, frequently occurring patterns. We call such patterns “motifs,” because of their close analogy to their discrete counterparts in computation biology. An efficient motif discovery algorithm for time series would be useful as a tool for summarizing and visualizing massive time series databases. In addition, it could be used as a subroutine in various other data mining tasks, including the discovery of association rules, clustering and classification. In this work we carefully motivate, then introduce, a non-trivial definition of time series motifs. We propose an efficient algorithm to discover them, and we demonstrate the utility and efficiency of our approach on several real world datasets.},
	author = {Lin, Jessica   and Keogh, Eamonn  J.  and Lonardi, Stefano   and Patel, Pranav  },
	booktitle = {2nd Workshop on Temporal Data Mining, at the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	citeulike-article-id = {3978002},
	keywords = {dtw, similarity, thesis},
	location = {Edmonton, Alberta, Canada},
	month = {July},
	posted-at = {2009-03-05 11:22:44},
	priority = {2},
	title = {Finding Motifs in Time Series},
	year = {2002}
}



@inproceedings{citeulike:2946589,
	address = {San Francisco, CA, USA},
	author = {Yi, Byoung-Kee   and Faloutsos, Christos  },
	booktitle = {VLDB '00: Proceedings of the 26th International Conference on Very Large Data Bases},
	citeulike-article-id = {2946589},
	isbn = {1558607153},
	keywords = {dtw, litreview, paa, similarity, thesis},
	pages = {385--394},
	posted-at = {2009-02-26 13:26:35},
	priority = {2},
	publisher = {Morgan Kaufmann Publishers Inc.},
	title = {Fast Time Sequence Indexing for Arbitrary {L}p Norms},
	url = {http://portal.acm.org/citation.cfm?id=645926.671689},
	year = {2000}
}



@article{citeulike:1736140,
	address = {New York, NY, USA},
	author = {Chakrabarti, Kaushik   and Keogh, Eamonn   and Mehrotra, Sharad   and Pazzani, Michael  },
	citeulike-article-id = {1736140},
	doi = {10.1145/568518.568520},
	issn = {0362-5915},
	journal = {ACM Trans. Database Syst.},
	keywords = {dtw, gemini, litreview, paa, similarity, thesis},
	month = {June},
	number = {2},
	pages = {188--228},
	posted-at = {2009-02-26 13:25:29},
	priority = {2},
	publisher = {ACM Press},
	title = {Locally adaptive dimensionality reduction for indexing large time series databases},
	url = {http://dx.doi.org/10.1145/568518.568520},
	volume = {27},
	year = {2002}
}



@inproceedings{citeulike:532335,
	address = {New York, NY, USA},
	author = {Lin, Jessica   and Keogh, Eamonn   and Lonardi, Stefano   and Chiu, Bill  },
	booktitle = {DMKD '03: Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery},
	citeulike-article-id = {532335},
	doi = {10.1145/882082.882086},
	keywords = {dtw, litreview, similarity, thesis},
	pages = {2--11},
	posted-at = {2009-02-26 12:51:56},
	priority = {2},
	publisher = {ACM Press},
	title = {A symbolic representation of time series, with implications for streaming algorithms},
	url = {http://dx.doi.org/10.1145/882082.882086},
	year = {2003}
}



@incollection{citeulike:4107287,
	abstract = {In this digital age, great interest has been shifted toward multimedia data manipulations. This includes videos, images, and audios, where typical manipulations require fairly large storage and are computationally intensive. Recent research has demonstrated the utilities of time series representation in various data mining tasks, allowing considerable reduction in time and space complexity. Specifically, the utilities of Uniform Scaling (US) and Dynamic Time Warping (DTW) have been shown to be necessary in several human-related domains, where uniform stretching or shrinking, as well as some local variation are typical. Classic examples include a query-by-humming system and motion capture data. However, all the past work has neglected the importance of data normalization before distance calculations, and therefore does not guarantee accurate retrievals. In this work, we discuss this concern and present a technique that accurately and efficiently searches under the US with DTW for normalized time series data, where no-false-dismissals are guaranteed.},
	author = {Euachongprasit, Waiyawuth   and Ratanamahatana, Chotirat  },
	citeulike-article-id = {4107287},
	doi = {10.1007/978-3-540-68125-0\_11},
	journal = {Advances in Knowledge Discovery and Data Mining},
	pages = {100--111},
	posted-at = {2009-02-26 12:45:40},
	priority = {2},
	title = {Accurate and Efficient Retrieval of Multimedia Time Series Data Under Uniform Scaling and Time Warping},
	url = {http://dx.doi.org/10.1007/978-3-540-68125-0\_11},
	year = {2008}
}



@inproceedings{citeulike:4043115,
	abstract = {Dynamic Time Warping (DTW) is a pattern matching approach that can be used for limited vocabulary speech recognition, which is based on a temporal alignment of the input signal with the template models. The main drawback of this method is its high computational cost when the length of the signals increases. This paper presents a modified ver- sion of the DTW, based on the Discrete Wavelet Transform (DWT), that reduces its original complexity. Many wavelet families with different support-sizes are experimented and the corresponding results are reported.},
	author = {Junior, Sylvio  B.  and Guido, Rodrigo  C.  and Chen, Shi-Huang   and Vieira, Lucimar  S.  and Sanchez, Fabricio  L. },
	booktitle = {Multimedia Workshops, 2007. ISMW '07. Ninth IEEE International Symposium on},
	citeulike-article-id = {4043115},
	doi = {10.1109/ISM.Workshops.2007.51},
	journal = {Multimedia Workshops, 2007. ISMW '07. Ninth IEEE International Symposium on},
	keywords = {dtw, litreview, thesis},
	pages = {256--263},
	posted-at = {2009-02-13 11:04:31},
	priority = {2},
	title = {Improved Dynamic Time Warping Based on the Discrete Wavelet Transform},
	url = {http://dx.doi.org/10.1109/ISM.Workshops.2007.51},
	year = {2007}
}



@inproceedings{citeulike:4036334,
	abstract = {A variety of techniques currently exist for measuring the similarity between time series datasets. Of these techniques, the methods whose matching criteria is bounded by a specified \&\#949; threshold value, such as the LCSS and the EDR techniques, have been shown to be robust in the presence of noise, time shifts, and data scaling. Our work proposes a new algorithm, called the Fast Time Series Evaluation (FTSE) method, which can be used to evaluate such threshold value techniques, including LCSS and EDR. Using FTSE, we show that these techniques can be evaluated faster than using either traditional dynamic programming or even warp-restricting methods such as the Sakoe-Chiba band and the Itakura Parallelogram.},
	address = {New York, NY, USA},
	author = {Morse, Michael  D.  and Patel, Jignesh  M. },
	booktitle = {SIGMOD '07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data},
	citeulike-article-id = {4036334},
	doi = {10.1145/1247480.1247544},
	isbn = {978-1-59593-686-8},
	keywords = {dtw, lcs, litreview, thesis},
	location = {Beijing, China},
	pages = {569--580},
	posted-at = {2009-02-11 18:02:03},
	priority = {2},
	publisher = {ACM},
	title = {An efficient and accurate method for evaluating time series similarity},
	url = {http://dx.doi.org/10.1145/1247480.1247544},
	year = {2007}
}



@article{citeulike:4031866,
	abstract = {We address the handling of time series search based on two important distance definitions: Euclidean distance and time warping distance. The conventional method reduces the dimensionality by means of a discrete Fourier transform. We apply the Haar wavelet transform technique and propose the use of a proper normalization so that the method can guarantee no false dismissal for Euclidean distance. We found that this method has competitive performance from our experiments. Euclidean distance measurement cannot handle the time shifts of patterns. It fails to match the same rise and fall patterns of sequences with different scales. A distance measure that handles this problem is the time warping distance. However, the complexity of computing the time warping distance function is high. Also, as time warping distance is not a metric, most indexing techniques would not guarantee any false dismissal. We propose efficient strategies to mitigate the problems of time warping. We suggest a Haar wavelet-based approximation function for time warping distance, called Low Resolution Time Warping, which results in less computation by trading off a small amount of accuracy. We apply our approximation function to similarity search in time series databases, and show by experiment that it is highly effective in suppressing the number of false alarms in similarity search.},
	author = {Chan, F. K. P.  and Fu, A. W. C.  and Yu, C. },
	booktitle = {Knowledge and Data Engineering, IEEE Transactions on},
	citeulike-article-id = {4031866},
	doi = {10.1109/TKDE.2003.1198399},
	journal = {Knowledge and Data Engineering, IEEE Transactions on},
	keywords = {dtw, litreview, similarity, thesis, wavelet},
	number = {3},
	pages = {686--705},
	posted-at = {2009-02-10 21:43:38},
	priority = {2},
	title = {Haar wavelets for efficient similarity search of time-series: with and without time warping},
	url = {http://dx.doi.org/10.1109/TKDE.2003.1198399},
	volume = {15},
	year = {2003}
}



@inproceedings{citeulike:4031865,
	abstract = {Mining time series data is an important approach for the analysis in many application areas as diverse as biology, environmental research, medicine, or stock chart analysis. As nearly all data mining tasks on this kind of data depend on a distance function between two time series, a huge number of such functions has been developed during the last decades. The introduction of threshold-based distance functions presented a new concept of time series similarity and these functions were applied to data mining techniques on a wide spectrum of time series data. In this demonstration, we present the Java toolkit T-Time which is able to perform several data mining tasks for a complete range of threshold values in an interactive way. The results are visually presented in a very concise way so that the user can easily identify important threshold values. Combined with domain-specific knowledge, these pivotal values can yield novel insights beyond the means of the underlying data mining techniques the analysis is based on.},
	author = {Assfalg, J.  and Kriegel, H. P.  and Kroger, P.  and Kunath, P.  and Pryakhin, A.  and Renz, M. },
	booktitle = {Data Engineering, 2008. ICDE 2008. IEEE 24th International Conference on},
	citeulike-article-id = {4031865},
	doi = {10.1109/ICDE.2008.4497636},
	journal = {Data Engineering, 2008. ICDE 2008. IEEE 24th International Conference on},
	keywords = {dtw, litreview, similarity, thesis},
	pages = {1620--1623},
	posted-at = {2009-02-10 21:41:29},
	priority = {2},
	title = {T-Time: Threshold-Based Data Mining on Time Series},
	url = {http://dx.doi.org/10.1109/ICDE.2008.4497636},
	year = {2008}
}



@article{citeulike:3357064,
	author = {Rinzivillo, Salvatore   and Pedreschi, Dino   and Nanni, Mirco   and Giannotti, Fosca   and Andrienko, Natalia   and Andrienko, Gennady  },
	citeulike-article-id = {3357064},
	doi = {10.1057/palgrave.ivs.9500183},
	issn = {1473-8716},
	journal = {Information Visualization},
	keywords = {litreview, publication, similarity, thesis},
	number = {3-4},
	pages = {225--239},
	posted-at = {2009-02-10 21:31:06},
	priority = {2},
	publisher = {Palgrave Macmillan},
	title = {Visually driven analysis of movement data by progressive clustering},
	url = {http://dx.doi.org/10.1057/palgrave.ivs.9500183},
	volume = {7},
	year = {2008}
}



@article{citeulike:2427286,
	address = {New York, NY, USA},
	author = {Schreck, Tobias   and Teku\vsov\'a, Tatiana   and Kohlhammer, J\"orn   and Fellner, Dieter  },
	citeulike-article-id = {2427286},
	doi = {10.1145/1345448.1345454},
	issn = {1931-0145},
	journal = {SIGKDD Explor. Newsl.},
	keywords = {dtw, litreview, similarity, thesis},
	month = {December},
	number = {2},
	pages = {30--37},
	posted-at = {2009-02-10 21:26:54},
	priority = {2},
	publisher = {ACM},
	title = {Trajectory-based visual analysis of large financial time series data},
	url = {http://dx.doi.org/10.1145/1345448.1345454},
	volume = {9},
	year = {2007}
}



@inproceedings{citeulike:4025073,
	abstract = {Time series data poses a significant variation to the traditional segmentation techniques of data mining because the observation is derived from multiple instances of the same underlying record. Additionally, the standard segmentation methods employed in traditional clustering require instances to be classified exactly by attaching an event to a specific cluster at the exclusion of other clusters. This paper is an investigation into the predictive power of the clustering technique on stock market data and its ability to provide stock predictions that can be utilised in strategies that outperform the underlying market. This uses a brute force approach to the prediction of stock prices based on the formation of a cluster around the query sequence. The prediction is then applied in a model designed to capitalise on the derived prediction. The predictive accuracy of minimum distance clusters produced promising results with a prediction error incorporated into the forecast strategy.},
	address = {Darlinghurst, Australia, Australia},
	author = {Nayak, Richi   and Braak, Paul  T. },
	booktitle = {AIDM '07: Proceedings of the 2nd international workshop on Integrating artificial intelligence and data mining},
	citeulike-article-id = {4025073},
	isbn = {978-1-920682-65-1},
	keywords = {litreview, similarity, thesis},
	location = {Gold Coast, Australia},
	pages = {95--103},
	posted-at = {2009-02-09 09:56:43},
	priority = {2},
	publisher = {Australian Computer Society, Inc.},
	title = {Temporal pattern matching for the prediction of stock prices},
	url = {http://portal.acm.org/citation.cfm?id=1386993.1387003},
	year = {2007}
}



@article{citeulike:973785,
	address = {New York, NY, USA},
	author = {Li, Tao   and Li, Qi   and Zhu, Shenghuo   and Ogihara, Mitsunori  },
	citeulike-article-id = {973785},
	doi = {10.1145/772862.772870},
	issn = {1931-0145},
	journal = {SIGKDD Explor. Newsl.},
	keywords = {litreview, thesis, wavelet},
	month = {December},
	number = {2},
	pages = {49--68},
	posted-at = {2009-02-09 09:54:19},
	priority = {2},
	publisher = {ACM Press},
	title = {A survey on wavelet applications in data mining},
	url = {http://dx.doi.org/10.1145/772862.772870},
	volume = {4},
	year = {2002}
}



@article{citeulike:2693625,
	address = {New York, NY, USA},
	author = {Maier, David  },
	citeulike-article-id = {2693625},
	doi = {10.1145/322063.322075},
	issn = {0004-5411},
	journal = {J. ACM},
	keywords = {lcs, litreview, thesis},
	month = {April},
	number = {2},
	pages = {322--336},
	posted-at = {2009-02-09 09:33:53},
	priority = {2},
	publisher = {ACM},
	title = {The Complexity of Some Problems on Subsequences and Supersequences},
	url = {http://dx.doi.org/10.1145/322063.322075},
	volume = {25},
	year = {1978}
}



@inproceedings{citeulike:4024793,
	abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article.  ACM has opted to expose the complete List rather than only correct and linked references.},
	address = {New York, NY, USA},
	author = {Hadlock, F. },
	booktitle = {IEA/AIE '88: Proceedings of the 1st international conference on Industrial and engineering applications of artificial intelligence and expert systems},
	citeulike-article-id = {4024793},
	doi = {10.1145/55674.55676},
	isbn = {0-89791-271-3},
	keywords = {lcs, litreview, thesis},
	location = {Tullahoma, Tennessee, United States},
	pages = {645--653},
	posted-at = {2009-02-09 09:31:17},
	priority = {2},
	publisher = {ACM},
	title = {An efficient algorithm for pattern detection and classification},
	url = {http://dx.doi.org/10.1145/55674.55676},
	year = {1988}
}



@inproceedings{citeulike:2659746,
	abstract = {The aim of this paper is to give a comprehensive comparison of well-known longest common subsequence algorithms (for two input strings) and study their behaviour in various application environments. The performance of the methods depends heavily on the properties of the problem instance as well as the supporting data structures used in the implementation. We want to make also a clear distinction between methods that determine the actual lcs and those calculating only its length, since the execution time and more importantly, the space demand depends crucially on the type of the task. To our knowledge, this is the first time this kind of survey has been done. Due to the page limits, the paper gives only a coarse overview of the performance of the algorithms; more detailed studies are reported elsewhere},
	author = {Bergroth, L.  and Hakonen, H.  and Raita, T. },
	booktitle = {String Processing and Information Retrieval, 2000. SPIRE 2000. Proceedings. Seventh International Symposium on},
	citeulike-article-id = {2659746},
	doi = {10.1109/SPIRE.2000.878178},
	journal = {String Processing and Information Retrieval, 2000. SPIRE 2000. Proceedings. Seventh International Symposium on},
	keywords = {lcs, litreview, thesis},
	pages = {39--48},
	posted-at = {2009-02-09 09:26:20},
	priority = {2},
	title = {A survey of longest common subsequence algorithms},
	url = {http://dx.doi.org/10.1109/SPIRE.2000.878178},
	year = {2000}
}



@inproceedings{citeulike:4022060,
	abstract = {Monitoring predefined patterns in streaming time series is useful to applications such as trend-related analysis, sensor networks and video surveillance. Most current studies on such monitoring employ Euclidean distance to calculate the similarities between given query patterns and subsequences of streaming time series. Euclidean distance has been shown to be ineffective in measuring distances of time series in which shifting and scaling usually exist. Consequently, warping distances such as dynamic time warping (DTW), longest common subsequence (LCSS), have been proposed to handle warps in temporal dimension. However, they are inadequate in handling shifting and scaling in amplitude dimension. Moreover, they have been designed mainly for full sequence matching, whereas in online monitoring applications, we typically have no knowledge on the positions and lengths of possible matching subsequences. In this paper, we first discuss the weaknesses of existing warping distances on detecting patterns from streaming time series. We then propose a novel warping distance, which we name Spatial Assembling Distance (SpADe), that is able to handle shifting and scaling in both temporal and amplitude dimensions. We further propose an efficient approach for continuous pattern detection using SpADe, that is fundamental for subsequence matching on streaming data. Finally, our experimental results show that SpADe is effective and efficient for continuous pattern detection in streaming time series.},
	author = {Chen, Yueguo   and Nascimento, M. A.  and Ooi, Beng  C.  and Tung, A. K. H. },
	booktitle = {Data Engineering, 2007. ICDE 2007. IEEE 23rd International Conference on},
	citeulike-article-id = {4022060},
	doi = {10.1109/ICDE.2007.367924},
	journal = {Data Engineering, 2007. ICDE 2007. IEEE 23rd International Conference on},
	keywords = {dtw, litreview, thesis},
	pages = {786--795},
	posted-at = {2009-02-07 17:10:12},
	priority = {2},
	title = {SpADe: On Shape-based Pattern Detection in Streaming Time Series},
	url = {http://dx.doi.org/10.1109/ICDE.2007.367924},
	year = {2007}
}



@inproceedings{citeulike:4022058,
	abstract = {Matching video segments in order to detect their similarity is a necessary task in retrieval and summarization applications. In order to determine nearly identical content, such as repeated takes of the same scene, very precise matching of sequences of features extracted from the video segments needs to be performed. In this paper we compare the performance of three distance measures for the task of clustering multiple takes of the same scene: dynamic time warping (DTW) and two variants of longest common subsequence (LCSS). We also evaluate the influence of the quality of the input segmentation on the performance of the algorithms.},
	author = {Bailer, W. },
	booktitle = {Database and Expert Systems Application, 2008. DEXA '08. 19th International Conference on},
	citeulike-article-id = {4022058},
	doi = {10.1109/DEXA.2008.26},
	journal = {Database and Expert Systems Application, 2008. DEXA '08. 19th International Conference on},
	pages = {595--599},
	posted-at = {2009-02-07 17:06:10},
	priority = {2},
	title = {A Comparison of Distance Measures for Clustering Video Sequences},
	url = {http://dx.doi.org/10.1109/DEXA.2008.26},
	year = {2008}
}



@article{citeulike:4021682,
	abstract = {In a way similar to the string-to-string correction problem, we address discrete time series similarity in light of a time-series-to-time-series-correction problem for which the similarity between two time series is measured as the minimum cost sequence of edit operations needed to transform one time series into another. To define the edit operations, we use the paradigm of a graphical editing process and end up with a dynamic programming algorithm that we call time warp edit distance (TWED). TWED is slightly different in form from dynamic time warping (DTW), longest common subsequence (LCSS), or edit distance with real penalty (ERP) algorithms. In particular, it highlights a parameter that controls a kind of stiffness of the elastic measure along the time axis. We show that the similarity provided by TWED is a potentially useful metric in time series retrieval applications since it could benefit from the triangular inequality property to speed up the retrieval process while tuning the parameters of the elastic measure. In that context, a lower bound is derived to link the matching of time series into down sampled representation spaces to the matching into the original space. The empiric quality of the TWED distance is evaluated on a simple classification task. Compared to edit distance, DTW, LCSS, and ERP, TWED has proved to be quite effective on the considered experimental task.},
	author = {Marteau, P. F. },
	booktitle = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
	citeulike-article-id = {4021682},
	doi = {10.1109/TPAMI.2008.76},
	journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
	number = {2},
	pages = {306--318},
	posted-at = {2009-02-07 14:07:24},
	priority = {2},
	title = {Time Warp Edit Distance with Stiffness Adjustment for Time Series Matching},
	url = {http://dx.doi.org/10.1109/TPAMI.2008.76},
	volume = {31},
	year = {2009}
}



@book{citeulike:1454223,
	abstract = {{A timeless classic in how complex information should be presented graphically.  The Strunk \& White  of visual design. Should occupy a place of honor--within arm's reach--of everyone  attempting to understand or depict numerical data graphically.  The design of the book is an exemplar of the principles it espouses:  elegant typography and layout, and seamless integration of lucid text and perfectly chosen graphical examples. Very Highly Recommended.}},
	author = {Tufte, Edward  R. },
	citeulike-article-id = {1454223},
	howpublished = {Hardcover},
	isbn = {096139210X},
	keywords = {dtw, litreview, thesis},
	posted-at = {2009-02-06 14:23:45},
	priority = {2},
	publisher = {{Graphics Press}},
	title = {The Visual Display of Quantitative Information},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/096139210X}
}



@inproceedings{citeulike:4015923,
	abstract = {Visualisations play an important part in the development of ideas. They make ingredients and relations explicit, guide thinking processes of the designer or scientist, and support communications, often across the boundaries of disciplines. In the fields of cognitive psychology and human-machine interaction, block diagrams have been a dominant means for representing cognitive systems. However, we believe that this form of representation may constrain how we think about cognition in undesirable ways. This form of representation biases viewers to see cognition as a sequential, step-by-step process, under emphasizing the dynamical properties of closed-loop, adaptive processes. Block diagrams emphasize activity and internal mental operations (awareness) and occlude the ecological or work domain (situational) constraints},
	author = {Stappers, P. J.  and Flach, J. M. },
	booktitle = {Systems, Man and Cybernetics, 2004 IEEE International Conference on},
	citeulike-article-id = {4015923},
	doi = {10.1109/ICSMC.2004.1398404},
	journal = {Systems, Man and Cybernetics, 2004 IEEE International Conference on},
	keywords = {dtw, litreview, thesis},
	pages = {821--826 vol.1},
	posted-at = {2009-02-06 14:04:52},
	priority = {2},
	title = {Visualizing cognitive systems: getting past block diagrams},
	url = {http://dx.doi.org/10.1109/ICSMC.2004.1398404},
	volume = {1},
	year = {2004}
}



@article{citeulike:3994742,
	author = {Tinbergen, J. },
	citeulike-article-id = {3994742},
	journal = {Zeitschrift f\"{u}r National- \"{o}konomie 1},
	pages = {669--679},
	posted-at = {2009-02-02 11:58:41},
	priority = {2},
	title = {Bestimmung und Deutung von Angebotskurven. Ein Beispiel.},
	year = {1930}
}



@misc{citeulike:3994561,
	author = {Hanau, A. },
	citeulike-article-id = {3994561},
	institution = {Vierteljahrshefte zur Konjunkturforschung 7},
	keywords = {dtw, litreview, thesis},
	posted-at = {2009-02-02 11:40:50},
	priority = {2},
	publisher = {Vierteljahrshefte zur Konjunkturforschung 7},
	title = {Die Prognose der Schweinepreise},
	year = {1928}
}



@misc{citeulike:3994263,
	author = {Koopmans, T. C. },
	citeulike-article-id = {3994263},
	institution = {Netherlands Economic Institute},
	keywords = {dtw, litreview, thesis},
	location = {Haarlem},
	posted-at = {2009-02-02 11:33:05},
	priority = {2},
	publisher = {Netherlands Economic Institute},
	title = {Linear Regression Analysis of Economic Time Series},
	year = {1937}
}



@inproceedings{citeulike:1237593,
	author = {Fu, Ada  W.  and Keogh, Eamonn   and Leo and Ratanamahatana, Chotirat  A. },
	booktitle = {VLDB '05: Proceedings of the 31st international conference on Very large data bases},
	citeulike-article-id = {1237593},
	isbn = {1595931546},
	pages = {649--660},
	posted-at = {2009-02-01 20:27:56},
	priority = {2},
	publisher = {VLDB Endowment},
	title = {Scaling and time warping in time series querying},
	url = {http://portal.acm.org/citation.cfm?id=1083592.1083668},
	year = {2005}
}



@article{citeulike:1082000,
	abstract = {Senior Member-Walid G. Aref and Senior Member-Ahmed K. Elmagarmid},
	address = {Piscataway, NJ, USA},
	author = {Elfeky, Mohamed  G. },
	citeulike-article-id = {1082000},
	doi = {10.1109/TKDE.2005.114},
	issn = {1041-4347},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	month = {July},
	number = {7},
	pages = {875--887},
	posted-at = {2009-02-01 20:11:27},
	priority = {2},
	publisher = {IEEE Educational Activities Department},
	title = {Periodicity Detection in Time Series Databases},
	url = {http://dx.doi.org/10.1109/TKDE.2005.114},
	volume = {17},
	year = {2005}
}



@inproceedings{citeulike:825581,
	address = {New York, NY, USA},
	author = {Faloutsos, Christos   and Ranganathan, M.  and Manolopoulos, Yannis  },
	booktitle = {SIGMOD '94: Proceedings of the 1994 ACM SIGMOD international conference on Management of data},
	citeulike-article-id = {825581},
	doi = {10.1145/191839.191925},
	issn = {0163-5808},
	keywords = {dtw, litreview, thesis},
	pages = {419--429},
	posted-at = {2009-02-01 20:11:02},
	priority = {2},
	publisher = {ACM Press},
	title = {Fast subsequence matching in time-series databases},
	url = {http://dx.doi.org/10.1145/191839.191925},
	year = {1994}
}



@article{citeulike:3991527,
	author = {Young, P.  and Shellswell, S. },
	booktitle = {Automatic Control, IEEE Transactions on},
	citeulike-article-id = {3991527},
	journal = {Automatic Control, IEEE Transactions on},
	keywords = {dtw, litreview, thesis},
	number = {2},
	pages = {281--283},
	posted-at = {2009-02-01 16:52:35},
	priority = {2},
	title = {Time series analysis, forecasting and control},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1099963},
	volume = {17},
	year = {1972}
}



@book{citeulike:3449765,
	abstract = {Time Series Analysis With Applications in R, Second Edition, presents an
accessible approach to understanding time series models and their
applications. Although the emphasis is on time domain ARIMA models and their
analysis, the new edition devotes two chapters to the frequency domain and
three to time series regression models, models for heteroscedasticity, and
threshold models. All of the ideas and methods are illustrated with both real
and simulated data sets.

A unique feature of this edition is its integration with the R computing
environment. The tables and graphical displays are accompanied by the R
commands used to produce them. An extensive R package, TSA, which contains
many new or revised R functions and all of the data used in the book,
accompanies the written text. Script files of R commands for each chapter are
available for download. There is also an extensive appendix in the book that
leads the reader through the use of R commands and the new R package to carry
out the analyses.},
	author = {Cryer, Jonathan  D.  and Chan, Kung-Sik  },
	citeulike-article-id = {3449765},
	edition = {2nd},
	howpublished = {Hardcover},
	isbn = {0387759581},
	keywords = {dtw, litreview, thesis},
	month = {October},
	posted-at = {2009-02-01 12:18:27},
	priority = {2},
	publisher = {Springer},
	title = {Time Series Analysis: With Applications in R (Springer Texts in Statistics)},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0387759581},
	year = {2008}
}



@book{citeulike:2206845,
	abstract = {{<B>Time Series: Theory and Methods</B> is a systematic account of linear time series models and their application to the modelling and prediction of data collected sequentially in time. The aim is to provide specific techniques for handling data and at the same time to provide a thorough understanding of the mathematical basis for techniques. Both time and frequency domain methods are discussed, but the book is written in such a way that either approach could be emphasized. The book intended to be a text for graduate students in statistics, mathematics, engineering, and the natural or social sciences. It contains substantial chapters on multivariate series and state-space models (including applications of the Kalman recursions to missing-value problems) and shorter accounts of special topics including long-range dependence, infinite variance processes and non-linear models. Most of the programs used in the book are available on diskettes for the IBM-PC. These diskettes, with the accompanying manual, <I>ITSM: The Interactive Time Series</I> <I>Modelling </I> <I>Package</I> <I> for the</I> <I>PC</I>, also by Brockwell and Davis, can be purchased from Springer-Verlag.}},
	author = {Brockwell, Peter  J.  and Davis, Richard  A. },
	citeulike-article-id = {2206845},
	howpublished = {Hardcover},
	isbn = {0387974296},
	keywords = {dtw, litreview, thesis},
	month = {September},
	posted-at = {2009-02-01 11:02:02},
	priority = {2},
	publisher = {Springer},
	title = {Time Series: Theory and Methods (Springer Series in Statistics)},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0387974296},
	year = {1998}
}



@book{citeulike:3991239,
	abstract = {This book presents modern developments in time series econometrics that are
applied to macroeconomic and financial time series. It attempts to bridge the
gap between methods and realistic applications. This book contains the most
important approaches to analyse time series which may be stationary or
nonstationary. Modelling and forecasting univariate time series is the
starting point. For multiple stationary time series Granger causality tests
and vector autoregressive models are presented. For real applied work the
modelling of nonstationary uni- or multivariate time series is most important.
Therefore, unit root and cointegration analysis as well as vector error
correction models play a central part. Modelling volatilities of financial
time series with autoregressive conditional heteroskedastic models is also
treated.},
	author = {Kirchg\"{a}ssner, Gebhard   and Wolters, J\"{u}rgen  },
	citeulike-article-id = {3991239},
	edition = {1},
	howpublished = {Hardcover},
	isbn = {354073290X},
	keywords = {dtw, litreview, thesis},
	month = {October},
	posted-at = {2009-02-01 10:47:08},
	priority = {2},
	publisher = {Springer},
	title = {Introduction to Modern Time Series Analysis},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/354073290X},
	year = {2007}
}



@book{citeulike:3991222,
	abstract = {This monograph of carefully collected articles reviews recent developments in
theoretical and applied statistical science, highlights current noteworthy
results and illustrates their applications; and points out possible new
directions to pursue. With its enlightening account of statistical discoveries
and its numerous figures and tables, Probability and Statistical Models with
Applications is a must read for probabilists and theoretical and applied
statisticians.},
	citeulike-article-id = {3991222},
	edition = {1},
	howpublished = {Hardcover},
	isbn = {1584881240},
	keywords = {dtw, litreview, thesis},
	month = {September},
	posted-at = {2009-02-01 10:12:49},
	priority = {2},
	publisher = {Chapman \& Hall/CRC},
	title = {Probability and Statistical Models with Applications},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/1584881240},
	year = {2000}
}



@book{citeulike:3989988,
	abstract = {The Wiley Classics Library consists of selected books that have become
recognized classics in their respective fields. With these new unabridged and
inexpensive editions, Wiley hopes to extend the life of these important works
by making them available to future generations of mathematicians and
scientists. Currently available in the Series: T. W. Anderson Statistical
Analysis of Time Series T. S. Arthanari \& Yadolah Dodge Mathematical
Programming in Statistics Emil Artin Geometric Algebra Norman T. J. Bailey The
Elements of Stochastic Processes with Applications to the Natural Sciences
George E. P. Box \& George C. Tiao Bayesian Inference in Statistical Analysis
R. W. Carter Simple Groups of Lie Type William G. Cochran \& Gertrude M. Cox
Experimental Designs, Second Edition Richard Courant Differential and Integral
Calculus, Volume I Richard Courant Differential and Integral Calculus, Volume
II Richard Courant \& D. Hilbert Methods of Mathematical Physics, Volume I
Richard Courant \& D. Hilbert Methods of Mathematical Physics, Volume II D. R.
Cox Planning of Experiments Harold M. S. Coxeter Introduction to Modern
Geometry, Second Edition Charles W. Curtis \& Irving Reiner Representation
Theory of Finite Groups and Associative Algebras Charles W. Curtis \& Irving
Reiner Methods of Representation Theory with Applications to Finite Groups and
Orders, Volume I Charles W. Curtis \& Irving Reiner Methods of Representation
Theory with Applications to Finite Groups and Orders, Volume II Bruno de
Finetti Theory of Probability, Volume 1 Bruno de Finetti Theory of
Probability, Volume 2 W. Edwards Deming Sample Design in Business Research
Amos de Shalit \& Herman Feshbach Theoretical Nuclear Physics, Volume 1
—Nuclear Structure J. L. Doob Stochastic Processes Nelson Dunford \& Jacob T.
Schwartz Linear Operators, Part One, General Theory Nelson Dunford \& Jacob T.
Schwartz Linear Operators, Part Two, Spectral Theory—Self Adjoint Operators in
Hilbert Space Nelson Dunford \& Jacob T. Schwartz Linear Operators, Part Three,
Spectral Operators Herman Fsehbach Theoretical Nuclear Physics: Nuclear
Reactions Bernard Friedman Lectures on Applications-Oriented Mathematics
Gerald d. Hahn \& Samuel S. Shapiro Statistical Models in Engineering Morris H.
Hansen, William N. Hurwitz \& William G. Madow Sample Survey Methods and
Theory, Volume I—Methods and Applications Morris H. Hansen, William N. Hurwitz
\& William G. Madow Sample Survey Methods and Theory, Volume II—Theory Peter
Henrici Applied and Computational Complex Analysis, Volume 1—Power
Series—lntegration—Conformal Mapping—Location of Zeros Peter Henrici Applied
and Computational Complex Analysis, Volume 2—Special Functions—Integral
Transforms—Asymptotics—Continued Fractions Peter Henrici Applied and
Computational Complex Analysis, Volume 3—Discrete Fourier Analysis—Cauchy
Integrals—Construction of Conformal Maps—Univalent Functions Peter Hilton \&
Yel-Chiang Wu A Course in Modern Algebra Harry Hochetadt Integral Equations
Erwin O. Kreyezig Introductory Functional Analysis with Applications William
H. Louisell Quantum Statistical Properties of Radiation All Hasan Nayfeh
Introduction to Perturbation Techniques Emanuel Parzen Modern Probability
Theory and Its Applications P.M. Prenter Splines and Variational Methods
Walter Rudin Fourier Analysis on Groups C. L. Siegel Topics in Complex
Function Theory, Volume I—Elliptic Functions and Uniformization Theory C. L.
Siegel Topics in Complex Function Theory, Volume II—Automorphic and Abelian
integrals C. L Siegel Topics in Complex Function Theory, Volume III—Abelian
Functions \& Modular Functions of Several Variables J. J. Stoker Differential
Geometry J. J. Stoker Water Waves: The Mathematical Theory with Applications
J. J. Stoker Nonlinear Vibrations in Mechanical and Electrical Systems},
	author = {Anderson, T. W. },
	citeulike-article-id = {3989988},
	edition = {First},
	howpublished = {Paperback},
	isbn = {0471047457},
	keywords = {dtw, litreview, thesis},
	month = {June},
	posted-at = {2009-02-01 10:02:38},
	priority = {2},
	publisher = {Wiley-Interscience},
	title = {The Statistical Analysis of Time Series},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0471047457},
	year = {1994}
}



@article{citeulike:2090933,
	abstract = {Time series data, due to their numerical and continuous nature, are difficult to process, analyze, and mine. However, these tasks become easier when the data can be transformed into meaningful symbols. Most recent works on time series only address how to identify a given pattern from a time series and do not consider the problem of identifying a suitable set of time points for segmenting the time series in accordance with a given set of pattern templates (e.g., a set of technical patterns for stock analysis). However, the use of fixed-length segmentation is an oversimplified approach to this problem; hence, a dynamic approach (with high controllability) is preferable so that the time series can be segmented flexibly and effectively according to the needs of the users and the applications. In view of the fact that this segmentation problem is an optimization problem and evolutionary computation is an appropriate tool to solve it, we propose an evolutionary time series segmentation algorithm. This approach allows a sizeable set of pattern templates to be generated for mining or query. In addition, defining similarity between time series (or time series segments) is of fundamental importance in fitness computation. By identifying the perceptually important points directly from the time domain, time series segments and templates of different lengths can be compared and intuitive pattern matching can be carried out in an effective and efficient manner. Encouraging experimental results are reported from tests that segment both artificial time series generated from the combinations of pattern templates and the time series of selected Hong Kong stocks.},
	author = {Chung, Fu-Lai   and Fu, Tak-Chung   and Ng, V.  and Luk, R. W. P. },
	booktitle = {Evolutionary Computation, IEEE Transactions on},
	citeulike-article-id = {2090933},
	doi = {10.1109/TEVC.2004.832863},
	journal = {Evolutionary Computation, IEEE Transactions on},
	keywords = {dtw, litreview, thesis},
	number = {5},
	pages = {471--489},
	posted-at = {2009-01-27 14:11:56},
	priority = {2},
	title = {An evolutionary approach to pattern-based time series segmentation},
	url = {http://dx.doi.org/10.1109/TEVC.2004.832863},
	volume = {8},
	year = {2004}
}



@inproceedings{citeulike:3513035,
	abstract = {Recognition of hand-drawn shapes is an important and widely studied problem. By adopting a generative probabilistic framework we are able to formulate a robust and flexible approach to shape recognition which allows for a wide range of shapes and which can recognize new shapes from a single exemplar. It also provides meaningful probabilistic measures of model score which can be used as part of a larger probabilistic framework for interpreting a page of ink. We also show how Bayesian model comparison allows the trade-off between data fit and model complexity to be optimized automatically.},
	address = {Washington, DC, USA},
	author = {Krishnapuram, Balaji   and Bishop, Christopher  M.  and Szummer, Martin  },
	booktitle = {IWFHR '04: Proceedings of the Ninth International Workshop on Frontiers in Handwriting Recognition},
	citeulike-article-id = {3513035},
	doi = {10.1109/IWFHR.2004.46},
	isbn = {0-7695-2187-8},
	keywords = {dtw, litreview, thesis},
	pages = {20--25},
	posted-at = {2009-01-26 13:43:27},
	priority = {2},
	publisher = {IEEE Computer Society},
	title = {{G}enerative {M}odels and {B}ayesian {M}odel Comparison for Shape Recognition},
	url = {http://dx.doi.org/10.1109/IWFHR.2004.46},
	year = {2004}
}



@book{citeulike:3822011,
	abstract = {This collection of twenty-three original papers represents the first effort to
bring together the work of constraint programming researchers scattered across
multiple disciplines and across the world. The collection contributes to the
understanding of the common principles of this emerging general paradigm, the
investigation of its theoretical foundations as well as applications to real-
world computing problems. It is organized around themes of concurrency and
reactive systems, languages and environments, algorithms, computer graphics,
and artificial intelligence. Constraint programming aims at supporting a wide
range of complex applications which are often modeled naturally in terms of
constraints. Early work, in the 1960s and 1970s, made use of constraints in
computer graphics, user interfaces, and artificial intelligence. Such work
introduced a declarative component in otherwise-procedural systems to reduce
the development effort. The mid-1980s have witnessed the emergence of general-
purpose programming languages based on constraints, such as constraint logic
programming and concurrent constraint programming, with significant
applications in academia and industry. Today, an increasing number of
researchers from all over the map of computing are looking at different
aspects of this new computational paradigm.},
	citeulike-article-id = {3822011},
	howpublished = {Hardcover},
	isbn = {0262193612},
	keywords = {dtw, litreview, thesis},
	month = {May},
	posted-at = {2008-12-23 13:02:48},
	priority = {2},
	publisher = {The MIT Press},
	title = {Principles and Practice of Constraint Programming},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0262193612},
	year = {1995}
}



@inproceedings{citeulike:3821484,
	abstract = {A Query by Humming system allows the user to find a song by humming part of the tune. No musical training is needed. Previous query by humming systems have not provided satisfactory results for various reasons. Some systems have low retrieval precision because they rely on melodic contour information from the hum tune, which in turn relies on the error-prone note segmentation process. Some systems yield better precision when matching the melody directly from audio, but they are slow because of their extensive use of Dynamic Time Warping (DTW). Our approach improves both the retrieval precision and speed compared to previous approaches. We treat music as a time series and exploit and improve well-developed techniques from time series databases to index the music for fast similarity queries. We improve on existing DTW indexes technique by introducing the concept of  envelope transforms , which gives a general guideline for extending existing dimensionality reduction methods to DTW indexes. The net result is high scalability. We confirm our claims through extensive experiments.},
	address = {New York, NY, USA},
	author = {Zhu, Yunyue   and Shasha, Dennis  },
	booktitle = {SIGMOD '03: Proceedings of the 2003 ACM SIGMOD international conference on Management of data},
	citeulike-article-id = {3821484},
	doi = {10.1145/872757.872780},
	isbn = {1-58113-634-X},
	keywords = {dtw, litreview, thesis},
	location = {San Diego, California},
	pages = {181--192},
	posted-at = {2008-12-23 05:35:52},
	priority = {2},
	publisher = {ACM},
	title = {Warping indexes with envelope transforms for query by humming},
	url = {http://dx.doi.org/10.1145/872757.872780},
	year = {2003}
}



@article{citeulike:3056920,
	address = {New York, NY, USA},
	author = {Hjaltason, Gisli  R.  and Samet, Hanan  },
	citeulike-article-id = {3056920},
	doi = {10.1145/958942.958948},
	issn = {0362-5915},
	journal = {ACM Trans. Database Syst.},
	keywords = {dtw, litreview, thesis},
	month = {December},
	number = {4},
	pages = {517--580},
	posted-at = {2008-12-22 22:38:00},
	priority = {2},
	publisher = {ACM},
	title = {Index-driven similarity search in metric spaces (Survey Article)},
	url = {http://dx.doi.org/10.1145/958942.958948},
	volume = {28},
	year = {2003}
}



@inproceedings{citeulike:3816328,
	abstract = {The problem of finding patterns of interest in time series databases (query by content) is an important one, with applications in virtually every field of science. A variety of approaches have been suggested. These approaches are robust to noise, offset translation, and amplitude scaling to varying degrees. However, they are all extremely sensitive to scaling in the time axis (longitudinal scaling). We present a method for similarity search that is robust to scaling in the time axis, in addition to noise, offset translation, and amplitude scaling. The method has been tested on medical, financial, space telemetry and artificial data. Furthermore the method is exceptionally fast, with the predicted 2 to 4 orders of magnitude speedup actually observed. The method uses a piecewise linear representation of the original data. We also introduce a new algorithm which both decides the optimal number of linear segments to use, and produces the actual linear representation},
	author = {Keogh, E. },
	booktitle = {Tools with Artificial Intelligence, 1997. Proceedings., Ninth IEEE International Conference on},
	citeulike-article-id = {3816328},
	doi = {10.1109/TAI.1997.632306},
	journal = {Tools with Artificial Intelligence, 1997. Proceedings., Ninth IEEE International Conference on},
	keywords = {dtw, litreview, thesis},
	pages = {578--584},
	posted-at = {2008-12-22 01:04:00},
	priority = {2},
	title = {Fast similarity search in the presence of longitudinal scaling in time series databases},
	url = {http://dx.doi.org/10.1109/TAI.1997.632306},
	year = {1997}
}



@inproceedings{citeulike:3816327,
	abstract = {We introduce a new model of similarity of time sequences that captures the intuitive notion that two sequences should be considered similar if they have enough non-overlapping time-ordered pairs of subsequences thar are similar. The model allows the amplitude of one of the two sequences to be scaled by any suitable amount and its offset adjusted appropriately. Two subsequences are considered similar if one can be enclosed within an envelope of a specified width drawn around the other. The model also allows non-matching gaps in the matching subsequences. The matching subsequences need not be aligned along the time axis. Given this model of similarity, we present fast search techniques for discovering all similar sequences in a set of sequences. These techniques can also be used to find all (sub)sequences similar to a given sequence. We applied this matching system to the U.S. mutual funds data and discovered interesting matches.},
	author = {Agrawal, Rakesh   and Lin, King-Ip   and Sawhney, Harpreet  S.  and Shim, Kyuseok  },
	booktitle = {In VLDB},
	citeulike-article-id = {3816327},
	keywords = {dtw, litreview, thesis},
	pages = {490--501},
	posted-at = {2008-12-22 01:02:20},
	priority = {2},
	title = {Fast Similarity Search in the Presence of Noise, Scaling, and Translation in Time-Series Databases},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.4179},
	year = {1995}
}



@article{citeulike:3816322,
	abstract = {Motivation: Increasingly, biological processes are being studied   through time series of RNA expression data collected for large   numbers of genes. Because common processes may unfold at varying   rates in different experiments or individuals, methods are needed that will allow corresponding expression states in different time series to be mapped to one another.  Results: We present implementations of time warping algorithms   applicable to RNA and protein expression data and demonstrate their application to published yeast RNA expression time series. Programs executing two warping algorithms are described, a simple warping algorithm and an interpolative algorithm, along with programs that generate graphics that visually present alignment information. We show time warping to be superior to simple clustering at mapping corresponding time states. We document the impact of statistical measurement noise and sample size on the quality of time alignments,  and present issues related to statistical assessment of alignment   quality through alignment scores. We also discuss directions for algorithm improvement including development of multiple time series alignments and possible applications to causality searches and non-temporal processes ( concentration warping').  Availability: Academic implementations of alignment programs   genewarp and genewarpi and the graphics generation programs grphwarp and grphwarpi are available as Win32 system DOS box executables on our web site along with documentation on their use. The publicly available data on which they were demonstrated may be found at http://genome-www.stanford.edu/cellcycle/.   Postscript files generated by grphwarp and grphwarpi may be directly printed or viewed using GhostView software available at http://www.cs.wisc.edu/\~{}ghost/.  Contact: church@arep.med.harvard.edu  Supplementary information: http://arep.med.harvard.edu/timewarp/supplement.htm. 10.1093/bioinformatics/17.6.495},
	author = {Aach, John   and Church, George  M. },
	citeulike-article-id = {3816322},
	doi = {10.1093/bioinformatics/17.6.495},
	journal = {Bioinformatics},
	keywords = {dtw, litreview, thesis},
	month = {June},
	number = {6},
	pages = {495--508},
	posted-at = {2008-12-22 00:51:21},
	priority = {2},
	title = {Aligning gene expression time series with time warping algorithms},
	url = {http://dx.doi.org/10.1093/bioinformatics/17.6.495},
	volume = {17},
	year = {2001}
}



@incollection{citeulike:3816243,
	abstract = {Widespread interest in discovering features and trends in time- series has generated a need for tools that support interactive exploration.This paper introduces timeboxes: a powerful direct-manipulation metaphor for the specification of queries over time series datasets. Our TimeSearcher implementation of timeboxes supports interactive formulation and modification of queries, thus speeding the process of exploring time series data sets and guiding data mining.},
	author = {Hochheiser, Harry   and Shneiderman, Ben  },
	citeulike-article-id = {3816243},
	doi = {10.1007/3-540-45650-3\_38},
	journal = {Discovery Science},
	keywords = {dtw, litreview, thesis},
	pages = {441--446},
	posted-at = {2008-12-21 23:13:53},
	priority = {2},
	title = {Interactive Exploration of Time Series Data},
	url = {http://dx.doi.org/10.1007/3-540-45650-3\_38},
	year = {2001}
}



@book{citeulike:1109201,
	address = {Menlo Park, CA, USA},
	author = {Fayyad, Usama  M.  and Piatetsky-Shapiro, Gregory   and Smyth, Padhraic   and Uthurusamy, Ramasamy  },
	citeulike-article-id = {1109201},
	editor = {Fayyad, Usama  M.  and Piatetsky-Shapiro, Gregory   and Smyth, Padhraic   and Uthurusamy, Ramasamy  },
	isbn = {0262560976},
	keywords = {dtw, litreview, thesis},
	posted-at = {2008-12-21 23:12:28},
	priority = {2},
	publisher = {American Association for Artificial Intelligence},
	title = {Advances in knowledge discovery and data mining},
	url = {http://portal.acm.org/citation.cfm?id=257938},
	year = {1996}
}



@inproceedings{citeulike:3816224,
	abstract = {Sequential data is easily understood through a simple line graph, yet systems to search such data typically rely on complex interfaces or query languages. This paper presents QuerySketch, a financial database application in which graphs are used for query input as well as output. QuerySketch allows users to sketch a graph freehand, then view stocks whose price histories match the sketch. Using the same graphical format for both input and output results in an interface that is powerful, flexible, yet easy to use.},
	address = {New York, NY, USA},
	author = {Wattenberg, Martin  },
	booktitle = {CHI '01: CHI '01 extended abstracts on Human factors in computing systems},
	citeulike-article-id = {3816224},
	doi = {10.1145/634067.634292},
	isbn = {1-58113-340-5},
	keywords = {dtw, litreview},
	location = {Seattle, Washington},
	pages = {381--382},
	posted-at = {2008-12-21 22:53:35},
	priority = {2},
	publisher = {ACM},
	title = {Sketching a graph to query a time-series database},
	url = {http://dx.doi.org/10.1145/634067.634292},
	year = {2001}
}



@inproceedings{citeulike:3816213,
	abstract = {Query-by-Example is a query language for use by non-programmers querying a relational data base. In an earlier paper, the features of this language were introduced; however, it was assumed that the data base was already defined and available to the user.},
	address = {New York, NY, USA},
	author = {Zloof, Mosh\'e  M. },
	booktitle = {VLDB '75: Proceedings of the 1st International Conference on Very Large Data Bases},
	citeulike-article-id = {3816213},
	doi = {10.1145/1282480.1282482},
	keywords = {dtw, litreview, thesis},
	location = {Framingham, Massachusetts},
	pages = {1--24},
	posted-at = {2008-12-21 22:48:45},
	priority = {2},
	publisher = {ACM},
	title = {Query-by-example: the invocation and definition of tables and forms},
	url = {http://dx.doi.org/10.1145/1282480.1282482},
	year = {1975}
}



@incollection{citeulike:3815893,
	abstract = {Conceptually, the techniques of linguistic pattern recognition are largely independent of the medium, but overall performance is influenced by the preprocessing to such an extent that until a few years ago the pattern recognition step was generally viewed as a small appendix to the main body of signal processing knowledge. To this day, it remains impossible to build a serious system without paying close attention to preprocessing, and deep algorithmic work on the recognizer will often yield smaller gains than seemingly more superficial changes to the front end. In Section 9.1, we introduce a speech coding method, linear prediction, that has played an important role in practical application since the 1970s.We extend the discussion of quantization started in Section 8.1 from scalars to vectors and discuss the Fourier transform-based (homomorphic) techniques that currently dominate the field.},
	citeulike-article-id = {3815893},
	doi = {10.1007/978-1-84628-986-6\_9},
	journal = {Mathematical Linguistics},
	keywords = {dtw, litreview, thesis},
	pages = {219--246},
	posted-at = {2008-12-21 17:43:45},
	priority = {2},
	title = {Speech and handwriting},
	url = {http://dx.doi.org/10.1007/978-1-84628-986-6\_9},
	year = {2008}
}



@incollection{citeulike:3815889,
	abstract = {Efficient retrieval of time series data has gained recent attention from the research community. In particular, finding meaningful distance measurements for various applications is one of the most important issues in the field, since no single distance measurement works for all applications. In this paper, we propose a different distance measurement for time series applications based on Constraint Continuous Editing Distance (CCED) that adjusts the potential energy of each sequence for optimal similarity. Furthermore, we also propose a lower bounding distance for CCED for efficient indexing and fast retrieval, even though CCED does not satisfy triangle inequality.},
	author = {Chhieng, Van   and Wong, Raymond  },
	citeulike-article-id = {3815889},
	doi = {10.1007/978-3-540-71703-4\_51},
	journal = {Advances in Databases: Concepts, Systems and Applications},
	keywords = {dtw, litreview, thesis},
	pages = {598--610},
	posted-at = {2008-12-21 17:38:37},
	priority = {2},
	title = {Adaptive Distance Measurement for Time Series Databases},
	url = {http://dx.doi.org/10.1007/978-3-540-71703-4\_51},
	year = {2008}
}



@article{citeulike:3815887,
	abstract = {Abstract\&nbsp;\&nbsp;In this paper, we define time series query filtering, the problem of monitoring the streaming time series for a set of predefined patterns. This problem is of great practical importance given the massive volume of streaming time series available through sensors, medical patient records, financial indices and space telemetry. Since the data may arrive at a high rate and the number of predefined patterns can be relatively large, it may be impossible for the comparison algorithm to keep up. We propose a novel technique that exploits the commonality among the predefined patterns to allow monitoring at higher bandwidths, while maintaining a guarantee of no false dismissals. Our approach is based on the widely used envelope-based lower-bounding technique. As we will demonstrate on extensive experiments in diverse domains, our approach achieves tremendous improvements in performance in the offline case, and significant improvements in the fastest possible arrival rate of the data stream that can be processed with guaranteed no false dismissals. As a further demonstration of the utility of our approach, we demonstrate that it can make semisupervised learning of time series classifiers tractable.},
	author = {Wei, Li   and Keogh, Eamonn   and Van Herle, Helga   and Mafra-Neto, Agenor   and Abbott, Russell  },
	citeulike-article-id = {3815887},
	doi = {10.1007/s10115-006-0033-7},
	journal = {Knowledge and Information Systems},
	keywords = {dtw, litreview, thesis},
	month = {April},
	number = {3},
	pages = {313--344},
	posted-at = {2008-12-21 17:37:36},
	priority = {2},
	title = {Efficient query filtering for streaming time series with applications to semisupervised learning of time series classifiers},
	url = {http://dx.doi.org/10.1007/s10115-006-0033-7},
	volume = {11},
	year = {2007}
}



@incollection{citeulike:3815884,
	abstract = {It is well known that Dynamic Time Warping (DTW) is superior to Euclidean distance as a similarity measure in time series analyses. Use of DTW with the recently introduced warping window constraints and lower bounding measures has significantly increased the accuracy of time series classification while reducing the computational expense required. The warping window technique learns arbitrary constraints on the warping path while performing time series alignment. This work utilizes genetic algorithms to find the optimal warping window constraints which provide a better classification accuracy. Performance of the proposed methodology has been investigated on two problems from diverse domains with favorable results.},
	author = {Kumar, Pankaj   and Gupta, Ankur   and Jayaraman, Valadi  K.  and Kulkarni, Bhaskard  },
	citeulike-article-id = {3815884},
	doi = {10.1007/978-3-540-72960-0\_12},
	journal = {Advances in Metaheuristics for Hard Optimization},
	keywords = {dtw, litreview, thesis},
	pages = {251--261},
	posted-at = {2008-12-21 17:35:01},
	priority = {2},
	title = {Aligning Time Series with Genetically Tuned Dynamic Time Warping Algorithm},
	url = {http://dx.doi.org/10.1007/978-3-540-72960-0\_12},
	year = {2008}
}



@article{citeulike:785210,
	author = {Keogh, Eamonn   and Ratanamahatana, Chotirat  A. },
	citeulike-article-id = {785210},
	doi = {10.1007/s10115-004-0154-9},
	journal = {Knowledge and Information Systems},
	keywords = {dtw, litreview, thesis},
	month = {March},
	number = {3},
	pages = {358--386},
	posted-at = {2008-12-21 17:31:59},
	priority = {2},
	title = {Exact indexing of dynamic time warping},
	url = {http://dx.doi.org/10.1007/s10115-004-0154-9},
	volume = {7},
	year = {2005}
}



@incollection{citeulike:3815880,
	abstract = {Constraints are a natural mechanism for the specification of similarity queries on time-series data. However, to realize the expressive power of constraint programming in this context, one must provide the matching implementation technology for efficient indexing of very large data sets. In this paper, we formalize the intuitive notions of exact and approximate similarity between time-series patterns and data. Our definition of similarity extends the distance metric used in [2, 7] with invariance under a group of transformations. Our main observation is that the resulting, more expressive, set of constraint queries can be supported by a new indexing technique, which preserves all the desirable properties of the indexing scheme proposed in [2, 7].},
	author = {Goldin, Dina   and Kanellakis, Paris  },
	citeulike-article-id = {3815880},
	doi = {10.1007/3-540-60299-2\_9},
	journal = {Principles and Practice of Constraint Programming — CP '95},
	keywords = {dtw, litreview, thesis},
	pages = {137--153},
	posted-at = {2008-12-21 17:28:50},
	priority = {2},
	title = {On similarity queries for time-series data: Constraint specification and implementation},
	url = {http://dx.doi.org/10.1007/3-540-60299-2\_9},
	year = {1995}
}



@incollection{citeulike:3815871,
	citeulike-article-id = {3815871},
	doi = {10.1007/978-3-540-37014-7\_2},
	journal = {Dynamic Programming},
	keywords = {litreview},
	pages = {45--100},
	posted-at = {2008-12-21 17:15:50},
	priority = {2},
	title = {Applications of Dynamic Programming},
	url = {http://dx.doi.org/10.1007/978-3-540-37014-7\_2},
	year = {2007}
}



@inproceedings{citeulike:3815864,
	abstract = {We investigate techniques for similarity analysis of spatio-temporal trajectories for mobile objects. Such data may contain a large number of outliers, which degrade the performance of Euclidean and time warping distance. Therefore, we propose the use of non-metric distance functions based on the longest common subsequence (LCSS), in conjunction with a sigmoidal matching function. Finally, we compare these new methods to various L<sub>p</sub> norms and also to time warping distance (for real and synthetic data) and present experimental results that validate the accuracy and efficiency of our approach, especially in the presence of noise.},
	author = {Vlachos, M.  and Gunopulos, D.  and Kollios, G. },
	booktitle = {Database and Expert Systems Applications, 2002. Proceedings. 13th International Workshop on},
	citeulike-article-id = {3815864},
	journal = {Database and Expert Systems Applications, 2002. Proceedings. 13th International Workshop on},
	keywords = {dtw, litreview, thesis},
	pages = {721--726},
	posted-at = {2008-12-21 17:09:37},
	priority = {2},
	title = {Robust similarity measures for mobile object trajectories},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1045983},
	year = {2002}
}



@incollection{citeulike:2902692,
	abstract = {Similarity of objects is one of the crucial concepts in several applications, including data mining. For complex objects, similarity is nontrivial to define. In this paper we present an intuitive model for measuring the similarity between two time series. The model takes into account outliers, different scaling functions, and variable sampling rates. Using methods from computational geometry, we show that this notion of similarity can be computed in polynomial time. Using statistical approximation techniques, the algorithms can be speeded up considerably. We give preliminary experimental results that show the naturalness of the notion.},
	author = {Das, Gautam   and Gunopulos, Dimitrios   and Mannila, Heikki  },
	citeulike-article-id = {2902692},
	doi = {10.1007/3-540-63223-9\_109},
	journal = {Principles of Data Mining and Knowledge Discovery},
	keywords = {dtw, litreview, thesis},
	pages = {88--100},
	posted-at = {2008-12-21 17:07:58},
	priority = {2},
	title = {Finding similar time series},
	url = {http://dx.doi.org/10.1007/3-540-63223-9\_109},
	year = {1997}
}



@inproceedings{citeulike:2902876,
	address = {New York, NY, USA},
	author = {Gunopulos, Dimitrios   and Das, Gautam  },
	booktitle = {KDD '00: Tutorial notes of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining},
	citeulike-article-id = {2902876},
	doi = {10.1145/349093.349108},
	isbn = {1581133057},
	keywords = {dtw, litreview, thesis},
	pages = {243--307},
	posted-at = {2008-12-21 17:06:02},
	priority = {2},
	publisher = {ACM},
	title = {Time series similarity measures (tutorial PM-2)},
	url = {http://dx.doi.org/10.1145/349093.349108},
	year = {2000}
}



@inproceedings{citeulike:3815082,
	author = {Chu, Selina   and Keogh, Eamonn   and Hart, David   and Pazzani, Michael  },
	citeulike-article-id = {3815082},
	journal = {Proceedings of the Second SIAM Intl. Conf. on Data Mining},
	keywords = {litreview, thesis},
	posted-at = {2008-12-21 04:29:45},
	priority = {2},
	title = {Iterative Deepening Dynamic Time
Warping for Time Series},
	url = {http://www.siam.org/meetings/sdm02/proceedings/sdm02-12.pdf},
	year = {2002}
}



@inproceedings{citeulike:3815076,
	abstract = {After the generation of multimedia data turned digital, an explosion of interest in their data storage, retrieval, and processing has drastically increased. This includes videos, images, and audios, where we now have higher expectations in exploiting these data at hands. Typical manipulations are in some forms of video/image/audio processing, including automatic speech recognition, which require fairly large amount of storage and are computationally intensive. In our recent work, we have demonstrated the utility of time series representation in the task of clustering multimedia data using k-medoids method, which allows considerable amount of reduction in computational effort and storage space. However, k- means is a much more generic clustering method when Euclidean distance is used. In this work, we will demonstrate that unfortunately, k-means clustering will sometimes fail to give correct results, an unaware fact that may be overlooked by many researchers. This is especially the case when Dynamic Time Warping (DTW) is used as the distance measure in averaging the shape of time series. We also will demonstrate that the current averaging algorithm may not produce the real average of the time series, thus generates incorrect k-means clustering results, and then show potential causes why DTW averaging methods may not achieve meaningful clustering results. Lastly, we conclude with a suggestion of a method to potentially find the shape-based time series average that satisfies the required properties.},
	author = {Niennattrakul, V.  and Ratanamahatana, C. A. },
	booktitle = {Multimedia and Ubiquitous Engineering, 2007. MUE '07. International Conference on},
	citeulike-article-id = {3815076},
	doi = {10.1109/MUE.2007.165},
	journal = {Multimedia and Ubiquitous Engineering, 2007. MUE '07. International Conference on},
	keywords = {dtw, litreview, thesis},
	pages = {733--738},
	posted-at = {2008-12-21 04:17:26},
	priority = {2},
	title = {On Clustering Multimedia Time Series Data Using {K}-Means and {D}ynamic {T}ime {W}arping},
	url = {http://dx.doi.org/10.1109/MUE.2007.165},
	year = {2007}
}



@inproceedings{citeulike:3815040,
	abstract = {Fast similarity searching in large time sequence databases has typically used Euclidean distance as a dissimilarity metric. However, for several applications, including matching of voice, audio and medical signals (e.g., electrocardiograms), one is required to permit local accelerations and decelerations in the rate of sequences, leading to a popular, field tested dissimilarity metric called the \&ldquo;time warping\&rdquo; distance. From the indexing viewpoint, this metric presents two major challenges: (a) it does not lead to any natural indexable \&ldquo;features\&rdquo;, and (b) comparing two sequences requires time quadratic in the sequence length. To address each problem, we propose to use: (a) a modification of the so called \&ldquo;FastMap\&rdquo;, to map sequences into points, with little compromise of \&ldquo;recall\&rdquo; (typically zero); and (b) a fast linear test, to help us discard quickly many of the false alarms that FastMap will typically introduce. Using both ideas in cascade, our proposed method achieved up to an order of magnitude speed-up over sequential scanning on both real and synthetic datasets},
	author = {Yi, Byoung-Kee   and Jagadish, H. V.  and Faloutsos, C. },
	booktitle = {Data Engineering, 1998. Proceedings., 14th International Conference on},
	citeulike-article-id = {3815040},
	doi = {10.1109/ICDE.1998.655778},
	journal = {Data Engineering, 1998. Proceedings., 14th International Conference on},
	keywords = {dtw, litreview, thesis},
	pages = {201--208},
	posted-at = {2008-12-21 02:21:14},
	priority = {2},
	title = {Efficient retrieval of similar time sequences under time warping},
	url = {http://dx.doi.org/10.1109/ICDE.1998.655778},
	year = {1998}
}



@inproceedings{citeulike:3789964,
	abstract = {Abstract: Considerable effort has been put towards developing intelligent and natural interfaces between users and computer systems. This is done by means of a variety of modes of information (visual, audio, pen, etc.) either used individually or in combination. In this work, we focus on the visual sensory information to recognize human activity in form of hand-arm movements from a small, predefined vocabulary. We accomplish this task by means of a matching technique by determining the distance between the unknown input and a set of previously defined templates. A dynamic time warping (DTW) algorithm is used to perform the time alignment and normalization by computing a temporal transformation allowing the two signals to be matched. The system is trained with finite video sequences of single gesture performances whose start and end point are accurately known. Preliminary experiments are accomplished off-line and result in a recognition accuracy of up to 92\%.},
	address = {Washington, DC, USA},
	author = {Corradini, Andrea  },
	booktitle = {RATFG-RTS '01: Proceedings of the IEEE ICCV Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems (RATFG-RTS'01)},
	citeulike-article-id = {3789964},
	keywords = {dtw, litreview, thesis},
	posted-at = {2008-12-15 17:14:19},
	priority = {2},
	publisher = {IEEE Computer Society},
	title = {Dynamic Time Warping for Off-Line Recognition of a Small Gesture Vocabulary},
	url = {http://portal.acm.org/citation.cfm?id=882476.883586},
	year = {2001}
}



@inproceedings{citeulike:3789957,
	abstract = {In this paper an approach to classify hand shapes into different classes according to the similarity measures between features is proposed. We show how to use an Exploratory Data Analysis to extract novel, single feature of hand from images. Based on the obtained curve-like shape of the feature, hands are classified into one of 21 possible classes of Croatian sign language using Dynamic Time Warping and Longest Common Subsequence as similarity measures. Performance of the system was evaluated with 1260 images. Results show that high classification accuracy can be obtained from a single feature recognition and a small number of training sample.},
	author = {Kuzmanic, A.  and Zanchi, V. },
	booktitle = {EUROCON, 2007. The International Conference on "Computer as a Tool"},
	citeulike-article-id = {3789957},
	doi = {10.1109/EURCON.2007.4400350},
	journal = {EUROCON, 2007. The International Conference on "Computer as a Tool"},
	keywords = {dtw, litreview, thesis},
	pages = {264--269},
	posted-at = {2008-12-15 17:10:27},
	priority = {2},
	title = {Hand shape classification using {DTW} and {LCSS} as similarity measures for vision-based gesture recognition system},
	url = {http://dx.doi.org/10.1109/EURCON.2007.4400350},
	year = {2007}
}



@article{citeulike:2584345,
	abstract = {This survey describes the state of the art of online handwriting recognition during a period of renewed activity in the field. It is based on an extensive review of the literature, including journal articles, conference proceedings, and patents. Online versus offline recognition, digitizer technology, and handwriting properties and recognition problems are discussed. Shape recognition algorithms, preprocessing and postprocessing techniques, experimental systems, and commercial products are examined},
	author = {Tappert, C. C.  and Suen, C. Y.  and Wakahara, T. },
	booktitle = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
	citeulike-article-id = {2584345},
	doi = {10.1109/34.57669},
	journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
	keywords = {dtw, litreview, thesis},
	number = {8},
	pages = {787--808},
	posted-at = {2008-12-15 17:04:49},
	priority = {2},
	title = {The state of the art in online handwriting recognition},
	url = {http://dx.doi.org/10.1109/34.57669},
	volume = {12},
	year = {1990}
}



@article{citeulike:3789944,
	abstract = {This paper compares the current state of the art in online Japanese character recognition with techniques in western handwriting recognition. It discusses important developments in preprocessing, classification, and postprocessing for Japanese character recognition in recent years and relates them to the developments in western handwriting recognition. Comparing eastern and western handwriting recognition techniques allows learning from very different approaches and understanding the underlying common foundations of handwriting recognition. This is very important when it comes to developing compact modules for integrated systems supporting many writing systems capable of recognizing multilanguage documents.},
	author = {Jaeger, S.  and Liu, C. L.  and Nakagawa, M. },
	citeulike-article-id = {3789944},
	doi = {10.1007/s10032-003-0107-y},
	journal = {International Journal on Document Analysis and Recognition},
	keywords = {dtw, litreview, thesis},
	month = {October},
	number = {2},
	pages = {75--88},
	posted-at = {2008-12-15 16:59:51},
	priority = {2},
	title = {The state of the art in Japanese online handwriting recognition compared to techniques in western handwriting recognition},
	url = {http://dx.doi.org/10.1007/s10032-003-0107-y},
	volume = {6},
	year = {2003}
}



@article{citeulike:125682,
	author = {Yutao, Shou   and Nikos, Mamoulis   and David, Cheung  },
	citeulike-article-id = {125682},
	doi = {10.1007/s10994-005-5828-3},
	issn = {0885-6125},
	journal = {Machine Learning},
	keywords = {dtw, litreview, thesis},
	month = {February},
	number = {2-3},
	pages = {231--267},
	posted-at = {2008-12-15 16:45:58},
	priority = {2},
	publisher = {Kluwer Academic Publishers},
	title = {Fast and Exact Warping of Time Series Using Adaptive Segmental Approximations},
	url = {http://dx.doi.org/10.1007/s10994-005-5828-3},
	volume = {58},
	year = {2005}
}



@incollection{citeulike:3789897,
	abstract = {As the world has shifted towards manipulation of information and its technology, we have been increasingly overwhelmed by the amount of available multimedia data while having higher expectations to fully exploit these data at hands. One of the attempts is to develop content-based multimedia information retrieval systems, which greatly facilitate us to intuitively search by its contents; a classic example is a Query-by-Humming system. Nevertheless, typical content-based search for multimedia data usually requires a large amount of storages and is computationally intensive. Recently, time series representation has been successfully applied to a wide variety of research, including multimedia retrieval due to the great reduction in time and space complexity. Besides, an enhancement, Uniform Scaling, has been proposed and applied prior to distance calculation, as well as it has been demonstrated that Uniform Scaling can outperform Euclidean distance. These previous work on Uniform Scaling, nonetheless, overlook the importance and effects of normalisation, which make their frameworks impractical for real world data. Therefore, in this paper, we justify this importance of normalisation in multimedia data and propose an efficient solution for searching multimedia time series data under Uniform Scaling and normalisation.},
	author = {Euachongprasit, Waiyawuth   and Ratanamahatana, Chotirat  },
	citeulike-article-id = {3789897},
	doi = {10.1007/978-3-540-78646-7\_49},
	journal = {Advances in Information Retrieval},
	keywords = {dtw, litreview, thesis},
	pages = {506--513},
	posted-at = {2008-12-15 16:16:06},
	priority = {2},
	title = {Efficient Multimedia Time Series Data Retrieval Under Uniform Scaling and Normalisation},
	url = {http://dx.doi.org/10.1007/978-3-540-78646-7\_49},
	year = {2008}
}



@incollection{citeulike:3789894,
	abstract = {Relatively few query tools exist for data exploration and pattern identification in time series data sets. In previous work we introduced Timeboxes. Timeboxes are rectangular, direct-manipulation queries for studying time-series datasets. We demonstrated how Timeboxes can be used to support interactive exploration via dynamic queries, along with overviews of query results and drag-and-drop support for query-by-example. In this paper, we extend our work by introducing Variable Time Timeboxes (VTT). VTTs are a natural generalization of Timeboxes, which permit the specification of queries that allow a degree of uncertainty in the time axis. We carefully motivate the need for these more expressive queries, and demonstrate the utility of our approach on several data sets.},
	author = {Keogh, Eamonn   and Hochheiser, Harry   and Shneiderman, Ben  },
	citeulike-article-id = {3789894},
	doi = {10.1007/3-540-36109-X\_19},
	journal = {Flexible Query Answering Systems},
	keywords = {dtw, litreview, thesis},
	pages = {240--250},
	posted-at = {2008-12-15 16:14:09},
	priority = {2},
	title = {An Augmented Visual Query Mechanism for Finding Patterns in Time Series Data},
	url = {http://dx.doi.org/10.1007/3-540-36109-X\_19},
	year = {2002}
}



@misc{citeulike:3788829,
	author = {Dimitrios, Gunopulos  },
	citeulike-article-id = {3788829},
	keywords = {litreview, thesis},
	posted-at = {2008-12-15 06:48:48},
	priority = {2},
	title = {Time Series Similarity Measures},
	url = {http://mrw.interscience.wiley.com/emrw/9780470011812/eob/article/b2a12074/current/abstract}
}



@article{citeulike:3788783,
	abstract = {Abstract--In this paper, we give a comprehensive description of our writer-independent online handwriting recognition system frog on hand. The focus of this work concerns the presentation of the classification/training approach, which we call cluster generative statistical dynamic time warping (CSDTW). CSDTW is a general, scalable, HMM-based method for variable-sized, sequential data that holistically combines cluster analysis and statistical sequence modeling. It can handle general classification problems that rely on this sequential type of data, e.g., speech recognition, genome processing, robotics, etc. Contrary to previous attempts, clustering and statistical sequence modeling are embedded in a single feature space and use a closely related distance measure. We show character recognition experiments of frog on hand using CSDTW on the UNIPEN online handwriting database. The recognition accuracy is significantly higher than reported results of other handwriting recognition systems. Finally, we describe the real-time implementation of frog on hand on a Linux Compaq iPAQ embedded device.},
	address = {Washington, DC, USA},
	author = {Bahlmann, Claus   and Burkhardt, Hans  },
	citeulike-article-id = {3788783},
	doi = {10.1109/TPAMI.2004.1262308},
	issn = {0162-8828},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	keywords = {dtw, litreview, thesis},
	number = {3},
	pages = {299--310},
	posted-at = {2008-12-15 06:30:08},
	priority = {2},
	publisher = {IEEE Computer Society},
	title = {The Writer Independent Online Handwriting Recognition System frog on hand and Cluster Generative Statistical Dynamic Time Warping},
	url = {http://dx.doi.org/10.1109/TPAMI.2004.1262308},
	volume = {26},
	year = {2004}
}



@article{citeulike:3744226,
	abstract = {Poor handwriting is a diagnostic criterion for developmental coordination disorder. Typical of poor handwriting is its low overall quality and the high variability of the spatial characteristics of the letters, usually assessed with a subjective handwriting scale. Recently, Dynamic Time Warping (DTW), a technique originally developed for speech recognition, was introduced for pattern recognition in handwriting. The present study evaluates its application to analyze poor handwriting. Forty children attending Dutch mainstream primary schools were recruited and based on their scores on the Concise Evaluation Scale for Children's Handwriting (Dutch abbreviation: BHK), 20 good and 20 poor writers (of whom 13 were scheduled for handwriting intervention) were identified. The groups were matched for age (7-9 years), school grade (grades 2 and 3) and handedness. The children subsequently wrote sequences of the letter "a" on a graphics tablet in three conditions (normal, fast, and accurate). Classical kinematics were obtained and for each individual letter DTW was used to calculate the distance from the mean shape. The DTW data revealed much higher variability in the letter forms of the poor writers that was independent of the kinematic results of larger trajectories, faster movements, and higher pen pressure. The current results suggest that DTW is a valid and objective technique for letter-form analysis in handwriting and may hence be useful to evaluate the rehabilitation treatments of children suffering from poor handwriting. In education research it may be exploited to explore how children (should) learn to write.},
	author = {Di Brina, C.  and Niels, R.  and Overvelde, A.  and Levi, G.  and Hulstijn, W. },
	citeulike-article-id = {3744226},
	doi = {10.1016/j.humov.2008.02.012},
	issn = {0167-9457},
	journal = {Human movement science},
	keywords = {dtw, litreview, thesis},
	month = {April},
	number = {2},
	pages = {242--255},
	posted-at = {2008-12-04 02:15:21},
	priority = {2},
	title = {Dynamic time warping: a new method in the study of poor handwriting.},
	url = {http://dx.doi.org/10.1016/j.humov.2008.02.012},
	volume = {27},
	year = {2008}
}



@article{citeulike:3736775,
	abstract = {We present an efficient and robust multiscale DTW (Ms-
DTW) approach to music synchronization for time-aligning
CD recordings of different interpretations of the same piece.
The general strategy is to recursively project an alignment
path computed at a coarse resolution level to the next higher
level and then to refine the projected path. As main contributions,
we address several crucial issues including the design
and specification of robust and scalable audio features, suitable
local cost measures,MsDTWlevels, constraint regions,
as well as sampling rate adaptation and structural enhancement
strategies. Extensive experiments on Western classical
music show that our MsDTW-based algorithm yields the
same alignment result as the classical DTW-based strategy
while significantly reducing the running time and memory
requirements. Even for pieces of a duration of 10 to 15 minutes,
the alignment (based on previously extracted feature
sequences) can be computed in less than a second.},
	author = {Muller, M.  and Mattes, H.  and Kurth, F. },
	booktitle = {in Proc. ISMIR},
	citeulike-article-id = {3736775},
	keywords = {dtw, litreview, thesis},
	location = {Victoria, Canada},
	pages = {192--197},
	posted-at = {2008-12-02 19:47:57},
	priority = {2},
	title = {An efficient multiscale approach to
audio synchronization},
	url = {http://www.google.com/url?sa=t\&\#38;source=web\&\#38;ct=res\&\#38;cd=2\&\#38;url=http\%3A\%2F\%2Farnetminer.org\%2Fviewpub.do\%3Fpid\%3D2106739\%26mode\%3Dpub\&\#38;ei=5I81SYviKZmWsAOrmoSHBg\&\#38;usg=AFQjCNGOl-fUcp4Q2PJKISmoUrdy16Fm3g\&\#38;sig2=MB2z\_J\_cyQ766tYPoeXoKg},
	year = {2006}
}



@article{citeulike:3736765,
	abstract = {Dynamic Time Warping (DTW) has a quadratic time and space complexity that limits its use to small time series. In this paper we introduce FastDTW, an approximation of DTW that has a linear time and space complexity. FastDTW uses a multilevel approach that recursively projects a solution from a coarser resolution and refines the projected solution. We prove the linear time and space complexity of FastDTW both theoretically and empirically. We also analyze the accuracy of FastDTW by comparing it to two other types of existing approximate DTW algorithms: constraints (such as Sakoe-Chiba Bands) and abstraction. Our results show a large improvement in accuracy over existing methods.},
	address = {Amsterdam, The Netherlands, The Netherlands},
	author = {Salvador, Stan   and Chan, Philip  },
	citeulike-article-id = {3736765},
	issn = {1088-467X},
	journal = {Intell. Data Anal.},
	keywords = {dtw, litreview, thesis},
	number = {5},
	pages = {561--580},
	posted-at = {2008-12-02 19:42:38},
	priority = {2},
	publisher = {IOS Press},
	title = {Toward accurate dynamic time warping in linear time and space},
	url = {http://portal.acm.org/citation.cfm?id=1367993},
	volume = {11},
	year = {2007}
}



@inproceedings{citeulike:3733947,
	abstract = {In this paper, we discuss an on-line signature verification system based on dynamic time-warping (DTW). The DTW-algorithm originates from the field of speech recognition, and has been applied successfully in the signature verification area more than once. However, until now, few adaptations have been made in order to take the specific characteristics of signature verification into account. According to us, one of the most important differences is the availability of a rather large number of reference patterns, making it possible to determine which parts of a reference signature are important and which are not. By disconnecting the DTW-stage and the feature extraction process we are able to deal efficiently with this extra amount of information. We demonstrate the benefits of our approach by building and evaluating a complete system},
	author = {Martens, R.  and Claesen, L. },
	booktitle = {Pattern Recognition, 1996., Proceedings of the 13th International Conference on},
	citeulike-article-id = {3733947},
	doi = {10.1109/ICPR.1996.546791},
	journal = {Pattern Recognition, 1996., Proceedings of the 13th International Conference on},
	keywords = {dtw, litreview, thesis},
	pages = {38--42 vol.3},
	posted-at = {2008-12-02 03:27:24},
	priority = {2},
	title = {On-line signature verification by dynamic time-warping},
	url = {http://dx.doi.org/10.1109/ICPR.1996.546791},
	volume = {3},
	year = {1996}
}



@inproceedings{citeulike:3733945,
	abstract = {We focus on the use of the dynamic time warping (DTW) technique in the signature verification area. The DTW algorithm originates from the field of speech recognition, where it is a highly appreciated component of speaker specific isolated word recognisers. A few years ago the DTW algorithm was successfully introduced in the area of online signature verification. The characteristics of speech recognition and signature verification are however rather different. Starting from these dissimilarities, our objective is to extract an alternative DTW approach that is better suited to the signature verification problem},
	author = {Martens, R.  and Claesen, L. },
	booktitle = {Document Analysis and Recognition, 1997., Proceedings of the Fourth International Conference on},
	citeulike-article-id = {3733945},
	doi = {10.1109/ICDAR.1997.620587},
	journal = {Document Analysis and Recognition, 1997., Proceedings of the Fourth International Conference on},
	keywords = {dtw, litreview, thesis},
	pages = {653--656 vol.2},
	posted-at = {2008-12-02 03:26:10},
	priority = {2},
	title = {Dynamic programming optimisation for on-line signature verification},
	url = {http://dx.doi.org/10.1109/ICDAR.1997.620587},
	volume = {2},
	year = {1997}
}



@article{citeulike:1807913,
	abstract = {Despite their known weaknesses, hidden Markov models (HMMs) have been the dominant technique for acoustic modeling in speech recognition for over two decades. Still, the advances in the HMM framework have not solved its key problems: it discards information about time dependencies and is prone to overgeneralization. In this paper, we attempt to overcome these problems by relying on straightforward template matching. The basis for the recognizer is the well-known DTW algorithm. However, classical DTW continuous speech recognition results in an explosion of the search space. The traditional top-down search is therefore complemented with a data-driven selection of candidates for DTW alignment. We also extend the DTW framework with a flexible subword unit mechanism and a class sensitive distance measure-two components suggested by state-of-the-art HMM systems. The added flexibility of the unit selection in the template-based framework leads to new approaches to speaker and environment adaptation. The template matching system reaches a performance somewhat worse than the best published HMM results for the Resource Management benchmark, but thanks to complementarity of errors between the HMM and DTW systems, the combination of both leads to a decrease in word error rate with 17\% compared to the HMM results},
	author = {De Wachter, M.  and Matton, M.  and Demuynck, K.  and Wambacq, P.  and Cools, R.  and Van Compernolle, D. },
	booktitle = {Audio, Speech and Language Processing, IEEE Transactions on [see also Speech and Audio Processing, IEEE Transactions on]},
	citeulike-article-id = {1807913},
	doi = {10.1109/TASL.2007.894524},
	journal = {Audio, Speech and Language Processing, IEEE Transactions on [see also Speech and Audio Processing, IEEE Transactions on]},
	keywords = {dtw, litreview, thesis},
	number = {4},
	pages = {1377--1390},
	posted-at = {2008-12-02 03:20:17},
	priority = {2},
	title = {Template-Based Continuous Speech Recognition},
	url = {http://dx.doi.org/10.1109/TASL.2007.894524},
	volume = {15},
	year = {2007}
}



@inproceedings{citeulike:1033529,
	abstract = {For many performance analysis problems, the ability to reason across traces is invaluable. However, due to non-determinism in the OS and virtual machines, even two identical runs of an application yield slightly different traces. For example, it is unlikely that two identical runs of an application will suffer context switches at exactly the same points. These sorts of variations across traces make it difficult to reason across traces. This paper describes and evaluates an algorithm, dynamic time warping (DTW) that can be used to align traces, thus enabling us to reason across traces. While DTW comes from prior work our use of DTW is novel. Also we describe and evaluate an enhancement to DTW that significantly improves the quality of its alignments. Our results show that for applications whose performance varies significantly over time, DTW does a great job at aligning the traces. For applications whose performance stays largely constant for significant periods of time, the original DTW does not perform well; however, our enhanced DTW performs much better.},
	author = {Mytkowicz, T.  and Diwan, A.  and Hauswirth, M.  and Sweeney, P. F. },
	citeulike-article-id = {1033529},
	doi = {10.1109/IPDPS.2006.1639592},
	journal = {Parallel and Distributed Processing Symposium, 2006. IPDPS 2006. 20th International},
	keywords = {dtw, litreview, thesis},
	pages = {8 pp.+},
	posted-at = {2008-12-02 03:17:17},
	priority = {2},
	title = {Aligning traces for performance evaluation},
	url = {http://dx.doi.org/10.1109/IPDPS.2006.1639592},
	year = {2006}
}



@incollection{citeulike:3733930,
	abstract = {In this paper, we report the results of recognition of online handwritten Tamil characters. We experimented with two different approaches. One is subspace based method wherein the interactions between the features in the feature spate are assumed to be linear. In the second approach, we investigated an elastic matching technique using dynamic programming principles. We compare the methods to find their suitability for an on-line form-filling application in writer dependent, independent and adaptive scenarios. The comparison is in terms of average recognition accuracy and the number of training samples required to obtain an acceptable performance. While the first criterion evaluates effective recognition capability of a scheme, the second one is important for studying the effectiveness of a scheme in real time applications. We also perform error analysis to determine the advisability of combining the classifiers.},
	author = {Joshi, Niranjan   and Sita, G.  and Ramakrishnan, A. G.  and Madhvanath, Sriganesh  },
	citeulike-article-id = {3733930},
	journal = {Neural Information Processing},
	keywords = {dtw, litreview, thesis},
	pages = {806--813},
	posted-at = {2008-12-02 03:12:44},
	priority = {2},
	title = {Tamil Handwriting Recognition Using Subspace and DTW Based Classifiers},
	url = {http://www.springerlink.com/content/v8xlqt222kvw63kj
},
	year = {2004}
}



@article{citeulike:3733907,
	abstract = {One of the most challenging areas in the field of automatic control is the design of automatic control devices that 'learn' to improve their performamce based upon experience, i.e., that can adapt themselves to circumstances as they find them. The military and commercial implications of such devices are impressive, and interest in the two main areas of research in the field of control, the USA and the USSR, runs high. Unfortunately, though, both theory and construction of adaptive controllers are in their infancy, and some time may pass before they are commonplace. Nonetheless, development at this time of adequate theories of processes of this nature is essential.  The purpose of our paper is to show how the functional equation technique of a new mathematical discipline, dynamic programming, can be used in the formulation and solution of a variety of optimization problems concerning the design of adaptive devices. Although, occasionally, a solution in closed form can be obtained, in general, numerical solution via the use of high-speed digital computers is contemplated.  We discuss here the closely allied problems of formulating adaptive control processes in precise mathematical terms and of presenting feasible computational algoritbms for determining numerical solutioms.  To illustrate the general concepts, consider a system which is governed by the inhomogeneous Van der Pol equation<tex>ddot{x} + mu(x^{2} - 1) dot{x} + x = r(t), 0 leq t leq T</tex>, where<tex>r(t)</tex>is a random function whose statistical properties are only partially known to a feedback control device which seeks to keep the system near the unstable equilibrium state<tex>x = 0, dot{x} = 0</tex>. It proposes to do this by selecting the value of \&\#956; as a function of the state of the system at time<tex>t</tex>, and the time<tex>t</tex>itself. By observing the random process<tex>r(t)</tex>, the controller may, with the passage of time, infer more and more concerning the statistical properties of the function<tex>r(t)</tex>and thus may be expected to improve the quality of its control decisions. In this way the controller adapts itself to circumstances as it finds them. The process is thus an interesting example of adaptive control, and, conceivably, with some immediate ap- plications.  Lastly, some areas of this general domain requiring additional research are indicated.},
	author = {Bellman, R.  and Kalaba, R. },
	booktitle = {Automatic Control, IRE Transactions on},
	citeulike-article-id = {3733907},
	journal = {Automatic Control, IRE Transactions on},
	keywords = {dtw, litreview, thesis},
	number = {2},
	pages = {1--9},
	posted-at = {2008-12-02 02:50:59},
	priority = {2},
	title = {On adaptive control processes},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1104847},
	volume = {4},
	year = {1959}
}



@article{citeulike:3733894,
	abstract = {Comprehensive two-dimensional gas chromatography (GC \&\#xd7; GC) is now recognized as the preferred technique for the detailed analysis and characterization of complex mixtures of volatile compounds. However, for comparison purposes, taking into account all the information contained in the chromatogram is far from trivial. In this paper, it is shown that the combination of peak alignment by dynamic time warping and multivariate analysis facilitated the comparison of complex chromatograms of tobacco extracts. The comparison is shown to be efficient enough to provide a clear discrimination among three types of tobacco. A tentative interpretation of loadings is presented in order to give access to the compounds which differ from one sample to another. Once located, mass spectrometry was used to identify markers of tobacco type.},
	author = {Vial, J.  and Nocairi, H.  and Sassiat, P.  and Mallipatu, S.  and Cognon, G.  and Thiebaut, D.  and Teillet, B.  and Rutledge, D. },
	citeulike-article-id = {3733894},
	doi = {10.1016/j.chroma.2008.09.027},
	issn = {00219673},
	journal = {Journal of Chromatography A},
	keywords = {dtw, litreview, thesis},
	month = {September},
	posted-at = {2008-12-02 02:32:00},
	priority = {2},
	title = {Combination of dynamic time warping and multivariate analysis for the comparison of comprehensive two-dimensional gas chromatogramsApplication to plant extracts},
	url = {http://dx.doi.org/10.1016/j.chroma.2008.09.027},
	year = {2008}
}



@incollection{citeulike:3733893,
	abstract = {The problem of similarity search in time series database has attracted a lot of interest in the data mining field. DTW(Dynamic Time Warping) is a robust distance measure function for time series, which can handle time shifting and scaling. The main defect of DTW lies in its relatively high computational complexity of similarity search. In this paper, we develop a simple but efficient approximation technique for DTW to speed up the search process. Our method is based on a variation of the traditional histograms of the time series. This method can work with a time linear with the size of the database. In our experiment, we proved that the proposed technique is efficient and produces few false dismissals in most applications.},
	author = {Gu, Jie   and Jin, Xiaomin  },
	citeulike-article-id = {3733893},
	doi = {10.1007/11875581\_101},
	journal = {Intelligent Data Engineering and Automated Learning – IDEAL 2006},
	keywords = {dtw, litreview, thesis},
	pages = {841--848},
	posted-at = {2008-12-02 02:31:17},
	priority = {2},
	title = {A Simple Approximation for Dynamic Time Warping Search in Large Time Series Database},
	url = {http://dx.doi.org/10.1007/11875581\_101},
	year = {2006}
}



@inproceedings{citeulike:3731715,
	abstract = {Finding similar patterns in a time sequence is a well-studied problem. Most of the current techniques work well for queries of a prespecified length, but not for variable length queries. We propose a new indexing technique that works well for variable length queries. The central idea is to store index structures at different resolutions for a given dataset. The resolutions are based on wavelets. For a given query, a number of subqueries at different resolutions are generated. The ranges of the subqueries are progressively refined based on results from previous subqueries. Our experiments show that the total cost for our method is 4 to 20 times less than the current techniques including linear scan. Because of the need to store information at multiple resolution levels, the storage requirement of our method could potentially be large. In the second part of the paper we show how the index information can be compressed with minimal information loss. According to our experimental results, even after compressing the size of the index to one fifth, the total cost of our method is 3 to 15 times less than the current techniques},
	author = {Kahveci, T.  and Singh, A. },
	booktitle = {Data Engineering, 2001. Proceedings. 17th International Conference on},
	citeulike-article-id = {3731715},
	doi = {10.1109/ICDE.2001.914838},
	journal = {Data Engineering, 2001. Proceedings. 17th International Conference on},
	keywords = {dtw, litreview, thesis},
	pages = {273--282},
	posted-at = {2008-12-01 06:05:35},
	priority = {2},
	title = {Variable length queries for time series data},
	url = {http://dx.doi.org/10.1109/ICDE.2001.914838},
	year = {2001}
}



@inproceedings{citeulike:3731713,
	abstract = {We investigate the problem of searching similar multiattribute time sequences. Such sequences arise naturally in a number of medical, financial, video, weather forecast, and stock market databases where more than one attribute is of interest at a time instant. We first solve the simple case in which the distance is defined as the Euclidean distance. Later we extend it to shift and scale invariance. We formulate a new symmetric scale and shift invariant notion of distance for such sequences. We also propose a new index structure that transforms the data sequences and clusters them according to their shiftings and scalings. This clustering improves the efficiency considerably. According to our experiments with real and synthetic datasets, the index structure's performance is 5 to 45 times better than competing techniques, the exact speedup based on other optimizations such as caching and replication.},
	author = {Kahveci, T.  and Singh, A.  and Gurel, A. },
	booktitle = {Scientific and Statistical Database Management, 2002. Proceedings. 14th International Conference on},
	citeulike-article-id = {3731713},
	doi = {10.1109/SSDM.2002.1029718},
	journal = {Scientific and Statistical Database Management, 2002. Proceedings. 14th International Conference on},
	keywords = {dtw, litreview, thesis},
	pages = {175--184},
	posted-at = {2008-12-01 06:04:10},
	priority = {2},
	title = {Similarity searching for multi-attribute sequences},
	url = {http://dx.doi.org/10.1109/SSDM.2002.1029718},
	year = {2002}
}



@inproceedings{citeulike:3731711,
	abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article.  ACM has opted to expose the complete List rather than only correct and linked references.},
	address = {New York, NY, USA},
	author = {Kelvin and Wong, Man  H. },
	booktitle = {PODS '99: Proceedings of the eighteenth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems},
	citeulike-article-id = {3731711},
	doi = {10.1145/303976.304000},
	isbn = {1-58113-062-7},
	keywords = {dtw, litreview, thesis},
	location = {Philadelphia, Pennsylvania, United States},
	pages = {237--248},
	posted-at = {2008-12-01 06:02:47},
	priority = {2},
	publisher = {ACM},
	title = {Fast time-series searching with scaling and shifting},
	url = {http://dx.doi.org/10.1145/303976.304000},
	year = {1999}
}



@article{citeulike:3731706,
	abstract = {We consider the problem of finding similar patterns in a time sequence. Typical applications of this problem involve large databases consisting of long time sequences of different lengths. Current time sequence search techniques work well for queries of a prespecified length, but not for arbitrary length queries. We propose a novel indexing technique that works well for arbitrary length queries. The proposed technique stores index structures at different resolutions for a given data set. We prove that this index structure is superior to existing index structures that use a single resolution. We propose a range query and nearest neighbor query technique on this index structure and prove the optimality of our index structure for these search techniques. The experimental results show that our method is 4 to 20 times faster than the current techniques, including sequential scan, for range queries and 3 times faster than sequential scan and other techniques for nearest neighbor queries. Because of the need to store information at multiple resolution levels, the storage requirement of our method could potentially be large. In the second part, we show how the index information can be compressed with minimal information loss. According to our experimental results, even after compressing the size of the index to one fifth, the total cost of our method is 3 to 15 times less than the current techniques.},
	author = {Kahveci, T.  and Singh, A. K. },
	booktitle = {Knowledge and Data Engineering, IEEE Transactions on},
	citeulike-article-id = {3731706},
	doi = {10.1109/TKDE.2004.1269667},
	journal = {Knowledge and Data Engineering, IEEE Transactions on},
	keywords = {dtw, litreview, thesis},
	number = {4},
	pages = {418--433},
	posted-at = {2008-12-01 05:59:02},
	priority = {2},
	title = {Optimizing similarity search for arbitrary length time series queries},
	url = {http://dx.doi.org/10.1109/TKDE.2004.1269667},
	volume = {16},
	year = {2004}
}



@inproceedings{citeulike:2264785,
	abstract = {We investigate techniques for analysis and retrieval of object trajectories in two or three dimensional space. Such data usually contain a large amount of noise, that has made previously used metrics fail. Therefore, we formalize non-metric similarity functions based on the longest common subsequence (LCSS), which are very robust to noise and furthermore provide an intuitive notion of similarity between trajectories by giving more weight to similar portions of the sequences. Stretching of sequences in time is allowed, as well as global translation of the sequences in space. Efficient approximate algorithms that compute these similarity measures are also provided. We compare these new methods to the widely used Euclidean and time warping distance functions (for real and synthetic data) and show the superiority of our approach, especially in the strong presence of noise. We prove a weaker version of the triangle inequality and employ it in an indexing structure to answer nearest neighbor queries. Finally, we present experimental results that validate the accuracy and efficiency of our approach},
	author = {Vlachos, M.  and Kollios, G.  and Gunopulos, D. },
	booktitle = {Data Engineering, 2002. Proceedings. 18th International Conference on},
	citeulike-article-id = {2264785},
	doi = {10.1109/ICDE.2002.994784},
	journal = {Data Engineering, 2002. Proceedings. 18th International Conference on},
	keywords = {dtw, litreview, thesis},
	pages = {673--684},
	posted-at = {2008-12-01 05:57:25},
	priority = {2},
	title = {Discovering similar multidimensional trajectories},
	url = {http://dx.doi.org/10.1109/ICDE.2002.994784},
	year = {2002}
}



@article{citeulike:603020,
	abstract = {The technique of dynamic programming for the time registration of a reference and a test pattern has found widespread use in the area of isolated word recognition. Recently, a number of variations on the basic time warping algorithm have been proposed by Sakoe and Chiba, and Rabiner, Rosenberg, and Levinson. These algorithms all assume that the test input is the time pattern of a feature vector from an isolated word whose endpoints are known (at least approximately). The major differences in the methods are the global path constraints (i.e., the region of possible warping paths), the local continuity constraints on the path, and the distance weighting and normalization used to give the overall minimum distance. The purpose of this investigation is to study the effects of such variations on the performance of different dynamic time warping algorithms for a realistic speech database. The performance measures that were used include: speed of operation, memory requirements, and recognition accuracy. The results show that both axis orientation and relative length of the reference and the test patterns are important factors in recognition accuracy. Our results suggest a new approach to dynamic time warping for isolated words in which both the reference and test patterns are linearly warped to a fixed length, and then a simplified dynamic time warping algorithm is used to handle the nonlinear component of the time alignment. Results with this new algorithm show performance comparable to or better than that of all other dynamic time warping algorithms that were studied.},
	author = {Myers, C.  and Rabiner, L.  and Rosenberg, A. },
	citeulike-article-id = {603020},
	journal = {Acoustics, Speech, and Signal Processing [see also IEEE Transactions on Signal Processing], IEEE Transactions on},
	keywords = {dtw, litreview, thesis},
	number = {6},
	pages = {623--635},
	posted-at = {2008-12-01 03:52:05},
	priority = {2},
	title = {Performance tradeoffs in dynamic time warping algorithms for isolated word recognition},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1163491},
	volume = {28},
	year = {1980}
}



@inproceedings{citeulike:964832,
	address = {Washington, DC, USA},
	author = {Zhang, Zhang   and Huang, Kaiqi   and Tan, Tieniu  },
	booktitle = {ICPR '06: Proceedings of the 18th International Conference on Pattern Recognition (ICPR'06)},
	citeulike-article-id = {964832},
	doi = {10.1109/ICPR.2006.392},
	keywords = {dtw, litreview, thesis},
	pages = {1135--1138},
	posted-at = {2008-12-01 03:51:45},
	priority = {2},
	publisher = {IEEE Computer Society},
	title = {Comparison of Similarity Measures for Trajectory Clustering in Outdoor Surveillance Scenes},
	url = {http://dx.doi.org/10.1109/ICPR.2006.392},
	year = {2006}
}



@article{citeulike:3568196,
	abstract = {To recognize speech, handwriting or sign language, many hybrid approaches have been proposed that combine Dynamic Time Warping (DTW) or Hidden Markov Models (HMM) with discriminative classifiers. However, all methods rely directly on the likelihood models of DTW/HMM. We hypothesize that time warping and classification should be separated because of conflicting likelihood modelling demands. To overcome these restrictions, we propose to use Statistical DTW (SDTW) only for time warping, while classifying the warped features with a different method. Two novel statistical classifiers are proposed (CDFD and Q-DFFM), both using a selection of discriminative features (DF), and are shown to outperform HMM and SDTW. However, we have found that combining likelihoods of multiple models in a second classification stage degrades performance of the proposed classifiers, while improving performance with HMM and SDTW. A proof-of-concept experiment, combining DFFM mappings of multiple SDTW models with SDTW likelihoods, shows that also for model-combining, hybrid classification can provide significant improvement over SDTW. Although recognition is mainly based on 3D hand motion features, these results can be expected to generalize to recognition with more detailed measurements such as hand/body pose and facial expression.},
	author = {Lichtenauer, J. F.  and Hendriks, E. A.  and Reinders, M. J. },
	booktitle = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
	citeulike-article-id = {3568196},
	doi = {10.1109/TPAMI.2008.123},
	journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
	keywords = {dtw, litreview, thesis},
	number = {11},
	pages = {2040--2046},
	posted-at = {2008-12-01 03:49:25},
	priority = {2},
	title = {Sign Language Recognition by Combining Statistical DTW and Independent Classification},
	url = {http://dx.doi.org/10.1109/TPAMI.2008.123},
	volume = {30},
	year = {2008}
}



@article{citeulike:2737624,
	abstract = {Continuously monitoring through time the correlation/distance of multiple data streams is of interest in a variety of applications, including financial analysis, video surveillance, and mining of biological data. However, distance measures commonly adopted for comparing time series, such as Euclidean and Dynamic Time Warping (DTW), either are known to be inaccurate or are too time-consuming to be applied in a streaming environment. In this paper we propose a novel DTW-like distance measure, called Stream-DTW (SDTW), which unlike DTW can be efficiently updated at each time step. We formally and experimentally demonstrate that SDTW speeds up the monitoring process by a factor that grows linearly with the size of the window sliding over the streams. For instance, with a sliding window of 512 samples, SDTW is about 600 times faster than DTW. We also show that SDTW is a tight approximation of DTW, errors never exceeding 10\%, and that it consistently outperforms approximations developed for the case of static time series.},
	author = {Capitani, Paolo   and Ciaccia, Paolo  },
	booktitle = {Including special issue: 20th Brazilian Symposium on Databases (SBBD 2005)},
	citeulike-article-id = {2737624},
	doi = {10.1016/j.datak.2006.08.012},
	journal = {Data \& Knowledge Engineering},
	keywords = {dtw, litreview, thesis},
	month = {September},
	number = {3},
	pages = {438--458},
	posted-at = {2008-12-01 03:48:48},
	priority = {2},
	title = {Warping the time on data streams},
	url = {http://dx.doi.org/10.1016/j.datak.2006.08.012},
	volume = {62},
	year = {2007}
}



@article{citeulike:2713605,
	abstract = {Abstract\&nbsp;\&nbsp;A new data mining technique used to classify normal and pre-seizure electroencephalograms is proposed. The technique is based on a dynamic time warping kernel combined with support vector machines (SVMs). The experimental results show that the technique is superior to the standard SVM and improves the brain activity classification.},
	author = {Chaovalitwongse, W.  and Pardalos, P. },
	citeulike-article-id = {2713605},
	doi = {10.1007/s10559-008-0012-y},
	journal = {Cybernetics and Systems Analysis},
	month = {January},
	number = {1},
	pages = {125--138},
	posted-at = {2008-12-01 03:43:26},
	priority = {2},
	title = {On the time series support vector machine using dynamic time warping kernel for brain activity classification},
	url = {http://dx.doi.org/10.1007/s10559-008-0012-y},
	volume = {44},
	year = {2008}
}



@article{citeulike:2838910,
	address = {Norwell, MA, USA},
	author = {Efrat, Alon   and Fan, Quanfu   and Venkatasubramanian, Suresh  },
	citeulike-article-id = {2838910},
	doi = {10.1007/s10851-006-0647-0},
	issn = {0924-9907},
	journal = {J. Math. Imaging Vis.},
	keywords = {dtw, litreview, thesis},
	month = {April},
	number = {3},
	pages = {203--216},
	posted-at = {2008-12-01 03:42:32},
	priority = {2},
	publisher = {Kluwer Academic Publishers},
	title = {Curve Matching, Time Warping, and Light Fields: New Algorithms for Computing Similarity between Curves},
	url = {http://dx.doi.org/10.1007/s10851-006-0647-0},
	volume = {27},
	year = {2007}
}



@incollection{citeulike:3728229,
	abstract = {As we have seen in Chap. 4, dynamic time warping is a flexible tool for comparing time series in the presence of nonlinear time deformations. In this context, the choice of suitable local cost or distance measures is of crucial importance, since they determine the kind of (spatial) similarity between the elements (frames) of the two sequences to be aligned. For the mocap domain, we introduce two conceptually different local distance measures – one based on joint angle parameters and the other based on 3D coordinates – and discuss their respective strengths and weaknesses (Sect. 10.1). The importance of DTW is then illustrated by some synthesis and analysis applications (Sect. 10.2). By comparing a motion data stream to itself, one obtains a cost or distance matrix that exhibits self-similarities within the motion. In Sect. 10.3, we describe how this idea can be exploited for motion retrieval. Finally, in Sect. 10.4, we discuss some work related to DTW-based motion retrieval.},
	citeulike-article-id = {3728229},
	doi = {10.1007/978-3-540-74048-3\_10},
	journal = {Information Retrieval for Music and Motion},
	keywords = {dtw, litreview, thesis},
	pages = {211--226},
	posted-at = {2008-11-29 22:30:44},
	priority = {0},
	title = {{DTW}-Based Motion Comparison and Retrieval},
	url = {http://dx.doi.org/10.1007/978-3-540-74048-3\_10},
	year = {2007}
}



@incollection{citeulike:3728228,
	abstract = {Dynamic time warping (DTW) is a well-known technique to find an optimal alignment between two given (time-dependent) sequences under certain restrictions (Fig. 4.1). Intuitively, the sequences are warped in a nonlinear fashion to match each other. Originally, DTW has been used to compare different speech patterns in automatic speech recognition, see [170]. In fields such as data mining and information retrieval, DTW has been successfully applied to automatically cope with time deformations and different speeds associated with time-dependent data. In this chapter, we introduce and discuss the main ideas of classical DTW (Sect. 4.1) and summarize several modifications concerning local as well as global parameters (Sect. 4.2). To speed up classical DTW, we describe in Sect. 4.3 a general multiscale DTW approach. In Sect. 4.4, we show how DTW can be employed to identify all subsequence within a long data stream that are similar to a given query sequence (Sect. 4.4). A discussion of related alignment techniques and references to the literature can be found in Sect. 4.5.},
	citeulike-article-id = {3728228},
	doi = {10.1007/978-3-540-74048-3\_4},
	journal = {Information Retrieval for Music and Motion},
	keywords = {dtw, litreview, thesis},
	pages = {69--84},
	posted-at = {2008-11-29 22:27:36},
	priority = {0},
	title = {{D}ynamic {T}ime {W}arping},
	url = {http://dx.doi.org/10.1007/978-3-540-74048-3\_4},
	year = {2007}
}



@inproceedings{citeulike:2019923,
	abstract = {Recently, we are attending to a huge evolution on the development of high performance computing platforms. Among these platforms, the GPU (Graphics Processing Units) stimulated by game industries, constantly demanding more graphical processing power, evolved from a simple graphical card to a general purpose computation parallel data processing device. This article shows the GPU's viability to general purpose computation, developing a speech recognition application inside. Dynamic Time Warping (DTW) is applied on a voice password identification. Normally, DTW requires large amount of data and processing time, so that it is an efficient technique to simple vocabulary, when the voice commands set is small. Using NVIDIA GeForce 8800 GTX, with 128 processing unit cores, and a CUDA (Compute Unified Device Architecture) software platform development architecture, the DTW application was implemented, and tested its performance.},
	author = {Poli, Gustavo   and Mari, Joao  F.  and Hiroki and Levada, Alexandre  L. },
	booktitle = {Computer Architecture and High Performance Computing, 2007. SBAC-PAD 2007. 19th International Symposium on},
	citeulike-article-id = {2019923},
	doi = {10.1109/SBAC-PAD.2007.21},
	journal = {Computer Architecture and High Performance Computing, 2007. SBAC-PAD 2007. 19th International Symposium on},
	keywords = {dtw, thesis},
	pages = {19--25},
	posted-at = {2008-11-20 17:42:35},
	priority = {2},
	title = {Voice Command Recognition with Dynamic Time Warping (DTW) using Graphics Processing Units (GPU) with Compute Unified Device Architecture (CUDA)},
	url = {http://dx.doi.org/10.1109/SBAC-PAD.2007.21},
	year = {2007}
}



@article{citeulike:3578001,
	abstract = {A computer system is described in which isolated words, spoken by a designated talker, are recognized through calculation of a minimum prediction residual. A reference pattern for each word to be recognized is stored as a time pattern of linear prediction coefficients (LPC). The total log prediction residual of an input signal is minimized by optimally registering the reference LPC onto the input autocorrelation coefficients using the dynamic programming algorithm (DP). The input signal is recognized as the reference word which produces the minimum prediction residual. A sequential decision procedure is used to reduce the amount of computation in DP. A frequency normalization with respect to the long-time spectral distribution is used to reduce effects of variations in the frequency response of telephone connections.  The system has been implemented on a DDP-516 computer for the 200-word recognition experiment. The recognition rate for a designated male talker is 97.3 percent for telephone input, and the recognition time is about 22 times real time.},
	author = {Itakura, F. },
	booktitle = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
	citeulike-article-id = {3578001},
	journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
	keywords = {dtw, litreview, thesis},
	number = {1},
	pages = {67--72},
	posted-at = {2008-11-19 12:36:49},
	priority = {4},
	title = {Minimum prediction residual principle applied to speech recognition},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1162641},
	volume = {23},
	year = {1975}
}



@mastersthesis{citeulike:3577984,
	author = {Myers, C. S. },
	citeulike-article-id = {3577984},
	journal = {MS and BS thesis,  MIT Jun 20 1980,},
	keywords = {dtw, litreview, thesis},
	posted-at = {2008-11-19 12:30:07},
	priority = {2},
	title = {A Comparative Study Of Several Dynamic Time Warping Algorithms For Speech Recognition},
	url = {http://dspace.mit.edu/bitstream/1721.1/27909/1/07888629.pdf }
}



@article{citeulike:1920382,
	author = {Dunfield, Peter  F.  and Yuryev, Anton   and Senin, Pavel   and Smirnova, Angela  V.  and Stott, Matthew  B.  and Hou, Shaobin   and Ly, Binh   and Saw, Jimmy  H.  and Zhou, Zhemin   and Ren, Yan   and Wang, Jianmei   and Mountain, Bruce  W.  and Crowe, Michelle  A.  and Weatherby, Tina  M.  and Bodelier, Paul  L. E.  and Liesack, Werner   and Feng, Lu   and Wang, Lei   and Alam, Maqsudul  },
	citeulike-article-id = {1920382},
	doi = {10.1038/nature06411},
	issn = {0028-0836},
	journal = {Nature},
	keywords = {publication},
	month = {November},
	posted-at = {2008-11-08 22:50:08},
	priority = {0},
	publisher = {Nature Publishing Group},
	title = {Methane oxidation by an extremely acidophilic bacterium of the phylum Verrucomicrobia},
	url = {http://dx.doi.org/10.1038/nature06411},
	year = {2007}
}



@article{citeulike:2709869,
	author = {Ming, Ray   and Hou, Shaobin   and Feng, Yun   and Yu, Qingyi   and Dionne-Laporte, Alexandre   and Saw, Jimmy  H.  and Senin, Pavel   and Wang, Wei   and Ly, Benjamin  V.  and Lewis, Kanako  L.  and Salzberg, Steven  L.  and Feng, Lu   and Jones, Meghan  R.  and Skelton, Rachel  L.  and Murray, Jan  E.  and Chen, Cuixia   and Qian, Wubin   and Shen, Junguo   and Du, Peng   and Eustice, Moriah   and Tong, Eric   and Tang, Haibao   and Lyons, Eric   and Paull, Robert  E.  and Michael, Todd  P.  and Wall, Kerr   and Rice, Danny  W.  and Albert, Henrik   and Wang, Ming-Li   and Zhu, Yun  J.  and Schatz, Michael   and Nagarajan, Niranjan   and Acob, Ricelle  A.  and Guan, Peizhu   and Blas, Andrea   and Wai, Ching  M.  and Ackerman, Christine  M.  and Ren, Yan   and Liu, Chao   and Wang, Jianmei   and Wang, Jianping   and Na, Jong-Kuk   and Shakirov, Eugene  V.  and Haas, Brian   and Thimmapuram, Jyothi   and Nelson, David   and Wang, Xiyin   and Bowers, John  E.  and Gschwend, Andrea  R.  and Delcher, Arthur  L.  and Singh, Ratnesh   and Suzuki, Jon  Y.  and Tripathi, Savarni   and Neupane, Kabi   and Wei, Hairong   and Irikura, Beth   and Paidi, Maya   and Jiang, Ning   and Zhang, Wenli   and Presting, Gernot   and Windsor, Aaron   and Navajas-Perez, Rafael   and Torres, Manuel  J.  and Feltus, Alex  F.  and Porter, Brad   and Li, Yingjun   and Burroughs, Max  A.  and Luo, Ming-Cheng   and Liu, Lei   and Christopher, David  A.  and Mount, Stephen  M.  and Moore, Paul  H.  and Sugimura, Tak   and Jiang, Jiming   and Schuler, Mary  A.  and Friedman, Vikki   and Mitchell-Olds, Thomas   and Shippen, Dorothy  E.  and Depamphilis, Claude  W.  and Palmer, Jeffrey  D.  and Freeling, Michael   and Paterson, Andrew  H.  and Gonsalves, Dennis   and Wang, Lei   and Alam, Maqsudul  },
	citeulike-article-id = {2709869},
	doi = {10.1038/nature06856},
	journal = {Nature},
	keywords = {publication},
	month = {April},
	number = {7190},
	pages = {991--996},
	posted-at = {2008-11-08 22:49:20},
	priority = {0},
	publisher = {Nature Publishing Group},
	title = {The draft genome of the transgenic tropical fruit tree papaya (Carica papaya Linnaeus)},
	url = {http://dx.doi.org/10.1038/nature06856},
	volume = {452},
	year = {2008}
}



@article{citeulike:2949501,
	author = {Hou, Shaobin   and Makarova, Kira  S.  and Jimmy and Senin, Pavel   and Ly, Benjamin  V.  and Zhou, Zhemin   and Ren, Yan   and Wang, Jianmei   and Galperin, Michael  Y.  and Omelchenko, Marina  V.  and Wolf, Yuri  I.  and Yutin, Natalya   and Koonin, Eugene  V.  and Stott, Matthew  B.  and Mountain, Bruce  W.  and Crowe, Michelle  A.  and Smirnova, Angela  V.  and Dunfield, Peter  F.  and Feng, Lu   and Wang, Lei   and Alam, Maqsudul  },
	citeulike-article-id = {2949501},
	doi = {10.1186/1745-6150-3-26},
	issn = {1745-6150},
	journal = {Biology Direct},
	keywords = {publication},
	month = {July},
	pages = {26+},
	posted-at = {2008-11-08 22:48:37},
	priority = {0},
	title = {Complete genome sequence of the extremely acidophilic methanotroph isolate V4, "Methylacidiphilum infernorum", 
a representative of the bacterial phylum Verrucomicrobia},
	url = {http://dx.doi.org/10.1186/1745-6150-3-26},
	volume = {3},
	year = {2008}
}



@article{citeulike:3496861,
	abstract = {This paper reports on an optimum dynamic progxamming (DP) based time-normalization algorithm for spoken word recognition. First, a general principle of time-normalization is given using time-warping function. Then, two time-normalized distance definitions, called symmetric and asymmetric forms, are derived from the principle. These two forms are compared with each other through theoretical discussions and experimental studies. The symmetric form algorithm superiority is established. A new technique, called slope constraint, is successfully introduced, in which the warping function slope is restricted so as to improve discrimination between words in different categories. The effective slope constraint characteristic is qualitatively analyzed, and the optimum slope constraint condition is determined through experiments. The optimized algorithm is then extensively subjected to experimental comparison with various DP-algorithms, previously applied to spoken word recognition by different research groups. The experiment shows that the present algorithm gives no more than about two-thirds errors, even compared to the best conventional algorithm.},
	author = {Sakoe, H.  and Chiba, S. },
	booktitle = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
	citeulike-article-id = {3496861},
	journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
	keywords = {dtw, litreview, thesis},
	number = {1},
	pages = {43--49},
	posted-at = {2008-11-08 22:11:03},
	priority = {0},
	title = {Dynamic programming algorithm optimization for spoken word recognition},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1163055},
	volume = {26},
	year = {1978}
}




