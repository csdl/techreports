\section{Planned evaluation in the classroom}\label{classroom}
In order to evaluate Trajectory performance through the discovery of both: well known and novel software process recurrent behavioral patterns, I am planning to conduct a classroom case study. The approach I am taking is based on continuous collection of software process and product artifacts by Hackystat and their daily indexing and mining with Trajectory. This software process and product dataset will be collected from classroom software project development during the Fall'09 and Spring'10 Software Engineering classes. Performing analyses daily within the data collection interval provides me with ability to communicate immediately with students about observed patterns improving their empirical evaluation. Once I identify emerging, strongly supported patterns, I will communicate via e-mail with students in order to conform observed phenomena. I am planning to perform two types of analyses: an individual developer software process analysis (personal software process discovery) and a ``collective software process'' analysis.

\begin{table}
\begin{center}
    \begin{tabular}{ | c | l | }
    \hline
    Symbolic abbreviation & Description \\ 
     of Events 						& 	  \\ 
    \hline
    $CI$                  & code implementation event, \\
    											& this corresponds to the new code entry \\
    \hline    											
    $CR$                  & code refactoring event, this includes renaming \\
    											& or deleting of functional units, and buffer transactions \\
    \hline
    $CD$                  & debugging event \\
		\hline
		$CCS$                 & successful compilation event \\
		\hline
		$CCF$                 & unsuccessful compilation event \\
    \hline
		$UTS$                 & successful unit test run \\
		\hline
		$UTF$                 & unsuccessful unit test run \\
		\hline
		$CM$                  & code commit to SCM \\
		\hline
		$CU$                  & code update from SCM \\
		\hline
    $CAS$                 & code analysis success event, corresponds to a \\
                          & successful invocation of one of the code analysis tools \\
    \hline
		$CAS$                 & code analysis failure event, corresponds to a \\
                          & unsuccessful invocation of one of the code analysis tools \\
    \hline    
    $CCP$                 & positive delta in the code size (churn) \\
    \hline
    $CCN$                 & negative delta in the code size (churn) \\
    \hline
    \end{tabular}
    \caption{Taxonomy of the Symbolic Events to be used in the classroom evaluation of Trajectory analyses.}
    \label{fig:data_collected_points}
    \end{center}
\end{table}

\subsection{Personal software process discovery}
For this type of experimental validation, I plan to perform Trajectory analyses using Hackystat data collected from individual developers. The data collected by Hackystat from each of developers will be aggregated into symbolic streams of two types: symbolic Events series and symbolic Episodes series:
\begin{itemize}
	\item Symbolic Events series. This dataset will consist of the thirteen types of Events listed in Table \ref{fig:data_collected_points}. These Events represent a set of essential code-development activities which will constitute a multivariate symbolic time-point series for further analyses through data mining. The goal of these analyses will be to discover recurrent behavioral patterns in the sequence of activities by an individual developer. 
	\item Symbolic Episodes data. This dataset will consist of the thirteen types of Episodes listed in the table \ref{fig:data_collected_intervals}. These Episodes represent a set of code-development activities which intended to capture dynamic recurrent behavioral patterns.
\end{itemize}

The goal of the Event series analysis is to build a taxonomy of symbolic software process patterns corresponding to one of the behaviors such as test-driven design or the more traditional test-last design.

In contrast to the analysis of symbolic Events series, which treats the process as a set of sequential activities, the analysis of symbolic Episodes series is intended to support discovery of overlapping, dynamic patterns. For example, I might find that an unsuccessful unit testing episode is usually happening within a code refactoring episode. In contrast, during code implementation, unit tests are usually successful and accompanied by decreasing coverage. Also, I expect to be able to infer a developers' ``reaction'' patterns. For example, as a reaction to the continuous failure of unit tests, one might start broad refactoring with the goal of reducing it's complexity. 

\begin{table}
\begin{center}
    \begin{tabular}{ | c | l | }
    \hline
    Symbolic abbreviation 	& Description \\ 
    of Episodes 						& of metric	  \\ 
    \hline
    $CI$ 									& code implementation episode, \\ 
    											& this corresponds to the new code entry events \\
	  \hline
    $CR$ 									& code refactoring episode, this includes renaming \\
    											& or deleting of functional units, and buffer transactions \\
    \hline
    $CD$ 									& code debugging episode, \\
    											& corresponds to debugging events \\
		\hline
		$CCS$ 								& successful code compilation episode, corresponds \\
													& to two or more successful compilation events \\
		\hline
		$CCF$ 								& unsuccessful code compilation episode, corresponds \\
													& to two or more successful compilation events \\
    \hline
		$UTS$ 								& successful unit test episode, corresponds \\
													& to two or more successful unit-test events \\
		\hline
		$UTF$ 							  & unsuccessful unit test episode, corresponds \\
													& to two or more unsuccessful unit-test events \\
		\hline
		$TCG$ 								& code test coverage growth, corresponds to a positive \\
													& delta between at least three code coverage analysis \\
													& tool invocations \\
		\hline
		$TCD$ 								& code test coverage decrease, corresponds to a negative \\
													& delta between at least three code coverage analysis \\
													& tool invocations \\
		\hline
		$CSG$ 								& code size growth, corresponds to a positive \\
													& delta between at least three code size analysis \\
													& tool invocations or three commits \\
		\hline
		$CSD$ 								& code size decrease, corresponds to a negative \\
													& delta between at least three code size analysis \\
													& tool invocations or three commits \\																										
		\hline
		$CCG$ 								& code complexity growth, corresponds to a positive \\
													& delta between at least three complexity analysis \\
													& tool invocations \\
		\hline
		$CCD$ 								& code complexity decrease, corresponds to a negative \\
													& code complexity delta between at least three complexity \\
													& analysis tool invocations \\													
		\hline
		$CAS$ 								& ``clean code development'' episode, corresponds to \\
													& at least three successful code analysis tools invocations \\
		\hline
		$CAF$ 								& ``unclean code development'' episode, corresponds to \\
													& at least three unsuccessful code analysis tools invocations \\
		\hline		
	  \end{tabular}
    \caption{Taxonomy of the Symbolic Episodes to be used in the classroom evaluation of Trajectory analyses.}
    \label{fig:data_collected_intervals}
    \end{center}
\end{table}

Once I have collected a sufficient amount of data, I expect to see some recurrent patterns gaining enough support to be considered as ``candidate patterns''. The support function I am considering will be a product of two metrics: one is based on the support function from the AprioriAll algorithm and quantifies the fraction of the developers demonstrating the pattern, and the second one is based on the total frequency of the pattern appearance. The reason for using two components is that the sampling space is limited to only eight to ten students (``developers'') in each of the classroom studies, and it is likely that there will be ``non-shared'' strong patterns - ones that observed only within a data collected from the single person.

Once a new pattern emerges in the ``candidate patterns'' pool, I will review it and classify as a ``meaningful'' or a ``unmeaningful'' on the basis of the process it is inferring. By performing these reviews, I intend to fulfill two goals: first, to prune the candidate patterns pool to truly useful and interesting patterns, and second, to develop a knowledge base of the data-mining algorithms in which the amount of reported unmeaningful or trivial patterns is limited. The ability to communicate with students will provide me with additional feedback that I will use to resolve complex cases.

\subsection{Collective software process discovery}
Traditionally students in class are divided by groups. It will be interesting to see whether the ``collective behavior'' and corresponding recurrent behavioral patterns are different from those in the personal software process. For the ``collective software process'' recurrent behavioral pattern discovery I plan to summarize individual developer streams by combining overlapping Events and Episodes of same activities. It is hard to guess at this time whether or not these aggregated streams will be more or less noisy than individual streams, but in any case the application of the Trajectory analyses might lead to interesting findings. For example, I might infer frequent communications between team members through the high fraction of synchronous (overlapping) development events.
