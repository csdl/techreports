\section{Planned evaluation in the classroom}\label{classroom}
In order to evaluate Software Trajectory performance in the discovery of well known and novel recurrent behavioral patterns in software process, I am planning to conduct two classroom case studies followed by survey of participants (Section \ref{survey}). 

This studies will be conducted in two Software Engineering classes during the Fall'09 and Spring'10 semesters taught by Dr. Philip Johnson at the University of Hawaii. One of this classes will be for undergraduate students, and the other for graduate students. By curriculum design, students will be required to collect metrics and perform analyses on their own data in order to develop understanding of software metrics and improve thier software development process. About 20 - 30 students are expected to participate in the study. 

By the beginning of the data collection in the Fall'09 class I am planning to implement a web interface (a Hackystat ProjectBrowser plugin) of the Software Trajectory framework and begin the first phase of the classroom study. This plugin will provide students with visualization of the recurrent patterns found in  the performed software process. Whether this visualization, and information about discovered patterns will be an useful addition to the curriculum and student personal toolkit will be a research question of this study. At the end of the study, I will compose an online questionnaire to collect the students' opinion about Software Trajectory analyses. 

This questionaire will cover questions concerned about Software Trajectory analyses usefullness, usability and utility. Another important question will be the students' perception of whether or not Software Trajectory analyses is a reasonable approach to software process discovery, analysis and improvement in ``real world'' settings. I plan to design questions as complete statements like ``\textit{Software Trajectory analyses revealed recurrent behaviors in my software process}'' providing ranking options in the form of statements from \textit{``Strongly disagree''} to \textit{``Strongly agree''} with an option to skip the question and option to write an extended comments for elaboration of an opinion. By the end of the questionaire I will ask students to provide additional impressions, suggestions and comments about the Software Trajectory framework. The experience collected during the first phase of the classroom study will be used in the improvement of the Software Trajectory analyses and user interface.

By the beginning of the data collection in the Spring'10 class I am planning to release a next version of the Software Trajectory framework incorporating experience from the first classroom study. The second classroom study will be very similar to the first one in the flow. 

During both classroom experiments I will log the use of the online version of Software Trajectory framework. System usage, performed analyses and their parameters will be logged and associated with users. The pattern of the system utilization during the classroom evaluation will provide additional valuable statistics for the Software Trajectory evaluation.

\subsection{Classroom study design}
During the classroom study, the software product and process data will be collected from continuous integration as well as from individual students by Hackystat. This data will be daily converted into the symbolic representation and analysed with Software Trajectory. Once Software Trajectory will be able to identify strongly supported (``candidate'') patterns, they will be reported and will appear in the Software Trajectory web user interface with indication of their support values.

The support function I am considering for this study will be a product of two metrics: the first one is based on the support function from the AprioriAll algorithm and will quantify the fraction of participating students demonstrating the pattern, and the second one is based on the total frequency of the pattern appearance. The reason for using two components is that the classroom ``sampling space'' is limited to only eight to ten students and it is likely that there will be ``non-shared'' strong patterns - ones that observed only within a data collected from the single student.

I understand that this choice of the support function introduces a bias into my analyses focusing too much on the subjective patterns which might be too specific to the observed individuals or classroom curicculum settings. Here I am compromising in planned research between the ability to capture recurrent behaviors at all and the usefulness and significance of captured behaviors. As I said at this point of my research it is impossible to foresee the cognitive impact of observing and understanding  of discovered patterns and guess where I should draw the support function threshold. By iteratively reviewing Software Trajectory results and collecting relevant feedback from students through the instructor, I plan to improve the Trajectory knowledge base in order to prune reported patterns to only useful and meaningful ones. This ability to communicate with students and iteratively improve the system design is crucial for in-process knowledge development and overall experiment success.

I am planning to perform two types of analyses: an individual developer behavioral software process analysis (personal software process discovery) and a ``collective software process'' analysis.

\begin{table}
\begin{center}
    \begin{tabular}{ | c | l | }
    \hline
    Symbolic abbreviation & Description \\ 
     of Events 						& 	  \\ 
    \hline
    $CI$                  & code implementation event, \\
    											& this corresponds to the new code entry \\
    \hline    											
    $CR$                  & code refactoring event, this includes renaming \\
    											& or deleting of functional units, and buffer transactions \\
    \hline
    $CD$                  & debugging event \\
		\hline
		$CCS$                 & successful compilation event \\
		\hline
		$CCF$                 & unsuccessful compilation event \\
    \hline
		$UTS$                 & successful unit test run \\
		\hline
		$UTF$                 & unsuccessful unit test run \\
		\hline
		$CM$                  & code commit to version control \\
		\hline
		$CU$                  & code update from version control \\
		\hline
    $CAS$                 & code analysis success event, corresponds to a \\
                          & successful invocation of one of the code analysis tools \\
    \hline
		$CAF$                 & code analysis failure event, corresponds to a \\
                          & unsuccessful invocation of one of the code analysis tools \\
    \hline    
    $CCP$                 & positive delta in the code size (churn) \\
    \hline
    $CCN$                 & negative delta in the code size (churn) \\
    \hline
    \end{tabular}
    \caption{Taxonomy of the Symbolic Events to be used in the classroom evaluation of Trajectory analyses.}
    \label{fig:data_collected_points}
    \end{center}
\end{table}

\subsection{Personal software process discovery}
For this type of experimental validation, I plan to perform Trajectory analyses using Hackystat data collected from individual developers. The data collected by Hackystat from each of developers will be aggregated into symbolic streams of two types: symbolic Events series and symbolic Episodes series:
\begin{itemize}
	\item Symbolic Events series. This dataset will consist of the thirteen types of Events listed in Table \ref{fig:data_collected_points}. These Events represent a set of essential code-development activities which will constitute a multivariate symbolic time-point series for further analyses through data mining. The goal of these analyses will be to discover recurrent behavioral patterns in the sequence of activities by an individual developer. 
	\item Symbolic Episodes data. This dataset will consist of the fifteen types of Episodes listed in the table \ref{fig:data_collected_intervals}. These Episodes represent a set of code-development activities which intended to capture dynamic recurrent behavioral patterns.
\end{itemize}

The goal of the Event series analysis is to build a taxonomy of symbolic software process patterns corresponding to one of the behaviors such as test-driven design or the more traditional test-last design.

In contrast to the analysis of symbolic Events series, which treats the process as a set of sequential activities, the analysis of symbolic Episodes series is intended to support discovery of overlapping, dynamic patterns. For example, I might find that an unsuccessful unit testing episode is usually happening within a code refactoring episode. 

By combining Events and Episodes in the analyses I might discover that during code implementation, unit tests are usually successful but accompanied with decreasing test coverage. Also, I expect to be able to infer a developers' ``reaction'' patterns. For example, as a reaction to the continuous failure of unit tests, one might start broad refactoring of the code with the goal of reducing its complexity. 

\begin{table}
\begin{center}
    \begin{tabular}{ | c | l | }
    \hline
    Symbolic abbreviation 	& Description \\ 
    of Episodes 						& of metric	  \\ 
    \hline
    $CI$ 									& code implementation episode, \\ 
    											& this corresponds to the new code entry events \\
	  \hline
    $CR$ 									& code refactoring episode, this includes renaming \\
    											& or deleting of functional units, and buffer transactions \\
    \hline
    $CD$ 									& code debugging episode, \\
    											& corresponds to debugging events \\
		\hline
		$CCS$ 								& successful code compilation episode, corresponds \\
													& to two or more successful compilation events \\
		\hline
		$CCF$ 								& unsuccessful code compilation episode, corresponds \\
													& to two or more successful compilation events \\
    \hline
		$UTS$ 								& successful unit test episode, corresponds \\
													& to two or more successful unit-test events \\
		\hline
		$UTF$ 							  & unsuccessful unit test episode, corresponds \\
													& to two or more unsuccessful unit-test events \\
		\hline
		$TCG$ 								& code test coverage growth, corresponds to a positive \\
													& delta between at least three code coverage analysis \\
													& tool invocations \\
		\hline
		$TCD$ 								& code test coverage decrease, corresponds to a negative \\
													& delta between at least three code coverage analysis \\
													& tool invocations \\
		\hline
		$CSG$ 								& code size growth, corresponds to a positive \\
													& delta between at least three code size analysis \\
													& tool invocations or three commits \\
		\hline
		$CSD$ 								& code size decrease, corresponds to a negative \\
													& delta between at least three code size analysis \\
													& tool invocations or three commits \\																										
		\hline
		$CCG$ 								& code complexity growth, corresponds to a positive \\
													& delta between at least three complexity analysis \\
													& tool invocations \\
		\hline
		$CCD$ 								& code complexity decrease, corresponds to a negative \\
													& code complexity delta between at least three complexity \\
													& analysis tool invocations \\													
		\hline
		$CAS$ 								& ``clean code development'' episode, corresponds to \\
													& at least three successful code analysis tools invocations \\
		\hline
		$CAF$ 								& ``unclean code development'' episode, corresponds to \\
													& at least three unsuccessful code analysis tools invocations \\
		\hline		
	  \end{tabular}
    \caption{Taxonomy of the Symbolic Episodes to be used in the classroom evaluation of Trajectory analyses.}
    \label{fig:data_collected_intervals}
    \end{center}
\end{table}

\subsection{Collective software process discovery}
Traditionally students in class are divided by groups. It will be interesting to see whether the ``collective behavior'' and corresponding recurrent behavioral patterns are different from those in the personal software process. For the ``collective software process'' recurrent behavioral pattern discovery I plan to summarize individual developer streams by combining overlapping Events and Episodes of same activities. It is hard to guess at this time whether or not these aggregated streams will be more or less noisy than individual streams, but in any case the application of the Trajectory analyses might lead to interesting findings. For example, I might infer peer-programming episodes through the high fraction of synchronous (overlapping) development events between team members.
