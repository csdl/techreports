\chapter{Experimental evaluation} \label{experiments}
I propose to conduct three case studies: a pilot study, a classroom case study, and a public data case study in order to empirically evaluate the capabilities and performance of the Trajectory framework. The main difference among these experiments lies in the structure of the data used and in approaches for evaluation. 

The exploratory, evaluative pilot study consists of a set of a small research experiments run on small, custom datasets, and conducted with a primary goal to help in the architectural design and development of the Trajectory framework. As the secondary goal, these experiments are helping to outline the boundaries of applicability of my approach to certain problems. Section \ref{pilot.evaluation} discussess some of the insights yielded by the pilot study.

The public data case study is based on the use of publicly available Software Configuration Management (SCM) audit trails of the big ongoing software projects such as Eclipse, GNOME etc. Mining of the SCM repositories is a well-developed area of the research with much work published. SCM repositories data present coarse software product artifacts which usually mined with a purpose of discovering various characteristics enlightening software evolution. I am using a mixed approach in this study. I plan to perform SCM audit trail data mining using Trajectory in order to discover known software process artifacts and evaluate Trajectory's performance when compared to similar tools in the first phase of the study. The second phase, and the main goal, is in the development of the application-specific taxonomy mapping of software process artifacts into recurrent behaviors. This mapping will be build by extending well-known behaviors with newly discovered and qualitatively evaluated ones. Section \ref{public.evaluation} outlines this study in details.

The classroom case study is based on the most sophisticated data set. This data will be collected by the Hackystat from Continuous Integration and from individual developers and will contain a fine-grain information about performed software process. The approach I am taking in this study is very similar to the public data case study. I will apply Trajectory analyses to the low-level software process artifacts in order to discover recurrent behaviors. If discovered, these behaviors will be qualitatively exmined by interviewing 


found  collected concurrently from Continuous Integration and from individual developers. This study will examine the proposed ability of my approach to discover low-level behavioral patterns through the indexing and mining of the symbolic abstractions of the software process, Section \ref{classroom}. 

My intent behind this studies is to assess the ability of the Trajectory to recognize well known recurrent behavioral patterns (for example Test Driven Development), as well as its ability to discover useful software processes. In addition, these studies will support a classification and extension of the current Hackystat sensor family in order to improve Trajectory's performance. It is quite possible that some of the currently collected sensor data will be excluded from the Trajectory Analysis datasets, while some new ones will be designed and developed in order to capture important features from the studied software development data streams.

While the information collected from previous research during the review of the literature, and preliminary experimental results strongly suggest that the proposed approach is efficient in the discovery of the recurrent patterns from the symbolic temporal data and likely to yield proposed contribution, it is quite possible, that for some reasons it will fail. Whether it will happen due to the specificity of domain, or due to the insufficiency of the information enclosed in the collected artifacts, or maybe because of inefficiency of augmented methods, through the thorough analyses of failed experiments I will apply every effort to fulfill original goals by modifying proposed methods and improving data collection mechanism. If this effort will not improve the situation, I will explain pitfalls of the process data mining from the symbolic temporal data obtained through the approximation of the software process. 

\section{Pilot study}\label{pilot.evaluation}
In order to demonstrate the ability of Trajectory to perform telemetry indexing and temporal recurrent patterns extraction, I have conducted two experiments which provide insights in the use of the motif frequencies and software process event taxonomies. 

\begin{figure}[tbp]
   \centering
   \includegraphics[height=185mm]{cluster_streams.eps}
   \caption{Clustering of telemetry streams for classroom pilot dataset using symbolic approximation and vectors of motif frequencies. While it seems to be meaningful to find correlation between \textit{UnitTest\_Failure} and \textit{CodeIssue} streams unit test, this grouping happened due to the similarity of behavior pattern - short, high amplitude bursts; but note, there is no correlation of features in time .}
   \label{fig:cluster_streams}
\end{figure}

\subsection{Clustering of the Hackystat Telemetry streams}
The main purpose of the this study was to evaluate the ability of PAA and SAX approximations and indexing to capture a temporal specificity of telemetry streams through the discovery of recurrent temporal patterns. Knowing about the frequently misleading results of a time-series clustering \cite{citeulike:227029}, I did not expect to capture much interesting facts, nevertheless the results were encouraging.

The data used in this study was collected from student users of Hackystat during Spring, 2009. This dataset represents Hackystat metrics collected during sixty days of a classroom project by eight students. The following clustering experiments were conducted using the distance between vectors of motif frequencies extracted by indexing of telemetry streams:

\begin{figure}[tbp]
   \centering
   \includegraphics[height=90mm]{dev_clustering.eps}
   \caption{Clustering of developers behavior using symbolic approximation and vectors of motif frequencies. This analysis captured similar development behavior among developers. Developers \#2 and \#7 were consistent (no bursts observed) in both, coding and measuring effort during whole time interval, while all others can be characterized with bursty, inconsistent effort.}
   \label{fig:cluster_developers}
\end{figure}

\begin{itemize}
	\item Clustering of software process related telemetry streams collected from individual developers. I was able to group developers with similar behavioral patterns within clusters, which indicates the feasibility of the classification approach. Figure \ref{fig:cluster_developers} depicts results of this analysis.
	\item Clustering of software product-related telemetry streams by using motif frequencies. I was able to group telemetry streams, but while these groups look intuitively meaningful, the close examination of the stream features suggests that this grouping happened due to the similar temporal behavior on the short stretches. This result, while proving the correctness of approach, indicates it's limitation, pointing that instead of using just motif frequencies, some temporal ordering should be taken into account. Figure \ref{fig:cluster_streams} displays results of this analysis.
\end{itemize}

\subsection{Sequential patterns search}
The second pilot study, focusing on discovery of sequential patterns, was conducted using real data from my own concurrent development of two software projects. While working on the Trajectory framework, I made decision to split the code into two parts: an algorithm implementation library that I named JMotif, and user-interface part called TrajectoryBrowser. While this decision simplified development, it introduced a dependency of TrajectoryBrowser on the JMotif API. As a result of iterative and incremental pattern in my development, I changed the JMotif public API three times, which consequently involved extensive refactoring in the ProjectBrowser code. This dependency can be clearly seen from observing DevTime streams at Figure \ref{fig:sequential_growth} panel $a$. 

\begin{figure}[tbp]
   \centering
   \includegraphics[height=80mm]{sequential_growth.eps}
   \caption{The illustration of finding of sequential $growth \; pattern$ in two DevTime telemetry streams. Panel $a$: The Hackystat ProjectBrowser showing telemetry streams. Panel $b$: the TrajectoryBrowser showing same telemetry streams along with identified pattern. Panel $c$: the symbolic representation of streams with highlighted pattern.}
   \label{fig:sequential_growth}
\end{figure}

In order to capture this dependency pattern in two Telemetry streams, representing the daily amount of development time spent on the TrajectoryBrowser and JMotif projects, I defined a synthetic \textit{growth pattern} as the large positive delta value between previous and current day effort. By transforming Telemetry streams with this simple rule in the symbolic form, I obtained a two dimensional symbolic time series, where letter $G$ represents a growth pattern, see Figure \ref{fig:sequential_growth} panel $c$. I defined a formal rule for \textit{sequential growth} pattern as the pattern like $G_{JMotif}\; \rightarrow \; G_{TrajectoryBrowser}$ where distance between these $G$s is less than three days. By application of this rule I identified a pattern which exactly corresponds to my experience. 

While this experiment was designed with a purpose and does not provide any value in the dependencies discovery, the ``sequential growth'' pattern and past effort information can be used in the estimation of effort needed for requested software changes.
