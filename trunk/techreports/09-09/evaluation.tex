\chapter{Experimental evaluation} \label{experiments}
I propose to conduct two case studies: \textit{Public data case study} (Section \ref{public.evaluation}), and \textit{Classroom case study} (Section \ref{classroom}) in order to empirically evaluate the capabilities and performance of Software Trajectory framework. These studies differ in the granularity of data used, and in the approaches for evaluation. 

During my work on the pilot version of Software Trajectory framework, I have designed a set of small experiments with a goal to aid the architectural design and algorithms implementation. In addition, these experiments helped me to outline the boundaries of applicability of my approach to certain problems in software engineering. I call these experiments as \textit{Pilot study} and Section \ref{pilot.evaluation} discusses some of the yielded by this study insights.

My intent behind these empirical studies is to assess the ability of the Software Trajectory to recognize well known recurrent behavioral patterns and software processes (for example Test Driven Development), as well as its ability to discover new ones. In addition, these studies will support a classification and extension of the current Hackystat sensor family in order to improve Trajectory's performance. It is quite possible that some of the currently collected sensor data will be excluded from the Software Trajectory datasets, while some new ones will be designed and developed in order to capture important features from the studied software development data streams. 

Before proceeding with presentation of design of these studies and approaches for evaluation I will discuss my research paradigm.

\section{Review of evaluation strategies}
In contemporary literature, research methods are categorized into three paradigms: quantitative, qualitative and mixed-methods  \cite{citeulike:5410262}. Despite the arguments presented in the past for integrating first two methods \cite{GubaLincoln-CompetingParadigms}, combining qualitative and quantitative methods in a single, mixed-method study, is currently widely practiced and accepted in many areas of research, and in particular, in the Empirical software engineering \cite{citeulike:5361927} \cite{citeulike:5361791}. 

\subsection{The two paradigms}
The quantitative paradigm is based on positivism. The positivism philosophy presumes, that there exists only one truth, which is an objective reality independent from human perception, which, in turn, means that a researcher and a researched phenomena unable to influence this truth or being influenced by it \cite{GubaLincoln-CompetingParadigms}. Quantitative studies are focused on finding of this truth through extensive study of the phenomena, and often characterized with very large sample sizes, control groups etc. in order to ensure proper statistical derivations.

In contrast, qualitative study is based on the constructivism \cite{citeulike:209817} \cite{GubaLincoln-CompetingParadigms} and interpretivism \cite{citeulike:447180}. Both presume existence of multiple truths based on the one's construction of reality, as well as non-existence of such a reality prior to the investigation. Moreover, once such a reality is solely or socially constructed, it is not fixed, - it is changing over time, shaped by new findings and experiences of researcher. Techniques of qualitative studies are not meant to represent large population. They rather focusing on the small, rich in the features samples which can provide valuable information. In other words, subjects in a qualitative study picked not because they represent large groups, but because they can articulate important for a study high-quality information.

These assumptions behind two approaches extend differences beyond just methodological or philosophical to the one's perceptions of the reality. According to Guba et al. \cite{GubaLincoln-CompetingParadigms}, qualitative and quantitative methods do not study the same phenomena. Reflecting the authors' statement into Empirical software engineering, we can infer a limitations of qualitative studies by its inability to access some of the phenomena that researchers are interested in, such as prior experiences, social interactions, and the behavioral variations of individuals performing software process.  

\subsection{Mixed methods and Exploratory research}
Surprisingly, the fundamental differences of two paradigms are rarely discussed in the mixed-methods research which is based on the pragmatism. The finding of truth is more important in pragmatism paradigm then the question of methods or philosophy. Researcher practicing mixed-method approach are free to use methods, techniques and protocols which meet their purposes, and which, after all, provide the best understanding of the research problem. This unifying logic in understanding of studied phenomena regardless of approach, is the central in mixed methods.

While three discussed paradigms at least assume the existence of a problem and allow to outline methods of data collection and design of a study, sometimes, a clear statement of a problem is missing from research. In such a case, Exploratory research must be conducted in order to determine a problem existence through insights into a given situation.

\section{Software Trajectory approach evolution}
When I started exploring Hackystat telemetry streams during Software Engineering class taught by my present adviser Philip Johnson, I realized the ``coolness'' of the software process visualization at providing insights into software process. By using web-interface of Hackystat I was able to pull and visualize various telemetry streams reflecting my development. Working in the class, I was comparing metrics of my software process to the ones from my team members. Performing such analyses, and discussing their results we were able to improve our individual and team software processes, which resulted in excellent grades. 

After joining CSDL team, I started my research exploring the boundaries of telemetry visualization and telemetry analyses, and finding possibilities for improvements. At that point of time, I was working on the two problems: improving the visualization of telemetry streams and introduction of the metrics for quantitative comparison of telemetry streams (trajectories). While working on these two problems, I have realized, that the real problem I am trying to solve lies beyond the reach of these two. The problem is that user is not provided with enough details about software process in order to be able to perform comparative analyses of two projects (two software processes). I am envisioning the solution of this problem in the extension of Hackystat analyses with the ability to discover detailed features from software process.

In other words, by conducting \textit{exploratory research} of visualization tools and techniques as well as designing ``flexible enough'' metrics appropriate for telemetry streams comparison, I have realized the importance of understanding of the generative software process. This knowledge about performed software process, inferred from the set of collected artifacts, is crucial in the process improvement and in the comparative analysis of software projects.

Once I was able to clearly formulate this problem, I have started another exploratory study - my pilot study. By performing this exploration of the tools and techniques for software process mining and inference, I am trying to build my own toolkit which will allow me to approach the next problem - problem of quantifying of software process through the discovery of recurrent behaviors. 

\section{Software Trajectory case studies and evaluation design}
The proposed public data case study is based on the use of publicly available Software Configuration Management (SCM) audit trails of the big, ongoing software projects such as Eclipse, GNOME etc. Mining of SCM repositories is a well-developed area of research with much work published \cite{citeulike:5043676}. SCM repositories contain coarse software product artifacts which are usually mined with a purpose of discovering of various characteristics enlightening software evolution and software process. I am using a mixed approach in this study. In the first phase of this study, I plan to perform SCM audit trail data mining following published work and using Software Trajectory as a tool in order to discover confirmed patterns in software process artifacts, and thus quantitatively evaluate Trajectory's performance when compared to existing tools. In the second phase, I will develop my own pre-processing and taxonomy mapping of software process artifacts into temporal symbolic series. By using this data and Trajectory, I plan to develop a new approach for SCM audit trail mining and possibly discover new evolutionary behaviors within software process. These discovered knowledge will be evaluated through the peer-reviewed publication submitted for the annual MSR challenge \cite{citeulike:5043676}.

The classroom case study is based on the most sophisticated data set. This data will be collected by the Hackystat from Continuous Integration and from individual developers and will contain a fine-grain information about performed software process. The approach I am taking in this study is very similar to the public data case study. I will develop my own taxonomy for mapping of software process artifacts into symbolic temporal data and will apply Trajectory analyses to this data in order to discover recurrent behaviors. In turn, these discovered knowledge will be evaluated through interviewing for usefulness and meaningfulness. Results of interviewing will be augmented in the Software Trajectory and will constitute part of my thesis and following publication.

Both case studies are exploratory in nature. At this point of my research, I can only see that the properties of my approach and its current implementation in the Software Trajectory framework appear to be very promising. The wealth of developed techniques for temporal symbolic data mining and recent development of SAX approximation allow me to overcome many computational limitations in existing approaches for mining of software process artifacts. The current implementation of Hackystat provides the ability to capture fine-grain software product and process metrics providing a richness of data, which, potentially, might reveal new insights. Current research in software process discovery indicates the overall feasibility of proposed goals in the discovery of unknown recurrent behaviors in software process. 

Nevertheless, there is no prior knowledge about application of these techniques to software process mining. Moreover, at this stage of my research, it is impossible to foresee if new recurrent behaviors will be discovered or their meaningfulness or usefulness for real world applications. For this reason I am undertaking a constructivism paradigm in my research \cite{citeulike:209817}, and will develop knowledge about the applicability of my approach to software process mining during the development of Software Trajectory framework and its empirical evaluation. By designing, developing, deploying and observing working a software system, and by conducting interviews and surveys I will gain the desired experience and knowledge. By presenting my work in the peer-reviewed publications I will sharpen this knowledge confirming my hypotheses.

But it is possible that this part of my research will fail, and I will not be able to discover any meaningful novel knowledge about software process. If so, I will apply every effort to investigate and explain the pitfalls of my approach to the domain of software process data mining. It may be, failure is due to the specificity of domain, or due to the insufficiency of the information enclosed in the collected artifacts, or maybe due to inefficiency of augmented methods. Through the thorough analyses of failed experiments and collection of the feedback, I will outline boundaries of the approach taken in this work and possibility of the future development.
