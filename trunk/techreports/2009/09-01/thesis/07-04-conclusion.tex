\chapter{Research Summary, Contributions and Future Directions}
\label{ch:Conclusion}
This chapter finishes up this thesis. It begins with the research summary in Section \ref{sec:ResearchSummary}, followed by contributions in Section \ref{sec:Contributions}. Then it discusses future directions in Section \ref{sec:FutureDirections}.

\section{Research Summary}
\label{sec:ResearchSummary}
In the software engineering field, researchers and practitioners have put increasing effort on low-level software processes \cite{AgileAlliance,Larman:03} such as PSP, TSP and Agile Processes. Though proven to be useful in improving software quality\cite{Ferguson:97,Kamatar:00,MicrosoftTSP,Janzen:05}, low-level software processes are hard to execute correctly and repeatedly. Also, low-level software processes have the potential to require new skills from software organizations, project managers, and software developers. For example, in Test-Driven Development (TDD), each developer is a requirements analyst, designer, tester, and coder. As a result, a low-level software process could be used differently in different software organizations. Worse yet, an organization might think they are using a particular low-level process, such as TDD, but in reality, they are doing something quite different. Thus, I put my research effort on automated recognition of low-level software process development behaviors. As a step in this research area, I focused on one low-level software process called TDD and implemented the Zorro software system to automate the recognition of development behaviors of TDD. In addition, I proposed the Software Development Stream Analysis (SDSA) framework to assist the research on low-level software processes.

Test-Driven Development (TDD), a core practice of Extreme Programming, has been widely adopted by software industry and studied by software engineering researchers. So far, software engineering researchers have focused most of their energy on the outcomes that applying TDD brings to software products and software developers. However, compared to the claims made by practitioners, research findings of TDD on software quality and developer productivity are mixed. In fact, much of the research work on TDD suffers from the threat of ``construct validity'' \cite{Wang:04} because of the ``process conformance'' problem. Janzen and Saiedian \cite{Janzen:05} warn that the inability to accurately characterize process conformance is harmful to TDD research, and that it is so hard to measure the usage of a development method such as TDD that current reports on adoption of TDD are not valid. Fortunately, with the development of sophisticated software metrics collection system such as Hackystat \cite{Hackystat}, it is possible to study the process conformance of TDD. 

In order to study the process conformance of TDD, I implemented the Zorro software system with the aids of the Hackystat and SDSA frameworks. Hackystat sensors instrument the development environments to collect software process and product metrics. SDSA is a framework that I have created for studying low-level software processes. Using SDSA, Zorro abstracts a variety of software metrics into development activities, merges these activities together to form a time-series software development stream, and finally partitions the stream into a group of episodes ended with successful unit test invocations. In order to infer development behaviors of TDD, I defined a set of specific rules in Zorro according to Beck \cite{Beck:01,Beck:03} and others who have described the practices of TDD. The ``test-pass'' episodes are categorized as ``test-first'', ``refactoring'', ``test-addition'', ``regression'', ``code-production'', ``test-last'', ``long'', or ``unknown''. After inferring development behaviors in episodes and categorizing them, Zorro uses the classification results as well as the context of episodes to reason the conformance of TDD. Moreover, with the inferred results, Zorro implements a handful of analyses that are grouped into two categories. The first category of analyses study a single programming session and report different aspects of TDD. One analysis in this category is the ``TDD Episode Demography'' analysis that can be used to look for the development patterns of TDD. The second category of analyses leverage software project telemetry \cite{csdl2-04-11,csdl2-06-05} that can support interpersonal in-process project management and decision makings in the granularities of daily, weekly and monthly. For instance, I compared differences of developers' testing effort between ``The-TDD-Project'' and ``The-NON-TDD-Project'' on the weekly basis using the telemetry stream of ``TPRatio-DevTime-Chart'' in the industrial case study (Chapter \ref{ch:Industry}).

In order to empirically evaluate Zorro, I have conducted three case studies --- a pilot study (Chapter \ref{ch:Pilot}), a classroom case study (Chapter \ref{ch:Classroom}), and an industrial case study (Chapter \ref{ch:Industry}) to investigate whether Zorro can collect sufficient software metrics and how well it can infer TDD compliance. I summarize the research findings of these studies in the following.

\subsection{Data Collection}

One of my primary focuses was on evaluating Zorro's data collection because collecting necessary development activities is a must for Zorro to infer development behaviors. I implemented the Eclipse Screen Recorder (ESR \cite{esr}), an Eclipse plug-in that can record the software development activities occurred in the Eclipse IDE. Most importantly, it can be configured to  record the Eclipse screen in the frequency of one frame per second and the recorded video file size is just 5-7MB per hour. With the help of ESR, I validated Zorro's data collection in the pilot study and the classroom study. 

According to my research in these two studies, Zorro is capable of collecting development activities. In the classroom case study, I found that Zorro on average collected more development activities (16.8 per episode) than what I observed (14.9 per episode) in the recorded ESR videos (See Table \ref{tab:ActivityNumberSummary}). Given that ESR can capture almost every activity occurs in the Eclipse IDE, Zorro does a good job in collecting development activities. 

In both the pilot and the classroom studies, I compared two sources of data side by side to discover any hidden problems in Zorro's data collection. It turned out that a few problems (Sections \ref{sec:Pilot-Validation-Collection} and \ref{subsec:SensorDataValidation}) existed but none of them were significant. Instead, the G2-DevBehavior, a development behavior that I discovered in the classroom case study, significantly impacted Zorro's inference accuracy of development behaviors and compliance of TDD. If there were not G2-DevBehavior, Zorro could infer TDD compliance with 90+\% accuracy (Section \ref{subsubsec:EpisodeBehavior}). Thus, correctly recognizing the G2-DevBehavior has the potential to greatly improve Zorro's reliability. 

\subsection{Development Behavioral Inference} 

The other primary focus was on evaluating Zorro's inference of TDD development behaviors. I evaluated the development behavioral inference with the help of ESR in both the pilot study and the classroom study. In the data analyses, I watched the recorded ESR videos to observe participants' development behaviors and used the observed results to validate Zorro's inference. 

Compared to participant observation, Zorro's inference accuracy of development behaviors is 88.4\% in the pilot study (See Table \ref{tab:EsrPilotStudy}). This value is 70.1\% in the classroom study (See Table \ref{tab:EpisodeBehaviorAgreed}). However, these two values can not be directly compared since I revamped the classification of development behaviors in Zorro after the pilot study. A notable phenomenon occurred in the classroom case study was the so called G2-DevBehavior (Section \ref{subsec:ParticipantGroup}), which diverted Zorro's partitioning of episodes and inference of development behaviors. Further investigation in Chapter \ref{ch:Classroom} concludes that the inference accuracy is 89.6\% for group G1 who did not conduct G2-DevBehavior. All in all, it indicates that allowing Zorro to interpret G2-DevBehavior is necessary.  

%Additionally, I introduced compliance to TDD in Zorro after the pilot study. Zorro uses the inferred development behavior of an episode as well as its context to determine its conformance to TDD (See Section \ref{sec:Zorro-TDDConformance}). In the classroom case study, Zorro infers that 82.5\% episodes are compliant to TDD; whereas my observatin concludes that 93.4\% episodes are compliant to TDD. So Zorro is more conservative in determining developers' TDD compliance. 

\subsection{Usefulness}
Last, I also focused on evaluating Zorro's usefulness in the case studies. The ``Episode Demography'' and ``T/P Effort Ratio'' are two most useful analyses for beginners to understand and improve TDD practice based on evaluation results of the classroom study. In the industrial case study, I collaborated with Dr. Hanssen, a researcher who conducted a comparison study between TDD and an existing Test-Last practice in an European software company. This research supports the conclusion that Zorro's automated unobtrusive data collection and inference of development behaviors are useful for researchers to collect data without interrupting the development process. 

\section{Research Contributions}
\label{sec:Contributions}
%This research leverages Hackystat, an automated software metrics collection and analysis system,  to automatically infer development behaviors of low-level software processes. 
This research has three main contributions: 
\begin{itemize}
\item Software Development Stream Analysis Framework,
\item Automated recognition of TDD with Zorro,
\item Empirical evaluation of Zorro.
\end{itemize}

\subsection{Software Development Stream Analysis (SDSA) Framework}
The SDSA framework is the most significant contribution of this research. A problem with low-level software processes is that many organizations might use them differently based upon their understanding, which makes it hard to study their impacts on software development. The SDSA framework can automatically evaluate how well an organization executes software processes with only minimum interruption to the development process.

In order to study low-level software development processes, SDSA abstracts development activities of a programming session into a software development stream, a linear and time-series data structure. Corresponding to the incremental and iterative property of many low-level software processes, SDSA uses tokenizers to partition a long development stream into many short episodes, another abstract data structure representing a micro-iteration of a software process. 
%Once a development stream is constructed, many machine learning algorithms such as pattern matching can be applied to study the development process. 
Finally, SDSA uses JESS, a rule-based system in Java to recognize and classify development behaviors of partitioned episodes. 

In my thesis research, I instantiated the SDSA framework on Test-Driven Development (TDD), and the system resulting from this work is the Zorro software system. Zorro can automatically infer the development behaviors and the compliance of TDD. This research work demonstrates that the SDSA framework has the potential to be useful for researching other low-level software processes. 

\subsection{Automated recognition of TDD with Zorro}
Another significant contribution is the Zorro software system that was built on the top of the Hackystat and SDSA frameworks. Zorro recognizes TDD development behaviors conducted in an IDE as long as its sensor supplies all required metrics listed in Table \ref{tab:Zorro.Sensors}. In my research, I enhanced the Eclipse sensor and evaluated its data collection capability in the pilot and classroom case studies. As part of the industrial case study, I enhanced the Visual Studio .NET sensor to be Zorro compatible. 

Many analyses such as ``Episode Demography'' were developed to leverage Zorro's reasoning of development behaviors of TDD. Furthermore, with the help of software project telemetry, I developed a set of telemetry reducers of TDD to support management of TDD projects. 

Also, I implemented a rule set for TDD based upon the descriptions of many well-known TDD practitioners including Beck \cite{Beck:03}, Doshi \cite{TDDQuickReference} and Erdogmus \cite{Wang:04}, and additionally my grounded observation of TDD in practice. 

\subsection{Empirical evaluation of Zorro}

The third contribution is the empirical evaluation studies I conducted. The pilot and classroom studies exemplify a paradigm of empirical validation of Zorro. All the research materials were made public for others to validate Zorro in different environments. The industrial case study demonstrates how to conduct TDD research that does not suffer from the process conformance problem with the help of Zorro. In addition, the actual results of these studies are also my contributions. 
\begin{enumerate}
\item The Eclipse sensors collect sufficient in-process metrics for inference of TDD;
\item Zorro can identify TDD when it occurs in the IDEs of Eclipse and Visual Studio .NET; 
\item Zorro is helpful for beginners to understand and conform to TDD;
\item Zorro is useful for researchers to conduct TDD evaluation studies.
\end{enumerate}

\section{Future Directions}
\label{sec:FutureDirections}

\subsection{TDD Evaluation Studies}
First, Zorro can be used in evaluation studies of TDD to improve validity of research conclusions. The industrial case study I conducted in my thesis research is an attempt in this direction.

\subsection{Unified operational definition of TDD}
Second, reaching an unified operational definition of TDD is necessary. Beck uses the red/green/refactor to describe the order of TDD programming in \cite{Beck:03}. %But, in reality, developing software in TDD is significantly more complicated than this simple abstraction. 
Following this abstraction, I defined three types of development behaviors ``test-first'', ``refactoring'' and ``test-last'' in the prototype implementation of Zorro. Then I used this prototype in the pilot study in which I surprisingly found that half of the episodes were ``test-last'', which is very contradict to my intuition. With the pilot study, I realized that TDD in reality is quite different from in theory, and revamped Zorro's classification of TDD development behaviors (Section \ref{sec:ZorroBehaviorCategory}). Moreover, I introduced a heuristic algorithm for inference of TDD compliance (Section \ref{sec:Zorro-TDDConformance}) to Zorro. This is an operational definition of TDD. It works well according to the classroom case study conducted in my thesis research, but the conclusion is limited to the environment I tested. More replication studies need to be conducted in order to reach a unified operational definition of TDD that can be agreed upon by the community of TDD practitioners and researchers.

\subsection{More practical uses of Zorro's inference results}
Third, finding more practical use of Zorro is one more future direction. In my research, I designed and implemented some typical analysis such as ``Episode Demography'' and ``Episode Duration Distribution'' and some TDD telemetry streams. My initial evaluation concludes that they are useful for beginners and researchers of TDD. However, in order to fully use Zorro's potentials in software project management and software process improvement, finding more practical uses of Zorro is important. 

One practical use of Zorro is to study how to interpret Zorro's inference results. For example, will 100\% be necessary in actual software projects? Or how much compliance of TDD is acceptable?
Another interesting use of Zorro is to compare the inferred results to test coverage. Beck claims that TDD should yield 100\% test coverage automatically, but more research needs to be done to study this claim. 

Zorro can be used as a CASE (Computer Aided Software Engineering) tool for software project management. I applied Zorro to a TDD and a Non-TDD projects in the industrial case study. With the help of Zorro, I generated ``sensor installation status'' and ``TDD telemetry'' for the project manager. These reports helped the project manager to realize that problems existed in sensor installation and data collection. However, this research work only uses a few analyses Zorro provides. Additional research needs to be conducted to study how to use Zorro for software project managements.

\subsection{Other low-level software processes}
Zorro was built on the top of the more generic SDSA and Hackystat frameworks, this architecture makes it easily possible to study other low-level software processes or best practices of software development as well. For example, low-coupling is a desired property to objects in the object-oriented programming. With Hackystat and SDSA, we can easily define rules to find objects that are either excessively edited by developers, or overly referred by other objects in the development process. 

\subsection{Data mining}
At last, applying some data mining algorithms on software development streams may create interesting applications. The SDSA framework sorts a variety of software metrics collected by Hackystat sensors and organizes them as time-serious software development streams. I applied a rule-based system on them to infer development behaviors and compliance of TDD in my research. The rule-based system is powerful for recognizing well-defined development behaviors such as TDD. However, in cases that expected development behaviors are unknown, it will be better to use some data mining algorithms. For example, Heierman et. al. introduce the Episode Discovery \cite{Heierman04} algorithm to discover and classify naturally recurring patterns from temporal sequences of human-generated activities. This algorithm can be used to mine the software development streams to find recurring patterns of development activities.