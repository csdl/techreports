\chapter{Introduction}\label{chapter_introduction}
\textit{The central issue addressed in this dissertation is the possibility of recurrent 
behaviors discovery from publicly available software process artifacts. 
I have explored an approach to this problem based on the transformation of artifact trails into 
time series and application of data-mining technique based on finding characteristic features which 
are related to recurrent behaviors. 
This thesis presents results of my exploratory study consisting of two parts: 
a novel algorithm for interpretable time series classification that enables finding 
and ranking of class-characteristic features, 
and the results of few case studies, where I have explored the algorithm's applicability to the 
problem of recurrent behaviors discovery from public software process artifacts.}

\textit{The terms used throughout the thesis are defined in the Section \ref{sec_terminology}. 
Problem's background is given in Section \ref{sec_background}. 
Section \ref{section_software_process_design} discusses traditional approaches to software process 
design and contrasts them with open-source software processes design.
Sections \ref{sec_software_trajectory} reviews a previously developed technique to software process 
and product improvement based on continuous measurements and introduces software trajectory.
Section \ref{sec_research_hypothesis} presents the research hypothesis.
Section \ref{sec_knowledge_discovery} provides introduction into the problem of knowledge discovery 
from time-series.
Section \ref{section_trajectory_definition} connects two research areas and provides definitions.
Section \ref{section_contributions} enumerates main contributions of the thesis, 
while section \ref{section_organization} explains the thesis organization.}

\section{Basic terminology}\label{sec_terminology}
\begin{defn}\label{def_process}
A \textbf{\textit{Software Process}} defines resources, artifacts, but most importantly a set
of activities performed in order to design, develop, and maintain software systems.
\end{defn}
Examples of such activities include requirements collection and creation of UML diagrams, 
coding, system integration tests, etc. The intent behind a software process is to provide a 
control over software development by implementing a global strategy and by structuring
and coordinating human activities in order to achieve the \textit{goal} - to deliver a functional
software system on time and under the budget. 

\begin{defn}\label{def_process_desc}
A \textbf{\textit{Process Description}} is any sort of written process description defining 
some or all of needed resources, artifacts, actions and behaviors intended for development 
of a software system.
\end{defn}

\begin{defn}\label{def_process_model}
A \textbf{\textit{Process Model}} is a complete process description in some formal language 
that is mathematically unambiguous. Completeness of a model guarantees a rigorous specification
that is ready to be executed.
\end{defn}

\begin{defn}\label{def_metrics}
A \textbf{\textit{Software Metrics}} is a characteristic of a software system or a software process that can be 
objectively measured.
\end{defn}
Examples of software product metrics include the size of a software system measured in lines of code (LOC) or 
in functional points (FP), and the number of defects discovered in a delivered system; 
examples of software process metrics include the velocity of a software process called ``churn'' which is the 
amount of LOC added per day, the response time to fix an issue, and the ``technical debt'', 
that measures deterioration of the code quality over time. 
In addition, both product and process metrics, can be used to derive high-level software project metrics including 
cost, schedule, and productivity.

\begin{defn}\label{def_artifact}
A \textbf{\textit{Software Artifact}} is one of numerous byproducts of a software process - a use-case, 
an UML class diagram, a change record, or a bug report. 
\end{defn}
Artifacts are not only produced by a processes, but typically re-used within the process to support 
software development activities and to document the resulting software system.

\begin{defn}\label{def_behavior}
A \textbf{\textit{recurrent behavior}}, in context of software process, defines a \textit{frequent 
(i.e. supported by a numerous evidence) mannerism} in which a developer, or a team, conduct their everyday work.
\end{defn}

%
% >> section
%
\section{Background}\label{sec_background}
Contemporary software projects concern with development of complex software systems and typically have 
a considerably long life-cycle - well over decade.
A project's development and maintenance activities are usually carried out by geographically 
distributed teams and individuals. The development pace, the experience, and the structure of the 
development team continuously change with project progression and as developers joining and leaving. 
When combined with schedule and requirements adjustments, these create numerous difficulties 
for developers, users, and stakeholders, ultimately affecting the project success \cite{citeulike:2207657}. 

This software development complexity phenomena was identified in 1968 as ``Software crisis'' 
\cite{naur_crisis_68}, and was addressed by bringing the research and the practice of software development 
(or as it was called ``programming'') under the umbrella of Engineering - in an effort to provide 
the control over the process of software development. 
Following the engineering paradigm, numerous models of software design and development 
process, known as \textit{software processes}, were proposed \cite{citeulike:10002165}.
Some of these were formalized and evolved into industrial standards for software development processes 
such as CMM \textbf{cite}, ISO, PSP, and others \cite{citeulike:5043104}. 
However, in spite of this effort, industrial software  development remains error-prone and more 
than half of all commercial software development projects ending up failing or being very poorly executed 
(``Chaos Reports'', 2006 \cite{chaos2006}). Some of them are abandoned due to running 
over budget, some are delivered with such low quality, or so late, that they are useless, and some, 
when delivered, are never used because they do not fulfill requirements. 

Through the analyses of software project failures, it was acknowledged, that the engineering 
paradigm might not be the best way to provide a control over software development processes 
due to the fact that Software engineering is dealing with significantly different from other 
Engineering fields problems \cite{citeulike:3729379} \cite{citeulike:5203446} \cite{citeulike:2207657}.
The chief argument supporting this point of view is the drastic difference in the cost model:
while in Software Engineering there is almost no cost associated with materials and 
fabrication, these usually dominate cost in all other Engineering disciplines, but, 
ironically, Software Engineering is suffering from the costs and challenges associated with 
continuous re-design of the product and its design processes - the issue that is 
hardly seen at all in other Engineering areas. 
In addition, as pointed out by numerous research, the most of engineering-like models are 
prescriptive and rigid - i.e. they are difficult to tailor to the particular organizational structure,
to the project specificities and to changing requirements - thus, the degree to which an adopted model 
structures processes greatly varies between teams and projects and does not guarantee the success \cite{sacchi_2001}. 
Finally, increasing understanding and appreciation of the human role in software development 
processes over tools, technologies, and standards, suggests that the human-driven software process 
aspects are likely to define a software project fate \cite{citeulike:6580825} \cite{citeulike:149387} 
\cite{1605185} \cite{citeulike:113403} \cite{citeulike:12743107}. 

These engineering issues has been addressed in the software development communities practicing 
alternative to Software Engineering processes, where a number of flexible developer- and user-oriented 
software processes emerged from academy, hobbyists, and practitioners.
Among others, Free/Libre/Open-Source Software development model (FLOSS) and the 
Software Craftsmanship  approaches gained a significant credibility in the community \cite{citeulike:3729379}. 
While the former \textit{holistic} software process paradigm emphasizes loosely-organized 
collaboration, high degree of modularity, frequent releases, and effectively removes the boundary 
between developers and users, the latter, human-centric approach, is built upon the roles of highly 
motivated skilled individuals \cite{citeulike:262020} \cite{citeulike:2759198}. 

Nevertheless, the same complexity issues affects alternative processes.
As it was shown, most of FLOSS projects never reach a ``magic'' 1.0 version \cite{citeulike:12480029}. 
Among others, the great "infant mortality rate" of FLOSS projects was related to a burnout, 
inability to acquire a critical mass of users, loss of leading developer(s), and forking \cite{richter2007critique}. 
Software craftsmanship, from other hands, not only challenges developers with technological advances 
requiring continuous skills improvement, but creates significant cost and effort estimation difficulties for
stakeholders and project managers \cite{citeulike:11058784}. However, despite to these issues, 
it was proven, that the user- and developer-centric, disciplined manner of programming, along with the high 
degree of the software modularization allows to establish software processes that are not only comparable 
with industrial engineering-like processes in quality, but is capable of delivering of large and 
reliable software systems.

Currently, it is widely acknowledged, that there exists no single ``silver bullet'' process which 
guarantee to bring a software project to success \cite{citeulike:1986013}. 
Processes are numerous, each has advantages and drawbacks, and each accompanied with 
numerous success stories and failure experiences, making the process selection difficult 
and the results of its application unpredictable.
The uncertainty, and the alarming rate of projects failures suggest, that our understanding 
of software development ``mechanics'' is limited and insufficient \cite{citeulike:12550665}. 
The enormous cost of the lost effort, measured in hundreds of billions of US dollars 
\cite{citeulike:2207657} \cite{citeulike:2207653} \cite{citeulike:2207655}, 
continues to provide motivation for further research on software processes. 

%
% >> section
%
\section{Software process design}\label{section_software_process_design}
Traditionally, it was assumed that the software development is apriori performed for a profit 
in corporate, government, or military settings by people that are mostly collocated 
together. This assumption yielded many software development models describing 
``on-site, software manufacturing'' processes which were researched and discussed 
for decades in software  engineering literature. 
Currently, however, we see the rise of alternative software processes - people are coming 
together over the Internet and create high quality software which they distribute openly 
promoting its modification and re-distribution. Surprisingly, they provide a very little 
guidance on their software processes. 

In this section I would like to re-visit traditional and alternative approaches to software process
design and improvement, contrasting their specificities.

\subsection{Traditional approaches to software process design}\label{sec_traditional_software_processes}
Traditional approaches to software process design and improvement can be divided into two 
distinct categories. 

The first category consists of \textit{top-down} techniques through \textit{proposing a process} based 
on specific patterns of software development. 
For example, the Waterfall Model process proposes a sequential pattern in which developers first create a 
Requirements document, then create a Design, then create an Implementation, and finally develop Tests. 
The Test Driven Development process, from other hands, proposes an iterative behavioral pattern in which
the developer must first write a test case, then write the code to implement that test case, then re-factor the 
system for maximum clarity and minimal code duplication \cite{citeulike:6086365}. 

While the top-down approach follows the usual path of trials and errors, and seems to be an 
extension of natural to humans creative processes of invention and experimentation, 
the ``invention'' of an adequate to the task software process is far from trivial 
\cite{citeulike:5043104} \cite{citeulike:1986013}. 
Moreover, the evaluation cycle of an invented process is usually very expensive and considerably long.
In addition, it was shown that the process inventors are usually limited in their scope and tend to 
assume idealized versions of real processes, thus, often produce ``paper lions'' - process models which are 
likely to be disruptive and unacceptable for end users, at least in their proposed form \cite{citeulike:9758924}.

The second category of software design approaches consists of \textit{bottom-up} techniques that focus 
on the \textit{performed process reconstruction} through noticing of recurrent development events. 
Usually, the process reconstruction task is viewed as a two-levels problem where the first level 
consists of a event discovery (process segmentation) while the second level consists of process 
reconstruction through the events network analysis \cite{citeulike:2703162}.
One of the first works in this category by Cook and Wolf shows a possibility of automated extraction 
of a process model through the mining of recorded process event logs 
\cite{citeulike:328044} \cite{citeulike:5120757} \cite{citeulike:5128143}. 
Later work by Huo et al. shows that it is also possible to improve an existing process
through the event logs analysis \cite{citeulike:7691059} \cite{citeulike:7690766}. 

The bottom-up approaches, while appearing to be systematic and potentially less challenging than invention, 
are also affected by a number of issues, among which the observability is the most significant: 
while a live project observations are technically challenging to implement due to the high cost and 
privacy concerns, the post-process data collection affects its reconstruction due to 
discrepancies between actually performed and reported actions \cite{citeulike:7691059}. 
Yet another significant issue is the capacity of currently available process discovery and representation 
techniques - typically these need to be supervised by experts and finely tuned in order to reconstruct 
distributed and concurrent software processes. 

While the both approaches are opposite in their nature, they yield very similar process models that 
effectively are series of actions (or states) that must be performed (visited) successively in order 
to deliver a software. The ``process inventors'' put the best of their knowledge, experience, creativity,
and logical reasoning into the proposed sequence of steps, similarly, the process re-constructors 
strive to eliminate the noise and to converge to a concise sequence of steps that is supported by the 
majority of observations. 

This particular attention to the synthesis of sequential steps in traditional approaches, 
leaves other human-centric phenomenas, such as team's structure, work schedule, 
developer's discipline, behaviors, and motivation behind -- 
the issue that has been widely recognized \cite{citeulike:149387} \cite{citeulike:113403} 
\cite{citeulike:205322} \cite{citeulike:12798652} but still largely ignored in industrial practices 
mostly due to the  difficulties with human component benefits estimation 
\cite{citeulike:12798659} \cite{citeulike:12798662} \cite{csdl2-12-11}.

%
% >> section
%
\subsection{Free/Libre/Open Source processes}\label{sec_floss_processes}
Another phenomenon generating novel software processes is the social movement inspired by the philosophy 
of source code sharing and its collaborative improvement, that is called called ``free-software movement''. 
This social phenomenon originating from 1960s was partially formalized in 1983 by Richard Stallman,
who launched GNU Project and later, in 1985, founded the Free Software Foundation in order to support 
it. The ``open-source'' term, that is commonly used for description of free and libre open source software, 
was coined later, in 1998 at the very first Open Source Initiative (OSI) meeting \cite{osi-history}.

The open-source software development community, consists of self-organized individuals and teams of 
mostly non-professional programmers - amateurs, hobbyists, students, and academic software developers. 
The software developed by the community is distributed freely along with its source code and is 
usually called free/libre open-source software (FLOSS). Here, ``free'' corresponds to a freeware - 
a software that is distributed free of charge, whether ``libre'' corresponds to a software that is 
not necessarily free: typically, in order to partially cover the cost of the development, 
fund the software customization, maintenance, and support, open source developers are often paid 
by commercial users or the use of a software is licensed for a fee.

Over the years, this software development model has proven its ability to deliver increasingly complex 
and surprisingly popular software in a ``global'' scale - when thousands of non-professional developers 
and users are scattered all over the world. A number of open source projects such as Linux and its 
derivatives, Gnome, Apache HTTP Server, MySQL database, and others, succeeded to develop efficient 
distributed software processes that provide control over the large development team and code-base.
Moreover, these processes allowed to deliver the state of the art software whose quality is similar 
or exceeding that of industrial projects \cite{coverity2012}. 
In turn, this attracted a considerable attention not only from industrial companies that seek to emulate 
successful open source software processes in traditional closed-source commercial environment 
\cite{oss_virtual_organizations} \cite{oss_balance} \cite{oss_hp} \cite{oss_4industry}, 
but from software process research community, that is fond of finding of novel software processes
\cite{citeulike:12550640} \cite{citeulike:5043664} \cite{citeulike:5128808} \cite{citeulike:10377366}.

\begin{figure}[ht!]
   \centering
   \includegraphics[width=140mm]{figures/Linus.Kernel.ps}
   \caption{A Torvald's response in the mailing list suggesting that practical reasons, the ``real-life'', 
   should be always considered over specifications.
   Excerpt from Linux mailing list. \url{http://lkml.indiana.edu/hypermail/linux/kernel/0509.3/1441.html}}
   \label{fig:kernel}
\end{figure}

A number of studies conducted previously on open source processes discovered, that they differ from a 
traditional software development at many levels. 
One of the major found differences is the flexibility of FLOSS processes. 
For example, the most significant document in industrial software processes - the specification - is rarely 
considered in FLOSS projects, moreover, if exists, it is often discarded as not corresponding to changing 
project needs. 
Even in the Linux kernel development, which is probably one of the few strictly moderated FLOSS development 
processes, developers prise practical reasons over specifications \ref{fig:kernel}.

Obviously, the high degree of flexibility makes it difficult to control the project evolution. 
In order to overcome this issue in the concurrent development, many FLOSS projects view the final 
software ``look and feel'' and functionality as open-ended questions and encourage their developers to 
make only small incremental improvements and to commit code changes often 
\cite{so-checkin} \cite{git-best-practices1}. This frequent commits policy and the small change visibility 
practice are often cited as vital for the health of an open-source software process as allowing all developers 
to follow closely the project evolution and to respond to changes promptly.
Another way to manage the distributed development complexity practiced in open-source communities is the 
limitation of the access to a software main development tree and encouraging of ``forking'' by non-core developers. 
Later, if a forked version would be found as satisfying to the project needs, the changes are merged with the 
main tree by core developers.

Note, that both these techniques, while providing control over open-source development processes do not prescribe 
any of specific sequences of development actions or any formal methodology which developers are obliged to follow.
This characteristic freedom allowed in open-source projects provides a thriving human-centric environment for 
creative individuals and teams to invent and to immediately test any of ideas through their own experiences. 
Such environment, potentially, generates novel software processes, improves existing, and tests them in a real life
at the same time. 
In addition, the small incremental change philosophy and the frequent commit policy potentially contribute to the 
creation of numerous public artifacts providing a fine resolution into performed FLOSS processes making open-source 
software processes research the only viable option for large-scale software process research.

\subsection{Public software repositories}
Currently, with accessible personal computers, friendly software development toolkits, and massification of 
the high-speed Internet access and mobile devices, the small-scale recreational open-source programming 
become very popular. 
The web is commonly used as a platform for collaborative work: free code hosting sites such as SourceForge, 
GoogleCode, and GitHub host thousands of FLOSS projects offering free public access to numerous software 
process artifacts, such as design documents, source codes, bugs and issue records, and developers communications.
In addition, Q\&A and social websites for developers such as StackOverflow, TopCoder, and others, becoming 
increasingly popular among software developers and users as places to exchange expertise, to learn new tools, 
and to improving skills.

The proliferation of open-source software development and increasing abundance of publicly available 
software process artifacts continues to change the software process research landscape,
reviving the interest for process enactment and reconstruction, as well as bringing the attention to the 
human-specific components of software processes. 

The public availability of numerous software process artifacts covering the full life cycle of the software
development, effectively removes not only the high cost of observation, but most of the privacy concerns - 
the two issues that previously made any large-scale analysis of software projects unfeasible for most researchers.

Scientific community response on the availability of public artifacts was overwhelming, and a number of 
venues was established addressing the increased interest. 
Since 2004, the International Conference on Software Engineering (ICSE) hosts a Working Conference on 
Mining Software Repositories (MSR). The original call for papers stated MSR's purpose as 
\textit{``... to use the data stored in these software repositories to further understanding of software 
development practices ... [and enable repositories to be] used by researchers to gain empirically based 
understanding of software development, and by software practitioners to predict and plan various aspects 
of their project''} \cite{msr2004} \cite{citeulike:7853299}. 
Several other venues: International Conference on Predictive Models in Software Engineering \cite{promise12}, 
International Conference on Open Source Systems, the Workshop on Public Data about Software Development, 
and the International Workshop on Emerging Trends in FLOSS Research have also played
an important role in shaping and advancing this research domain.

Some of the published work addresses the software process discovery. Among others, most notable and 
relevant to my research is work by Jensen \& Scacchi. In their early work, they demonstrated, that 
information reflecting software processes can be gathered from public systems \cite{citeulike:12550640}. 
Later, in \cite{citeulike:5043664} and \cite{citeulike:5128808}, they show, that by manual mapping of 
collected process evidence to a pre-defined process meta-model it is possible to reconstruct some 
of the FLOSS processes. 
Another closely related to my research is work by Hindle et al. where they has shown that it is possible to 
discover software process evidence through partitioning \cite{citeulike:10377366}.

However, the research work based on mining of software process artifacts shows, that while public availability 
of artifacts is minimizing observability and privacy issues, the nature of these artifacts creates a number of 
challenges which I discuss in the chapter X, which limit the possible scope of the research and significantly 
elevate the complexity of the process discovery effectively rendering previously designed techniques inefficient.
Thus, the novel analysis and discovery techniques are needed to be developed for public software process artifacts 
analysis \cite{citeulike:7853299}.
% when ``\textit{... going beyond code and bugs...}'' 

%
% >> section
%
\section{Software Trajectory}\label{sec_software_trajectory}
In addition to the establishing of engineering-like software process paradigm, the acknowledgement of 
the software crisis led to the development of project management techniques based on software measurement, 
that was found as a key component in establishing of scientific basis for software engineering.

\subsection{Software measurement}\label{sec_software_metrics}
The goal of software measurement is to make objective judgments about software process and product quality. 
It has been shown that an effective measurement programs help organizations understand their capacities and 
capabilities - so that they can develop achievable plans for producing and delivering of software products. 
Furthermore, a continuous measurement effort provides an effective foundation for managing process 
improvement activities, such as PSP \cite{citeulike:8347315}, \cite{citeulike:5090131} 
\cite{citeulike:12929216}, CMM \cite{citeulike:9962021}, ISO 9001 \cite{iso-standard}, 
and SPICE \cite{spice-standard}.

In addition to practical applications, software measurement is extensively used in the research - it is the basis of 
Empirical Software Engineering research area where researchers base their conclusions on concrete evidence collected 
through experimentation and measurement of software systems and software processes \cite{citeulike:766768}.

\subsection{Software project telemetry}\label{section_software_telemetry}
Ideally, by using measurements, a software process and product can be assessed in real-time, allowing efficient 
in-process decision making.
Johnson et al. in \cite{citeulike:557296} pioneered this approach by defining software project telemetry as a 
particular style of software process and product metrics collection and analysis based on 
\textit{automated measurements over a specified time interval}. 
They implemented an in-process software engineering measurement and analysis system called Hackystat 
\cite{citeulike:12929227}, that is capable of metrics collection, processing, and telemetry streams visualization. 
The authors showed, that the visual analysis of multiple telemetry streams aids in the in-process decision making, 
and it is possible to improve existing software processes by using the knowledge extracted by experts through visual 
analysis of these streams. 
At the same time, they acknowledged, that it is impossible to extract an analytical model that is capable to 
automate that decision making process.

Later, Kou et al. extended Hackystat by adding architectural layer capable of collecting additional process metrics 
and partitioning telemetry streams sequences into development ``episodes'' \cite{citeulike:6180831}. 
Further, by designing ``operational definitions'' of test-driven development (TDD) encoded as rules,
they showed that it is possible determine whether the episode is compliant with
defined TDD patterns.
The approach was implemented as Hackystat extension and called Zorro \cite{citeulike:11538873}.

\subsection{Software trajectory}
One of the research directions, preceding this thesis work, was to explore the possibility of the creation of similar to Hackystat/Zorro 
system that would be capable of automated patterns mining from telemetry streams obtained by measurements of publicly 
available software process artifacts. 
Unfortunately, it turned out that the granularity level of public artifacts is too high to apply this technique. 
In order to keep the research direction, I defined an alternative to software telemetry - Software Trajectory \cite{csdl2-10-09}:
\begin{defn}\label{def_trajectory}
A \textbf{\textit{software trajectory}} is a curve, possibly multi-dimensional, that describes a software project 
progression in the space of chosen software metrics.
\end{defn}
Software trajectory abstracts software process artifact trail by representing it as multidimensional vector of ordered, 
non-equidistantly spaced measurements that is essentially time series. This time series describes an approximate path that 
the software, or the process draw in a space, by the analogy to approximate trajectories of objects in a physical space, 
or reduced in complexity sequence of states of a dynamic system (Poincare' maps).
The study of software trajectory properties led to this dissertation.

%
% >> section
%
\section{Research hypothesis, scope of the dissertation}\label{sec_research_hypothesis}
In previous sections, I have outlined the evidence of the limited performance of traditional engineering-like 
software processes \ref{section_background}, as well as the oversight by traditional approaches to software process 
design of a variety of human factors falling beyond a typical sequence of development actions 
\ref{section_software_process_design}.
Then, I have identified a number of key differences of FLOSS processes that can potentially shed light on 
human-driven aspects of software development \ref{floss_processes}.
Finally, I have pointed out a growing wealth of publicly available software process artifacts enabling quantitative 
and qualitative analyses of software development processes \ref{section_public_repositories} and outlined a need for 
new knowledge discovery techniques capable of processing these datasets.

All this provided the motivation for my exploratory study, in which, I attempted to explore the possibility 
of discovery of a particular human-driven aspect - \textit{\textbf{recurrent behaviors}} - from FLOSS software 
processes artifacts.
\begin{defn}\label{def_process}
A \textbf{\textit{recurrent behavior}}, in a context of software process, defines a \textit{frequent 
(i.e. supported by a numerous evidence) mannerism} in which a developer, or a team, conduct their everyday work.
\end{defn}

For example, if one developer frequently runs unit tests before committing changes into repository, 
while another typically commits changes without running the tests, the first developer's habit of 
preceding to the commit code testing is a recurrent behavior that may reflect the developer's discipline
or an unusual attention to some particular part of the code. 
Consider another example, if one of the developers usually commits code changes in mornings, while another 
developer late in the day, these two recurrent behaviors, might indicate a constraints that are put on the 
project, its software process, or on the developers themselves.
Obviously, latter behaviors should be possible to quantify by simple analysis of commit timestamps, while 
the former can be possibly discovered by the analysis of co-occurring changes in the source code. 

These, and similar recurrent behaviors could be further associated with certain project or process 
traits, such as pace, agility, size, complexity, code quality and others, extending our knowledge of human 
factors influence on software processes and laying a foundation for future research in software processes.

Based on this rationale, and accounting for the proven existence and the effect of recurrent behaviors on 
all levels of software development hierarchy \cite{citeulike:8347315} in open-source \cite{citeulike:200721}
and industrial \cite{citeulike:5090131} development, I hypothesized, that \textbf{\textit{it is also possible 
to discover recurrent behaviors from publicly available software process artifacts}}. 

Following the hypothesis, I have investigated a number of publicly available software repositories,
their artifacts, and a number of applicable data-mining techniques in a preliminary exploratory study 
\cite{csdl2-10-09}. However, similarly to other studies in the field, I have discovered two issues 
significantly affecting efficiency of currently available process mining techniques. 
The first issue is that while FLOSS process artifacts are numerous and readily accessible, 
they are irregular and represent not sequences of development actions, but rather series of software 
state snapshots. 
The second issue is that there is no baseline process exists for any given software project, making the 
assessment of its specificity difficult.

Addressing this issues, I cast the initial problem of event-based recurrent behaviors discovery into more 
generic problem of knowledge discovery from temporally ordered software measurements 
represented as time series, and approached this by developing of a novel technique for interpretable 
comparative analysis of time series that allows class-characteristic patterns discovery and ranking.
I have implemented this approach in the software artifacts analysis framework called 
Software Trajectory Analysis (STA) that automates public software artifacts collection, 
their measurements, software trajectories creation, and their characteristic patterns discovery. 

This dissertation proposes SAX-VSM, a novel algorithm for time-series classification, presents
results of its performance evaluation, details Software Trajectory Analysis framework implementation, 
and discusses its performance evaluation based on three case-studies.

%
% >> section
%
\section{Interpretable time-series classification}\label{section_knowledge_discovery}
In data mining, time series are used as a proxy representing a large variety of real-life phenomena 
in a wide range of fields including, but not limited to physics, medicine, meteorology, music, 
motion capture, image recognition, signal processing, and text mining. 
While time series usually directly represent observed phenomenas by recording their measurable evolution 
in time, the pseudo time series often used for representation of various high-dimensional data 
by combining data points into ordered sequences. 
For example in spectrography data values are ordered by the component wavelengths \cite{citeulike:12550833};
in shape analysis the order is the clockwise walk direction starting from a specific point in the outline 
\cite{citeulike:12550835}, in image classification the numbers of pixels are sorted by color component 
values \cite{citeulike:2900542}.

Many important problems of knowledge discovery from time series reduce to the core task of finding 
characteristic, likely to be repeated, sub-sequences in a longer time series. 
In the early work these were called as 
\textit{frequent patterns} \cite{citeulike:5159615}, 
\textit{approximate periodic patterns} \cite{citeulike:1959582},
\textit{primitive shapes} \cite{citeulike:5898869}, 
\textit{class prototypes} \cite{citeulike:4406444}, 
or \textit{understandable patterns} \cite{citeulike:3978076}. 
Later, similarly to Bioinformatics, these were unified under the term \textit{motif} \cite{citeulike:3977965}.
Once found, motifs can be used for a research hypothesis generation by their association with known
or possible phenomena \cite{citeulike:3977965}. 

The recent advances in semi-supervised and unsupervised finding of such characteristic sub-sequences, 
in particular work based on \textit{shapelets} \cite{citeulike:7344347} \cite{citeulike:11957982}
\cite{citeulike:12552293} and \textit{bag of patterns} \cite{citeulike:10525778}, show a great potential 
of application of time series data-mining techniques to a wide variety of high-dimensional data.

Unfortunately, both techniques provide a limited insight into the data and suffer from performance issues. 
While exact shapelet techniques allow discovery of class-characteristic patterns and facilitate classification,
algorithm is almost quadratic and provides limited insight into class specificities. 
The bag of patterns algorithm, while performs in a linear time, requires a previous knowledge for input parameters 
selection and does not offer the class generalization.

In this thesis I propose an alternative solution to class-characteristic patterns discovery from time series called 
SAX-VSM. As I shall show, the proposed algorithm not only facilitates classification, but provides insights into 
the both: classification results and class specificities. These, in turn, enable the discovery of novel phenomenas.

\section{Contributions}\label{section_contributions}
Main contributions of my work can be summarized as follows: 
\begin{itemize}
\item I propose a novel, generic algorithm for interpretable time series classification: SAX-VSM. 
While the classification performance of this algorithm is at the level of current state of the art, 
it offers an outstanding feature - discovery, generalization, and ranking of class-characteristic features. 
This, in turn, enables knowledge discovery by offering much clearer insight into classification results than any of 
competing techniques.
In addition, SAX-VSM is very fast in classification and has a small memory footprint. 
Overall, I expect this algorithm to play an important role in future because of the growing ubiquity of time series and 
a growing interest in behaviors.
\item Powered by SAX-VSM, I design a Software Trajectory Analysis (STA) framework, and through case-studies 
show its capacity for recurrent behaviors discovery from publicly available software process
artifacts. While case studies are obviously limited, I argue that STA is a useful knowledge discovery tool applicable for a 
variety of software process artifacts and metrics. 
\item Finally, I provide SAX-VSM and STA implementations to community.
\end{itemize}

\section{Dissertation Outline}\label{section_organization}
The rest of this dissertation is organized as follows. Chapter \ref{chapter_background_work} discusses the history 
of Software Engineering, previous work in software process discovery, mining of software repositories, and current 
state of the art in time series mining. Chapter \ref{chapter_sax_vsm} proposes an algorithm for interpretable 
time series classification. Chapter \ref{chapter_sta} discusses the design of STA framework and presents case studies.
Chapter \ref{chapter_conclusions} concludes and discusses several directions for future study.