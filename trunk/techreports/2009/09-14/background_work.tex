\chapter{Prior and related work}\label{chapter_background_work}
Software Trajectory Analysis consists of two components: 
the \textit{software artifacts retrieval and measurement machinery}, i.e. data assimilation layer, 
and the \textit{software trajectory characteristic patterns discovery module}, i.e. analysis layer. 
The high-level overview of these layers show at Figure \ref{fig:STA2-results}.

The artifacts retrieval and measurement machinery refers to a way in which software artifacts are 
collected, measured, and enriched with metadata. 
Currently, STA is capable of retrieving data from typical to OSS development Software Configuration 
Management system (SCM) components such as version control, defect management, and communications 
management systems. In addition it is capable to work with data from other sources, such as Q\&A 
websites, or directly from the Hackystat system \cite{citeulike:557296}.

Note however that STA is not limited to these data sources.
As public data sources are highly heterogeneous, STA adopts the usual to software repository mining 
field strategies for data assimilation, unification, and off-line enrichment \cite{german04_softchange}, 
where public artifacts are retrieved and stored ``\textit{as is}'' (i.e. mirrored) at first, 
enriched with with metadata at second, and measured at the final step.
Similarly to other systems, the relational database is used by STA for data storage and indexing, 
this not only enables an interactive workflow and a federated access to data, but allows an efficient 
measurements partitioning and aggregation, which is essential for software trajectories construction.
Overall, STA data assimilation layer is designed in a way which conforms with the field's best practices
allowing its extension for any data source capable of providing data worth the upstream analyses.

The software trajectory characteristic patterns discovery module relies on the implementation of 
SAX-VSM algorithm that I shall propose and discuss in the Chapter \ref{chapter:sax-vsm} of this thesis.
The algorithm enables discovery and ranking of recurrent patterns from software trajectories, 
which, as I shall show in Chapter \ref{chapter:results}, likely to correspond to recurrent behaviors -- 
the antecedent features of software processes that allow better understanding of software process dynamics, 
and aid in problem detection. Moreover, quantitative assessment of behaviors enables not only process 
improvement hypothesis generation, but, most importantly, its empirical validation.
The overview of the analysis module is shown at Figure \ref{fig:sta-full-overview}.
%\ref{fig:sta-overview} Classes construction, however, is performed by the user depending on the research 
% question and requires some advanced knowledge and experience.

\begin{figure}[h]
   \centering
   \includegraphics[width=150mm]{figures/Flow-analysis.eps}
   \caption{High-level STA overview. Software artifacts are retrieved, enhanced, and measured within the
   data assimilation layer. Next, based on the user input, classes of software trajectories are constructed.   
   In turn, the data analysis layer perform comparative analyses of software trajectories yielding sets
   of ranked class-characteristic behaviors.
   Note, that while for the clarity only two classes of trajectories shown, STA works with many classes and 
   reports many characteristic patterns.}
   \label{fig:sta-full-overview}
\end{figure}

In this chapter, in order to relate Software Trajectory Analysis to other research and to position it 
among other work, I am going to discuss previous work from few research areas. 
At first, since STA is designed for measurements analysis, I provide a background on software measurements 
and their tight coupling with software process management. 
Next, I discuss previous STA implementations. 
Finally, I review relevant to mining of software repositories research focusing on recurrent behaviors discovery.
The work relevant to SAX-VSM will be reviewed in the next Chapter.

\section{Software measurements}
As in all other Engineering fields, measurements are used in Software Engineering in order to establish a 
systematic approach to software development which provides the control over software processes, facilitates
their improvement, and, most importantly, makes their result predictable. 

\subsection{Software measurements history}
According to Fenton \cite{citeulike:1525462}, the history of measurements in Software Engineering dates 
back to mid-1960's  ``\textit{...when the Lines of Code metric was used as the basis for measuring the 
productivity and effort...}'', which, in fact, predates the establishment of Software Engineering as an 
independent discipline \cite{naur_crisis_68}. 
While much of early research work concerning measurements in Software engineering dates to late 1970's 
\cite{citeulike:13158802}, probably the earliest published work outlining relevance of software measurements 
to software process management is ``Software project forecasting'' by DeMillo and Lipton 
\cite{demillo1980software}, where they point out that software measurements create a basis which allows 
practitioners and researchers to be ``rational and objective'' about software processes. 
In their same work, the authors cite notes by Perils, Sayward and Shaw, who emphasized the role of software 
measurements in the software process management, saying that 
`\textit{the purpose of software metrics is to provide aids for making optimal choice at several points 
in the life cycle}''.

While the early work on software measurements was heavily driven by the need for resource prediction model
and forecasting \cite{citeulike:1525462}, later work focused on the importance of measurements in the 
software process management and improvement. For example, one of the pioneering strategies for global software 
process improvement, Total Software Quality Management (TSQM), relies on the set of ten explicitly defined software 
process and product metrics ranging from low level metrics of LOC and design complexity to high-level process 
management metrics such as schedule and system testing progress \cite{citeulike:13071448}.
Similarly, the local strategy for software process improvement, Personal Software Process (PSP), relies on 
the broad range of software metrics \cite{citeulike:13072239}.

Over years, the increasing understanding of software measurements importance in software process management, 
as well as the success of the measurements-based predictive modeling prompted the creation of the research 
field of Empirical Software Engineering (ESE), where scientists use measurements as the basis for research 
hypotheses generation and for their investigation \cite{citeulike:766768}. 

Recently, due to the proliferation of open source software development model and advancements in public software
project hosting solutions, a new research area called Mining Software Repositories (MSR) was established 
within ESE field. MSR is specifically concerned with application of analytical techniques to public software 
repositories \cite{citeulike:12550438} \cite{citeulike:4534888} \cite{citeulike:2710928}, thus, the work from 
this field is the most relevant to my research.

\subsection{Software measurement theory}
In science, and in engineering, measurements allow to formally characterize attributes of an entity 
by assigning them a numerical, boolean, or a symbolic value. 
The choice of the value type depends on the used measurements criteria, such as a dimension, a level, or a degree. 
Ultimately, the chosen criteria and the scale of used values shall enable the intuitive and precise quantitative 
comparison between attributes regardless of their qualitative similarity or difference,
as pointed by Chapin \cite{citeulike:13158806}. 
Typically, the measurement units and scales are standardized.

An entity in Software Engineering can be a physical object, such as a program or a use case diagram, 
an event, such as a software release, or a software artifact, such as a bug report.
A measurable entity's attribute can be its property or a feature, such as the program's size, the amount of 
defects discovered during testing, and the usability of a software system.

Further, attributes are generally divided into two categories: internal and external. 
While measures for internal attributes are computed based on the entity itself, external attributes measures 
depend on the both an entity and an environment in which the entity resides - for example a system testing 
time varies depending on the performance of a test server.

Finally, as pointed by Fenton \cite{citeulike:1803429} there are two broad types of measurements: direct
and indirect. While direct measurement of an attribute do not depend on any other attributes, 
indirect measurement involves the measurement of one or more other attributes. 
As an example of a direct measurement consider the size of system source code or a time developers spent on 
project. In contrast, a module defect density (ratio of defects and module size), 
or a requirement stability (ratio of initial requirements and total requirements) are indirect measurements.

\subsection{Software measurements in STA}
Software Trajectory Analysis is built for the analysis of software measurements with purpose to enable recurrent 
behaviors discovery. It relies on the previous work that has indicated a possibility of software processes inference 
through observation of their effect on software product evolution. 
In particular, STA exploits the sequential dependency of consecutive measurements discovering recurrent patterns 
in their dynamics, which, I hypothesize reflect the recurrent behaviors.

\begin{figure}[t]
   \centering
   \includegraphics[width=115mm]{figures/SEI-measurements.eps}
   \caption{An illustration of relation between software measurements and key responsibilities in project management 
     from SEI Guidebook \cite{citeulike:10567306}.}
   \label{fig:sei-measures}
\end{figure}

The idea of studying software processes through the measurements of software product evolution is not new.
Throughout the history of the research in Software Engineering, the tight correlation between software process and 
product dynamics was discussed in numerous studies where measurable evolution of software system attributes 
often used for software process research.

As a particular example discussing the observability of software processes through software product measurements 
consider a de-facto industrial standard in software measurements provided by Software Engineering Institute (SEI) in 
their guidebook \cite{citeulike:10567306}. In particular, the authors discuss the variability in software processes 
execution which significantly affects the resulting software system and project faith. To address this issue, the 
discussed throughout the guidebook methodology recommends to implement a continuous software product and 
process measurement program that would allow to continuously control the software process variability 
enabling a ``real-time'' software process adjustments. 
Figure \ref{fig:sei-measures} shows a schematic illustration of such a program.

Hackystat, the ``parent'' system of STA, is another related study that extends the applicability of continuous 
measurements and confirms the possibility of software process understanding through the analysis of recurrent 
behaviors\cite{citeulike:557296}. 
As pointed by the authors, the visual comprehension of measurements variability and patterns collocation enables 
``emergent knowledge that one state variable appears to co-vary with another in the current project context'',
which naturally enables self-improvement process for software developers \cite{citeulike:557296}. 

STA extends both mentioned approaches by providing an automation for characteristic patterns discovery, which, 
as I expect, provide improved understanding of software processes dynamics and their effect on the software product.

\section{Previous work on STA}
Current implementation of Software Trajectory Analysis is generic by the design. In fact, it can be applied to almost 
any kind of sequential software measurements that carry useful to the research question information. This generality is granted 
by the STA's core algorithm for characteristic patterns discovery called SAX-VSM that I shall propose and discuss in the 
following Chapter \ref{chapter_sax_vsm}. Previous STA implementations were not generic -- they were ``hard-coded'' 
for specific studies which I discuss in this section. These exploratory studies provided valuable insights into the problem 
of recurrent behaviors discovery and into aspects of the system design and implementation.

\begin{figure}[t]
   \centering
   \includegraphics[width=150mm]{figures/STA12-schema-draft.eps}
   \caption{The Schematic overview of two first STA implementations. 
   Left panel shows a schematic representation of information flow in Hackystat -- raw measurements, metadata, and its 
   abstractions seamlessly absorbed by STA, processed, and presented to the user.
   The right panel shows an information flow in the more generic STA implementation designed for the analysis of 
   Android OS public repository. developed for Android OS repository mining.}
   \label{fig:STA12-schema}
\end{figure}

\subsection{STA v. 1.0: mining Hackystat software telemetry streams}
The very first Software Trajectory Analysis implementation was designed specifically for the analysis of Hackystat 
data called ``software telemetry streams''. As this data is collected automatically by so-called ``sensors'' installed 
at the developer's system and deployment environment, it is characterized by consistency and regularity, which enables 
unprecedented insight into performed processes as it was discussed in \ref{section_software_telemetry}. 
Effectively, by offering efficient data collection and storage mechanisms and consistent, fine-grained data, 
Hackystat provided ideal testbed for STA feasibility study.

The overview of the very first, Hackystat-based STA implementation targeting the possibility of recurrent behaviors 
discovery is shown at the left panel of Figure  \ref{fig:STA12-schema}.
The pilot STA implementation for mining of recurrent behaviors was based on two techniques: the first is the discretization 
of time-series with SAX \cite{sax}, that effectively translated real-valued telemetry streams into strings, and the 
occurrence frequency-based discovery of recurrent patterns.

As I have shown in \cite{csdl2-10-09}, this approach demonstrated the feasibility of recurrent behaviors discovery 
through the mining of frequently occurring symbolic patterns, i.e. motifs. 
Consider an example of recurrent behaviors discovery shown at the Figure \ref{fig:STA1-results}, where trajectories of 
measurements corresponding to development effort shown at the left panel while their clustering based on Euclidean 
distance among vectors of symbolic patterns occurrences shown at the right. Clearly, clustering separated two 
developers (\#2 and \#7) from the rest. Further investigation of the data revealed that these two developers 
demonstrated the most consistent development behavior as they devoted time for development activities almost every day, 
whether the rest of study participants did not.

\begin{figure}[t]
   \centering
   \includegraphics[width=145mm]{figures/STA1.eps}
   \caption{Clustering of developers behavior using symbolic approximation and vectors of motif frequencies. 
   This analysis captures similar development behavior among developers. 
   Developers \#2 and \#7 were consistent (no bursts observed) in both, coding and measuring effort during whole time interval, 
   while all others can be characterized with bursty, inconsistent effort.}
   \label{fig:STA1-results}
\end{figure}

In addition to indication of feasibility of automated recurrent behaviors discovery through the analysis of measurements, 
the experience with the pilot system highlighted a number of issues to be addressed in the future work. 
Among them, the chief issue was the external validity of the study, as the small scale of class-room 
experimentation does not provide an adequate coverage of the studied phenomena and as it has significant external 
treats. For example, in the discussed above development activity behaviors study, it is possible that some developers 
that demonstrated inconsistent behaviors may simply had their systems mis-configured.
The second significant issue identified by the pilot version was the problem of the mining algorithm parameter selection:
the sliding window size and two discretization parameters must be defined before mining, but their values are difficult 
to guess.

Note, that the pilot STA also implemented a recurrent behaviors mining workflow based on application of Apriori algorithm 
\cite{citeulike:775528} to development event records. As I have shown in \cite{citeulike:13159603}, this approach also
offered a satisfactory performance. However, since development events are impossible to recover from public software 
artifacts, this approach was not implemented in following STA implementations.

\begin{figure}[t]
   \centering
   \includegraphics[width=145mm]{figures/STA2-draft.eps}
   \caption{Clustering of developers behavior using symbolic approximation and vectors of motif frequencies. 
   This analysis captured similar development behavior among developers. 
   Developers \#2 and \#7 were consistent (no bursts observed) in both, coding and measuring effort during whole time interval, 
   while all others can be characterized with bursty, inconsistent effort.}
   \label{fig:STA2-results}
\end{figure}

\subsection{STA v. 2.0: experience with Android OS repository}
The second STA implementation was developed targeting analyses of measurement streams obtained by measuring of public
software repositories.

The decision to use public software repositories in the next to pilot exploratory study was made in order to increase its 
significance by addressing all of the proposed by Gasser et al. \cite{citeulike:13058334} essential characteristics for 
empirical studies based on mining of software artifacts:  
(1) they must reflect a real-life phenomena, 
(2) provide adequate phenomena's coverage, 
(3) examine representative levels of variance, 
(4) demonstrate an adequate level of statistical significance,
(5) provide results that are comparable across projects,
(6) be reproducible. 

Unfortunately, due to much coarser granularity and inconsistency of measurements collected from artifacts, as discussed 
further in this Chapter, the original approach to the data analysis based on the observed patterns frequency failed, 
and an additional exploratory study of time series mining techniques was conducted.
By experimenting with a number of discretization and aggregation techniques, as well as with various
distance functions and ranking schema, I found, that the common to Information Retrieval (IR) toolkit called 
Vector Space Model (VSM) based on \tfidf weighting schema and Cosine similarity demonstrated a satisfiable performance. 
As I have shown in \cite{csdl2-11-10}, STA based on discretization with SAX and mining with VSM was capable to discover 
characteristic behaviors.

Consider an example shown at Figure \ref{fig:STA2-results} for two sets of software measurements: pre- and post- release 
counts of new lines of code in Android OS kernel repository. While the details of data processing and recurrent behaviors 
discovery will be discussed later in the Chapter \ref{chapter_results}, left panel of the figure show that it was possible to
obtain clustering of characteristic behaviors sets corresponding to different time intervals where pre- and post- release 
behaviors are clearly separated. By using cluster's centroids, it was also possible to correctly classify other time-intervals,
which validates the approach applicability and correctness.

Similarly to the pilot implementation, the experience with this STA highlighted the same problem of an expected pattern 
length and the discretization parameters selection. This problem I have addressed by further development of the proposed 
in \cite{csdl2-11-10} approach into a generic algorithm for time series characteristic pattern discovery and classification 
called SAX-VSM \cite{sax-vsm} which I present in the next Chapter.

Note, that while I shall discuss difficulties associated with mining of public software repositories further in this Chapter, 
in order to combat the lack of their explicit external and internal connectivity along with the heterogeneity of data formats,
the second STA implementation followed typical to MSR approaches for data integration \cite{citeulike:13058334} \cite{cvsanaly}. 
In particular, similarly to previously developed solution called softChange \cite{german04_softchange} whose 
architecture is shown at Figure \ref{fig:softchange} STA builds its own data storage facility called STA-DB and 
relies on its own data storage format, as shown at the Figure \ref{fig:sta-assimilation}.

\section{Mining Software Repositories}
The discussed in this thesis STA implementation deals with data (i.e. artifacts and metadata) collected from public software 
repositories, and is concerned with the discovery of recurrent behaviors which are abstracted as recurrent, structurally 
similar, subsequences of longer sequences composed of temporally ordered artifacts measurements. 
This research direction is not new and much work has been done previously in mining of software repositories and in mining 
of public software repositories in particular. 
For the last decade, researchers working on these problems discuss their approaches and findings in two specialized venues: 
The Predictive Model in Software Engineering (PROMISE) Workshop and the Working Conference on Mining Software Repositories 
(MSR) which held within annual International Conference on Software Engineering (ICSE).
In order to enable comparative analyses of studied techniques, both venues encourage researchers to investigate 
reference datasets. While PROMISE maintains the same datasets over years \cite{promise12}, MSR offers a so-called 
MSR challenge dataset annually \cite{MSRChallenge2013}.
Note however, that PROMISE research is typically concerned with development of predictive models \cite{Menzies13},
MSR traditionally \cite{citeulike:12550438} \cite{citeulike:2710928} \cite{citeulike:7853299}, thus, when discussing the 
specificity of public data from software repositories I mostly refer to MSR research.

First of all, note, that while previous work in MSR \cite{citeulike:4534888} suggests source-control systems, 
defect-tracking systems, and archived communications as the main data sources, in this work, I also use public data from 
StackOverflow -- a popular software development community-based question answering service \cite{MSRChallenge2013}.
In particular, I am going to study recurrent behaviors in the daily activity of top-rated contributors.

Secondly, note, that previous research in MSR pointed out a number of issues which a typical study has to deal with and which
not only create technical difficulties, but also affect its validity. 
The chief problem is that software project public repositories are highly heterogeneous - each is managed and operated 
mostly in isolation serving a particular project needs having no explicit interactions with other projects. 
Moreover, within a project's repository, its SCM subsystems, such as version control, defect-tracking, and mailing list, 
are rarely ``connected'  \cite{citeulike:13058334}. 
This issue of heterogeneity directly affects MSR studies generality as tools working for and results obtained from one repository,
are not applicable to another.
Another issue is that while public availability of software artifacts is minimizing observability and privacy issues, 
the nature of these artifacts creates a number of other challenges, which limit the possible scope of the research and 
significantly elevate the complexity of the process discovery. Among others, four issues are typically cited as the most significant:
\begin{itemize}
 \item First of all, the artifacts are created by developers and users not in order to enable the research,
but merely to support software development activities. Thus, the process-related information content of these
artifacts is rather poor, and additional evidence is often needed \cite{citeulike:342840} \cite{citeulike:7954249} 
\cite{citeulike:7260421}.
 \item Secondly, the majority of these artifacts (change records, defect reports, assigned tasks, etc) 
typically represent a snapshot of the software project state rather than reflect any of performed actions, 
thus it might be simply impossible to infer any of completed software development events \cite{citeulike:1296888}.
This fact effectively renders obsolete a majority of previously developed event-based process discovery tools.
 \item Thirdly, developers and users not only create and submit to repositories artifacts on their own volition,
but most of the change management system (such as Git, Subversion, and Gerrit) offer an asynchronous workflow, 
where the locally created artifacts might never be committed \cite{citeulike:2280690} \cite{citeulike:9037939}. 
Therefore, artifacts are displaced in time and it is often impossible to know exactly when their content was created.
 \item Finally, the high volume of produced artifacts and their dimensionality demands for automated, high throughput 
techniques robust to the noise \cite{citeulike:12550438}, \cite{citeulike:7853299}, \cite{citeulike:4534888}.
\end{itemize}

These data-specific issues associated with mining of public software repositories not only create significant external treats 
to MSR research, but they are impossible to resolve without altering the normal flow of generative software process, 
for example by implementing special measurement programs or by introducing instrumented source code editing or building tools. 
Typically, MSR researchers deal with these external treats by finding of additional evidence supporting their 
claims \cite{citeulike:5043664} \cite{citeulike:5128808}.

\subsection{Public software artifacts}
Public software repositories software offer a wide range of software process and product artifacts for analyses.
Among others, source code change records, defect reports, feature requests, assigned tasks, developers communications, 
tutorials, etc. All these public artifacts allow a developer or a user to instantly obtain a ``snapshot'' of the project - 
i.e. retrieve the latest (or any previous) source code revision and obtain a complete overview of the software project 
state - lists of open and closed issues, past and future plans, etc.

Note however, that being exceptionally convenient in aiding of project management, this snapshot-oriented nature of 
public software artifacts creates difficulties for software process research as ``snapshots'' rarely reflect any of the 
performed actions. 
Thus, as pointed in previous work \cite{citeulike:1296888}, for some projects it may be simply impossible to infer 
many of software development events and consequently software processes which happen in between these snapshots,
which affects software process traceability and with the credibility of results as shown 
in \cite{citeulike:2280690} \cite{citeulike:9037939}. I acknowledge this process observability issue while working with 
public software process artifacts and intentionally avoid discussing and concluding on software processes. 
What I shall focus on, will be the validation of the ability of proposed techniques to capture process-characteristic 
recurrent behaviors (i.e. characteristic patterns).

Further in this section I review kinds of public software repositories, their artifacts, and data types, to whose 
measurements STA was or potentially can be applied. 

\subsubsection{Source code}
As the main output of a software project is the source code, it is also the main subject of the scientific research. 
Metrics that derived from the source code are predominant in studies concerned with software complexity, maintainability,
and code quality studies, as well as those that concerned with productivity, project planning, 
and cost estimation \cite{citeulike:4534888}. 

Typically, evolution of source code is recorded as a sequence of consecutive change records, which are simple artifacts 
tracking changes of each source code line. Despite to the their simplicity, tracing the code evolution through the analysis 
of change records can become increasingly difficult as developers branch the source code tree, merge it back later, 
or abandon the branches \cite{citeulike:13156191}.

A large number of metrics can be derived through source code and change records analyses, it offers probably the most functional one - the count of physical 
lines of code (LOC). 

While another metrics exists -- the count of logical lines of code (LLOC), it is language-dependent and its derivation involves 
significant overhead.


\subsubsection{Defect tracking system}
Typically, the defect repository serves as a centralized system for managing a software project QA (Quality Assurance) activities
providing users and project contributors with means to report and to discuss an improper system behavior.
In addition, in some projects, the defect repository is also used to keep a track of requests for future system features and related
discussions.

Artifacts from repositories provide users and contributors with up to date information about system defects and their severity, and, 
if implemented in the tracking system, with additional comments about their technical nature, resolution plans, and other information. 

\subsubsection{Developers communications}
As OSS projects usually developed by highly distributed teams, email, mailing lists, and newsgroups typically are 
used as primary communication channels between project participants. Therefore, these data are particularly useful for
identifying process agents, actions,  and activities related to process coordination between participants. 

Developer communication artifacts, such as email messages, mailing list posts, and newsgroup messages include agent identification, 
timestamps, topics, and other useful information. 

\subsubsection{Q\&A websites}
Frequently, professional software developers and hobbyists programmers seek answers to various questions using various websites. 
Among others, Stack Overï¬‚ow (SO) explicitly targets programmers and is dedicated to software-, hardware-, and computer system 
administration-related issues.

A number of recent MSR studies which explored this data has shown its potential in providing insights into a number of problems.
While most of the studies are concerned with programming related questions, such as identifying topics relevant to particular 
development communities \cite{kartik:msr14}, mining additional technical expertise \cite{VenkataramaniGAMB13} \cite{SaxeMG13}, 
or identifying problematic APIs \cite{KavalerPGCDF13} \cite{Linares2013Exploratory} or software package documentation 
\cite{Campbell2013Deficient}, 
a number of studies address broad phenomena such as those concerned with collaborative problem solving 
\cite{Tausczik2014Collaborative}, and knowledge sharing \cite{VasilescuCSCW14} \cite{Schenk2013Geo}. 
Finally, some researcher investigate contributors temporal behaviors \cite{Bosu2013Building} or attempt their profiling 
\cite{GinscaP13}.

\subsubsection{Metadata}
Often, as reported by Begel
et al. \cite{citeulike:7260421} who conducted a survey at Microsoft, in order to understand performed 
by software developers processes, the quantitative source code changes information is simply not sufficient. Through the survey,
the authors accounted for 31 different information needs for understanding and coordinating team software processes, among 
which needs for accompanying software change metadata were well pronounced. Among other needs, it was found that engineers want new 
solutions for
learning the rationale behind
changes, finding responsible people, discovering and tracking dependencies, 
learning
about the status of items in progress. The authors emphasize, that the majority of needs were concerned with people, not 
the code, and that metadata is essential in meeting the requests.

Similarly, Kim et al. \cite{citeulike:4000311} who proposed a system for software repositories data extraction, data storage, 
and a universal data-exchange language, emphasized the metadata importance for information collection and partitioning, moreover, 
the authors has shown that it is possible to create a metadata-based system for mining of software repositories system for 
close-source projects.

STA DB has expendable, metadata-centric design. New types of metadata can be defined and related to existing and newly collected
artifact entities. Later, in the process of software trajectories definitions, the metadata class tags and values can be used 
for data partitioning and aggregation.

\section{Data assimilation}
Currently, there is a voluminous amount of the research literature concerned with mining of public software repositories 
\cite{citeulike:2710928} which, in fact, extends probably even larger body of earlier work that have focused on studying 
of historic private databases on software development \cite{citeulike:393158} \cite{citeulike:13125375} \cite{citeulike:13125481}.

Among others, the research questions discussed in this literature usually concerned with extracting of 
historical information for better understanding of software systems evolution \cite{citeulike:277045} \cite{citeulike:4000311}, 
better understand and improving of software processes \cite{citeulike:5803126}, 
and studying the impact of software tools on the processes and products \cite{citeulike:13125389}. 
Yet, some effort was made towards automation of the process of data retrieval and measurements. 
While some of the proposed solutions allow real-time interactive repositories exploration by extending 
repository management tools such as CVS, SVN, etc. with a front-end engine, such as Bonsai \cite{bonsai},
or JReflex \cite{citeulike:3017440}, others, such as CVSAnalY \cite{citeulike:6544724}, softChange \cite{citeulike:13125395},
and {TA}-{RE} \cite{citeulike:4000311} propose a workflow based on off-line artifacts retrieval, 
pre-processing, and on-demand analysis.
Similarly to the latter, STA relies on the off-line retrieval and pre-processing of public software artifacts as
shown at the Figure \ref{fig:sta-assimilation}. Note that since STA has been initially designed as a Hackystat 
extension \cite{csdl2-10-09}, it does not need any specific parser and is capable of real-time 
collection of Hackystat data.

However, when the snapshots are viewed in dynamics, it significantly clarifies the picture. 
With time, the uncommitted changes of individual developers are merged with the main source tree, 
current defects getting fixed, while new ones discovered. 
The understanding of the dynamics allows researchers to create better models of software processes, 
forecast future events and to predict the evolution of the variety of metrics.
The software project evolution seen through the multiple snapshots allows the better 
assessment of the project dynamics and facilitates predictions. Moreover, as pointed by the Hackystat authors, 
\cite{citeulike:557296} \textit{the ability to compare}

\begin{figure}[t]
   \centering
   \vspace{1cm}
   \includegraphics[width=115mm]{figures/Flow.eps}
   \caption{Detailed overview of Software Trajectory Analysis data assimilation layer. 
    At first, software artifacts are collected from software repositories, measured, converted into 
    universal to STA format, and stored in a dedicated database.
    If Hackystat used as a data source, typically, no additional processing required and data can be used in the real-time.
    Stored in STA DB software artifact entities can be further enhanced by additional measurements and metadata.
    Finally, the database allows efficient data partitioning and its aggregation into software trajectories.}
   \label{fig:sta-assimilation}
\end{figure}

\section{Relevant MSR research on recurrent behaviors discovery}
Some research within MSR, specifically concerns with application of analytical techniques to sequences of software 
artifact measurements -- the approach which is similar to STA.

\subsection{Itemsets mining}
In data mining, frequently occurring items (actions, events) often used in order to discover implicit knowledge from
large datasets. As I have mentioned earlier in Section \ref{section_software_process_design}, these techniques were 
applied for software process mining before by Cook and Wolf \cite{citeulike:328044} \cite{citeulike:5120757} 
\cite{citeulike:5128143} and Rubin et al. \cite{citeulike:1885717}. Unfortunately, these techniques are not applicable
to public software repositories since they do not offer development event logs.

Nevertheless, sequential item mining was explored by Zimmermann et al. in \cite{citeulike:277045}. 
The authors developed a system called ROSE for identification of co-occurring changes in a software system aiming at 
prediction of future code changes based on the observed change. 
Similarly, Kagdi et al. \cite{citeulike:3929070} applied sequential-pattern mining to discover ordered sequences of 
frequently changed files in order to predict future changes. 
Livshits and Zimmermann \cite{citeulike:393158} developed DynaMine -- the system based on mining of call patterns 
capable to detect potential bugs.

Potentially, these techniques can be applied for STA results. For example it may be possible to discover ordered, or
unordered sequences of frequent behaviors which can be classified as more coarser development actions.

\subsection{Time series analysis}
Because most of repositories have entities with temporal aspects, MSR research seeks to quantitatively analyze ordered 
in time sequences of software artifacts or measurements. In particular time-series analysis and the extraction of 
temporal patterns thought to be quite useful for re-enacting and describing behaviors within a software repository. 

\begin{figure}[t!]
   \centering
   \includegraphics[width=145mm]{figures/FourrierMySQL.eps}
   \caption{}
   \label{fig:mysql-fourrier}
\end{figure}

For example Herraiz et al. \cite{citeulike:6544685} applied ARIMA models to software evolution measurements aiming 
prediction of future changes. 
Similarly, Antoniol et al. \cite{citeulike:3378725} worked on mining of repeated behaviors using linear prediction 
coding (LPC) and Cepstrum coefficients. 
Hindle et al. in \cite{citeulike:10377345} described how to discover recurrent behaviors from software measurements 
by Fourier analysis. The left panel of the Figure \ref{fig:mysql-fourrier} from their work indicates that the studied 
signal carries potentially distinguishable periodic behavior, moreover, they were able to detect a promising a smear 
of frequencies between 18 and 19 (right panel), unfortunately this direction was not further investigated.


\epigraph{Without the right information, you're just another person with an opinion.}{Tracy O'Rourke, CEO of Allen-Bradley}