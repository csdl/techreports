\chapter{Prior and related work}\label{chapter_background_work}
Software Trajectory Analysis consists of two components: 
the \textit{software artifacts retrieval and measurement machinery} (i.e. a data assimilation layer), 
and the \textit{software trajectory characteristic patterns discovery module} (i.e. an analysis layer). 
The high-level overview of these layers show at the Figure \ref{fig:STA2-results}.

The artifacts retrieval and measurement machinery refers to a way in which software artifacts are 
collected, measured, and enriched with metadata. 
Currently, STA is capable of retrieving and processing data from typical to OSS development 
Software Configuration Management system (SCM) components such as version control, 
defect management, and communications management systems. 
In addition, as I shall show in this thesis, it is capable to work with other data sources such as Q\&A 
websites or Hackystat \cite{csdl2-10-09}.

Note, that STA is not limited only to these data sources.
As public repositories are highly heterogeneous and continuously evolving, STA adopts a common to the 
Software Repository Mining field (MSR) strategy for data assimilation, unification, and off-line enrichment,
where public artifacts are retrieved and stored ``\textit{as is}'' (i.e. mirrored) at first, 
measured at second, and enriched with with metadata at the final step 
\cite{citeulike:12550438} \cite{german04_softchange} \cite{cvsanaly}.
Similarly to other systems for mining software repositories, the relational database is used in STA 
for data storage and indexing -- this solution not only enables an interactive workflow and a federated access to 
data, but allows for effective measurements partitioning and aggregation, which is 
an \textit{essential capability} for efficient software trajectories construction.
Overall, STA data assimilation layer is designed in a way that conforms to the field's best practices
allowing its extension for any data source that is capable of providing data for analyses.
%\fxnote{maybe cite the data enrichment with geolocation metadata for stakcoverflow analyses?}

The software trajectory characteristic patterns discovery module refers to an analytical machinery that 
is responsible for discovery of characteristic recurrent patterns in a set of software trajectories provided as 
an input. Conceptually, this module can embed \textit{any data mining algorithm} which is capable of 
discovering recurrent patterns from sequential data, such as one of the numerous algorithms for time series 
motifs discovery \cite{citeulike:13197378}.
However, the specificity of software trajectories and the pattern of interest, i.e. recurrent behavior, 
is placing a number of constraints that limit the applicability and the performance of known algorithms.
First of all, the algorithm must be capable \textit{to discover recurrent patterns without any prior knowledge 
about their length, shape, amplitude, and occurrence frequency}, as these are expected to naturally differ 
between projects, problems, or even subsets of trajectories from the same project.
Secondly, it must be capable to \textit{learn from a very small training data set} --
the property that has been shown crucial in predictive modeling and knowledge mining from software 
repositories where data is sparse \cite{citeulike:6055293}.
And finally, the algorithm must provide an automated mechanism for 
\textit{patterns ranking according to their relevance} in order to allow their efficient review by human experts 
since it is impossible to set an ``importance threshold'' or a number of ``interesting'' patterns in advance.

The discussed in this thesis STA characteristic patterns discovery module implementation relies on SAX-VSM,
a novel algorithm for characteristic patterns discovery from time series, that I shall propose in the following Chapter. 
This algorithm has been designed specifically in order to address aforementioned requirements.

\begin{figure}[t]
   \centering
   \includegraphics[width=150mm]{figures/Flow-analysis.eps}
   \caption{The high-level STA overview. Software artifacts are retrieved, enhanced, and measured within the
   data assimilation layer. Next, based on the user input, classes of software trajectories are constructed.   
   In turn, the data analysis layer performs comparative analyses of software trajectories yielding sets
   of ranked class-characteristic behaviors.
   Note, that while for the clarity only two classes of trajectories shown, STA analysis module is capable of
   discovering class-characteristic patterns from many classes at once.}
   \label{fig:sta-full-overview}
\end{figure}

Further in this chapter, in order to relate Software Trajectory Analysis to other research and to position it among 
other work, I shall discuss previous work from few research areas.
At first, since STA is designed for software measurements analysis, I provide a background on software 
measurements and evidence for their tight coupling with software processes. 
Next, I briefly discuss my earlier exploratory studies conducted with previous STA implementations. 
Finally, I review relevant to mining software repositories research focusing on recurrent behaviors discovery.
The work relevant to time series characteristic patterns discovery and SAX-VSM will be discussed in the next Chapter.

\section{Software measurements}
As in all other Engineering fields, measurements are used in Software Engineering in order to establish a 
systematic approach to software development which provides the control over software processes, facilitates
their improvement, and, most importantly, makes their result predictable. 
In addition, software measurements enable scientific research.

\subsection{Software measurement history}
According to Fenton \cite{citeulike:1525462}, the history of measurements in Software Engineering dates 
back to mid-1960's  ``\textit{...when the Lines of Code metric was used as the basis for measuring the 
productivity and effort...}'' , which in fact predates the establishment of Software Engineering as the 
independent discipline \cite{naur_crisis_68}. 
Much of early research concerned with software measurements has been driven by the need for resource model 
prediction and forecasting \cite{citeulike:1525462} whereas later research has extended towards the problem 
of software process management \cite{citeulike:13158802}.

Probably the earliest published work outlining close relations of software measurements and software 
processes is the ``Software project forecasting'' by DeMillo and Lipton \cite{demillo1980software} where they 
point out that software measurements create a basis which allows practitioners and researchers to be 
``\textit{rational and objective}'' about software processes. 
Remarkably, the authors refer to the earlier notes by Perils, Sayward and Shaw, who emphasized the role of software 
measurements in the software process management, saying that `\textit{the purpose of software metrics is 
to provide aids for making optimal choice at several points in the life cycle}''.

With time, the increasing understanding of software measurements objectiveness and their ability to reflect 
the state of software processes led to the development of measurement-based strategies for software 
process management and improvement. 
For example, one of the pioneering strategies for global software process improvement, 
Total Software Quality Management (TSQM), relies on the set of ten explicitly defined software 
process and product metrics ranging from low level metrics of Lines of Code and Design Complexity to 
high-level process management metrics such as Schedule and System Testing Progress \cite{citeulike:13071448}.
Similarly, the local strategy for software process improvement, Personal Software Process (PSP), relies on 
the broad range of software metrics \cite{citeulike:13072239}.

In addition to playing an important role in software process management and forecasting, software 
measurements become ubiquitous in scientific research. 
In the field of Empirical (or as it also called Experimental) Software Engineering (ESE), 
researchers use measurements and experimentation as the basis for research hypotheses 
generation and their investigation \cite{citeulike:766768}. 

Recently, due to the proliferation of open source software development and advancements in public software
project hosting solutions, a new research area called Mining Software Repositories (MSR) has been established 
within ESE field. MSR is specifically concerned with application of analytical techniques to public software 
repositories \cite{citeulike:12550438} \cite{citeulike:4534888} \cite{citeulike:2710928}, thus, 
the research work from this field is one of the most relevant to my research.

\subsection{Software measurement theory}
In science and in Engineering, measurements allow to formally characterize attributes of an entity by assigning 
them a numerical, boolean, or a symbolic values. 
The choice of the value type depends on the used measurement criteria, such as a dimension, a level, 
or a degree. Ultimately, the chosen criteria and the scale of used values shall enable the intuitive 
and precise quantitative comparison between attributes regardless of their qualitative similarity or 
difference, as pointed by Chapin \cite{citeulike:13158806}. 
In addition, the measurement units and scales are typically standardized in order to enable the global comparability.

An entity in Software Engineering can be a physical object, such as a program or a use case diagram, 
an event, such as a software release, or a software artifact, such as a bug report.
A measurable entity's attribute can be its property or a feature, such as the program's size, the 
amount of defects discovered during testing, and the usability of a software system.

Further, attributes are usually divided into two categories: internal and external. 
While measures for internal attributes are computed based on the entity itself, external attribute 
measures depend on both the entity and the environment in which the entity resides - for example a 
system testing time varies depending on the performance of a test server.

Finally, as pointed by Fenton \cite{citeulike:1803429}, there are two broad types of measurements: direct
and indirect. While direct measurements of an attribute do not depend on any other attributes, 
indirect measurements involve measurements of one or more other attributes. 
As an example of a direct measurement consider the size of system source code or a time developers spent on 
project. In contrast, a module defect density (ratio of defects number and the module size), 
or a requirement stability (ratio of initial requirements and total requirements) are indirect measurements.

\subsection{Software measurements in STA}
Software Trajectory Analysis is built for the analyses of software measurements whose purpose is to enable 
recurrent behaviors discovery. In particular, STA exploits the sequential dependency of consecutive 
measurements for discovering recurrent patterns in their dynamics, which, as I hypothesize, reflect recurrent 
behaviors.

\begin{figure}[t]
   \centering
   \includegraphics[width=115mm]{figures/SEI-measurements.eps}
   \caption{An illustration of relation between software measurements and key responsibilities 
   in project management from SEI Guidebook \cite{citeulike:10567306}.}
   \label{fig:sei-measures}
\end{figure}

This approach builds upon previous work that confirmed the feasibility of software processes inference through 
observations (i.e. measurements) of their effect on software product evolution and indicated a possibility of 
recurrent behaviors discovery. 

As a particular example confirming the observability of software processes through software product measurements 
consider the de-facto industrial standard for software measurements application provided by 
Software Engineering Institute (SEI) in their guidebook \cite{citeulike:10567306}. 
In particular, the authors discuss the variability issue in software processes execution which significantly 
affects the resulting software system quality and the project faith. 
To address this issue, they propose a methodology based on implementation of a continuous software product 
and process measurement program that would allow for continuous assessment of software processes 
variability enabling a ``real-time'' software process control. The Figure \ref{fig:sei-measures} shows a schematic 
illustration of such a program.

Hackystat, the ``parent'' system of STA, is another related study that extends the applicability of continuous 
measurements and confirms the possibility of software process understanding through the analysis of recurrent 
behaviors \cite{citeulike:557296}. 
As pointed by the authors, the visual comprehension of measurements variability and patterns collocations enables 
``\textit{emergent knowledge that one state variable appears to co-vary with another in the current project context}'',
which allows for process improvement activities \cite{citeulike:557296}. 

As an example indicating the possibility of recurrent behaviors discovery through measurements, consider the 
study by Hindle et al. \cite{citeulike:10377345} discussed in the Section \ref{chapter2_section-tsanalysis} of 
this chapter that shows an example of recurrent behaviors detection by the Fourier Transform -based analysis.

STA extends previous approaches built for software measurements analysis by providing an automation for 
characteristic patterns discovery from software process and product measurements, which, as I expect, shall 
aid in understanding of recurrent behaviors and their role and effect in software processes.

\section{Previous work on STA}
Current implementation of Software Trajectory Analysis is generic by the design. In fact, it can be applied 
to almost any kind of sequential software measurements that carry useful to the research question information. 
This generality is granted by the STA core algorithm for characteristic patterns discovery (i.e. SAX-VSM). 
Previous STA implementations were not generic -- they were ``hard-coded'' for specific exploratory studies 
which I discuss in this section. These studies has provided valuable insights into the problem of recurrent 
behaviors discovery and into aspects of the system design and implementation.

\begin{figure}[t]
   \centering
   \includegraphics[width=150mm]{figures/STA12-schema-draft.eps}
   \caption{The Schematic overview of first two STA implementations. 
   Left panel shows a schematic representation of information flow in Hackystat: raw measurements, metadata, and its 
   abstractions seamlessly absorbed by STA, processed, and presented to the user.
   The right panel shows an information flow in the more generic STA implementation designed for the analysis of 
   Android OS public repository.}
   \label{fig:STA12-schema}
\end{figure}

\subsection{STA v. 1.0: mining Hackystat software telemetry streams}
The very first Software Trajectory Analysis (i.e. pilot) implementation has been designed specifically for the analysis 
of the Hackystat data called ``software telemetry streams''. As this data has been collected automatically by so-called 
``sensors'' installed at the developer's system and deployment environments, it is characterized by high consistency, 
which enables unprecedented insight into performed processes as I already discussed in \ref{section_software_telemetry}. 
Effectively, by offering efficient data collection and storage mechanisms, and most importantly consistent, fine-grained data, 
Hackystat provided an ideal testbed for STA feasibility study.

The overview of the pilot Hackystat-based STA implementation targeting the recurrent behaviors discovery is shown 
at the left panel of Figure  \ref{fig:STA12-schema}.
The pilot STA implementation has been based on two techniques: the discretization of time-series with SAX \cite{sax}, 
that effectively translated real-valued telemetry streams into strings, and the occurrence frequency (i.e. support) -based 
discovery of recurrent patterns.

As I have shown in \cite{csdl2-10-09}, this approach demonstrated the feasibility of recurrent behaviors discovery 
through the mining of frequently occurring symbolic patterns, i.e. time series motifs \cite{sax}. 
Consider an example of recurrent behaviors discovery shown at the Figure \ref{fig:STA1-results}, where software 
trajectories built of development effort measurements shown at the left panel while their clustering based on Euclidean 
distance among vectors of symbolic patterns occurrence frequencies shown at right. Clearly, the hierarchical clustering 
process divided the set of trajectories separating two developers (\#2 and \#7) from the rest. 
Further investigation of the data revealed, that these two developers demonstrated the most consistent development 
behavior as they spent considerable amounts of time working on the project almost every day whether the rest of the 
study participants did not. 
Thus, the results of STA analysis were found consistent with the ground truth.

In addition to indicating the feasibility of automated recurrent behaviors discovery through the analysis of measurements, 
the experience with the pilot system highlighted a number of issues.
The chief issue threating the external validity of the study has been the fact that the small scale class-room experimentation 
simply does not provide an adequate coverage of the studied phenomena. 
For example, it is possible that in the discussed above experiment some of the developers which demonstrated inconsistent 
behavior may simply had their ``sensors'' mis-configured or malfunctioning.
The second significant issue identified by the pilot STA has been the problem of the data mining algorithm parameter selection, 
as they have to be defined as the input, but their proper values are difficult to find.

Note, that the pilot STA also implemented a recurrent behaviors mining workflow based on the application of 
a frequent patterns mining algorithm called Apriori \cite{citeulike:775528} to development event records. 
As I have shown in \cite{citeulike:13159603}, this approach has shown a satisfactory performance. 
However, since development events are impossible to recover from public software artifacts, this workflow has not been 
used in the following STA implementations.

\begin{figure}[t]
   \centering
   \includegraphics[width=145mm]{figures/STA1.eps}
   \caption{Results of pilot STA study. 
   The left pane shows eight software trajectories that are Hackystat Development Effort telemetry streams \cite{citeulike:557296} 
   collected in the course of two months.
   The right pane shows a hierarchical clustering of developer behaviors obtained by computing Euclidean distance between vectors
   of recurrent patterns frequency built with SAX discretization \cite{sax}. 
   Note two groups discovered by clustering that correspond to consistent (developers \#2 and \#7) and inconsistent development effort.}
   \label{fig:STA1-results}
\end{figure}


\subsection{STA v. 2.0: experience with Android OS repository}
The second STA implementation has been developed targeting analyses of measurements obtained by measuring artifacts 
from public software repositories.

The decision to use public software repositories in the second exploratory study has been made in order to increase its 
significance by addressing all of the proposed by Gasser et al. \cite{citeulike:13058334} essential characteristics for 
empirical studies based on mining of software artifacts:  
(1) they must reflect a real-life phenomena, 
(2) provide adequate phenomena's coverage, 
(3) examine representative levels of variance, 
(4) demonstrate an adequate level of statistical significance,
(5) provide results that are comparable across projects,
(6) be reproducible. 

Unfortunately, due to much coarser granularity and inconsistency of measurements collected from public artifacts, 
the issues that are discussed further in this Chapter, the original approach to the data analysis based on the observed patterns 
frequency failed, and the additional exploratory study of time series mining techniques has been conducted using 
2012 MSR challenge data \cite{MSRChallenge2012}.
By experimenting with a number of time series discretization and aggregation techniques and with various distance 
functions and ranking schema, I found, that the common to the Information Retrieval (IR) research field toolkit called 
Vector Space Model (VSM) based on \tfidf weighting schema and Cosine similarity, demonstrated a satisfactory performance. 
As I have shown in \cite{csdl2-11-10}, STA based on the discretization with SAX and mining with VSM, has been found capable 
to discover characteristic behaviors in pre- and post- release software trajectories constructed of the New Lines of Code 
change records measurements.

In addition, to combat the lack of Android software repositories internal and external connectivity and the heterogeneity 
of data formats -- also the common issues in the MSR field -- in the second STA implementation I had followed state of the art 
MSR approaches for data integration \cite{citeulike:13058334} \cite{cvsanaly}. 
In particular, similarly to previously developed solution called softChange \cite{german04_softchange}, second STA mirrors 
repositories and builds its own data storage facility based on the relational database engine 
as it is shown at the Figure \ref{fig:sta-assimilation}.

While the details of data processing and recurrent behaviors discovery performed within the second exploratory study will be 
discussed later in the Chapter \ref{chapter_results}, consider an example shown at the Figure \ref{fig:STA2-results} 
for two classes of software trajectories that reflect pre- and post- release dynamics of counts of New Lines of Code in 
the Android OS kernel repository. 
The left panel of the figure shows that it is possible to cluster characteristic behaviors corresponding to different time intervals 
where pre- and post- release behaviors are clearly separated. 
The right panel shows that by using pre- and post- release clusters centroids it is also possible to classify other time-intervals, 
which validates the discovered recurrent patterns characteristic capacity and the overall correctness of the approach.

Similarly to the pilot implementation, the experience with second STA implementation highlighted the same problem of 
parameters selection. In order to address this issue, I have explored a possibility to employ a parameters optimization 
scheme and implemented a DIRECT-based approach \cite{citeulike:12563460} to aid in SAX-VSM parameters selection 
which I shall discuss in the next Chapter.

\begin{figure}[t]
   \centering
   \includegraphics[width=145mm]{figures/STA2-draft.eps}
   \caption{
   Left pane shows clustering of recurrent developer behaviors discovered by STA in software trajectories obtained by 
   measuring new code lines from Android OMAP kernel project software artifacts, note a distinct group of pre-release weekly 
   behaviors.  
   Right pane shows results of a cross-validation experiment where other pre- and post-release software trajectories 
   corresponding to new code lines were classified by computing their similarity with shown at the left pane pre- and 
   post- release  cluster centroids.}
   \label{fig:STA2-results}
\end{figure}

\section{Mining Software Repositories}
As mentioned before, mining software repositories is a well established research direction since 
mid-1970's, when Meir Lehman pioneered the software evolution theory by studying historical records from 
software repositories \cite{citeulike:2739216}. 
For the last decade, researchers working in this field discuss their approaches and findings in a number of venues. 
Among these, are Predictive Model in Software Engineering (PROMISE) Workshop and the Working Conference on Mining 
Software Repositories (MSR) which are held within annual International Conference on Software Engineering (ICSE).
In order to enable the comparison of proposed techniques performance, both venues encourage researchers to 
apply them to reference datasets. 
While PROMISE maintains the same reference dataset over years \cite{promise12}, 
MSR offers a so-called MSR challenge dataset annually \cite{MSRChallenge2012} \cite{MSRChallenge2013}.
Note however, that the {PROMISE} research is mainly concerned with development of predictive models for 
Software Engineering \cite{Menzies13}, whereas MSR traditionally uses public software repositories stimulating 
the diversification of possible research directions \cite{citeulike:12550438} \cite{citeulike:2710928} \cite{citeulike:7853299}.

\section{Understanding Public Software Repositories}
Traditionally, software repositories contain artifacts produced during the software life-cycle. 
Previous research classifies software repositories into three main categories: source-control systems, 
defect-tracking systems, and archived communications \cite{citeulike:4534888}, but many other repositories exist. 
These may contain various information, such as software system runtime logs, system testing logs, 
a variety of historical measurements, documentation, tutorial, and many other.
Recently, a novel type of repositories was proposed for MSR studies -- a historical information collected within the 
community-based question answering service Stack Overflow \cite{MSRChallenge2013}.

As pointed in previous review studies \cite{citeulike:12550438} \cite{citeulike:7853299} \cite{citeulike:7465518} there
is a number of issues associated with mining of \textit{public} repositories which not only create technical difficulties for 
the scientific research, but also affect its validity. 
The chief problem is that public project repositories are highly heterogeneous - each is managed and operated 
mostly in isolation serving a particular project and community needs, therefore having no explicit interactions with 
other projects. 
Moreover, within a project's repository, its SCM subsystems such as version control, defect-tracking, and mailing list, 
are rarely ``connected'  \cite{citeulike:13058334}. 
This issue of heterogeneity directly affects MSR studies generality since tools working for and results obtained from 
one repository, are rarely applicable to another.
Yet another issue is that while public availability of software artifacts minimizes observability and privacy issues, 
the nature of these artifacts creates a number of other challenges, which limit the possible scope of the research and 
significantly elevate its complexity. Among others, four issues are typically cited as the most significant:
\begin{itemize}
 \item First of all, the artifacts are created by developers and users not in order to enable the scientific research,
but merely to support software development activities. Thus, the informational content of these artifacts is rather 
poor and additional evidence (i.e. metadata) is often needed \cite{citeulike:342840} \cite{citeulike:7954249} 
\cite{citeulike:7260421}.
 \item Secondly, the majority of these artifacts (change records, defect reports, assigned tasks, etc.) 
typically represent a snapshot of the software project state rather than reflect any of performed processes.
Therefore, it might be simply impossible to infer performed software development events \cite{citeulike:1296888}.
This fact effectively renders obsolete most if not all of previously developed event-based process discovery tools.
 \item Thirdly, the project's contributors not only create and submit to repositories artifacts on their own volition,
but most of the change management system (such as Git, Subversion, and Gerrit) encourage the asynchronous workflow 
where the locally created artifacts might never be committed and thus will remain invisible 
\cite{citeulike:2280690} \cite{citeulike:9037939}. 
For that reason, it is often impossible to know exactly when the artifact's content was created, and what exactly 
has happened in between its snapshots.
 \item Finally, the vast volume of produced artifacts, their high dimensionality, and significant levels of noise demand 
 for automated, high throughput and robust techniques 
 \cite{citeulike:12550438}, \cite{citeulike:7853299}, \cite{citeulike:4534888}.
 \end{itemize}

These data-related issues associated with mining of public software repositories not only create significant external 
threats to MSR research validity, but often impossible to resolve without altering the normal flow of OSS software 
process, for example by implementing a special measurement program by introducing instrumented source code 
editors and building tools (as in Hackystat). 
Typically, MSR researchers deal with these issues by finding additional evidence in order to support their conclusions 
\cite{citeulike:5043664} \cite{citeulike:5128808}.

\subsection{Public software artifacts}
Public software repositories offer a wide range of software process and product artifacts for analyses.
Among others, source code change records, defect reports, feature requests, accepted, rejected and assigned tasks, 
developer communications, documentation, tutorials, etc. 
All these public artifacts allow the developer or the user to instantly obtain a ``snapshot'' of the project,  
i.e. to retrieve the latest (or any previous) source code revision and a complete overview of the software project state 
along with the lists of open and closed issues, past and future plans, etc.

Note however, that while being exceptionally convenient and aiding project management, this snapshot-oriented nature 
of public software artifacts creates difficulties for software process research as the ``snapshot'' rarely reflect finished, 
ongoing, or planned processes which compromises the quality of the performed studies as I have mentioned above. 

I acknowledge this software process observability issue when working with public software process artifacts and intentionally 
avoid discussing and concluding on software processes. Instead, what I shall focus on, is the validation of the proposed 
technique's ability to capture process-characteristic recurrent behaviors when snapshots are viewed in their dynamics. 
I hypothesize, that software measurements evolution reflects many recurrent development behaviors and that some of 
them are characteristic to certain aspects of software processes. Thus, by discovering recurrent patterns in software 
measurements evolution it shall be possible to at least partially infer and evaluate performed software development actions.

Further in this section I review a number of common public software repositories and their artifacts to whose measurements 
STA already has been or potentially can be applied. 

\subsubsection{Source code management system}
Source code management system keeps track of the main output of a software project -- its source code, which is also the 
main subject of the scientific research. Metrics that derived from the source code artifacts are predominant in studies concerned 
with software complexity, maintainability, and quality, as well as those that are concerned with productivity, project planning, 
and cost estimation \cite{citeulike:4534888}. 

Typically, the evolution of source code is recorded as a sequence of consecutive change records, which are simple artifact
tracking the change of each source code line. Despite to the artifact's simplicity, tracing the code evolution through the analysis 
of change records can become increasingly difficult as developers branch the source code tree, merge it back, or abandon 
branches \cite{citeulike:13156191}.

While a large number of metrics can be derived through source code and change records analyses, it offers probably 
the most functional one -- the count of physical lines of code (LOC). Other source code metrics, such as the count of logical lines 
of code (LLOC), function points (FP), or software system complexity are much less used as they are language-dependent and their 
derivation involves significant data processing overhead.

\subsubsection{Defect tracking system}
Normally, a software project defect repository serves as a centralized system for managing all of software project 
Quality Assurance (QA) activities providing users and developers with means to report and to discuss an improper system behavior.
However, in some projects, the defect repository is also used to keep a track of requests for future system features and related
discussions.

Artifacts from defect repositories are numerous as they may contain system logs, input and output files, screenshots etc. 
Their main purpose is to provide users with up to date information about system defects and their severity, and, if implemented 
in the system, with additional comments about their technical nature, resolution plans, and other information. 

By studying defect records, researchers has shown that it is possible to build predictive models for future bugs by their association 
with source code change patterns \cite{citeulike:6055293} (i.e. activity) and with particular code fragments \cite{citeulike:393158}. 
In addition, it has been shown that it is possible to optimize software testing processes by identifying source code 
``hot spots'' through the mining of bug reports history \cite{ostrand2004tool}.

\subsubsection{Developer communications}
As OSS projects usually developed by distributed teams lacking the ability for face-to-face meetings, emails, mailing lists, and 
newsgroups are typically used as primary communication channels between project participants. 
Therefore, these data is particularly useful for identifying process agents, their actions, and their activities related to the process 
coordination. 

Developer communications artifacts, such as email messages, mailing list posts, and newsgroup messages include agent identification, 
timestamps, topics, and many other useful information which can be studied in a variety of ways. 

For example Ying et al. in \cite{citeulike:1366052} proposed an interesting and novel research direction of mining developer 
communications content for understanding of the software quality, while Huang et al. in \cite{citeulike:9495129} used developer 
communications to build a developer interaction network and to categorize them by the level of their involvement and expertize.

\subsubsection{Q\&A websites}
Frequently, professional software developers and hobbyists programmers seek answers to various questions using the Internet. 
Among others resources the Internet offers community-driven platforms, such as Stack Overflow (SO) website that explicitly 
targets programmers and is dedicated to software-, hardware-, and computer system administration-related issues.

While other types of artifacts available, the ones distributed by SO team are probably the most used in MSR research. 
These are distributed monthly and contain a historical information about questions and answers along with their change history
including voting. In addition, SO team provides rich metadata about their service contributors. 
All these contributed to the selection of public Stack Overflow dump as the reference dataset in recent 2013 MSR Challenge 
\cite{MSRChallenge2013} at which a number of studies proposed interesting data analysis approaches.

Most of these are concerned with programming related questions, such as identifying topics relevant to particular 
development communities \cite{kartik:msr14}, mining additional technical expertise \cite{VenkataramaniGAMB13} \cite{SaxeMG13}, 
or identifying problematic APIs \cite{KavalerPGCDF13} \cite{Linares2013Exploratory} and documentation \cite{Campbell2013Deficient}.
Some studies address broad phenomena such as collaborative problem solving \cite{Tausczik2014Collaborative},
knowledge sharing \cite{VasilescuCSCW14} \cite{Schenk2013Geo}, and contributors behaviors \cite{Bosu2013Building} \cite{GinscaP13}.

\subsubsection{Metadata}
Often, as reported by Begel et al. \cite{citeulike:7260421} who conducted a survey at Microsoft, in order to understand performed 
software processes, the quantitative source code changes information is simply not sufficient. 
Through the survey, the authors accounted for 31 different information needs for understanding and coordinating team software 
processes, among which needs for accompanying software change metadata were clearly articulated. 
Among other purposes, it was found that metadata allows developers to learn the rationale behind software change,
find responsible people, discover and track dependencies, and to learn about the status of items in progress. 
Overall, the authors has emphasized, that the majority of developers needs were concerned with people, not the code, 
and that metadata is essential in meeting their requests.

Similarly, Kim et al. \cite{citeulike:4000311} who proposed a system for software repositories data extraction, data storage, 
and a universal data-exchange language, emphasized the metadata importance for information collection and partitioning, moreover, 
the authors has shown that it is possible to create a public metadata-based system for mining close-source software repositories.

For these reasons, STA DB has expendable, metadata-centric design. New types of metadata can be defined and related to existing 
and newly collected artifact entities. Later, within the process of software trajectories definitions, the metadata allows for efficient 
data partitioning and aggregation.

\section{Data assimilation}
Currently, there is a voluminous amount of the research literature concerned with mining of public software repositories 
\cite{citeulike:2710928} which, in fact, extends probably even larger body of earlier work that have focused on studying 
of historic private software repositories and databases \cite{citeulike:393158} \cite{citeulike:13125375} \cite{citeulike:13125481}.

Among others, the research questions discussed in this literature usually concerned with extraction of historical information 
for better understanding of software systems evolution \cite{citeulike:277045} \cite{citeulike:4000311}, 
better understand and improving of software processes \cite{citeulike:5803126}, 
and for studying the impact of software tools on the processes and products \cite{citeulike:13125389}. 

Yet, some effort has been made towards automation of the process of data retrieval and measurements. 
While some of the proposed solutions allow real-time interactive repositories exploration by extending 
repository management tools such as CVS, SVN, etc. with a front-end engine, such as Bonsai \cite{bonsai},
or JReflex \cite{citeulike:3017440}, others, such as CVSAnalY \cite{citeulike:6544724}, softChange \cite{citeulike:13125395},
and {TA}-{RE} \cite{citeulike:4000311} propose a workflow that is based on the off-line artifacts retrieval, 
pre-processing, and on-demand analyses.

Similarly to the latter, STA relies on the off-line retrieval, mirroring, and pre-processing of public software artifacts as
shown at the Figure \ref{fig:sta-assimilation}. Note, that since STA has been initially designed as a Hackystat extension 
\cite{csdl2-10-09}, it does not need any specific parser and is capable of real-time collection of Hackystat data.

\begin{figure}[t]
   \centering
   \includegraphics[width=115mm]{figures/Flow.eps}
   \caption{Detailed overview of Software Trajectory Analysis data assimilation layer. 
    At first, software artifacts are collected from software repositories, measured, converted into 
    universal to STA format, and stored in a dedicated database.
    If Hackystat used as a data source, typically, no additional processing required and data can be used in the real-time.
    Stored in STA DB software artifact entities can be further enhanced by additional measurements and metadata.
    Finally, the database allows efficient data partitioning and its aggregation into software trajectories.}
   \label{fig:sta-assimilation}
\end{figure}

\section{Relevant MSR research on recurrent behaviors discovery}
As I have shown above, MSR is a very diverse research field concerned with a variety of problems. 
Among others, researchers use information extracted from 
repositories to address research questions related to 
software system growth, 
its understanding,
software quality prediction,
refactoring and change patterns,
measuring individuals expertize and contribution,
understanding development teams social structure,
and with software processes understanding.
But in this section I focus on previous work from MSR that specifically concerned with application of analytical techniques 
to ordered sequences of software artifact measurements -- the approach which is similar to STA.

\subsection{Itemset mining}
In data mining, frequently occurring items (actions, events) often used in order to discover implicit knowledge from
large datasets. As I have mentioned earlier in Section \ref{section_software_process_design}, these techniques were 
applied for software process mining before by Cook and Wolf \cite{citeulike:328044} \cite{citeulike:5120757} 
\cite{citeulike:5128143} and Rubin et al. \cite{citeulike:1885717}. Unfortunately, these techniques are not applicable
to public software repositories since they do not offer development event logs.

Nevertheless, sequential item mining has found many applications in the MSR research.
Zimmermann et al. in \cite{citeulike:277045} developed a system called ROSE for identification of co-occurring changes 
in a software system aiming at the prediction of future code changes. 
Similarly, Kagdi et al. \cite{citeulike:3929070} applied the sequential-pattern mining to discover ordered sequences of 
frequently changed files in order to predict future changes. 
Livshits and Zimmermann \cite{citeulike:393158} developed DynaMine -- the system based on mining of function call 
patterns capable to detect potential bugs.

Potentially, these techniques can be applied to STA results. For example it may be possible to discover ordered, or
unordered sequences of frequent behaviors which can be further associated with particular development actions.

\subsection{Time series analysis}\label{chapter2_section-tsanalysis}
Because most of software artifacts are temporally marked, some MSR research seeks to quantitatively analyze ordered 
in time sequences of software artifacts or their measurements as these may carry useful information about software 
processes and recurrent behaviors. 

\begin{figure}[t!]
   \centering
   \includegraphics[width=145mm]{figures/FourrierMySQL.eps}
   \caption{}
   \label{fig:mysql-fourrier}
\end{figure}

For example Herraiz et al. \cite{citeulike:6544685} applied ARIMA models to software evolution measurements aiming 
prediction of future changes. The authors has shown that it is possible to predict a number of future changes in 
Eclipse by means of statistical non-explanatory model. 

Similarly, Antoniol et al. \cite{citeulike:3378725} have explored the application of common signal processing toolkit 
built upon Linear Predictive Coding (LPC) and Cepstrum coefficients to modeling of time varying software artifact 
histories. They have shown that it is possible to identify files with very similar size histories by using the 
proposed approach.

Temporal segmentation of time series has been applied to mining of Eclipse change log by Siy et al. \cite{citeulike:10896305}.
The authors has shown that by partitioning continuous developers activities into smaller segments, 
and in particular into segments whose duration is close to the software release cycle,
it is possible to discover ``stronger trends''. For example they have found that developers tend to focus on a particular file 
subset within a release cycle duration. In addition, they were able to detect similar change activity patterns among developers.

Finally, Hindle et al. in \cite{citeulike:10377345} described how to discover recurrent behaviors from software measurements 
by Fourier analysis. The left panel of the Figure \ref{fig:mysql-fourrier} from their work indicates that the studied 
signal carries potentially distinguishable periodic behavior, moreover, they were able to detect a promising smear 
of frequencies between 18 and 19 (right panel), unfortunately this direction was not further investigated.

\epigraph{Without the right information, you're just another person with an opinion.}{Tracy O'Rourke, CEO of Allen-Bradley}