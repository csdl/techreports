\documentclass[11pt,oneside]{article}
\usepackage{fullpage}

%%% Load some useful packages:
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{subfigure}
\usepackage{bbm}
\usepackage{tabularx}
\usepackage{setspace}
\onehalfspacing
%% Package to linebreak URLs in a sane manner.
\usepackage{url}

\usepackage{todonotes}
\usepackage{amsmath,amsfonts}
\numberwithin{equation}{subsection}
%% Define a new 'smallurl' style for the package that will use a smaller font.
\makeatletter
\def\url@smallurlstyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\ttfamily}}}
\makeatother
%% Now actually use the newly defined style.
\urlstyle{smallurl}

%% Define 'tinyurl' style for even smaller URLs (such as in tables)
\makeatletter
\def\url@tinyurlstyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\scriptsize\ttfamily}}}
\makeatother

%% Provides additional functionality for tabular environments
\usepackage{array}

%% Puts space after macros, unless followed by punctuation
\usepackage{xspace}

%% Make margins less ridiculous
\usepackage{fullpage}

%% Allows insertion of fixme notes for future work
\usepackage[footnote, nomargin]{fixme}

%% Make URLs clickable
\usepackage[colorlinks, bookmarks=true]{hyperref}

\begin{document}
\title{Contribution chapter abstract for Ph.D. dissertation: \\
       \textsc{Software Trajectory Analysis:} \\
       \textsc{An empirically based method for automated software process discovery} \\
       \author{Pavel Senin \\
               Collaborative Software Development Laboratory \\
               Department of Information and Computer Sciences \\
               University of Hawaii \\[0.3cm]
               \texttt{senin@hawaii.edu} \\[0.3cm]
               CSDL Technical Report 09-13 \\
               \url{http://csdl.ics.hawaii.edu/techreports/09-13/09-13.pdf}
       }
       \date{February 2013}
}
\maketitle

\clearpage


\section{Introduction}
Software engineering is a unique area of engineering field having almost no cost associated with
materials
and fabrication, which dominate cost in all other engineering disciplines. 
However, ironically, software engineering is suffering from the costs and challenges
associated with continuous re-design of the software design (development) processes - the issue
which is rarely seen at all in any other engineering areas. 

In order to efficiently deal with this issue, many universal design processes - known as 
software development methodologies - were proposed up to date. 
For example, the Waterfall Model process proposes a sequential pattern in which developers 
first create a Requirements document, then create a Design, then create an Implementation, 
and finally develop Tests. 
In contrast, the Test Driven Development process proposes an iterative pattern,
in which the developer  must first write a test case, then write the code to implement 
that test case, then refactor  the system for maximum clarity and minimal code duplication.

Nevertheless, despite to decades of the development, these universal methodologies known to fail in
delivering of the final product - the software. One of the reasons for these failures thought to be
in somewhat rigid structure of these models, which makes difficult accommodating the change.
Another reason is the overall immaturity of these models. These, in turn, thought to be the
consequences of the traditional, top-down approach to software process design which requires the
developer or manager to ``invent'' the process in the first place. 

Whether a software methodology invention happens through noticing of recurrent development
patterns, or through a decomposition of a larger process, invention is a non-trivial, difficult,
and error-prone task \cite{citeulike:5043104}. In addition, inventors, limited in their scope, 
tend to assume an idealized versions of real processes, thus, likely to produce ``paper lions''
- process models which are likely to be disruptive and unacceptable for end users, at least 
in their proposed form \cite{citeulike:9758924}. Finally, the full study cycle from a process
proposal, through its evaluation, to its full understanding and, potentially, acceptance is usually
measured by years \cite{citeulike:113403}.

In my research, I attempt to contribute to an alternative - the bottom-up - approach to software
process design. The idea ``at large'', is that through the observation of the performed 
by developers processes it is possible to \textit{automatically discover recurrent behaviors}, which
could be further generalized and associated with building blocks of a larger software development
process model. If such a tool is in place, it would potentially improve and shorten in time the
process analysis time, and, in addition, might shed new light on the development process itself.
In addition, such tool would benefit to other areas of process research - such as process
analysis, enactment, and to the process improvement.

This idea of inferring a process model through observing and studying performed processes is
not new and was around for decades \cite{citeulike:328044}. The problem with its further
development is usually associated with a number of ``show-stoppers'': the biggest issue is the
privacy concerns, the next is the overall cost of the observations, and the least, but not
the last, is the validity of such studies - it is difficult to make an unbiased judgment in
controlled environment. 

In my work, I am addressing outlined above issues through leveraging of knowledge
discovery techniques applied to software artifact trails. Thus, working with \textit{offline
public data}, I am avoiding privacy concerns and the cost of the surveillance, while, potentially, 
inferring \textit{true behaviors} from the real data.

\section{Background, Previous work}
Within my PhD Thesis Proposal, I have outlined the plan of working on Hackystat-collected data,
which consists of fine grained and well-organized data streams. However, later, the decision was
made to focus solely on publicly available software repositories. This decision, while increasing
reproducibility and statistical significance of findings, and applicability of my
contribution, brought a number of new challenges. First of all, the data collection,
storage, and indexing mechanisms had to be designed from scratch. Secondly, I had to
completely redesign the methods for the data analysis adapting to the public data specificities:
coarseness, noise and corruption.

Looking back on my proposal, the phrase which I wrote - ``...many temporal knowledge discovery and
data mining methods developed in the last decade can be applied to the software process domain...''
- turned out to be a very optimistic statement. During the evaluation of mentioned methods on the
real data, most of them produced insensible results, especially non-data adaptive ones - those which
assume sequential events, or based on a periodicity. While these would be my own, speculative
statement from my own experience, the failure of common sequential or associative pattern
mining techniques can be directly contributed to inherent aperiodicity, inconsistency and
granularity of public data. The numerous software process artifacts, that I have collected from
public software repositories, just do not reflect all those, atomic events such as coding,
compiling, building, and testing. Moreover, events which are possible to recognize, such as change,
are usually misplaced in time and very often merged together by the commit process. (I have tried a
number of techniques: Apriori-like algorithms, state-machines like techniques based on ProM, etc.)

What I would like to convey here, is that all the work in my thesis is done on the large volume
of incomplete, displaced in time, noisy, spurious and aperiodic data. Moreover, it is
important to understand, that this data is essentially a product of two components. First component
is a behavioral one - the human-driven, probably non-recurrent, aperiodic, creative process of
coding, while second component is the technology and methodology in use. While latter probably
provides a potentially well-measurable, marginal effect, it is still another source of noise in the
data since it is mostly impossible to recover any data about each individual development
environment and professional settings from software process artifacts.

Realizing these facts about the data in hands and the lack of proper instrumentation, in order to
advance my research work, I had no other choice, but turn to the literature review
again. Fortunately, through extensive review and experimentation, I was able to design \textit{a
novel, generic algorithm for temporal data analysis, which, in turn, demonstrate an excellent
performance and interpretability} of results. This method, which I called SAX-VSM, is one of the
contributions of my thesis and will be outlined in the following chapter.

Another, equally important contribution of my work, it the \textit{evaluation of SAX-VSM method
ability to discover recurrent behaviors}. For this, I have conducted three case studies. The
discovery of software release patterns My initial experimentation was performed on the data
collected from the Android SCM (Software Change Management) system. By application 

\subsection{Mining software process artifacts trails}
Software artifacts are abundant and thought to carry a significant amount of information about
performed processes. However, the vast majority of artifacts are concerned about the software itself
and largely associated  with a specific development methodology. Examples of such artifacts are
design documents, use cases, class diagrams and requirements, user manuals, etc. The payload of this
artifacts aids in understanding of a function, architecture, and the design of software, while
carrying a very little information about the applied effort and underlying behaviors. 

Due to this fact, I put the content of such artifacts outside of my immediate attention,
however, their layout in time - the density of their appearance, change, and death, are under
analysis.

Let me step back a bit here and explain a larger picture: the process recognition is built upon two
fundamental tasks: description and classification. Given a process, a recognition system first
generates a description of it (i.e., reconstructs the process in full or partially) and then
classifies the process based on that description (i.e., the recognition).

There are two general approaches for implementing such recognition systems - statistical and
structural. These employ different techniques for description and classification. Statistical
approaches to process recognition use theoretical concepts to discriminate among objects belonging
to different groups based upon their quantitative features. Structural approaches use syntactic
grammars to discriminate among objects belonging to different groups based upon the arrangement of
their morphological (structural organization) features. Finally, there are hybrid approaches to
pattern recognition combining aspects of both statistical and structural pattern recognition. This
last, hybrid approach I will take in the thesis.

The reasons for taking this route were explained above - the insufficient granularity of
available information and its spurious temporal arrangement. Thus, instead of inferring
process structures by ordering observed events, I rather look on these events densities and, by
using the statistical apparatus, I relate these patterns to performed activities. 

The idea is that 
By using larger
time intervals, 

Then, these

building structural
iasI focus on the layout of activities in time and their density. By using 


Many engineering, scientific, and production fields (such as movie-making or advertisement) have explicit 
and formalized design processes which are well studied to at least some degree. In contrast, in software 
development we are treating the process of design itself as a thing to be designed and, potentially, 
re-designed along the way. While there are ``best practices'', they prone to fail and it is commonplace
to alter these through combination or a systematic change.



Software artifacts are abundant and thought to carry a significant amount of information about performed 
processes.

However, the vast majority of artifacts are concerned about the software itself and largely associated 
with a specific development methodology. Examples of such artifacts are design documents, use cases, class 
diagrams and requirements, user manuals, etc. The payload of this artifacts aids in understanding of 
a function, architecture, and the design of software, while carrying a very little information about the 
applied effort and underlying behaviors. Due to this fact, I put such artifacts outside of this thesis
immediate attention.

What is studied in this work, is the informational content of software development process byproducts 
which accompany software change. It is long known that change Change not only provides an evidence about performed activities, and, potentially, 
carry an informational load about
recurrent behaviors. Such artifacts span in time, as behaviors do, and usually reflect both: the applied 
effort (process), and the evolution of the software itself. 
Examples of such artifacts are source code changes, bug reports, and developers discussions.

Note, however, that developers do not intentionally create these artifacts to enable research, or to keep 
things in some order - mainly, these artifacts are the pure byproduct helping to the development of a 
software project. Thus, we must assume here, that this data is inconsistent, that any kind of annotations 
used by developers might be erroneous, and the amount of disclosed information could simply be not enough
to determine the actual generative behavior - which ultimately leads to uncertainty of any claims about
process correctness, ``productivity'', or any other performance-related metrics. 

The focus of my work is to explore the informational content of software process artifacts designing 
a toolkit capable to handle the discovery of recurrent behaviors automatically. Ideally, such a toolkit 
must have following properties:
\begin{itemize}
 \item it must be Effective: the reported findings, with respect to behaviors reconstruction, must agree 
 with human intuition.
 \item it should be Scalable: currently software process artifact trails for a single project could easily 
       grow beyond dozens of gigabytes, thus, the computation technique should ideally be able to utilize 
       parallelization and be capable to pre-compute intermediate results alleviating the overall space-time 
       complexity to enable an online (fast turn-around) interactive mining.
 \item Efficient: the set of reported findings should not exceed a certain threshold simly becoming an 
        overwhelming stream of spurious facts.
\end{itemize}


\section{Methodology} \label{JMotif}
Given multiple trails of software process artifacts, how to find recurrent behaviors? In this chapter, 
I will describe the design and an implementation of research methodology employed in Software Trajectory 
Analysis. In short, the implemented approach enables aggregation, indexing and mining of software 
artifact trails allowing the discovery of recurrent behaviors. 

\section{Temporal attributes of software process artifacts}
The close examination and analysis of temporal dynamics of artifact-generating events laying the foundation 
of STA methodology. The extraction of software process-related metrics, their temporal partitioning and the
ability of finding the relevant information is the 



%%% Input file for bibliography
\bibliography{seninp}
%% Use this for an alphabetically organized bibliography
\bibliographystyle{plain}
%% Use this for a reference order organized bibliography
%\bibliographystyle{unsrt}
%% Try using this BibTeX style that hopefully will print annotations in
%% the bibliography. This will allow me to make notes on papers in the
%% BibTeX file and have them readable in the references section until
%% I turn them into a conceptual literature review 
%\bibliographystyle{annotation}

\end{document}
