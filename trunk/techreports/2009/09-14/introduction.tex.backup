\chapter{Introduction}

\textit{The research presented in this thesis rests concerns about recurrent behaviors extraction from
software process artifacts and rests on knowledge discovery from temporal data. Both topics are defined
in the beginning of this chapter (Sections \ref{section_background} - \ref{knowledge_discovery}.)
Then, in Section \ref{section_hypothesis}, I introduce the research hypothesis and define the general 
objectives of my research. Section \ref{section_contributions} enumerates main contributions of this thesis,
and Section \ref{section_organization} provides the thesis organization.
}

\section{Background.}\label{section_background}
Contemporary software projects typically have a considerably long life-cycle - well over decade.
Their development and maintenance usually carried out by geographically distributed teams 
and individuals. The development pace, the experience, and the structure of these development 
teams continuously change as developers are joining and leaving. When combined with
schedule and requirements changes, all these create numerous difficulties 
for developers, stakeholders, and community ultimately affecting the project success. 

This software development complexity phenomena was identified in 1968 as ``Software crisis'' \cite{crisis}, 
and was addressed by bringing the research and the practice of software development 
(or as it was called ``programming'') under the umbrella of Engineering - in an effort to provide a 
control over the process of software development. 
Following the engineering paradigm, numerous methodologies and models of \textit{software processes}
were proposed \cite{citeulike:10002165}.

A \textbf{\textit{software process}} is a set of activities performed in order to design, develop and maintain 
software systems. Examples of such activities include requirements collection and creation of UML diagrams;
requirements testing; code development, testing, and performance analysis. 
The intent behind a software process is to structure and coordinate human activities in order to achieve 
the goal - deliver a software system successfully in time and under a budget.
Since then, much work has been done in software process research resulting in a number of industrial standards for 
process models which were widely accepted (CMM, ISO, PSP etc. \cite{citeulike:5043104}) in industry. 

Nevertheless, industrial software development remains error-prone and more than half of all commercial software
development projects ending up failing or being very poorly executed (Rubinstein, ``Chaos Reports'', 2006). 
Some of them are abandoned due to running over budget, some are delivered with such low quality or so late that
they are useless, and some, when delivered, are never used because they do not fulfill requirements. 
fields. Through the years of experience, it was clearly understood, that software engineering is
very different from any other engineering field. There is almost no cost associated with materials 
and fabrication which dominate cost in all other engineering disciplines, but, ironically, 
software engineering is suffering from the costs and challenges associated with continuous 
re-design of the product and its design processes - the issue which is rarely seen at all 
in any of other engineering areas \cite{citeulike:5203446}.

Along with industrial, engineering-like, processes, a number of alternative software processes emerged from
academy, hobbyists, and practitioners. 
Among others, the Free/Libre/Open-Source Software model (FLOSS) and the software craftmanship  
approaches gained a significant credibility. While the former \textit{holistic} software process paradigm emphasizes
loosely-organized collaboration, frequent releases, and removes the boundary between developers and customers, 
the latter is focusing on the roles of highly motivated, skilled individuals in a process of software creation
\cite{citeulike:262020} \cite{citeulike:2759198}. 

Similarly, alternative processes affected by the same complexity issues. As was shown, most of FLOSS projects 
never reach a ``magic'' 1.0 version \cite{citeulike:12480029}. Among others, the great "infant mortality rate" of OSS
projects was related to a burnout, inability to acquire a critical mass of users, loss of leading developer(s), and
forking \cite{richter2007critique}. Software craftsmanship, from other hands, not only challenge developers
with technological advances requiring continuous education, but create cost and effort estimation difficulties for
stakeholders and project managers \cite{citeulike:11058784}.

%The recent study by Standish Group (Rubinstein, ``Chaos Reports'', 2006) indicates,
%that while `\textit{`Software development shops are doing a better job creating software than they were 
%12 years ago}'', still, only ``\textit{35\% of software projects in 2006 can be categorized 
%as successful meaning they were completed on time, on budget and met user requirements}‚Äù.
%Another, widely acknowledged problem with existing models is their rigidity - usually 
%once a project executed, the cost of incorporating a change in the process or a product 
%not only becomes significant, but also grows proportionally to the project execution time.

Currently, it is widely acknowledged, that there exists no single ``silver bullet'' process which can bring any 
software development project to success \cite{citeulike:1986013}. Processes are numerous, each has advantages 
and drawbacks, and each is accompanied with numerous application recommendations, success stories, and 
failures. However, the alarming rate of failing projects suggests, that all available knowledge is not enough for
making a proper process choice in any particular situation \cite{citeulike:12550665}. 
The enormous cost of the lost effort, and the lack of understanding of software process mechanics continue 
to fuel research community. 

\section{Software process design}\label{section_software_process}
Historically, there are two categories of approaches to software process design, analysis, and improvement. 
The first category consists of traditional to engineering \textit{top-down} approaches through \textit{proposing a
process based specific patterns of software development}. 
For example, the Waterfall Model process proposes a sequential pattern in which developers first create a 
Requirements document, then create a Design, then create an Implementation, and finally develop Tests. 
The Test Driven Development process, from other hands, proposes an iterative behavioral pattern in which
the developer must first write a test case, then write the code to implement that test case, then refactor the 
system for maximum clarity and minimal code duplication. 

While the top-down approach seems to be a natural extension of creative processes - such as invention
and experimentation, and follows the usual path of trial and error, one of the problems is that ``inventing''
the adequate 
to the task software process is far from trivial \cite{citeulike:5043104} \cite{citeulike:1986013}. 
Moreover, it was shown, that the process inventors are often limited in their scope and tend to assume an idealized
versions of real processes, thus, lekely to produce ``paper lions'' - process models which are likely to be
disruptive and unacceptable for end users, at least in their proposed form \cite{citeulike:9758924}. 
Finally, the evaluation cycle of an invented process is long and expensive. 

The second category consists of opposite, \textit{bottom-up} techniques for true \textit{process reconstruction 
through noticing of recurrent patterns of behaviors}. 
Within this category, typically, the process inference problem is viewed 
as a two-levels problem, where the first level consists of a patterns discovery, and the 
second level consists of pattern recognition and a network analysis.
One of the first works following this paradigm is by Cook and Wolf, where they show a
possibility of automated extraction of a process model through mining of recorded event logs
\cite{citeulike:328044} \cite{citeulike:5120757} \cite{citeulike:5128143}. 
Later work by Huo et al., shows that through event logs analyses it is also possible to 
improve existing processes \cite{citeulike:7691059} \cite{citeulike:7690766}. 

While the bottom-up approaches seem to be more systematic and less complex than invention, they 
also affected by a number of issues. A chief among these is the observability issue - 
it is usually very difficult to conduct a full depth study on a live project due to the privacy concerns. 
Moreover, it is expensive to observe a process performed by a team for a whole life-cycle of a project. 
Yet another issue is the capacity of the process discovery techniques - these often need to be supervised 
and fine tuned in order to reconstruct distributed and concurrent processes. 

\section{Public software repositories}\label{section_public_repositories}
Recently, however, the situation changed, and the interest for bottom-up process reconstruction has 
been revived. This change is driven by increase in public data that are made available by the 
proliferation of open source communities.
Currently, software artifacts are abundant: source code repositories, public bug/issue tracking systems, 
mailing list communications, social networks, Q\&A websites - all these are easily accessible. 
This effectively removes not only the high cost of observation, but most of the privacy concerns - both 
issues that previously made large-scale analysis of software projects unfeasible for most researchers.

Scientific community response on the availability of public artifacts was overwhelming, and a number of 
venues was established addressing increased interest. 
Since 2004, the International Conference on Software Engineering (ICSE) hosts a Working Conference on 
Mining Software Repositories (MSR). The original call for papers stated MSR's purpose as 
\textit{``... to use the data stored in these software repositories to further understanding of software 
development practices ... [and enable repositories to be] used by researchers to gain empirically based 
understanding of software development, and by software practitioners to predict and plan various aspects 
of their project''} \cite{msr2004} \cite{citeulike:7853299}. 
Several other venues: International Conference on Predictive Models in Software Engineering \cite{promise12}, 
International Conference on Open Source Systems, the Workshop on Public Data about Software Development, 
and the International Workshop on Emerging Trends in FLOSS Research have also played
an important role in shaping and advancing this research domain.

Within the published work, most notable is one by Jensen \& Scacchi, where they demonstrated that
 information reflecting software processes may be gathered from public systems in \cite{citeulike:12550640};
later, they has shown that by manual mapping of process evidence to a pre-defined process meta-model 
it is possible to discover some FLOSS processes \cite{citeulike:5043664} \cite{citeulike:5128808}. Another
closely related to my research work is by Hindle et al. where they has shown that it is possible to 
discover some processes through partitioning \cite{citeulike:10377366}.

Nevertheless, while availability of public software artifacts partially solves problems of observability and privacy, 
at the same time, they introduce a number of additional challenges, significantly elevating the complexity 
of process discovery problem. The main challenges facing research based on public software process 
artifacts are, in fact, due to these artifacts nature:
\begin{itemize}
\item First of all, the artifacts created by developers and users not in order to enable the research,
but merely to support software development activities. Thus, the process-related informational content of these
artifacts is questionable.
\item Secondly, majority of these artifacts (change records, defect reports, assigned tasks, etc) represent the current 
state - a snapshot - of a software project state, rather than reflect an action, and it is simply impossible to
infer any of low-level software development events \cite{citeulike:1296888}.
This effectively render obsolete a majority of previously developed event-based process discovery tools.
\item Thirdly, developers and users not only create and submit to repositories artifacts on their own volition,
but most of the change management system (such as Git, Subversion, and Gerrit) offer an asynchronous workflow, 
where the locally created artifacts might never be committed \cite{citeulike:2280690} \cite{citeulike:9037939}. In other
words, artifacts are displaced in time and it is often impossible to know exactly when their content was created.
 \item Finally, the high volume of artifacts demands for automated, high throughput techniques robust to noise
 \cite{citeulike:12550438}, \cite{citeulike:7853299}, \cite{citeulike:4534888}.
\end{itemize}
Overall, it is well pronounced, that novel analysis and discovery techniques are needed for public software process
artifacts when ``\textit{... Going Beyond Code and Bugs...}'' \cite{citeulike:7853299}.

In this thesis I show that it is possible to discover knowledge about performed processes from public software process
artifacts by application of temporal data mining techniques.

\section{Knowledge discovery from time series}\label{knowledge_discovery}
In many of data mining research areas time series used as a proxy representing a vast variety of
real-life phenomena in wide range of fields including, but not limited to physics, medicine, meteorology, 
music, motion capture, image recognition, signal processing, and text mining. 
While time series usually directly represent observed phenomenas by correlating data with time, the pseudo time 
often used for representation of various data by combining data points into ordered sequences. For example
in spectrography data values are ordered by component wavelengths \cite{citeulike:12550833};
in shape analysis the order often the clockwise walk direction starting from a
specific point in the outline \cite{citeulike:12550835}, in image classification the numbers of pixels
are sorted by color component values \cite{citeulike:2900542}.

In this work, I will use time series as a representation of a yet another phenomena - a software development 
process. This idea is inspired by the previous work by Johnson et al. \cite{citeulike:12550871} performed 
within the Hackystat project, where they used knowledge extracted by experts through visual correlation of
software metrics telemetry streams in order to improve software development management. Similarly, I use 
software metrics trajectories for extraction of software process knowledge. 

A \textit{\textbf{software trajectory}} is a line, or progression, which resembles a software state evolution.
It differs from time-series, or mentioned above telemetry streams, in a sense that trajectory assumed not to 
represent observed phenomena exactly. While time-series usually represent precise measurements obtained 
through direct observation, and telemetry streams assumed to be similar in their precision measurements 
obtained by remote sensing, trajectory is by precise in a
real 
explain a
phenomena preciselyTo clarify differences Working with software Since not all of the software metrics can be collected
and and the true state of the software cannot be assessed, The true state which

in order to extract automatiextract
Extending this 
approach, I show a system which build 


in yet
another my work I address exactly this problem - discovery of software process primitives from publicly available 
software process artifacts. I will apply knowledge discovery and data mining techniques to the

In my work, I focus on the single type of such primitives - recurrent behaviors. 
Since recurrent behaviors extraction While further, I show a novel algorithm and a reference workflow for that In this dissertation, that instead of deducting atomic events it is
possible to increase the level of basic tractable units to behaviors, and simplify they discovery through automated
application of data mining techniques.

\section{Research questions, hypothesis, and a scope of the dissertation}\label{section_hypothesis}
Software is coded by humans. Whether in team or individually, we perform a number of daily activities - 
commuting to an office, answering emails, attending meetings, writing a code, and many others. 
The ordering of these activities and their durations are driven and constrained by internal and external factors, 
such as a role, motivation, working schedule, project phase, physical location of the office, etc. 
When summarized together, the range of these activities and the mannerisms form our behavioral portraits.

A subset of daily activities can be attributed as relevant to the project goal - delivering a software. 
Thus, one of the preliminary research questions is the \textit{partitioning of the activities} into those that 
relevant to the process and those which are auxiliary. 


By limiting the scope of my research to a discovery of knowledge only from software process artifacts, 
I address the above question at large by obviously considering only related to software process entities.
This partitioning scheme, however, leads to another research questions: \textit{is it possible to discover 
recurrent behaviors from a very limited set of software process by-products?} and, if it is possible, 
\textit{will the discovered behaviors be meaningful, i.e. interpretable?}

Some of the previous research, especially in MSR field  \cite{citeulike:9114115, citeulike:7853299}, 
indicates, that by application of a variety of a techniques it is not only possible to discover evidence of 
software process \cite{citeulike:9007622}, but, at least partially, to infer the process as a 
whole \cite{citeulike:5128808}. However, in the mentioned research, software process artifact features 
were mostly captured by using of an expert knowledge of measurable discriminative properties of the 
event classes, which effectively limits analyses to The feature selection
process entails manual expert involvement and repeated experiments. Automatic feature
selection is necessary when (i) expert knowledge is unavailable, (ii) distinguishing features
among classes cannot be quantified, or (iii) when a fixed length feature description cannot
faithfully reflect all possible variations of the classes as in the case of sequential patterns
(e.g. time series data).

These combine into the research hypothesis - \textit{it is possible to discover recurrent behaviors 
from software process artifacts by application of appropriate data-mining techniques} - 
which I investigate in this dissertation. 

In order to approach this hypothesis questions, I choose to reduce the problem of mining of software artifacts to a narrower,
but more generic software treat software artifacts as time-series and design an interpretable time 
series classification technique, which, 
as I will show, temporal 


Exactly as it sounds, my research is interdisciplinary - it combines together knowledge discovery and software process 
analysis, focusing on a very narrow subject - exploring approaches for recurrent behaviors or ``programming habits'' 
discovery from software process artifacts.
While I will show, that recurrent and significant software-development behaviors can be discovered,
their precise categorization, effect, and performance are beyond the scope of this thesis.

\section{Contributions}\label{section_contributions}
I show a novel, generic algorithm for interpretable time series classification: SAX-VSM. 
While the performance of this algorithm is at the level of current state of the art, it offers an outstanding feature -
discovery, generalization and ranking of class-characteristic structural features. This feature, in turn, enables
knowledge discovery by offering much clearer insight into data specificity than any other competing technique.
In addition, SAX-VSM uses only N weight vectors for classification of unlabeled data by computing N cosines, where N is
a number of classes, - therefore it is very fast and has a very small memory footprint.
Overall, I expect this algorithm to play an important role in future because of the growing ubiquity of time series and
growing interest in behaviors.

I provide SAX-VSM implementation to the community. This implementation uses several computational tricks to optimize,
reduce, and reuse computation. Within last years, this implementation was regularly downloaded and used in academia and
industry. 

Powered by SAX-VSM, through the application of Software Trajectory Analysis (STA) to software process artifacts, I show
through the case studies: that it is possible to discover known recurrent behaviors, thus positively confirming the
research hypothesis. In the PostgreSQL study I was able to discover characteristic recurrent behaviors in source code
editing churns corresponding
to Software release and to the Commit Fest processes.
that STA and SAX-VSM can be used as a knowledge discovery tool in the StackOverflow case study. In particular, I show,
that the temporal primitives discovered by the algorithm provide not only quantitative evidence for processes
interpretation, but can be used for a qualitative assessment of discovered recurrent behaviors.
In Android case study...

Finally, I provide an implementation of STA analysis framework to the community. 

\section{Organization of the dissertation}\label{section_organization}