Return-Path: <@galaxy.net.hawaii.edu:johnson@natasha.ics.hawaii.edu>
Received: from uhunix.uhcc.Hawaii.Edu by uhics.ics.Hawaii.Edu (4.1/SMI-4.1)
	id AA17649; Sun, 5 Sep 93 10:28:03 HST
Received: from galaxy.net.Hawaii.Edu by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA01226; Sun, 5 Sep 93 10:26:58 HST
Received: from uhunix.uhcc.Hawaii.Edu ([128.171.44.6]) by galaxy.net.Hawaii.Edu with SMTP id <119645>; Sun, 5 Sep 1993 10:24:41 -1000
Received: from natasha.ics.Hawaii.Edu by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA01148; Sun, 5 Sep 93 10:26:29 HST
Message-Id: <9309052026.AA01148@uhunix.uhcc.Hawaii.Edu>
To: johnson@Hawaii.Edu, kreifelts@gmd.de,
        jacobs@picasso.informatik.rwth-aachen.de, d.r.newman@qub.ac.uk,
        ludwin.fuchs@gmd.de, konrad@nada.kth.se
Subject: Position paper
Date: 	Sun, 5 Sep 1993 10:27:20 -1000
From: Philip Johnson <johnson@uhics.ics.Hawaii.Edu>

  Position Statement for ECSCW '93 Tools and Technologies Workshop:
		      Defining Research Agendas


                Methodological Issues in CSCW Research
				   
			    Philip Johnson
	    Collaborative Software Development Laboratory
	   Department of Information and Computer Sciences
			 University of Hawaii
			  Honolulu, HI 96822



			     Introduction

This position paper provides a brief overview of the research
activities currently underway in the Collaborative Software
Development Laboratory.  It then elucidates some of the important
research methodology issues that have arisen during these activities.



			   Overview of CSDL

The Collaborative Software Development Laboratory (CSDL) at the
University of Hawaii is engaged in a variety of projects related to
the application of computer-supported cooperative work technology to
areas in software engineering.  

Most of our projects involve systems built upon a common
infrastructure: Egret (Johnson, 1992).  The EGRET system provides a
multi-user, hypertext, database application development environment
within a Unix/X window environment.  Egret has a client-server
architecture with a C++ back-end and Emacs front-ends.

Three of our current projects involve collaborative learning (CLARE),
collaborative formal technical review (CSRS), and collaborative
reading and analysis of Usenet (URN).

CLARE is a system that implements a data and process model to
facilitate understanding, analysis, and synthesis of research
literature. Through CLARE, a group of learners create structured
hypertext summaries of research articles that typically instantiate
canonical research representations (such as "concept paper",
"experience report", etc.).  In addition to the summaries, the
learners also create evaluative commentary on the article.  CLARE then
facilitates the learners' comparison of their analyses to each other,
and the generation of a common, consensual view on the relevence
and importance of the research artifact.  CLARE is currently 
being experimentally evaluated through a descriptive, protocol-analysis
study involving 15 undergraduate software engineering students.  
The results will form the basis of a Ph.D. thesis currently 
under preparation (Wan, 1993).

CSRS is a system for software review that supports the process of
analyzing software artifacts with the goal of discovering defects,
capturing design rationale information, clarifying requirements
specifications, and so forth.  CSRS implements a data and process
model that puts most of the process and products of software review
on-line.  More details about CSRS are provided in (Johnson and
Tjahjono, 1993).

URN is the most recent research project in our group, and is currently in
the design stage. URN is a system that is designed to overcome several of
the problems created by the extremely high volume of Usenet information and
access.  The basic premise is that through collaborative hypertext-based
restructuring and evaluation of Usenet articles, URN users can see a higher
volume of high interest articles, and archive and retrieve interesting
information more successfully.

Besides Egret, all of these systems share another common mechanism: the
presence of fine-grained instrumentation to measure the process and
products of their usage.  Semantically interesting actions taken by each
user are unobtrusively timestamped and recorded.  In addition to this
"top-down" mechanism, a "bottom-up", timer-based timestamp mechanism runs
once per minute to record whether or not any low-level editor activity has
recently occurred.  This combination of mechanisms allows us to both
capture the sequence and duration of activities occuring during the use of
our collaborative environment, as well as detect periods of idle time when
the user's focus of attention is elsewhere.



			Research Methodology Issues


Our current research method is an outgrowth of problems we have experienced
personally in our research group, as well as problems we see confronting
the field of CSCW in particular and software research in general.  The
fundamental issue is, "What constitutes `scientific' software-based
research, and is `scientific' software research always necessary or
desirable?"

Recall from your primary school science class that the scientific method is
supposed to proceed via (a) the generation of disprovable hypotheses; (b)
the design of an experiment that will generate data that either support or
disprove the hypothesis; (c) the collection of this data; and (d) the
analysis of the data and the generation of conclusions, if any.

What frequently occurs in software-based research is the following: (a) a
researcher has a "neat idea", often based upon the recent invention of a
particular piece of technology (for example, multi-media); (b) the
researcher implements the "neat idea"--usually not very well; (c) the
researcher gets the "neat idea" to run on a single demo example that is
carefully constructed for success; and (d) the researcher writes papers
claiming that this idea is interesting, feasible, scalable, and so forth.

This falls short of the classic scientific method along a number of
fronts: (a) there is no hypothesis, much less a disprovable
hypothesis; (b) there is little to no experimental design; (c) there
is little to no formal data collection; and (d) there is little to no
analysis.  This research method is similar to an "existance proof",
except there is no proof, only existance!  The merit of the research
is thus based primarily on how persuasively the researcher is able to
argue for it, rather than based upon any objective data.  The primacy
of persuasion means that other, subjective factors, such as the
author's advisors, current or prior institutions, "connectedness", and
so forth are frequently brought into account when "evaluating" the
merit of the research.

The "neat idea" paradigm of software research is especially treacherous for
beginning researchers, such as doctoral students. The fundamental problem
for these students is that the value of the "neat idea" paradigm is
intrinsically linked to how "neat" the "idea" is----if the idea isn't neat,
then the research has no value.  This contrasts with the traditional
scientific method, which is designed to allow hypotheses and experimental
designs that are interesting and generate scientifically important results
regardless of the outcome. (For example, disproving the Theory of
Relativity is as interesting as confirming it.)  

Thus, beginning researchers following the "neat idea" paradigm have an
especially arduous and stressful research process, since they must continue
to generate and test ideas (often requiring extensive software development
effort each time) until one of them proves to be sufficiently "neat" and
thus deserving of a conference paper acceptance, Ph.D. degree, and so
forth.  More accomplished researchers can often leverage off of previous
neat ideas to generate new ones, particularly since a minor change pto a
previously agreed upon "neat idea" is usually still "neat".

In our laboratory, we have replaced the "neat idea" paradigm of
research with a formal research process model with the following
characteristics: (a) all major research efforts begin with preparation
of a "research objective statement", that defines the goals, method,
and expected contribution of the research; (b) all major research
efforts have as their primary contribution the generation of new
empirical data about an interesting issue in software engineering of
CSCW; algorithms, techniques, and so forth are secondary; (c) all
major research efforts should lead to publishable results within 4-6
months of their initiation.  Masters and Ph.D. theses should consist
of 2-4 sequential, discrete, research objectives.

The CSDL research process model has both strengths and weaknesses.
The strength of this process model is that it overcomes the principle
risk associated with the "neat idea" paradigm: researchers do not
embark upon labor-intensive software development activities until they
have defined an empirically-based objective for this activity that
will result in publishable data within a short period of time.  This
dramatically decreases the risk and attendent stress of research
within our research group.

The major weakness of the CSDL research process model is that it
dramatically constrains the type of research performed in our
laboratory.  First, it substantially devalues research of a
non-empirical nature.  Second, it discourages research requiring
long-term investment that cannot be sliced up into intermediate,
publishable deliverables.  Third, it requires a top-down, waterfall
model, non-exploratory approach to research: the researcher must be
able to specify the research process and products in advance.  A
side-effect of this model is that new members to our group must be
handed a fully-specified research objective statement to execute; the
ability to write these documents requires substantial research and
domain-specific knowledge and experience.

Finally, it appears to be significantly more costly than the neat idea
paradigm.  In the case of CSDL, we invested over 2 years of
development effort into our underlying software infrastructure (the
Egret system and associated instrumentation) before we were in a
position to implement this research process model.  During this time,
we continued to publish, although the papers were in the "neat idea"
mode.

As we continue to refine and develop this research process model,
(which is itself instrumented and generating publishable data,
although this aspect will not be discussed in this position paper) we
are interested in exploring the following research methodology issues.
First, what is the appropriate motivation for building a CSCW system?
To what extent should one pre-determine the goals and evaluation of
the system as we do now?  When is good science sacrificed in the
interest of short-term results and reduced risk of failure?  Second,
how can CSCW research be made more "re-usable"? The current
landscape seems rather flat, with relatively little research building
off of previous work by other researchers.  How can research be
designed to be more influential?

			      References

Philip M. Johnson, "Supporting Exploratory CSCW with the Egret
Framework", in Proceedings of the 1992 Conference on Computer
Supported Cooperative Work, 1992.

Dadong Wan, "CLARE: A Computer-Supported Collaborative Learning
Environment Based on the Thematic Structure of Research and Learning
Artifacts", Ph.D. Dissertation (in preparation), University of Hawaii,
Department of Information and Computer Sciences, 1993.

Philip M. Johnson and Danu Tjahjono, "Improving Software Quality
through Computer Supported Collaborative Review", in Proceedings of
the Third European Conference on Computer Supported Cooperative Work,
September, 1993.



































