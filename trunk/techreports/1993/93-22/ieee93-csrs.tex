%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ieee93-csrs.tex -- 
%% RCS:            : $Id: ieee93-csrs.tex,v 1.5 93/12/27 08:43:09 johnson Exp Locker: johnson $
%% Author          : Philip Johnson
%% Created On      : Wed Dec  8 12:17:04 1993
%% Last Modified By: Philip Johnson
%% Last Modified On: Wed Nov 23 12:17:15 1994
%% Status          : Unknown
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1993 University of Hawaii
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% History
%% 8-Dec-1993		Philip Johnson	
%%    

\section{A Reviewer's Perspective of CSRS}
\label{sec:csrs}

With the FTArm method, formal technical review using CSRS is an almost entirely on-line,
asynchronous process.  Review typically begins with the downloading of
the review artifacts, such as requirements specifications, designs, source
code, test plans, and so forth from their ASCII files into the CSRS
database.  CSRS semi-automatically re-structures the artifacts as a
fine-grained hypertext network consisting of typed nodes and links. For
example, for source code artifacts, each function and variable definition
would be stored within its own node, with hypertext links to related nodes.
For a requirements specification, CSRS would re-structure the document
hierarchically into a set of nodes for each major section, each containing
subnodes for individual specifications typed according to their focus.

\begin{figure}
 {\centerline{\psfig{figure=initial-private.ps}}}
\caption{The initial CSRS screen when a reviewer starts private review.}
\label{fig:initial-screen}
\end{figure}

After an orientation session, reviewers begin to {\em privately} analyze
the on-line artifact.  Figure \ref{fig:initial-screen} shows the initial
screen that appears for the exemplary review experience, called Type-I,
that will be used throughout this paper\foot{This FTR occurred during Fall
of 1993 as a normal part of the software quality improvement practices of
our development group. The examples are presented ``as is'', with no
attempt to clean up misspellings or hacker's jargon.}. The left hand side
of the screen contains a window with a list of review artifact nodes (in
this case, source code variable, function, and macro definitions).
Reviewers ``middle-click'' over an artifact name to retrieve it for review.
Reviewers then analyze the artifact for quality-related problems and create
links to comment, issue, or action nodes as necessary.

Figure \ref{fig:issue-screen} illustrates the screen configuration during
issue generation.  The artifact generating the issue appears on the left
hand side of the screen, the top right hand side contains a list of issue
types for this particular review process, and the bottom right hand side
contains the text of the issue.  A link connects this particular review
artifact to the issue it generated, with anchors (in bold, italicized font)
displayed in both the source node window and the issue node window.
Reviewers typically traverse the network by middle-clicking on a link
anchor to retrieve that corresponding node.

\begin{figure}
 {\centerline{\psfig{figure=issue-private.ps}}}
\caption{A CSRS screen illustrating the generation of an issue.}
\label{fig:issue-screen}
\end{figure}

During the private review phase, reviewers are only permitted by
the system to view the issues they themselves create.  This prevents
``free-loading'', and enables reviewers to analyze the artifact without the
bias of other reviewer's viewpoints.  As reviewers work, CSRS unobtrusively
generates a timestamped log file containing a listing of semantically
meaningful actions taken by each reviewer. CSRS records any idle-time
detected during review in the log file. These measurements will be
described in more detail in Section \ref{sec:metrics}.

Once reviewers have completed private review, they begin a {\em public}
review phase.  During public review, the focus of effort shifts away from
the original artifact to the commentary generated by the reviewers during
private review. The goal of this phase is to generate consensus about the
validity, severity, and implications of the issues raised during private
review, as well as to address any additional issues that are discovered
during this process.  Reviewers analyze each of the issues raised, and vote
by specifyng one of: confirm, disconfirm, or neutral.  Any of these votes
can be annotated with an explanatory comment.

Figure \ref{fig:public-screen} shows a screen configuration from the
public review phase of the Type-I review.  This screen shows the same
review artifact from Figure \ref{fig:issue-screen} on the left hand side,
as well as the same issue in the lower right hand side.  In this figure,
however, that issue has been voted upon by other reviewers, and the upper
right hand side displays one comment created in reaction to this issue.  In
fact, the ``Follow-up'' field to this comment contains a link anchor to the
next node in this conversational thread.

\begin{figure}
 {\centerline{\psfig{figure=public.ps}}}
\caption{A CSRS screen from the public review phase.}
\label{fig:public-screen}
\end{figure}

As in public review, CSRS generates a log file containing a timestamped
listing of the essential semantic actions taken by reviewers, along with
idle-times.  In addition, CSRS automatically generates a daily e-mail
message to each reviewer if there are any new additions to the public
debate that the reviewer has not yet seen.

After public review, the moderator uses CSRS to ``boil down'' the
commentary generated by the reviewers into a Consolidated Review Report
that concisely represents the status of review. If all reviewers reached
consensus about the issues raised, then the review terminates at this
point, and the Consolidated Report becomes the final report of the review
concerning the software artifact. CSRS provides tools to support the
generation of a nicely formatted LaTeX document containing this report.
Figure \ref{fig:final-report} shows one page from the Consolidated Report
generated for the Type-I review.  The Consolidated Report is not the only
product of review, however. Other reports describing the measurements and
analysis of the FTR process are also available, as discussed below.

\begin{figure} 
\input{consolidated-example.tex}
\caption{An example page from the CSRS final report for the Type-I
review.}
\label{fig:final-report}
\end{figure}

For certain issues, consensus may not be reached through an on-line
process, or their resolution may require higher-bandwidth than an on-line,
asynchronous process.  To handle these requirements, public review is
optionally followed by a synchronous, face-to-face group review meeting.
In this meeting, only those issues not resolved through the on-line debate
are raised and resolved.  The results from this meeting are entered into
CSRS and a revised Consolidated Report is generated.

This description is intended only to provide a ``gestalt''
feel for CSRS, as context for the remainder of this paper, which
focusses on the design of high quality measurement for formal
technical review.  While the preceding discussion presented CSRS
from the reviewer's perspective, several other roles exist and
are crucial to fully appreciating the system:

\begin{itemizenoindent}
  
\item The {\em administrator} is responsible for defining the precise CSRS
  data and process model to be used during review.  For example, the
  administrator determines the specific artifact analysis technique, such as
  whether or not checklists will be used and what items will appear in the
  checklists. Administrative activities occur during the Process Improvement
  meta-phase, described in Appendix \ref{sidebar:csrs}.
  
\item The {\em moderator} is responsible for overseeing the actual review
  and monitoring the on-line process, including transitioning between phases
  and producing the Consolidated Report.
  
\item The {\em producer} is the person who created the artifact under
  review.  The producer is responsible for assisting the moderator in
  setting up the hypertext network, and for answering certain types of
  questions raised during any phase of review.
  
\item The {\em analyst} is responsible for reviewing the measurements
  collected during each review experience, comparing them to prior
  measurements, and generating hypotheses about how to improve review
  efficiency and effectiveness based upon this empirical data.

\end{itemizenoindent}

Of course, a single person may adopt more than one of these roles during a review. 

CSRS is implemented as a client-server system, where a C++ database server
process communicates with highly customized Lucid Emacs clients.  CSRS is
built on top of the EGRET collaborative work environment \cite{csdl-93-09}.

Our insights into FTR instrumentation have been gained through on-going
review activities in our research group over the past two years. (Initially,
each review experiment was followed by a fairly long period of redesign of
CSRS.  Currently, the system has stabilized and our group uses it regularly
for quality assurance of the software developed in our lab.)  
Figure \ref{fig:statistics} summarizes some key data concerning our review
experiments.  The next sections discuss the insights we have gained from 
these experiments into measurement of FTR and design for instrumentation.

\begin{figure}[h]
  \begin{center}
  \begin{tabular} {|l|c|} \hline   
    Review Experiments & 9 \\ \hline
    Artifact Size & 450--750 lines \\ \hline
    Review Rate & 200--500 lines/hr \\ \hline
    Group Size &  4--6 people\\ \hline
    Duration & 10--35 days \\ \hline
    CSRS Sessions & 2--28 logins/reviewer \\ \hline
    Issues Generated & 50--104 nodes\\ \hline
    Issue Generation Rate & 3--12 issues/hr \\ \hline 
  \end{tabular}
  \caption{{\em Ranges of key review statistics.}}
  \label{fig:statistics}
  \end{center}
\end{figure}







  






