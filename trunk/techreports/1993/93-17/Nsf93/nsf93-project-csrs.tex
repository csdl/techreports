%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% nsf93-project-csrs.tex -- 
%% RCS:            : $Id: nsf93-project-csrs.tex,v 1.6 93/09/01 17:20:27 johnson Exp $
%% Author          : Philip Johnson
%% Created On      : Mon Aug 16 14:32:13 1993
%% Last Modified By: Philip Johnson
%% Last Modified On: Wed Sep  1 17:20:08 1993
%% Status          : Unknown
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1993 University of Hawaii
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% History
%% 16-Aug-1993		Philip Johnson	
%%    


\subsection{CSRS}
\label{sec:csrs}

\subsubsection{Method}

This section provides an introduction to FTR using CSRS with illustrations
from a recent review.  A comprehensive specification of FTR using CSRS from
a user perspective is provided in \cite{csdl-93-11}; a detailed, current
design-level specification of CSRS is provided in \cite{csdl-92-11}.
Formal technical review using CSRS involves seven well-defined phases, as
illustrated by the diagram in Figure \ref{fig:process-model}.

\paragraph{Setup.} In this phase, the moderator and/or the producer decides upon
the composition of the review team and the artifacts to be reviewed.  The
moderator copies each program entity, such as a function definition, class
declaration, or variable definition into an individual hypertext node and
annotates it when necessary with supplemental information.  Various links
are created between these nodes to represent explicit and implicit
relationships. CSRS provides tools to automate much of the Setup phase in
the case where Lisp source code is the artifact under review.  However, the
CSRS process applies equally well to other artifacts (such as design
documents, test plans, and so forth), though support tools are not
currently as well developed.

\paragraph{Orientation.} This phase prepares the participants for private
review by introducing them to the artifacts under review.  The exact nature
of this phase depends upon the kind of review and the review group.  It may
range from a formal overview meeting with a presentation by the producer
about the review artifact structure and behavior, to an informal
notification through e-mail noting the presence of new nodes to review.
The Orientation phase assumes that the participants are familiar with CSRS;
if not, a separate CSRS training session must be provided.

\paragraph {Private review.} In this phase, reviewers inspect source nodes
privately and create issue, action and/or comment nodes.  Issue and action
nodes are not publicly available to other reviewers at this time, but
comment nodes are publicly available.  Comment nodes allow reviewers to
request clarification about the logic/algorithm of source nodes, or about
the review process, and may also contain answers to these questions by other
participants.

Figure \ref{fig:summary} illustrates a summary screen that reviewers use to
keep track of their progress during private review.  At this point, the
reviewer has partially completed private review, as indicated by the fact
that some of the source-nodes are reviewed, some have been read but have
not been completely reviewed, and some have not yet even been seen.

By mouse-clicking on a line or through menu operations, the reviewer can
traverse the hypertext network from this screen to a node containing a
source object for review, such as the one illustrated in Figure
\ref{fig:source}.  In this case, the object under review is the operation
{\tt gi*nbuff*make}.  Both pull-down and pop-up menus facilitate execution
of the most common operations during this phase, such as creating an issue
concerning the current source node under review (as illustrated in Figure
\ref{fig:issue}), or proposing a specific action to address an issue.

Reviewers explicitly mark each source node as reviewed when finished. While
reviewers do not have access to each other's state during private review,
the moderator does.  This allows the moderator to monitor the progress of
private review.  Private review normally terminates when all reviewers have
marked all source nodes as reviewed. In the unlikely event that no issues
have been raised, the review would terminate at this point.  If any issues
have been raised, public review begins.

\paragraph{Public review.} In this phase, all nodes are made public, and all review
participants (including the producer) react to the issues and actions by
voting and creating new nodes.  Participants can create new issue, action
or comment nodes based upon existing nodes.  Voting is used to determine
the degree of agreement within the group about the validity and
implications of issues and actions.  This phase normally concludes when all
nodes have been read by all reviewers, and when voting has stabilized on
all issues.

\paragraph{Consolidation.} In this phase, the moderator analyzes the results of
public and private review, and produces a condensed written report of the
review thus far.  In our experience, these consolidated reports are
more comprehensive, detailed, and accurate than typical review reports
from traditional review methods, such as those presented in (Pressman,
1992; Freedman and Weinberg, 1990; Humphrey, 1989).  Rather than simply a
checklist of characteristics with brief comments about the general quality
of the source, consolidation reports contain a re-organized and condensed
presentation of the analyses provided by reviewers in issue, action, and
comment nodes, thus providing contrasting opinions, the degree of consensus,
and proposals for changes.

CSRS provides the moderator with various tools to support the generation of
a nicely formatted LaTeX document containing
the consolidated report.  If the group reached consensus about all of the
issues and actions during public review, then this report presents the final
outcome of review with respect to artifact assessment.  A second review
outcome is the measurements of review process and products, as discussed
in Section \ref{sec:instrumentation} below.
 
\paragraph {Group review meeting.} If the consolidated report identifies issues or
actions that were not successfully resolved via public and private review,
a group meeting is the next step.  In the meeting, the moderator presents
the unresolved issues or actions and summarizes the differences of opinion.
After discussion, the group may vote to decide them, or the moderator may
unilaterally make the decision. The moderator then updates the CSRS
database, noting the decisions reached during the group meeting and then
generating a final hardcopy document representing the product of review.
In our review experiments thus far, we have not used CSRS for the group
review meeting, though CSRS support for this phase is underway.

\paragraph{External development.}  During this phase the software is
enhanced or corrected in response to the issues raised during review or due
to other development processes. 

\subsubsection{Instrumentation}
\label{sec:instrumentation}

Since the initial design of CSRS in 1991, research on effective
instrumentation for software review has been a primary research focus.
Instrumentation is a major feature distinguishing CSRS from other automated
review environments such as ICICLE \cite{Brothers90,Sembugamoorthy90},
Scrutiny \cite{Gintell93}, or INSPEQ \cite{Knight91}.  A detailed
description of the evolution of our instrumentation support
appears in \cite{csdl-93-07}.  CSRS supports both {\em outcome-oriented}
and {\em process-oriented} instrumentation.

\paragraph{Outcome-oriented instrumentation.} Such instrumentation allows review analysts
to query the hypertext database during or after review for information such
as the number of nodes generated of a given type, the set of nodes
containing a particular value in a particular field, or the set of nodes
partaking in a specific relationship to other nodes.

Outcome-oriented measures are very useful: at a minimum, the number and
severity of identified defects provides a first-order estimate of the
quality of the software under review.  Outcome-oriented measures can also
reveal important characteristics of the review team and review process.
For example, marginally productive members of the review team might be
identified (after a sufficient number of review instances) as those who
contribute little, or who contribute non-productively (by simply affirming
comments made by others), or who use review for political purposes.  Such a
method is less expensive and more objective than the strategy put forth in
\cite{Freedman90}, where behavioral data (such as ``Shows solidarity'',
``Shows Tension'', and so forth) is collected by a passive observer of the
review in order to provide feedback on the quality of reviewer
participation.

Outcome-oriented measures can also contribute to empirically-guided process
improvement.  For example, an organization may be able to fine-tune certain
characteristics of review, such as the number of participants or the number
of lines of code under review, by collecting data across a range of review
instances and correlating these factors to, for example, the number of
productive issues raised.

\paragraph{Process-oriented instrumentation.} This instrumentation allows review
analysts insight into the {\em sequence}\/ and {\em duration}\/ of
activities occurring during review.  The analysis and application of this
process instrumentation forms an exciting and intensive focus of our
current research.

Process instrumentation is implemented by a general purpose ``timestamp''
mechanism in EGRET (the collaborative environment underlying CSRS, which is
discussed in Section \ref{sec:infrastructure}).  This facility allows
EGRET-based applications such as CSRS to insert timestamp calls at
strategic points within their code to record the occurrence of
application-specific events of interest. For example, CSRS timestamps when
the user reads a node, writes a node, closes a node, traverses a link,
marks a node as reviewed, and so forth.  EGRET provides mechanisms to
fast-cache timestamps at the local client during a review session, and then
write the cache out to the server database at disconnect time.

One use of timestamp data is to calculate elapsed-time information.  For
example, subtracting the timestamp recorded when a user reads a particular
source node from the timestamp recorded when the user closes that node
yields the elapsed time during which the source node was displayed to the
user.

However, this elapsed display time is not a useful measure of the {\em
effort} spent reviewing a node, as we found out after experimenting with
our initial implementation.  Under a multi-window, multi-tasking
workstation environment, usage of CSRS is frequently interrupted by such
events as: phone calls; impromptu meetings with colleagues; reading and
answering e-mail; and so forth.

In other words, the elapsed display time has two components: the elapsed
time the user is actively working in CSRS while the node is displayed, and
the elapsed time that the node is displayed but the user's focus of
attention is elsewhere.

Our process instrumentation currently detects this latter {\em idle time}\/
via a bottom-up, timer-initiated timestamp that runs in parallel with the
top-down, user-initiated timestamp.  Once per minute, a timer wakes up,
determines whether or not there has been any recent low-level mouse or
keystroke activity within the CSRS application, and if so generates a
``busy'' timestamp. In our experiences thus far, this combination of
top-down and bottom-up timestamping provides high quality measures of
effort with a precision of plus or minus one minute.

Process instrumentation can provide substantial new insight into review.
First, the elapsed-time effort measures can augment the outcome-oriented
measures described above to measure the effort expended on review of
individual function objects and the effort expended upon review as a whole.
Figure \ref{fig:elapsed-time} illustrates some outcome-oriented measures
augmented with elapsed-time effort measures.

Precise and accurate measurement of the amount of effort expended upon
review (including the preparation phase) is notoriously difficult to obtain
in typical industrial settings \cite{Freedman90}.  Additionally, no other
manual or automated review method provides effort data at the grain size of
individual functions.  The availability of this fine-grained data makes
possible interesting new forms of experimentation, as discussed below.

\subsubsection{Infrastructure}
\label{sec:infrastructure}

CSRS is built on top of Egret, an environment for exploratory collaborative
group work. Egret provides a multi-user, hypertext environment for Unix and
X windows.  Egret has a client-server architecture, with a 13 KLOC C++
database back-end server, which is a completely re-written version of the
Hyperbase database server \cite{Wiil90}. The client side is a 15 KLOC Lisp
program using Lucid Emacs.  Finally, CSRS itself consists of a 9  KLOC
Lisp program written using Egret and Lucid Emacs services.

Egret has several noteworthy architectural features, and those, along with
other research experiences appear in
\cite{csdl-91-03,csdl-92-01,csdl-92-08,csdl-93-09}.  A detailed, current
design-level description of Egret appears in \cite{csdl-91-02}.  Egret also
provides infrastructure for two other systems under development in our
laboratory: a collaborative learning environment called CLARE
\cite{csdl-93-14}, and a collaborative interface to Usenet called URN
\cite{csdl-93-06}.

The Egret system was designed to determine the requirements for evolution
in the representational structure as well as contents of collaborative
systems for software development, and was developed with funding from the
principal investigator's NSF Research Initiation Award: ``Support for
Structural Evolution in Software Development'', Number CCR-9110861,
\$54,810, with original project duration of September 1, 1991-August 31,
1993.  A six month, no-cost extension to the end date of this award has
been approved.

\subsubsection{Current results}
\label{sec:results}

Our results fall into two categories: design results, involving the
impact of previous research on software review and collaborative work on
the current design of CSRS, and empirical results, involving the
measurements produced thus far by our use of CSRS.

\paragraph{Design results.} 

One realization from our research with CSRS is that introducing substantial
computer support into the fundamentally manual process of traditional
review creates a fundamentally different process.  As a result, we take
issue with research claiming to ``automate'' Fagan code inspection, as in
\cite{Brothers90}.  While ICICLE might provide a very effective form of
computer-supported review, the introduction of substantial computer support
creates a method fundamentally {\em different}\/ from Fagan's code
inspection\foot{In private conversation with developers of computer
supported review systems, we have learned that, in some cases,
industry-sponsored developers feel compelled to claim that their system
implements Fagan's code inspection.  This is because their corporate
software development standards require Fagan code inspection, and their
development would not be funded nor would the resulting system be used if
management believed otherwise.}.

The design of CSRS, therefore, has not been motivated by the goal of
translating a ``best practice'' from manual methods into a
computer-supported environment. Instead, our design has been influenced by
findings from computer-supported cooperative work technology, such as those
presented in \cite{Nunamaker91}, in conjunction with findings on current
review practice, such as those presented in \cite{Deimel91,Freedman90}.
Our design attempts to implement an environment and method that exploits
the known strengths of computer-supported cooperative work environments.
To illustrate, the next paragraphs discuss how the design of CSRS addresses
each of the pitfalls in manual review presented earlier in Section
\ref{sec:issues-manual-review}.

\begin{itemizenoindent}
  
\item {\em Insufficient preparation.} CSRS is designed to virtually
  eliminate the problem of detecting insufficient preparation, since the
  preparation phase in traditional review corresponds to private review in
  CSRS, and since the activities of each reviewer during private review is
  precisely instrumented and known to the moderator.  Of course, CSRS
  cannot force an intransigent participant to bring up the system and look
  at the materials, but it does eliminate the important problem of
  reviewers ``exaggerating'' their degree of preparedness to the review
  leader.
  
\item {\em Moderator domination.} CSRS is designed to prevent the
  moderator from most forms of domination that is possible in manual review.
  During private review, of course, all moderator influence is eliminated.
  During public review, the moderator can potentially influence outcome by
  ``flaming'', but cannot physically prevent contributions of other members
  by monopolizing air time as is possible in synchronous review.  Even the
  group meeting phase of CSRS is less vulnerable to moderator domination,
  since the set of issues has already been established and documented.
  
\item {\em Incorrect review rate.} Since most review in CSRS occurs
  asynchronously, the cost of review is much less sensitive to its rate.
  The desire of one participant to review slowly makes no impact on the
  rate (or cost) of review for any other participant.  An asynchronous
  style of interaction also eliminates ``air-time fragmentation''---the
  costly idle time spent by participants in manual face-to-face reviews
  while waiting for a turn to speak.
  
\item {\em Ego-involvement and personality conflict.} On-line and
  asynchronous review allows reviewers time to consider their choice of
  words carefully.  During private review, in fact, it is possible to log
  into CSRS the ``day after'' and modify comments that appear
  inappropriate, after a night's reflection, before any other reviewers
  see them.  In general, diplomacy is much easier in a non-real time
  environment.
  
\item {\em Issue resolution and meeting digression.} As noted previously,
  the constraints of a synchronous meeting context require participants to
  raise issues, but not to resolve them.  In CSRS, the problems of cost and
  digression arising from issue resolution are vastly minimized or
  eliminated.  
  
  More importantly, analysis of node content data from our review
  experiences reveals that issues are frequently difficult to articulate
  {\em except in the context of proposing an action to resolve it}, and
  that a natural tendency of most reviewers, immediately upon the
  identification of an issue, is to suggest a solution.  As a result, the
  current process model for CSRS explicitly {\em encourages} the
  interleaving of action proposal and discussion with issue proposal and
  dicussion during review.  Our experiences suggest that capturing action
  proposals when the reviewer first thinks of them, rather than
  artificially delaying their discussion until some future meeting time, is
  certainly a more natural and possibly a more efficient method of review.
  
\item {\em Recording difficulties and clerical overhead.} CSRS eliminates
  the role of scribe altogether, except for the group meeting phase where
  the role is considerably simplified.  CSRS also trivially resolves the
  problems of capturing review commentary completely and accurately.  CSRS
  provides the moderator with tools to support restructuring and
  reformatting of the data from its on-line, hypertext format into a form
  suitable for a linear, hardcopy presentation.  Such features
  substantially reduce the clerical overhead normally associated with
  review.
 
  Most important from a research perspective, CSRS collects data and
  process measures automatically and unobtrusively.  This means that
  research on review can be performed in any size organization with any
  amount of resources allocated to review.  The on-line CSRS Metric Database
  discussed below is one research activity made possible by automated and
  unobtrusive metric collection.

\end{itemizenoindent}

Taken as a whole, the CSRS method implements an incrementally increasing
level of both collaborative involvement and review cost.  During private
review, collaboration is actively prevented, and the cost of review to each
participant is restricted to the expense of their personal review efforts.
During public review, partial collaboration is supported by allowing each
reviewer on-line access to the comments made by others, which incurs some
additional cost.  Finally, the review group meeting provides the highest
cost collaboration, but is restricted in scope to only those issues that
cannot be resolved by the previous, lower-cost forms of collaboration.

\paragraph {Empirical results.}

The second category of results involves the empirical data derived from the
use of CSRS in our organization.  

Figure \ref{fig:review-summary-data} provides a general overview of our
review experiences performed with CSRS over the past year. Several things
should be noted about these review experiences. First, each of the early
reviews were followed by extensive redesign and enhancement of CSRS and its
instrumentation facilities, though the basic architecture and method now
appears to be stabilizing.  Second, the primary goals of these initial
experiences were to validate the essential characteristics of the system
and to support our own quality assurance activities.  The data was not
collected, nor is it presented here in an effort to make any
empirically-founded claims about the general nature of FTR.  However, the
data presented here {\em is}\/ intended to make a claim about the power of
CSRS for inquiry into the nature of formal technical review using this
system. We provide three examples below.

{\sl Complexity/effort relationships.} First, we are currently
investigating whether or not various measures of code complexity can serve
as a predictive measure of review effort.  Intuitively, it seems reasonable
that the more complicated a program entity, the more difficult it is to
understand and review, and the greater the number of issues and errors it
will contain.  We have hypothesized that these factors may lead to a
positive correlation between the complexity of a program entity and the
review effort required.  A very similar question is posed as an important
area for future research in \cite{Fagan86}.

A sufficiently precise correlation would allow us to predict the resources
required for review activities based upon the complexity computed for the
code, or conversely, to fit the set of artifacts to the resources available
for review.  CSRS is uniquely able to support such experimentation, since
no other review method is able to measure effort at the grain size required
to assess complexity-related relationships.  Such a predictive measure
would be very significant, since FTR activities can account for 15-20\% of
all development effort.

We are currently investigating three popular measures: Halstead's
volumetric complexity \cite{Halstead77}, McCabe's cyclometric complexity
\cite{McCabe76}, and simple lines of code. Currently, our analysis has been
restricted to the private review phase data from the Nbuff-2 review
experience, since later reviews are still in progress, and prior reviews
did not have idle time correction built into the instrumentation.

Figure \ref{fig:review-complexity-correlation} presents a scatter plot that
shows the relationship between the lines of code for each of the 36
functions reviewed in Nbuff-2 and the average private review effort
required (in seconds). The first result is that there is {\em no}
significant correlation ($r^{2} \approx 0.27$) between average effort and
complexity (in terms of LOC) for this data set, although with some
imagination, a general trend toward a positive correlation might be seen.

A second result is that lines of code correlates as well as Halstead's
program volume, and substantially better than McCabe's cyclometric
complexity, with average review effort. 

We did not expect to see ``instant significance'' from statistical analysis
of a single review experience.  What these initial experiences provide is
a set of more refined hypotheses to be investigated through analysis of 
forthcoming data from future review experiences:

\begin{itemizenoindent}
\item {\em Effort correlated with complexity, but more data is
  needed.} One reasonable hypothesis is that more data is needed to expose
  the relationship between complexity and effort.  
  
  If so, then an interesting question to pursue is the nature of this
  relationship: will a simply linear regression model suffice?  We
  hypothesize that the relationship may in fact be a bell-shaped curve,
  indicating that once the complexity of a function is sufficiently high,
  review effort actually drops off.
  
\item {\em Complexity is not strongly correlated with effort, but rather
  with other factors such as reviewer experience (or some combination of
  the two).} An alternative hypothesis, also supported by the timeplot
  data, is that other factors may be more important in predicting review
  effort than complexity.  For example, the code of inexperienced
  developers under review using CSRS leads to a wide spectrum of issue
  types, ranging from comments about the appropriate use of the programming
  language, to the application platform, to design aesthetics, and so
  forth.  The code of experienced developers has a much narrower range of
  comments.  We have observed that very simple functions written by
  inexperienced developers may engender a great deal of review activity
  involving these types of issues.

  If so, then we might expect to see that complexity will continue to be
  uncorrelated with review effort, at least when inexperienced developer's
  code is under review, since almost any function can trigger one of this
  broad spectrum of issues. 
\end{itemizenoindent}

{\sl Timeplot visualization.} Second, we are developing tools to help
visualize the process of FTR by creating more abstract representations of
the timestamp data.  As Figure \ref{fig:review-summary-data} reveals,
thousands of timestamps are generated during review, far too many for
non-automated analysis. To reconstruct the sequence of high-level
activities that occur during review, we have implemented a timeplot system
that produces a simple graphical representation of each review session.
Figure \ref{fig:timeplot} shows a timeplot for one of the review sessions
during the Nbuff-2 review.

The ultimate goal of the timeplot system is to help us formulate hypotheses
about the varieties of review behavior possible in CSRS and their impact
upon review outcome; to help us improve the CSRS system by identifying
bottlenecks and opportunities for additional support; and to eventually
lead to improved understanding of what makes a system easily reviewed.

{\sl Commentary classification.}
Third, to provide some perspective of the range of artifacts captured by
CSRS, we categorized the types of issues and commentatary raised during the
URN review, and found that while approximately 80\% raised traditional
review issues, the remaining 20\% provided either: (a) significant new
design rationale information; (b) new clarification of the specifications
or behavior of the application under development; or (c) new clarification
of the specifications or behavior of EGRET or some other underlying
infrastructure (such as the source language).  The presence and capturing
of these latter artifacts (which would not typically be recorded in a
conventional review---indeed, they would be viewed as a ``digression") have
helped us to improve the specifications and documentation of EGRET and
other systems, and provided significant aid to developers.

A final ``result'' worth mentioning is the subjective impression of our
organization about FTR using CSRS.  Put simply, all of the developers in
our (admittedly small) user community express extremely high satisfaction
with CSRS, viewing it as a highly effective and important part of their
current and future development activities.


