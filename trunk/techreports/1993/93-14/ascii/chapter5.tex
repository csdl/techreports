%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter5.tex -- 
%% RCS:            : $Id: chapter5.tex,v 1.17 94/04/07 21:38:38 dxw Exp $
%% Author          : Dadong Wan
%% Created On      : Sun Jul 11 00:56:07 1993
%% Last Modified By: Dadong Wan
%% Last Modified On: Thu Apr  7 21:38:04 1994
%% Status          : Unknown
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1993 University of Hawaii
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% History
%% 11-Jul-1993		Dadong Wan	
%%    created.
%%% \documentstyle [12pt,/group/csdl/tex/definemargins,
%%% /group/csdl/tex/lmacros]{report}
%%% \input{/home/3/dxw/c/tex/psfig}
%%% \special{header=/group/csdl/tex/psfig/lprep71.pro}
%%% 
%%% \begin{document}

\setcounter{chapter}{4}
\chapter{Hypotheses and Experiments}

This chapter describes the CLARE evaluation procedure. Specifically, it
elaborates ten empirical hypotheses, and a number of experiments designed
to explore these hypotheses. Section \ref{sec:problem-revisit} revisits the
research problem. Section \ref{sec:hypothesis} describes each of the ten
guiding hypotheses.  Section \ref{sec:data-collection} identifies and
relates three types of empirical data to be collected for each hypothesis:
{\it outcome\/}, {\it process\/}, and {\it assessment\/}. It also describes
the procedures for gathering this data, and methods for analyzing them. The
chapter concludes by discussing the actual experiments to be conducted,
including the task, subject, procedure, and the execution plan.


\section{The research problem revisited}
\label{sec:problem-revisit}

As described in Chapter 1, CLARE addresses the problem of representation,
or more specifically, the lack of representational support, in existing
collaborative learning systems. It explores issues related to the use of
the higher-order knowledge embedded in both scientific text as a basis of
differentiating, deliberating, and integrating different points of view and
interpretations from a group of learners, and of facilitating interactions
among them. There are three main suppositions underlying this approach.
First, scientific text, such as research papers, exhibits structural
patterns that embody certain dominant norms and conventions governing the
current practice in knowledge construction and presentation in a given
scientific community. Furthermore, such patterns can be characterized in
terms of a small number of primitives.  Selected aggregates of those
primitives form a core set of structural {\it ideal types\/} of selected
exemplary artifacts in that domain. And finally, the combination of those
primitives and aggregates provides a useful framework for learners to
collaboratively learn about the domain through the artifact-centered
exploration and discussion.

For example, one type of research artifacts in software engineering
research, called {\it experience paper,\/} typically contains the
description of a problem situation encountered by an organization, the
initial rationale for adopting a particular piece of software technology,
and resulted experiential evidence on whether or not that technology turns
out to be effective in that particular organizational context. In CLARE,
the thematic structure of such a paper can be expressed as follows:

\begin{quotation}
  \[ \left\{
\begin{array}{l}
  {\sf problem} \stackrel{ responds-to}{\longleftarrow} {\sf
   claim_{1}} \\

   {\sf  claim_{2}} \stackrel{presupposes}{\longleftarrow} {\sf
  claim_{1}} \\
  {\sf claim_{1} } \stackrel{supports or counters}{\longleftarrow} {\sf evidence} \\
\end{array} 
   \right \} \]
\end{quotation}

An example instantiation of {\sf problem}, {\sf
 claim\(_{1} \) }, {\sf  claim\(_{2} \)}, and {\sf
evidence\/} is as below:

\begin{itemize}
\item \fbox{{\sf Problem\/}}: ``Our organization has failed to produce
  high-quality software despite the increased staff and the
  state-of-the-art environment.''
  
\item \fbox{{\sf  Claim\(_{1}\)\/}}: ``Cleanroom engineering is the
  development methodology we should adopt to improve the software quality
  in our organization.''
  
\item \fbox{{\sf Claim\(_{2} \)\/}}: ``Cleanroom engineering helps produce
  zero defect software using statistical quality control.'' 
  
\item \fbox{{\sf Evidence\/}}: ``The average number of errors per KLOC
  has decreased substantially but it took us twice as long to complete a
  system of the same-size.''
\end{itemize}

A student in software engineering may use the above structural model as an
example {\it template\/} to guide his interpretation and evaluation of
other experience papers in this domain, and his efforts in constructing new
experience papers so that they may also conform the same structure. In a
group setting, such structural knowledge can be used as a shared framework
among learners to engage in discussions about the content of related
artifacts.

On the other hand, written artifacts such as published journal articles and
conference papers are likely to deviate from the canonical structural
protocols. For instance, a concept paper might fail to provide evidence to
substantiate the viability of the proposed approach, as suggested by the
standard structure.  Furthermore, both structural and content-level
ambiguities are commonly found in the research literature, some of which
are attributable to the lack of specificity on the part of the author,
while others are caused by the gap in the domain knowledge between the
author and the reader. Such inconsistency and a lack of precision in the
artifact itself often lead to different interpretations and controversies
among its readers regarding what the artifact, or a given segment of it,
really means.

The CLARE approach to collaborative learning is based on the structural
characterization of the scientific text described above. It asserts that
learning is knowledge construction through collaborative and critical
analyses of scientific text, and through subsequent deliberation and
integration of different interpretations and points of view among a group
of learners. At the core of CLARE is a thematic representation called
RESRA, which consists of a small number of primitives, and a set of
aggregates called CRFs (Refer to Chapter 3 for detailed description of
RESRA, and Chapter 4 for CLARE). It embodies a two-phase model of
collaborative learning, and a set of services designed to facilitate the
use of the RESRA to represent the content of selected artifacts, to
deliberate the reasoning behind different points of view, and to integrate
similar views into a coherent whole. See Figure \ref{fig:kr-role} for an
overview of the CLARE process model, and Figure \ref{fig:roadmap} for a
detailed function map of CLARE.


\section{Hypotheses}
\label{sec:hypothesis}

The ten research hypotheses to be described below are divided into three
groups: RESRA, SECAI, and CLARE. Despite their seemingly variety, most of
these hypotheses are closely related, and may thus be tested using the same
data sets. Table \ref{tab:hypothesis-summary} summarizes those hypotheses
and the dependencies among them.

\subsection{RESRA}
\label{sec:resra hypothesis}


\subsubsection{H\(_1\)}
\label{sec:RESRA1}

{\it RESRA is an effective mapping tool for characterizing essential
thematic components and relationships in scientific text.} The word {\it
essential\/} is used here to mean the skeleton of ideas as intended by the
author that form the backbone of an artifact. Certain components of the
artifact may seem essential to the reader/learner but not necessarily so to
the author, for example, those that coincide with the learner's interests
or points of view. Such semantic gaps between the producer and the reader
of a written artifact often exist and are even deemed desirable in the
context of collaborative learning, for they constitute a source of
potential controversy which, with proper facilitation, can lead to a better
understanding of the underlying problem and discovery of new knowledge.


\subsubsection{H\(_2\)}
\label{sec:RESRA2}

{\it RESRA is a useful organizational tool that enables learners to
incrementally integrate their fine-grained interpretations of an artifact
with those of their peers.} The RESRA-based integration takes places at two
levels, namely, integration of different interpretations by individual
learners on the same artifact, and the integration of the consolidated
group views on different artifacts. The focus of this hypothesis is on the
former. It is important to note that integration is not a simple
concatenation of two sets of related RESRA instances. Instead, it requires
careful characterization of the relationships among those instances, and
grouping through aggregations, such as perspectives.


\subsubsection{H\(_3\)}
\label{sec:RESRA3}

{\it RESRA provides a useful shared {\it frame of reference\/} for
collaborative construction of knowledge from scientific text.} RESRA, in
essence, is the {\it medium\/} of learning and interactions under
CLARE. The content of an artifact, or individual interpretations of it, is
summarized and evaluated in terms of RESRA instances. Furthermore, RESRA
also provides the basic grain size for comparing different viewpoints, a
framework for deliberating rationale behind those views and, ultimately,
for consolidating them into a coherent corpse of knowledge.  Therefore, the
collaborative utility of RESRA occupies the central stage in CLARE. This
hypothesis presupposes {\sf H\(_1\)}, {\sf H\(_2\)}, and {\sf H\(_6\)}.


\subsubsection{H\(_4\)}
\label{sec:RESRA4}

{\it The Canonical RESRA Form, or CRF, provides a viable means of
characterizing the thematic structure of exemplary scientific text.} A
corollary of this hypothesis is that RESRA can serve as a useful learning
tool for the newcomers of a domain about the norms and conventions (a.k.a.,
Kuhnian paradigms) that govern the formal communication of knowledge via
written artifacts.


\subsubsection{H\(_5\)}
\label{sec:RESRA5}

{\it The Canonical RESRA Form, or CRF, provides a useful heuristic basis
for exposing ambiguities and gaps in the existing work.} This hypothesis is
a natural extension of the mapping capability of RESRA (i.e., {\sf
H\(_1\)}); the suggestive power of the CRF depends heavily on the quality
of predefined CRF instances and the match between those instances and the
content structure of the current artifact.  Even though the CRF-based
suggestions are not always {\it right,\/} they nevertheless might be useful,
for any discrepancy between the two suggests that either the existing CRF
instance needs to be amended, or a new CRF instance should be added.


\subsubsection{H\(_6\)}
\label{sec:RESRA6}

{\it The use of CRF consultation functions can lead to a more consensual
view on an artifact among a group of learners.} In a sense, this hypothesis
is an instantiation of {\sf H\(_3\)\/}; more specifically, the CRF might be
used to guide consensus-building among a group of learners.


\subsection{SECAI}
\label{sec:SECAI hypothesis}

\subsubsection{H\(_7\)}
\label{sec:SM1}

{\it The dichotomy of exploration and consolidation in the SECAI model
represents a viable approach to collaborative learning under the CLARE
framework.} This hypothesis presupposes the primacy of adequate individual
preparation, which is carried out in the exploratory phase, to productive
collaboration, which is accomplished in the consolidation phase.


\subsubsection{H\(_8\)}
\label{sec:SM2}

{\it During an extensive period of using CLARE, learners exhibit a
noticeable migration from summarative activities to evaluative and
argumentative activities.} Such a migration may be explained by two
factors: first, most learners at the beginning may lack sufficient domain
knowledge.  Hence, their focus is mostly on internalizing the content of
the artifact rather than evaluating it. Second, since a typical group
consists of a mixture of learners, some of whom are primarily
summarization-oriented, while others are more critical-minded.  Over time,
the former may learn to lean toward the latter.


\subsection{CLARE}
\label{sec:clare hypothesis}

\subsubsection{H\(_9\)}
\label{sec:CLARE1}

{\it The ability to make fine-grained comparison between different points
of view on the same artifact is essential to collaborative deliberation
and, ultimately, the integration of those views; CLARE provides such a
capability.} The comparative view highlights key differences and
similarities of individual learners' representations at both the primitive
and the artifact levels, i.e., the CRF.


\subsubsection{H\(_{10}\)}
\label{sec:CLARE2}

{\it CLARE provides a viable platform for supporting artifact-based
collaborative learning.} This hypothesis forms the empirical basis behind
all exploratory activities using CLARE. Its formulation is non-comparative.
Hence, it does not require any direct comparison between CLARE and
alternative approaches.

\ls{1.0}
\small
\begin{table}[hbt]
    \caption{A synopsis of the ten hypotheses on CLARE}
    \begin{center}
    \begin{tabular} {||l|p{3.3in}|p{1.0in}||} \hline   
      {\bf Hypothesis} & {\bf Description} & {\bf Dependencies} \\ \hline
      
      H\(_1\) & RESRA is effective for representing essential
      content of scientific text. & None \\ \hline
      
      H\(_2\) & RESRA is effective for incremental, fine-grained
      integration of learners' views on an artifact. &
      H\(_1\)\\ \hline
      
      H\(_3\) & RESRA provides a viable framework for
      collaborative construction of knowledge. & H\(_1\);
      H\(_2\); H\(_6\) \\ \hline
      
      H\(_4\) & CRF provides a viable means of characterizing the
      thematic structure of scientific text. & H\(_1\) \\ \hline
      
      H\(_5\) & CRF is useful in exposing ambiguities and gaps in
      existing artifacts. & H\(_1\) \\ \hline
      
      H\(_6\) & CRF facilitates the generation of consensual views on
      scientific text. & None \\ \hline
      
      H\(_7\) & The dichotomy of exploratory and consolidation phases
      facilitates collaborative learning. & None \\ \hline
      
      H\(_8\) & Over time learners exhibit a migration from
      summarative to evaluative activities. & None \\ \hline
      
      H\(_9\) & Fine-grained comparison leads to effective
      deliberation and integration. & None \\ \hline
      
      H\(_{10}\) & CLARE is a viable platform to support collaborative
      learning. & H\(_6\); H\(_3\); H\(_7\); H\(_8\); H\(_9\)
      \\ \hline
    \end{tabular}
    \end{center}
    \label{tab:hypothesis-summary}
\end{table}
\normalsize
\ls{1.6}


\section{Data collection}
\label{sec:data-collection}

To evaluate CLARE and test the hypotheses described above requires
three types of data: {\it outcome\/}, {\it process\/}, and {\it
assessment}, each of which is described in turn in the
following sections. The specific data for each of the ten hypotheses
is also identified. Table \ref{tab:data} provides a synopsis of
such data.

\subsection{Outcome data}
\label{sec:outcome data}

The {\it outcome} data measures the quantity and quality of what individual
learners and groups have done in a CLARE session.  For example, {\sf
H\(_8\)} requires a comparison of the number and the content of summarative
and evaluative instances generated by the same group in different CLARE
sessions. While the quantitative outcome, such as the number of evaluative
nodes, is precisely measurable and automatically accumulated by CLARE, the
quality is not. The latter has to be assessed subjectively by the
researcher in cooperation with the course instructor.

To test the ten hypotheses described in the previous session
require several types of outcome measures, four of which are
identified here: {\it collaboration}, {\it criticality}, {\it
consensus}, {\it CRF conformance}. Two important characteristics
shared by all these measures are:

\begin{itemize}
\item They are raw and descriptive. 
  
\item They have both quantitative and qualitative aspects.
\end{itemize}

For example, {\it collaboration\/} measures include the number of nodes
created, the total size of these nodes, and the total amount of time spent
by the learners during consolidation. To gain a more accurate picture about
the group interaction, however, it also is necessary to examine the content
of actual database to find out what is discussed and how the discussion is
progressed.


\subsubsection{Collaboration measures}

The collaboration measure (COM) shows the degree of interactions among a
group of learners. Quantitatively, it includes three raw metrics: the
number of nodes created, the total size of those nodes, and the total
amount of effective time expended\footnote{CLARE automatically records the
number of {\it busy minutes\/} a learner spends in the system. It creates a
timestamp when the learner (1) invokes any nontrivial user-level CLARE
commands; or (2) in the case the user has not invoked any CLARE commands
within the last minute, a busy timestamp is generated if and only if the
user has generated any keyboard event (e.g., moving the mouse) within the
last predefined interval, which defaults to 2.5 minutes.  In other words,
at a given point if a learner has not done anything within the last 2.5
minutes, he is considered {\it idle\/}, and thus does not get credit for
the elapsed time. On the other hand, if in the next second or two, he
touches on the mouse or keyboard, a keyboard event is generated.  As a
result, CLARE considers him as {\it busy\/} and creates a busy timestamp
when next minute point is reached.} during consolidation. In addition, the
actual nodes and links created during this phase are examined to eliminate
certain spurious results, such as empty nodes, nonsense contents.

The COM represents an important measure, for the purpose of CLARE is to
facilitate collaboration among human learners.  Without a good measure of
collaboration, rigorous evaluation of the system will not be possible. The
COM is used in {\sf H\(_3\)\/}, {\sf H\(_7\)\/}, and {\sf H\(_9\)\/}, and
{\sf H\(_{10}\)\/}.


\subsubsection{Consensus measures}

The consensus measure (CSM) indicates the degree of consensus among a group
of learners in their interpretations and evaluations of the selected
artifact. The measure is derived in a two-step process.  First, the nodes
created by each learner are grouped by their types. If two nodes by two
different users are derived from the same semantic unit\footnote{Semantic units
(SUs) are the basic grain-size of reference in CLARE. They correspond to
sections, subsections, paragraphs, or even arbitrary regions in the
original artifact selected by the learner.  Currently, SUs are the same as
nodes, which typically correspond to sections in an artifact.}, the number
of overlapping nodes represent a preliminary indicator of the level of
consensus among the learners. Second, the actual content of all nodes are
examined to make sure that they represent more or less the same
information.

The importance of the CSM is twofold: first, since one primary goal of
CLARE is to facilitate the generation of consensual views on an artifact
among a group of learners, the CSM provides a measure of the degree to
which that goal is achieved.  Second, the CSM can serve as a indicator of
the group heterogeneity with respect to a given problem or artifact, and
thus be used as an independent variable to explain certain outcomes.  The
CSM is used in {\sf H\(_3\)\/}, {\sf H\(_6\)\/}, and {\sf H\(_9\)\/}, and
{\sf H\(_{10}\)\/}.


\subsubsection{Criticality measures}

The criticality measure (CRM) shows the degree of criticalness a learner
demonstrates in his view or interpretation of an artifact.  A learner who
points out two vital flaws in a particular method, for example, should have
a higher CRM value than a learner who merely summarizes the author's
original claims.  A simple indicator of CRM is the count of the number of
evaluative nodes, including \fbox{{\sf critique}}, \fbox{{\sf question}},
\fbox{{\sf suggestion}}. Like the previous measures, the actual node
content needs to be examined to avoid spurious interpretations.

The significance of CRM is twofold: as a measure of group composition and a
measure of the change in criticalness over time at both individual and
group levels. The latter is used in {\sf H\(_8\)\/}. The former, however,
can be used for assigning experimental groups, for a group with potentials
for collaboration probably needs to have a right combination of critical
individuals and not-so-critical ones.


\subsubsection{CRF conformance measures}

The CRF conformance measure (CCM) reveals the degree of congruence between
a learner's representation of the content of an artifact and the suggested
structural template by CLARE. A simple form of CCM is the count of the
missing tuples returned by CLARE's CRF function. The value, however, needs
to be verified with the actual node content to eliminate spurious tuples.
CCM is used in {\sf H\(_4\)} and {\sf H\(_{10}\)}.


\subsection{Process data}
\label{sec:process data}

The {\it process\/} data provides a detailed depiction of the path which
leads to a particular outcome. It includes such information as the type of
operations invoked, the sequence in which those operations are invoked, the
instance on which the operation is operated, and the amount of time the
learner spends on performing each of the operations. The exact types of
process data for individual hypotheses vary widely. In {\sf H\(_9\)\/}, for
example, appropriate process-level measures might include the frequency at
which the comparison mode is entered, the average amount of time the
learner spends in that mode, the type and frequency of the comparative
operations invoked, and, perhaps more importantly, whether or not the visit
of the comparison mode is immediately followed by some visible
argumentative activities, such as the creation of new nodes. The latter, if
commonly seen, allows one to infer that CLARE's comparison functions are
useful in highlighting differences and triggering controversies among
learners. The process data supplements the outcome measures in that it
provides detailed information which helps explain why a particular outcome
has taken place.  Such information is especially important when the outcome
turns out to be different from what was expected.

Detailed process data is captured automatically in CLARE through its
built-in, unobtrusive instrumentation mechanism. It can be output into a
format recognizable by standard analysis packages. Process data
corresponding to each hypothesis is described in Section
\ref{sec:hypotheses data}.


\subsection{Assessment data}
\label{sec:assessment data}

The {\it assessment\/} data represents the learners' subjective assessment
of their learning outcomes and processes, and aspects of CLARE, such as the
utility of a given system feature, SECAI, RESRA.  Such data is gathered
through two questionnaires (see Appendix \ref{sec:assessment} and
\ref{sec:feedback} for details) that are filled out by learners at the end
of each CLARE session. Despite the subjective nature of the assessment
questions, they provide important feedback on the effectiveness of CLARE in
areas in which more objective measures are not available.


\subsection{Hypotheses and data collection}
\label{sec:hypotheses data}

\subsubsection{H\(_1\)}
\label{sec:resra1 data}

To gain insights about whether RESRA is a viable mapping tool, it is not
sufficient to merely look at one or more outcome indicators, i.e., CSM, or
learners' subjective assessments, i.e., Questions\(_{1,
15-16}\)\footnote{All numbered questions referred in this and following
sections are the questions from the questionnaire in the Appendix
A.}. Instead, it requires answering a wide range of process-level
questions. For example, how do learners use RESRA to summarize and evaluate
the content of a research paper using CLARE? Do they do summarization,
followed by evaluation, or do they engage in the two activities
simultaneously? During summarization, do learners create summarative nodes
first, and then try to connect them together by adding links of appropriate
type, or are they essentially {\it tuple-oriented,\/} i.e., creating one
tuple at a time? What exact types of nodes and tuples are created by
individual learners? Are they all different and partitioning, or are they
largely overlapped with one another? Are all important elements and
relationships in the artifact identified by individual learners, by a
group, or by a number of groups?  Are there mis-represented elements, for
example, treating a \fbox{{\sf problem\/}} as a \fbox{{\sf claim\/}}.  If
so, how are they distributed? How often are the open-ended, node and link
types are used? Answers to these and other related questions can be
obtained by analyzing the detailed tracking data captured by CLARE, and by
examining the content of the database generated by the group. It is
important to note that an effective mapping tool does not imply a uniform
pattern of using RESRA, nor an elimination of mis-represented nodes or
links.  In contrary, an effective RESRA should promote heterogeneity at the
exploratory phase, which is the exact focus of the current hypothesis.


\subsubsection{H\(_2\)}
\label{sec:resra2 data}

To determine whether RESRA is an effective organizational tool for
integrating divergent interpretations of an artifact requires answering two
sub-questions: does RESRA {\it promote\/} the variety of interpretations of
an artifact in some consistent fashion? And, does RESRA allow easy {\it
integration\/} of the resulted views?  The two questions are of equal
importance: without the former, the latter becomes unnecessary; without the
latter, group collaboration does not exist. In terms of outcomes, the
answers to both questions can be approximated through a good CSM indicator,
measured immediately after the exploration and consolidation, respectively.
The learners' answers to Question\(_{2,4}\) indicate their perceived
effectiveness of RESRA at the two levels.  However, the most important
source of insights probably comes from the analyses of the process data
gathered by CLARE, since it is likely to reveal a detailed picture of how
various views, if any, are derived (see the above paragraph), and how those
views are eventually integrated.  It will answer the key question, what
exactly happened during integration?  What integrative links were
identified first? What was added subsequently?  How did learners arrive at
a consensus on a given relationship between two nodes? Was integration
accompanied by argumentation, or vice versa?  Did {\it group
leaders\/}\footnote{A {\it group leader\/} is visible among the crowd
because he is the person who creates most significant nodes and/or links
and, more importantly, who has the strongest influence on other learners.},
emerge in the consolidation process? Did the learners attempt to construct
perspectives to {\it group\/} various points of view?


\subsubsection{H\(_3\)}
\label{sec:resra3 data}

The hypothesis that RESRA is a useful framework for collaborative learning
presumes that RESRA is an effective mapping and organizational tool (see
{\sf RESRA1\/} and {\sf RESRA2\/} in Section \ref{sec:hypothesis}). Hence,
the process, outcome, and assessment data described above is directly
applicable here. More importantly, the process data from argumentation,
i.e., the process of collaboratively deliberating the outcomes from
exploration, should provide insights to such questions as, how did
argumentation get started at the first place?  Was it triggered through
comparing instances of particular types, such as leading questions or
problems, or through navigating a series of related nodes? Where were
{\it centers of controversy\/}: the exact semantics of a RESRA primitive, a
position taken by a particular learner, or a problem identified in the
artifact? Was argumentation centered on summarative or evaluative
instances?  What proportion of discussions was devoted to RESRA itself,
e.g., what constitutes a {\it claim\/}? Or, should {\bf X} be a {\it
claim\/} instead of a {\it theory\/}?  How was the learner's participation
distributed within the group? Were all learners able to become involved
constructively regardless what they did during the exploration phase?


\subsubsection{H\(_4\)}
\label{sec:resra4 data}

Whether the CRF is a viable way of characterizing the thematic structure of
scientific text is in part determined by the completeness and the
representativeness of the set of predefined CRF instances. The more
complete and representative the set is, the stronger is the proof for the
genericity of the CRF, and the more examples the learner can rely on to
show them how to use the CRF. At the process level, the tracking data will
reveal the pattern in which the CRF is used by the learner. Specifically,
it will provide answers to questions such as, did learners start
summarization with a CRF, followed by efforts to {\it fill-in\/} each blank
spot by searching the content of the artifact? Or, did they create RESRA
tuples first and then {\it retrofit\/} those tuples to a given CRF template?
How do the learners deviate from the selected CRF in their summarization?
The viability of the CRF does not dictate the answers to above questions.


\subsubsection{H\(_5\)}
\label{sec:resra5 data}

The most important data source for determining whether the CRF helps
exposing thematic ambiguities and gaps in an artifact comes from the
Questions\(_{5,17}\), which represent the learner's assessment of the
heuristic value of the CRF. In addition, the process data can also provide
evidence about this hypothesis. For example, one might infer that the CRF
indeed helped the problem discovery process if a consistent pattern was
found between the invocation of the CRF functions and the subsequent
creation of critique and question nodes.


\subsubsection{H\(_6\)}
\label{sec:resra6 data}

The degree of consensus among learners with respect to their views on the
selected artifact is measurable by examining the content of summarative
nodes created. If, for example, all learners have created a similar set of
tuples, and corresponding nodes share the same content, the level of
consensus is high. Conversely, if the similarity is only found at either
the tuple level or the content level but not both, the consensus is low.
The level of consensus, however, may not provide any meaningful indication
of the heuristic value of the CRF if the CRF was not used by the learner.
Thus, the process-level data must be interpreted along with the outcome
measure. By doing so, it can answer such questions as, was the CRF actually
used by the learner during summarization and, if so, what was the common
pattern of usage? Was the reference to the CRF immediately followed by the
creation of new summarative nodes and/or links? Was there any correlation
between the usage of the CRF and the resulted consensus among the learners
who were CRF users?  In addition, the process data will also be useful in
validating the learner's answers to Question\(_{6}\). For instance, if an
learner's answer to Question\(_6\) is affirmative, but his process data
reveals that he has never used the CRF consultation function, the
assessment data for the current learner might be ignored.


\subsubsection{H\(_7\)}
\label{sec:SECAI1 data}

At first glance, it seems evident that preparation should always precede
collaboration, and that better preparation will result in high-quality
collaboration. Is it truly so in CLARE? To test this hypothesis requires a
detailed analysis of the resulted data and usage behavior at both the
individual and group levels. Were {\it good explorers,\/} measured in terms
of the process steps they have followed and the quality of their
summarization and evaluation, are also {\it good collaborators,\/} as
reflected by the frequency and quality of questions raised on other
learners' positions, explanations given on their own positions,
alternatives proposed in response to what is already suggested, and
relationships identified between views of different learners?  Or, is it
the case that there exists a {\it partition\/} of good explorers and good
collaborators in a group, and that the group which has the most balanced
mixture of the two tends to have the highest level of interactions, and
generated the highest quality representation of the artifact? The process
data will allow us to explore answers to questions such as the ones listed
above. The user assessments from Questions\(_{7-8}\) will provide a
different data point for the current hypothesis.


\subsubsection{H\(_8\)}
\label{sec:SECAI2 data}

During an extensive period of using CLARE, is there a noticeable shift of
focus from summarization to evaluation at individual and/or group levels?
If so, what are potential factors accounting for it? The answer to the
latter in part comes from Question\(_{9-10}\). The analysis of the types
and the content of node and link instances created by the learner, and the
process by which those instances are created will illuminate both
questions. In particular, it will help answer such questions as, is there a
difference in the pattern of using CLARE across experimental sessions? Is
there a difference in the outcomes of those experiments? If the answers to
both are positive, is there any correlation between the two?


\subsubsection{H\(_9\)}
\label{sec:clare1 data}

While the presence of divergent views on a given artifact is a necessary
condition for effective collaboration, the ability to discern the
differences and similarities between those views plays a no less important
role in triggering constructive controversies. In CLARE, this ability is
realized through its comparison/contrast mechanisms, which provide four
types of comparisons: {\it RESRA template\/}, {\it summarization\/}, {\it
evaluation\/}, and {\it leading question\/}. The process data will provide
clues to such questions as, how is this facility used by the learner? Is
there a consistent pattern of using this function, such as, learner A has
spent substantially more time than Learner B and C in comparing the answers
to leading questions? If yes, is the usage pattern in some way correlated
with the quantity, quality, and the content of the learner's exploration,
his participation in the subsequent consolidation, or his answer to
Question\(_{12}\)? How often is the invocation of the comparison function
followed immediately by a {\it burst\/} of new questions, criticisms, or
other visible argumentative activities? Which type of comparison is most
often used?


\subsubsection{H\(_{10}\)}
\label{sec:clare2 data}

In terms of data collection, the current hypothesis can be viewed at two
levels. First, it is an all-encompassing claim about CLARE. As such, the
process, assessment, and outcome data for all previous formulations are
directly applicable here. Second, since collaboration in CLARE takes place
mostly in the consolidation phase, this hypothesis requires evidence to
demonstrate the degree to which such activities have in fact taken place.
The process data from the consolidation phase contains answers to a wide
range of questions. For instance, how much and what types of interactions
took place among learners in terms of the RESRA instances they have
created?  What are the sequences of events leading to, for example, the
emergence of a new perspective or a consensus on the relationships between
two points of view?  How were argumentative and integrative actions
intertwined within each learner and across learners? How did the consensus,
if any, arrive?  Are learners' answers to Questions\(_{13-14, 18-21}\)
consistent with quantity and quality of the RESRA instances they have
created, and the process which led to the creation of those instances?

\ls{1.0}
\small
\begin{table}[hbt]
    \caption{A synopsis of data to be gathered}
    \begin{center}
    \begin{tabular} {||l|p{1.3in}|p{1.8in}|p{1.5in}||} \hline
      {\bf Hypothesis} & {\bf Outcome} & {\bf Process} & {\bf Assessment} \\ \hline
      
      H\(_1\) & CSM; frequency of RESRA instances of the type
      \fbox{{\sf other\/}}.  & Action chains leading to RESRA
      instance creation. & Questions\(_{1, 15-16}\) \\ \hline
      
      H\(_2\) & CSMs for pre- and post- integration. &
      Deliberation action chains leading to the integration of
      points of view. & Question\(_{2, 4}\) \\ \hline
      
      H\(_3\) & COM; Rating on the quality of online discussions.
      & Network of action sequences during interaction of various
      points of view. & Questions\(_{2-4}\) \\ \hline
      
      H\(_4\) & Quality of existing CRFs; CCM. & None. &
      Question\(_{5}\) \\ \hline
      
      H\(_5\) & CCM; CSM. & Usage patterns of the {\sf Consult CRF\/}
      function. & Questions\(_{5, 17}\) \\ \hline
      
      H\(_6\) & CSM. & Usage patterns of the {\sf Consult CRF\/}
      function. & Question\(_{6}\) \\ \hline
      
      H\(_7\) & COM. Rating on the quality of online discussions.
      & Usage patterns in both exploratory and consolidation phases.
      & Questions\(_{7-8}\) \\ \hline
      
      H\(_8\) & CRM. & Usage patterns in both exploratory and/or
      consolidation phases. & Questions\(_{9-10}\) \\ \hline
      
      H\(_9\) & COM. & Usage pattern of the comparison mode. &
      Question\(_{12}\). \\ \hline
      
      H\(_{10}\) & COM. CSM. & Usage patterns in both exploratory
      and consolidation phases. & Questions\(_{13-14, 18-21}\) \\
      \hline
    \end{tabular}
    \end{center}    
    \label{tab:data}
\end{table}
\normalsize
\ls{1.6}


\section{Data analyses}
\label{sec:data analysis}

This primary purpose of the evaluation component of the current research is
to find out how real learners use and view CLARE. The approach is
exploratory. Its intent is to provide a basis on which more rigorous
comparative studies might be performed. Hence, the analysis techniques to
be employed in this research are restricted to primarily descriptive
statistics, e.g., frequency counts.  Such summary statistics on outcome and
assessment data are supplemented by a substantial amount of qualitative
analyses of the detailed process data and the content of the database
generated by the group, guided by the questions identified in Section
\ref{sec:hypotheses data}. Consistent and important usage patterns of
various CLARE functions will be identified, plotted, and compared
graphically. Data-based activity maps will also be drawn, which will show
graphically the patterns of RESRA usages across learners and at the group
level.

\section{Experiments}
\label{sec:experiments}

The viability and the effectiveness of CLARE as an alternative
collaborative learning environment, as formulated in the ten hypotheses
(see Section \ref{sec:hypothesis}), are tested through a series of
experiments. This section describes the tasks, subjects, procedures, and
the execution plan of those experiments.

\subsection{Task}

All CLARE evaluation experiments involve the same task, namely, learning
about a subject domain by reviewing selected scientific text from that
domain. The task consists of two components: individual reviews, followed
by group deliberation and integration. Traditionally, such activities
typically take place in a seminar setting, where students are assigned to
read a common set of research papers from current journals or conference
proceedings. They are asked to write reviews of those papers that not only
summarize the key contributions, but also discuss major strengths and
weaknesses of the author's approach, and problems or questions the student
might have on the content of the artifact. The subsequent classroom
discussion allows selected individuals to present their reviews of those
papers. It also provides opportunity for the interaction of various points
of view held by different students regarding the assigned papers.  Through
these activities, students are expected to gain deeper understanding of
concepts, problems, methods, theories, et al, that are important to the
paper and domain.  At the same time, they are also expected to improve
their critical skills in evaluating other people's work, identifying
problems, developing alternative solutions, and working with other
learners.

The experiment shares almost the same learning goal as described above.
However, the procedure for realizing it is quite different (see
\ref{sec:procedures}). Unlike traditional seminars in which activities are
either paper-based or carried out face-to-face, the experiment is conducted
in the CLARE-mediated environment. All learning activities are governed by
the process and data protocols defined in CLARE. For example, there is no
writing of paper reviews in the original sense, nor face-to-face
discussions. Instead, students study papers by creating nodes and links of
selected types in CLARE. They interact with other students in a similar
fashion, i.e., via reading and reacting to the online artifacts they create
in the CLARE database.

The outcome from the experiment is a database containing a collection of
integrated artifacts created by all students in the group, and a group of
more knowledgeable learners. A linearized hardcopy of the database content
can also be generated.


\subsection{Subjects}

The subjects are 16 upper-level undergraduates (i.e., juniors and
seniors), who are enrolled in ICS414 (Software engineering II), and
8 graduate students who are enrolled in ICS613 (advanced software
engineering). Both classes are from the Department of Information
and Computer Sciences at the University of Hawaii in the Fall,
1993. These subjects are divided into groups of 4: four groups in
the first class and two groups in the second class.  The group
stays the same through the experiment period.


\subsection{Procedures}
\label{sec:procedures}

Each experiment involves a group of 4 students, whom are randomly assigned
from the subject pool. First-time users receive a 30-minute overview of the
objective, the basic approach and the overall architecture of CLARE,
followed by a 30-minute demo of the basic functionality and interface
features using pre-existing examples. A detailed user guide is provided to
the user at this time.

Prior to the experiment, the selected research papers are input into CLARE
database by the experiment coordinator in collaboration with the course
instructor. All papers are broken down into a set of nodes, typically
corresponding to sections in the paper. These nodes are connected together
through a set of links. Additional processing, such as the conversion of
explicit references into links, are also done at this time.  Since graphics
and figures are currently not handled by CLARE, hard-copies are provided to
all subjects.

The actual experiment is divided into two phases: {\it exploration\/} and
{\it consolidation.\/}  The former consists of two activities: summarization
and evaluation, both of which are carried out privately by each student.
The purpose of summarization is to identify key elements and relationships
in the paper as viewed by the student. The student is expected to use
predefined RESRA templates to guide his/her summarative activities. The key
to this step is to suspend judgement. Evaluation goes beyond just
critiquing of the original paper; it also involves questioning and
suggesting alternatives.

The consolidation phase includes three activity types: comparison,
deliberation, and integration. The comparison mode enables the student to
see what the other students have done during the previous phase and,
perhaps more importantly, to discern ambiguities, inconsistencies,
differences, similarities in their interpretations. The deliberation
process involves challenging other students' positions through request for
clarifications, critiquing, identifying what is missing, suggesting
additional sources or alternative views, etc.  In response to the
challenges from others, the student is expected to defend and elaborate his
positions.

As a close-up step, integration requires students to connect together nodes
and links created by different students through such actions as declaring
two nodes as similar, subsuming, sharing-the-same-perspective, or related.
The outcome from this process is a more complete, consistent, and perhaps
consensual view on a selected artifact.

After finishing the exploration and consolidation, all subjects are asked
to fill out two questionnaires: assessment and feedback.  This step is
followed by an informal discussion on learners' experience and problems
encountered in the CLARE session just completed.

\subsection{Execution Plan}
\label{sec:exec-plan}

The experiment is conducted between September and early October, 1993.
There are five batches of experiments: three for ICS414 and two for ICS613.
In ICS414, each batch consists of 4 concurrent experiment groups, which
results in a total of 12 experiments. All concurrent sessions use the same
research papers and operate under the identical experimental conditions. In
ICS613, each batch consists of two concurrent sessions, which results in to
a total of four experiments.

All CLARE experimental activities is carried out in the asynchronous mode.
Subjects are told that they are not supposed to discuss the papers outside
CLARE. The average length of the experiment is one week, which is about
equally divided for exploration and consolidation. Pilot tests are
conducted prior to the actual experiments to validate the questionnaires.



%%%\end{document}



