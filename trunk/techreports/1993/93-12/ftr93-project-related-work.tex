%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ftr93-project-related-work.tex -- 
%% RCS:            : $Id: nsf93-project-related-work.tex,v 1.9 93/10/06 16:50:50 johnson Exp $
%% Author          : Philip Johnson
%% Created On      : Thu Aug 12 16:29:25 1993
%% Last Modified By: Philip Johnson
%% Last Modified On: Mon Oct 11 15:15:15 1993
%% Status          : Unknown
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1993 University of Hawaii
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% History
%% 12-Aug-1993		Philip Johnson	
%%    

\subsection{Related work}
\label{sec:external-research}

Research on formal technical review tends to fall into two forms, which can
be termed {\em descriptive}\/ and {\em prescriptive}.  The descriptive
literature describes the process and products of review abstractly,
advocates that organizations must create their own individualized form of
review, but provides little prescriptive support for this process.
\cite{Schulmeyer87,Dunn90,Freedman90} exemplify this literature.  Such work
provides only vague and qualified answers to many central questions
concerning review, such as: How much should be reviewed at one time? What
issues should be raised during review, and are standard issue lists
effective? What is the relationship between time spent in various review
activities and their productivity? How many people should be involved in a
review? What artifacts should be produced and consumed during a review?

The prescriptive literature, on the other hand, takes a relatively hard
line stance on both the process and products of review.
\cite{Fagan76,Fagan86,Russell91} exemplify this literature.  Such
literature comes to precise conclusions about the process and products
(meetings should last a maximum of two hours; each line of code must be
paraphrased; lines of code should be read at rate of 150 lines per hour;
etc).  This literature certainly presents evidence for its effectiveness
within the authors' organizations.  However, the strict prescriptions
appear to suggest that all organizations should simply adapt to the review
method, rather than that the review method should be substantially adapted
to the needs and characteristics of the organization and application
domain.

\subsubsection{Issues in the research literature}

Research and development of a range of review methods is healthy, since it
is unlikely that any single review technique is most effective under all
organizational and application contexts.  However, the current state of
research on review fails to provide clear guidance to an organization in
choosing a well-suited review method.

One deficiency in the literature is the lack of high quality, empirical
studies comparing different review methods to each other.  Past studies
compare review to testing \cite{Myers78,Hetzel76,Basili85} or compare
different factors within a single review method (usually Fagan's
inspection), such as the effect of the number of participants or group
composition on review effectiveness \cite{Bisant89,Martin90}.

Unfortunately, these latter comparative studies do not specify the actual
activities that occurred in precise detail, leaving unanswered such
questions as whether reviewers strictly followed the checklist, whether the
same checklist was used during the meeting, whether the reader applied the
test cases prepared by reviewers, and so forth. As a result, although each
study claimed to use the same approach (Fagan code inspection), there may
have been substantial and significant differences between the methods.

Another problem with the current state of research on review is conflicting
and/or anecdotal explanations of the causal factors underlying review
outcomes.  For example, \cite{Selby85} attributes review effectiveness to a
technique called stepwise abstraction.  \cite{Dunn90,Peele82} attribute the
success of software review to the presence of group synergy, yet
\cite{Humphrey90} finds that 75\% of errors are found during private
preparation rather than in the public meeting.
\cite{Myers78,Parnas85,Peele82} attribute the success of review to the
synergy effect between producer and reviewers, and yet others discourage
active participation of the producer during the meeting
\cite{Ackerman89,Russell91,Pfleeger91}.  Finally, \cite{Fagan76} attributes
review effectiveness to paraphrasing, \cite{Ackerman89} to the use of
selective test cases, and \cite{Knight91} to checklists.

These issues in the state of review research are not raised with the intent
of denigrating the research or the researchers.  Instead, they are raised
to highlight the difficulty and cost of obtaining empirically-founded
understanding of a highly manual, knowledge-intensive, and collaborative
activity.  Later in this proposal, we will describe how our on-line,
instrumented, collaborative environment for FTR addresses many of the
methodological difficulties confronting review researchers.  First,
however, some of the practical problems in obtaining good review outcomes
with traditional manual review methods are presented.

\subsubsection{Issues in manual review}
\label{sec:issues-manual-review}

Despite the methodological issues and credit assignment variations noted
above, most research tends to agree that manual review, when properly
carried out, is effective.  Research also tends to agree that manual review
is expensive.  For example, \cite{Russell91} shows that in his
organization, one person-year of technical staff time was required per 20
KLOC for FTR, and that this cost added 15-20\% new overhead onto
development. Boeing Computer Services found reviews to be ``extremely
expensive'' \cite{Glass82}. Such admissions are usually followed by
analyses demonstrating that this upstream investment is more than recouped
through decreases in downstream rework costs.

Although manual FTR, when properly carried out, is typically cost-effective
in the long run, this is a significant qualification, since manual FTR is
very difficult to properly carry out. \cite{Deimel91,Freedman90,Gilb88}
together catalog many common pitfalls,  including:

\begin{itemizenoindent}
\item {\em Insufficient preparation.} A major responsibility of the
  review leader is to ensure that participants adequately prepare for the
  review by reading and reflecting upon the review materials.
  Insufficiently prepared reviewers are a major cause of low quality review
  outcomes, where reviewers attend the meeting and attempt to ``bluff''
  their way through and/or read the code for the first time during the meeting.
  This problem is serious enough that fairly devious remedies are presented
  in the literature. One such approach is to deliberately leave out one
  page of the review materials when distributing them to participants:
  those who prepare will notice the absence and contact the review leader.
  
\item {\em Moderator domination.} In a group meeting, it is easy for the
  moderator to inadvertantly or premeditatedly abuse this role by
  inhibiting or intimidating the other participants.  This results in
  reduced participation and reduced review quality.

\item {\em Incorrect review rate.} Each minute of a review meeting is
  intrinsically expensive, since it requires the simultaneous attendance
  and involvement of at least three and frequently six to eight technical
  staff personnel.  Thus, the speed of review is critical to its
  cost-effectiveness: too slow and the time (and salaries) of several
  technical personnel is wasted; too fast and the quality of review
  decreases.

\item {\em Ego-involvement and personality conflict.} The fact that one of
  the review member's work artifacts is under review can lead to
  significant problems during review.  Review always requires substantial
  diplomacy and care on the part of each member.  Otherwise, the review
  process can degenerate or dissolve completely.

\item {\em Issue resolution and meeting digression.} The expense of
  review meetings and the complexity of software dictates that review
  sessions not evolve into problem-solving sessions.  All instructional
  materials we have seen cite this issue as crucial to review success,
  stating that reviewers should ``raise issues, don't resolve them.''  They
  also note that it requires significant reviewer effort and continual
  moderator vigilence to prevent such discussion.
  
\item {\em Recording difficulties and clerical overhead.} Manual review
  requires a scribe to record the outcome of the process.  Capturing the
  information generated during a review meeting completely and accurately is
  extremely difficult, as noted in the literature, and as anyone who has ever
  attempted the role of scribe will attest. Methods involving audio-visual
  aids and a ``telegram style'' of note-taking have been proposed to support
  this process.
  
  Finally, substantial additional clerical overhead is required to
  collect data adequate for research on FTR or FTR process improvement.
  Perhaps for this reason, published review data has only come from very
  large organizations able to allocate the additional resources to this
  activity.

\end{itemizenoindent}

These problems are not specific to code review, but have analogues in most
traditional meeting-based group work.  \cite{Nunamaker91} discusses the
problems of manual meeting-based group work to motivate the design of an
electronic meeting room system, and each of the above issues appear in his
research as well. Section \ref{sec:results} describes how the design
of CSRS addresses each of these issues. 



\subsubsection*{Summary}

The previous two sections provide evidence for a central claim in our
research: the current manual nature of FTR makes it difficult to
effectively carry out review, and makes it difficult to measure the process
and products of review in such a manner as to understand review, compare
review experiences across organizations, and improve the process based upon
empirical findings.

The next section presents an overview of CSRS, our computer-based support
environment for formal technical review.  This section will acquaint the
reader with the process and products of review using CSRS, the relationship
of CSRS to other computer-based review systems, and its current status. The
following section will present specific research to be performed with CSRS
as part of this research agenda.





