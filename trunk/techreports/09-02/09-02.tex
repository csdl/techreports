%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 09-02.tex --     ESEM 2009 Submission
%% Author          : Philip Johnson
%% Created On      : Wed Jan 07 14:06:37 2009
%% Last Modified By: Philip Johnson
%% Last Modified On: Mon Feb 23 16:29:11 2009
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 2009 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
\documentclass{acm_proc_article-sp}

%
\def\sharedaffiliation{%
\end{tabular}
\begin{tabular}{c}}
%
\begin{document}

\title{I need more coverage, stat!  \\
Experiences with the Software Intensive Care Unit}

\numberofauthors{2} 

\author{
  \alignauthor Philip Johnson\\
  \email{johnson@hawaii.edu}
%
  \alignauthor Shaoxuan Zhang \\
  \email{sz@hawaii.edu}
%
  \sharedaffiliation
  \affaddr{Collaborative Software Development Laboratory}\\
  \affaddr{Department of Information and Computer Sciences}\\
  \affaddr{University of Hawaii}
}

\date{15 March 2008}

\maketitle
\begin{abstract}
One goal of the Hackystat Framework is to facilitate the teaching of
software metrics in classroom settings.  To that end, we have conducted
classroom evaluations in 2003, 2006, and 2008.  This paper reports in
detail on our most recent approach to teaching software metrics in the
classroom by way of an approach called the ``Software ICU''.  In this
approach, students learn about ten empirical project ``vital signs'' and
use the Hackystat Framework to put their students projects into a virtual
``intensive care unit'' where these vital signs can be assessed and
monitored.  We conducted a questionnaire-based evaluation that provides
insight into the strengths and weaknesses of this approach, how it compares
to previous approaches using the Hackystat Framework, and promising future
directions.
\end{abstract}

\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures, software quality measures]

\section{Introduction}

Introducing students to software measurement in particular and empirical
software engineering in general is a challenging task.  If one merely
lectures from the literature, much of the subtleties involved in the
practice of collecting and analyzing process and product data are lost.
Students can easily form the erroneous impression that software measurement
is ``easy''.  For example, (1) collect complexity; (2) set a threshold
using a published reference such as \cite{Clark08}, and (3) require
developers to ``fix'' any classes that exceed the established threshold.
The problem with this approach is that individual metrics never capture the
spectrum of trade-offs implicit in a design. For example, a natural result
of performance optimization on a section of code is an increase in
complexity (and coupling). Measurements on such classes might exceed
thresholds for important reasons.  Without such real world grounding, such
students could grow up to be the stereotypical process improvement
managers who impose ``best practices'' for measurement and analysis
without understanding the potential for misinterpretation and, ultimately,
measurement dysfunction \cite{Austen96}.

Requiring students to gather and analyze measurements themselves also
presents problems.  For example, while the Personal Software Process
\cite{Humphrey95} provides a well structured approach to data gathering and
analysis by students, the PSP suffers from problems including high overhead
\cite{csdl2-01-12}, data quality \cite{csdl-98-13}, and adoption
\cite{Borstler02}.  Students introduced to metrics in this way can easily
form the impression that software measurement is ``too hard'' to be
practical in most real-world situations.

For five years, we have been experimenting with the use of Hackystat in a
classroom setting in order to better understand the issues involved in
teaching software measurement. In 2003, we performed a case study in which 
we used Hackystat to automate data collection and analysis and used 
a survey to assess student reactions \cite{csdl2-03-12}.  We performed a 
partial replication of this study in 2006 with a redesigned version of
Hackystat \cite{csdl2-07-02}. 

In this paper, we present the results of our third partial replication with
yet another redesigned version of Hackystat in the Fall of 2008.  One major
change in our current approach is the complete abandonment of terminology
like ``software measurement'' or ``metrics'' as the pedagogical focus.
Instead, we present the material through the metaphor of a medical
intensive care unit (ICU).  Instead of ``metrics'', we taught the students
about how to acquire software ``vital signs''.  The goal of vital sign
collection was to assess whether the ``patient'' (software system under
development) was ``healthy'' or ``sick''.  The user interface was modeled
after a medical ICU vital sign monitor, with the ability to display both
the current value as well as the trends (heartbeats) over time.  Just as a
vital sign monitor can be set with alarms, the Software ICU monitor can be
configured with thresholds to color the current value or trend either red,
yellow, or green.  Finally, just as an ICU supports multiple patients, our
Software ICU can show the status of multiple projects simultaneously,
supporting ease of comparative evaluation.

The software measurements underlying the ICU were collected automatically
through two mechanisms. First, the students installed Hackystsat sensors
into their IDE (Eclipse) and build system (Ant) which would send process
metrics regarding their development activities.  Second, their projects
used the Hudson system to perform continuous integration, which meant that
after each commit of their code, the system would be automatically built
and tested.  The Hudson system was also configured to automatically gather
certain product metrics such as coverage, coupling, and complexity.

The study involved 18 students from a senior-level undergraduate software
engineering course at the University of Hawaii from Fall, 2008.  They used
the Software ICU for the final five weeks of the semester, then filled out
an online questionnaire regarding the system.  In addition, logs were kept
that tracked their usage of the system.

Our results showed that, in general, the Software ICU was an effective
approach to teaching students about process and product measurement.  The
overhead involved in data collection appeared acceptably low, and most
students found the data to be useful, although some ``vital signs'' were
more useful than others.  The log data showed that students displayed the
Software ICU analysis almost 600 times during the five weeks of the study,
confirming that they did indeed utilize the system. With respect to
replication, we used some of the same questions in 2008 as we did in 2006
and 2003.  The replicated questions shows that Hackystat's utility for
students appears to be increasing over time, though its overhead in certain
areas (sensor installation) has actually decreased since 2006.







\section {Related Work}

We've worked on this before \cite{csdl2-07-02,csdl2-03-12,csdl2-03-13}.

\section{The Software ICU}

An overview of how it works.  

\section{Evaluation}

How we evaluated it.

\section{Conclusions and future directions}

Where we will go next.

\section{Acknowledgments}

We will acknowledge some folks here.

\bibliographystyle{abbrv}
\bibliography{csdl-trs}  
\end{document}

