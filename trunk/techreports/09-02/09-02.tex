%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 09-02.tex --     ESEM 2009 Submission
%% Author          : Philip Johnson
%% Created On      : Wed Jan 07 14:06:37 2009
%% Last Modified By: Philip Johnson
%% Last Modified On: Thu Feb 26 11:13:06 2009
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 2009 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
\documentclass{acm_proc_article-sp}

%
\def\sharedaffiliation{%
\end{tabular}
\begin{tabular}{c}}
%
\begin{document}

\title{I need more coverage, stat!  \\
Classroom experience with the Software ICU}

\numberofauthors{2} 

\author{
  \alignauthor Philip Johnson\\
  \email{johnson@hawaii.edu}
%
  \alignauthor Shaoxuan Zhang \\
  \email{sz@hawaii.edu}
%
  \sharedaffiliation
  \affaddr{Collaborative Software Development Laboratory}\\
  \affaddr{Department of Information and Computer Sciences}\\
  \affaddr{University of Hawaii}
}

\date{15 March 2008}

\maketitle
\begin{abstract}
One goal of the Hackystat Framework is to facilitate the teaching of
software metrics in classroom settings.  To that end, we have conducted
classroom evaluations in 2003, 2006, and 2008.  This paper reports in
detail on our most recent approach to teaching software metrics in the
classroom by way of an approach called the ``Software ICU''.  In this
approach, students learn about ten empirical project ``vital signs'' and
use the Hackystat Framework to put their students projects into a virtual
``intensive care unit'' where these vital signs can be assessed and
monitored.  We conducted a questionnaire-based evaluation that provides
insight into the strengths and weaknesses of this approach, how it compares
to previous approaches using the Hackystat Framework, and promising future
directions.
\end{abstract}

\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures, software quality measures]

\section{Introduction}

Introducing students to software measurement in particular and empirical
software engineering in general is a challenging task.  

On the one hand, if one merely lectures from the literature, much of the
subtleties involved in the practice of collecting and analyzing process and
product data are lost.  An overly superficial presentation can lead
students to form the erroneous impression that software measurement is
``easy'' to institute into development practices. For example, simply (1)
collect complexity; (2) set a threshold using a published reference such as
\cite{Clark08}, and (3) require developers to ``fix'' any classes that
exceed the established threshold.  The problem is that individual metrics
never capture the spectrum of trade-offs implicit in a design. For example,
a natural result of performance optimization on a section of code is an
increase in complexity (and coupling). Measurements on such classes might
exceed thresholds for important reasons.  Without such real world
grounding, such students could grow up to be the stereotypical process
improvement managers who impose ``best practices'' for measurement and
analysis without understanding the potential for misinterpretation and,
ultimately, measurement dysfunction \cite{Austin96}.

On the other hand, requiring students to gather and analyze measurements
themselves can potentially lead students to believe that measurement is too
``hard''.  For example, while the Personal Software Process
\cite{Humphrey95} provides a well structured approach to data gathering and
analysis by students, independent research reveals a number of problems
including high overhead \cite{csdl2-01-12}, data quality \cite{csdl-98-13},
and low adoption \cite{Borstler02}.  Students introduced to metrics via the
PSP (or its successor, the Team Software Process) can easily form the
impression that software measurement imposes too much overhead for (at
least) ``agile'' software development situations.

For the past five years, one research thrust of the Hackystat Framework has been
to explore the issues involved in teaching software measurement in a
classroom setting.  Hackystat provides a middle ground between excessively
high overhead approaches like the PSP/TSP and excessively low overhead
approaches like literature review.  Extensive automation of both data
collection and analysis lowers the overhead required to give students
practical experience with measurement, while creating opportunities to
experiment with the way that measurements are analyzed, presented, and
interpreted.

In this paper, we present the results of a case study experiment we
performed in the Fall of 2008 in which we used the metaphor of a medical
intensive care unit (ICU) to explain and motivate the use of metrics in
software development.  We built a new user interface for metric data called
the ``Software ICU'' that is similar in many ways to a medical ICU
monitoring device.  Just as a medical ICU automatically gathers vital signs
of patients such as heart rate and respiration in order to detect changes
in health, our software ICU automatically monitors the process and product
``vital signs'' of its software ``patients''---in this case, the student
teams and the projects they were developing.  Just as a medical ICU
generates alarms when a vital sign falls outside a established range for
normalacy, our software ICU was configured to color the data as red,
yellow, or green.

We collected two types of data: an online questionnaire and log data that
recorded all student interactions with the Software ICU.  Our results
provide some evidence that, in general, the Software ICU is currently the
most effective Hackystat-based approach to teaching students about process
and product measurement.  Student feedback indicates that the overhead
involved in data collection and analysis was acceptably low, and almost all
of the students found the data to be useful, although students found some
``vital signs'' to be more useful than others. Most students believed that
the Software ICU would be feasible for use in professional situations.  The
log data provided independent confirmation of the usage of the system, as
most students invoked the Software ICU almost 20 times per week during the
course of the study.

The remainder of the paper is organized as follows.  Section
\ref{sec:related} presents related work.  Section \ref{sec:icu} provides a
brief overview of the system. Section \ref{sec:evaluation} presents the
case study design and its results.  Section \ref{sec:conclusions} presents
our conclusions and future directions.

\section {Related Work}
\label{sec:related}

Perhaps the most extensively studied curriculum for measurement-based
software engineering is the Personal Software Process \cite{Humphrey95} and
the Team Software Process \cite{Humphrey00}.  Both of these approaches
require students to develop a series of software projects, typically six to
eight during a single semester.  Both process and product measures are
gathered about each project, and the measurements become increasingly
detailed as the semester proceeeds.

We've worked on this before \cite{csdl2-07-02,csdl2-03-12,csdl2-03-13}.

In 2003, we performed a case study in which 
we used Hackystat to automate data collection and analysis and used 
a survey to assess student reactions \cite{csdl2-03-12}.  We performed a 
partial replication of this study in 2006 with a redesigned version of
Hackystat \cite{csdl2-07-02}. 

Also need to talk about dashboards. 

\section{The Software ICU}
\label{sec:icu}

An overview of how it works.  

\section{Evaluation}
\label{sec:evaluation}

The software measurements underlying the ICU were collected automatically
through two mechanisms. First, the students installed Hackystsat sensors
into their IDE (Eclipse) and build system (Ant) which would send process
metrics regarding their development activities.  Second, their projects
used the Hudson system to perform continuous integration, which meant that
after each commit of their code, the system would be automatically built
and tested.  The Hudson system was also configured to automatically gather
certain product metrics such as coverage, coupling, and complexity.

The study involved 18 students from a senior-level undergraduate software
engineering course at the University of Hawaii from Fall, 2008.  They used
the Software ICU for the final five weeks of the semester, then filled out
an online questionnaire regarding the system.  In addition, logs were kept
that tracked their usage of the system.


How we evaluated it.

\section{Conclusions and future directions}
\label{sec:conclusions}

Where we will go next.

\section{Acknowledgments}

We will acknowledge some folks here.

\bibliographystyle{abbrv}
\bibliography{csdl-trs,hackystat,psp}  

%% In this paper, we present the results of our third partial replication with
%% yet another redesigned version of Hackystat in the Fall of 2008.  One major
%% change in our current approach is the complete abandonment of terminology
%% like ``software measurement'' or ``metrics'' as the pedagogical focus.
%% Instead, we present the material through the metaphor of a medical
%% intensive care unit (ICU).  Instead of ``metrics'', we taught the students
%% about how to acquire software ``vital signs''.  The goal of vital sign
%% collection was to assess whether the ``patient'' (software system under
%% development) was ``healthy'' or ``sick''.  The user interface was modeled
%% after a medical ICU vital sign monitor, with the ability to display both
%% the current value as well as the trends (heartbeats) over time.  Just as a
%% vital sign monitor can be set with alarms, the Software ICU monitor can be
%% configured with thresholds to color the current value or trend either red,
%% yellow, or green.  Finally, just as an ICU supports multiple patients, our
%% Software ICU can show the status of multiple projects simultaneously,
%% supporting ease of comparative evaluation.

\end{document}

