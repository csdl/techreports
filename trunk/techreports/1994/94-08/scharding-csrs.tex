%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% scharding-csrs.tex -- 
%% RCS:            : $Id: icse94-csrs.tex,v 1.1 93/09/02 10:59:26 johnson Exp Locker: johnson $
%% Author          : Philip Johnson
%% Created On      : Mon Aug 16 14:32:13 1993
%% Last Modified By: Philip Johnson
%% Last Modified On: Sun Jun 19 10:36:02 1994
%% Status          : Unknown
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1993 University of Hawaii
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% History
%% 16-Aug-1993		Philip Johnson	
%%    
 


\section{CSRS: The principle-centered design}

The principal result of our first two years of research was CSRS: a
computer-supported cooperative work environment for formal technical
review. CSRS was implemented using Egret \cite{csdl-92-01}, an environment
for exploratory collaborative group work.  Egret provides a multi-user,
hypertext environment for Unix and X windows.  It is based upon a
client-server architecture, where a database back-end server process
written in C++ communicates over TCP/IP to client processes implemented via
a customized version of Lucid Emacs.

In the design of CSRS, we were motivated solely by the desire to resolve as
many of the issues we discovered in the FTR literature as possible.  The
result was a highly instrumented environment enacting a specialized
process for FTR.  We called this process model FTArm, for ``Formal,
Technical, Asynchronous review method.''


\begin{figure*} [t]
 {\centerline{\psfig{figure=/group/csdl/techreports/93-22/process.ps}}}
\caption{{\em The seven phases in the FTArm method, along with the primary
entry condition for each phase.}}
\label{fig:process-model}
\end{figure*}

\subsection{The FTArm Method}

The FTArm process consists of seven well-defined phases, as illustrated by
the diagram in Figure \ref{fig:process-model}. The FTArm method is not
specific to a review artifact type or development phase.

\paragraph{Setup.} In this phase, the moderator and/or the producer
decide upon the composition of the review team and the artifacts to be
reviewed. The moderator or producer then restructures the review artifact
into a multi-node, interlinked hypertext document stored within the CSRS
database.  Regular expression-based parsing tools available in CSRS can
partially or fully automate this database entry and restructuring process.

\paragraph{Orientation.} This phase prepares the participants for the
private review phase through an overview of the review artifacts.  The
exact nature of this overview depends upon the complexity of the
review artifact and the familiarity of the reviewers with it, and can range
from a simple e-mail message to a formal, face-to-face meeting.

\paragraph {Private review.} In this phase, reviewers analyze the
review artifact nodes (termed ``source'' nodes) privately and create issue,
action and/or comment nodes.  Issue and action nodes are not publicly
available to other reviewers, though comment nodes are publicly available.
Comment nodes allow reviewers to request clarification about the
logic/algorithm of source nodes, or about the review process, and may also
contain answers to these questions by other participants.

Figure \ref{fig:issue-screen} contains a snapshot of one reviewer's
screen during the private review phase.  The function {\tt
t*node-schema!combine-field-IDs} is the review artifact under
analysis, as displayed in the left hand window.  A checklist of defect
classifications appears in the upper right window, while a defect
concerning this function is being documented in the lower right
window.

\begin{figure*} [t]
 {\centerline{\psfig{figure=/group/csdl/techreports/93-22/issue-private.ps}}}
\caption{{\em A CSRS screen illustrating the generation of an issue.}}
\label{fig:issue-screen}
\end{figure*}

In FTArm, reviewers must explicitly mark each source node as reviewed
when finished. While reviewers do not have access to each other's
state during private review, the moderator does.  This allows the
moderator to monitor the progress of private review.  Private review
normally terminates when all reviewers have marked all source nodes as
reviewed. In the event that no reviewer has created any issues, review
would terminate at this point.  Otherwise, public
review begins.

\paragraph{Public review.} In this phase, all nodes are now
accessable to reviewers, and all participants (including the producer)
react to the issues and actions by voting (a modified Delphi process).
Participants can also create new issue, action or comment nodes based upon
the votes or nodes of others.  Voting is used to determine the degree of
agreement within the group about the validity and implications of issues
and actions.  Public review normally concludes when all nodes have been read
by all reviewers, and when voting has stabilized on all issues.

\paragraph{Consolidation.} In this phase, the moderator analyzes the results of
public and private review, and produces a condensed written report of the
review thus far.  These consolidated reports are more
comprehensive, detailed, and accurate than typical review reports from
traditional review methods. Rather than simply a checklist of
characteristics with brief comments about the general quality of the
source, consolidation reports contain a re-organized and condensed
presentation of the analyses provided by reviewers in issue, action, and
comment nodes, thus providing contrasting opinions, the degree of
consensus, and proposals for changes.

CSRS provides the moderator with various tools to support the generation of
a nicely formatted LaTeX document containing the consolidated report.  If
the group reached consensus about all of the issues and actions during
public review, then this report presents the review outcome with
respect to artifact assessment.  A second review outcome is the
measurements of review process and products, as discussed in Section
\ref{sec:instrumentation} below.
 
\paragraph {Group review meeting.} If the consolidated report 
identifies issues or actions that were not successfully resolved via
public and private review, the FTArm method requires a face-to-face,
group meeting as the final phase.  Here the moderator presents only
the unresolved issues or actions and summarizes the differences of
opinion.  After discussion, the group may vote to decide them, or the
moderator may unilaterally make the decision. The moderator then
updates the CSRS database, noting the decisions reached during the
group meeting and then generating a final consolidated report
representing the product of review.

This description is intended only to provide a ``gestalt'' feel for FTArm,
and thus illustrates the process from the perspective of a reviewer.
However, several other roles exist in FTArm and are crucial to the review
process:

\begin{itemize}
  
\item The {\em administrator} is responsible for defining the precise CSRS
  data and process model to be used during review.  For example, the
  administrator determines the specific artifact analysis technique, such as
  whether or not checklists will be used and what items will appear in the
  checklists. 
  
\item The {\em moderator} is responsible for overseeing the actual review
  and monitoring the on-line process, including transitioning between phases
  and producing the Consolidated Report.
  
\item The {\em producer} is the person who created the artifact under
  review.  The producer is responsible for assisting the moderator in
  setting up the hypertext network, and for answering certain types of
  questions raised during any phase of review.
  
\item The {\em analyst} is responsible for reviewing the measurements
  collected during each review experience, comparing them to prior
  measurements, and generating hypotheses about how to improve review
  efficiency and effectiveness based upon this empirical data.

\end{itemize}


\subsection{FTR research issues: the principle-centered solution}
\label{sec:instrumentation}

To resolve issues in the research on FTR, CSRS provided a precisely
defined process model along with sophisticated instrumentation.  
Such instrumentation was a major feature distinguishing CSRS from other
automated review environments such as ICICLE \cite{Brothers90}, Scrutiny
\cite{Gintell93}, or INSPEQ \cite{Knight91}. The instrumentation facilities
had two basic goals: generation of {\em process} and {\em outcome} oriented data. 


\paragraph{Process instrumentation.} 

Process mechanisms provide insight into the {\em sequence}\/ and {\em
duration}\/ of review activities.  They are implemented by a general
purpose timestamp subsystem in Egret, the collaborative infrastructure for
CSRS.  This facility enables calls at strategic points within CSRS to
record the occurrence of application-specific events of interest. For
example, CSRS timestamps when the user reads a node, writes a node, closes
a node, traverses a link, marks a node as reviewed, and so forth.  Egret
provides mechanisms to cache timestamp data at the local client during a
review session, and write the cache contents out to the server database at
disconnect time.

One use of timestamp data is to calculate elapsed-time
information.  For example, subtracting the timestamp recorded when a
user reads a particular source node from the timestamp recorded when
the user closes that node yields the elapsed time during which the
source node was displayed to the user.

However, we quickly learned that this elapsed display time is not a useful
measure of the {\em effort} spent reviewing a node.  In a multiple window,
multi-tasking workstation environment, review using CSRS is typically
interrupted by phone calls, impromptu meetings with colleagues, reading and
answering e-mail, and so forth. Thus, the elapsed display time has two
components: the time during which the user was actually reviewing, and the
during which the node was displayed but the user's focus of attention was
elsewhere.

As a result, we modified the process instrumentation to detect this latter
``idle time'' via a bottom-up, timer-initiated timestamp process that runs
in parallel with the top-down, user-triggered timestamps.  Once per minute,
the timer process wakes up, determines whether or not there has been any
recent low-level mouse or keystroke activity within the CSRS application,
and if so generates a ``busy'' timestamp. In our experiences thus far, this
combination of top-down and bottom-up timestamping provides high quality
measures of effort with a precision of plus or minus one minute.

One goal of the process instrumentation was to provide insight into the 
behavior of participants during review.  To aid in this investigation, we 
developed a simple graphical display program called TimePlot that
illustrates which nodes were displayed to the user during each minute that
they used CSRS.  Figure \ref{fig:timeplot} illustrates a portion of one 
session generated by TimePlot.



\begin{figure}
\tiny
\input{/group/csdl/techreports/93-22/timeplot.tex}
\normalsize
\caption{An excerpt from an example Timeplot graph from one review
session. This session actually lasted for 
22 minutes, but only the first 10 minutes are shown here.}
\label{fig:timeplot}
\end{figure}


\paragraph{Outcome instrumentation.} 

These mechanisms allow review analysts to query the CSRS database
during or after review for such information as the number of nodes
generated of a given type, the set of nodes containing a particular
value in a particular field, or the set of nodes partaking in a
specific relationship to other nodes.

Outcome measures are very useful. First, the number and severity
of identified defects provides a first-order estimate of the quality of the
software under review.

Second, outcome measures can be used to reveal certain problems with the
review team or review process.  For example, marginally productive members
of the review team might be identified (after a sufficient number of review
instances) as those who contribute little, or who contribute
non-productively (by simply affirming comments made by others), or who use
review for political purposes.  The CSRS design appeared to be less
expensive and more objective than techniques where behavioral data (such as
``Shows solidarity'', ``Shows Tension'', and so forth) is collected by a
passive observer of the review in order to provide feedback on the quality
of reviewer participation \cite{Freedman90}.

Third, outcome measures can contribute to empirically-guided process
improvement.  For example, an organization may be able to bound certain
review factors (such as the number of participants or the size of artifact
to review) by measuring these factors across a large number of otherwise
similar review instances, and then correlating them to review
effectiveness.

Finally, outcome measures can combine with process measures to indicate
the effort expended on review by each participant on {\em individual}
source nodes, as well as the cumulative effort expended by all participants
across the entire review artifact. It is well known that precise and
accurate measurement of cumulative review effort (including preparation) is
notoriously difficult to obtain in typical industrial settings
\cite{Freedman90}.   Figure \ref{fig:coarse-grained} illustrates the this
granularity of information provided by the system about reviewer behavior.


\begin{figure} 
 {\centerline{\psfig{figure=/group/csdl/techreports/93-12/nbuff-data.ps}}}
\caption{Outcome-oriented review measures.}
\label{fig:coarse-grained}
\end{figure}


\subsection{FTR practice issues: the principle-centered solution}
\label{sec:results}

The FTArm process model was carefully designed to address as many issues
in the practice of manual FTR as possible.  The following paragraphs
discuss how this occurs.

\begin{itemize}
  
\item {\em Insufficient preparation.} FTArm eliminates the problem of
  detecting insufficient preparation, since the preparation phase in
  traditional review corresponds to private review in FTArm, and since the
  activities of each reviewer during private review is precisely
  instrumented and known to the moderator.  Of course, the method cannot
  force an intransigent participant to bring up the system and look at the
  materials, but it does eliminate the most important problem of reviewers
  ``exaggerating'' their degree of preparedness to the review leader. 
  
\item {\em Moderator domination.} FTArm is designed to prevent the
  moderator from most forms of domination that are possible in manual review.
  During private review, of course, all moderator influence is eliminated.
  During public review, the moderator can potentially influence outcome by
  ``flaming'', but cannot physically prevent contributions of other members
  by monopolizing air time as is possible in synchronous review.  Even the
  group meeting phase of FTArm is less vulnerable to moderator domination,
  since the set of issues has already been established and documented.
  
\item {\em Incorrect review rate.} Since most review in FTArm occurs
  asynchronously, the cost of review is much less sensitive to its rate.
  The desire of one participant to review slowly makes no impact on the
  rate (or cost) of review for any other participant.  Asynchronous
  interaction also eliminates ``air-time fragmentation''---the costly idle
  time spent by participants in face-to-face reviews while waiting
  for a turn to speak.
  
\item {\em Ego-involvement and personality conflict.} On-line and
  asynchronous review allows reviewers time to consider their choice of
  words carefully.  During private review, in fact, it is possible to use
  CSRS the ``day after'' to modify comments that appear inappropriate,
  after a night's reflection, before any other reviewers see them.  In
  general, diplomacy is much easier in a non-real time environment.
  
\item {\em Issue resolution and meeting digression.} As noted previously,
  the constraints of a synchronous meeting context require participants to
  raise issues, but not to resolve them.  In FTArm, the problems of cost and
  digression arising from issue resolution are vastly minimized or
  eliminated.  
  
  More importantly, analysis of node content data from our review
  experiences reveals that issues are frequently difficult to articulate
  {\em except in the context of proposing an action to resolve it}, and
  that a natural tendency of most reviewers, immediately upon the
  identification of an issue, is to suggest a solution.  As a result,
  FTArm explicitly {\em encourages} the interleaving of action proposal
  and discussion with issue proposal and dicussion during review.  Our
  experiences suggest that capturing action proposals when the reviewer
  first thinks of them, rather than artificially delaying their
  discussion until some future meeting time, is certainly a more natural
  and possibly a more efficient method of review.
  
\item {\em Recording difficulties and clerical overhead.} FTArm
  eliminates the role of scribe altogether, except for the group meeting
  phase where the role is considerably simplified.  The CSRS system
  trivially resolves the problems of capturing review commentary completely
  and accurately.  CSRS also provides the moderator with tools to support
  restructuring and reformatting of the data from its on-line, hypertext
  format into a form suitable for a linear, hardcopy presentation.  Such
  features substantially reduce the clerical overhead normally associated
  with review.
 
  Most important from a research perspective, CSRS collects data and
  process measures automatically and unobtrusively.  This means that
  research on review can be performed in any size organization with any
  amount of resources allocated to review.

\end{itemize}

Taken as a whole, the FTArm method is designed to reduce the cost of review
by implementing an incrementally increasing level of both collaborative
involvement and review cost.  During private review, collaboration is
actively prevented, and the cost of review to each participant is
restricted to the expense of their personal review efforts.  During public
review, partial collaboration is supported by allowing each reviewer
on-line access to the comments made by others, which incurs some additional
cost.  Finally, the review group meeting provides the highest cost
collaboration, but is restricted in scope to only those issues that cannot
be resolved by the previous, lower-cost forms of collaboration.

The FTArm method is a classic result of ``laboratory'' design: through 
extensive literature research and in-house experimentation, a system was
designed that appeared to fix many problems occuring in industry.
The next section discusses what happened when industry started to look at
the fruits of our laboratory.


