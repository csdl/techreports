%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% new -- Hackystat-UH ISESE Paper
%% Author          : Philip Johnson
%% Created On      : Mon Sep 23 11:52:28 2002
%% Last Modified By: Philip M. Johnson
%% Last Modified On: Thu Aug 12 08:27:08 2004
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 2002 Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 

\documentclass[11pt,twocolumn]{article} 
\input{/export/home/csdl/tex/psfig/psfig}
\usepackage{/export/home/csdl/tex/icse2003/latex8}
\usepackage{times}
%% A verbatim-like environment which allows font changes
%%\usepackage{alltt}
%% New LaTeX2e graphics support
\usepackage[final]{graphicx}
% uncomment the % away on next line to produce the final camera-ready version
% and uncomment the \thispagestyle{empty} following \maketitle
\pagestyle{empty}

\begin{document}

\title{Improving Software Development Management through Software Project Telemetry}


\author{\protect\begin{tabular}{ccc}
Philip M. Johnson & Hongbing Kou & Mike Paulding  \\
Qin Zhang & Aaron Kagawa & Takuya Yamashita \\
\end{tabular}\\
\em  Collaborative Software Development Laboratory \\
\em  Department of Information and Computer Sciences \\
\em  University of Hawai'i \\
\em  Honolulu, HI 96822 \\
\em  johnson@hawaii.edu}
\maketitle
\thispagestyle{empty}

\begin{abstract}  % 200 words

Abstract goes here.  

\end{abstract}

\Section{Introduction}
\label{sec:intro}

It is conventional wisdom in the software engineering research community
that metrics can improve the effectiveness of project management.
Proponents of software metrics quote theorists and practitioners from
Galileo's ``What is not measurable, make measurable'' \cite{Finkelstein82}
to DeMarco's ``You can neither predict nor control what you cannot
measure'' \cite{DeMarco82}.  Software metrics have been applied to ...

Despite the potential of metrics in theory, their effective application
appears to be far from mainstream in practice. For example, a recent case
study of over 600 software professionals revealed that only 27\% viewed
metrics as ``very'' or ``extremely'' important to their software project
decision making process \cite{Kulik2003}. The study also revealed that cost
and schedule estimation was the only metrics application attempted
by a majority of respondents.

At least two hypotheses could account for this chasm separating the theory
and practice of software measurement. The first is that three quarters of
practitioners are simply ignorant: if they would subscribe to the journals
and/or partake of the many educational opportunities regarding software
metrics, they would find that implementing current ``best practices'' would
make a substantial, positive impact on their project management decision
making.

While all of us, theorists and practitioners alike, can always benefit from
additional reading and education, an alternative hypothesis also explains
this chasm between metrics theory and practice. Perhaps the metrics
methodology used by theorists yields results not easily translated into
practice?  Consider that much metrics research involves the following basic
methodology: (1) collect data on completed software projects; (2) generate
a model that fits the collected data points; (3) claim that this model can
now be used to predict characteristics of future projects, for example,
that a future project of size X will require Y person-months of effort, or
that the future implementation of a module with complexity S will be prone
to defects with probability T.

Practitioners encounter several barriers to adoption of metrics research
findings using this methodology. First, for the practitioner to use the
model directly for their own management decision-making, they must confirm
that the set of projects used to calibrate the model are similar to the
theirs, at least for the project characteristics important to the model.
Second, they must confirm that their future projects will also be similar
to the past projects, at least with respect to the model.  If the
practitioner cannot confirm these similarities, then they confront a third
barrier: the cost and time required to build a custom repository of
historical project data from their own organizatoin of sufficient
completeness and quality to generate a new model calibrated to their own
circumstances. This generates yet another barrier: the risk that changes in
their organization or development context could invalidate the predictive
power of the model they spent so much time and effort creating and
calibrating.

Faced with these barriers, it is no wonder that many practitioners find it
daunting to apply measurement ``best practices'' to their own
situation. Indeed, the agile community argues against model-based metrics
applications, promoting much ``softer'' metrics for decision-making.

An intriguing alternative method for measurement is software project
telemetry.

\Section {Software Project Telemetry}

According to Encyclopedia Brittanica, telemetry is a ``highly automated
communications process by which measurements are made and other data
collected at remote or inaccessible points and transmitted to receiving
equipment for monitoring, display, and recording.''  Perhaps the best known
example of telemetry is NASA's Mission Control Center in Houston, in which
telemetry data has been used since 1965 to monitor the Gemini, Apollo,
SkyLab, and Space Shuttle flights.  At Mission Control, dozens of
specialists monitor streams of data sent from a space vehicle, watching for
anomalous readings in one or more sensors, and making incremental course or
mission adjustments based upon the current state of the mission as
reflected by this telemetry.

Since 2001, we have been designing, implementing, and evaluating
infrastructure to support a telemetry-based approach to empirically-guided
software project management. Rather than measure past projects and hope
future projects are similar, software project telemetry uses measures from
the current project for decision-making.  Rather than build a model from
old projects to predict the exact threshold of allowable complexity for a
new class, telemetry indicates whether complexity is increasing,
decreasing, or staying the same as the current project progresses, and
allows the managers to check whether or not complexity is co-varying with
defects.

We define "software project telemetry" as a style of software metrics
definition, collection, and analysis with the following three essential
properties:

\begin{enumerate}

\item {\em Software project telemetry data is collected automatically by tools
   that are unobtrusively monitoring some form of state in the project
   development environment.}  In other words, the software developers are
   working in a ``remote or inaccessable location'' from the perspective of
   metrics collection activities. This contrasts with software metrics data
   that requires human intervention or developer effort to collect, such as
   PSP/TSP metrics.
        
\item {\em Software project telemetry data consists of a stream of time-stamped
   events, where the time-stamp is significant for analysis.} Software
   project telemetry data is thus focused on evolutionary processes in
   development.  This contrasts, for example, with Cocomo, where the time
   at which the calibration data was collected about the organization is
   not particularly significant (or, at least, the time scale of
   significance would be measured in years or decades, as opposed to
   minutes, days or weeks.)
        
\item {\em Software project telemetry is useful for in-process, local
   monitoring, control, and short-term prediction.} Telemetry analyses
   provide representations of current project state and how it is changing
   at the grain sizes of days, weeks, or months.  Control thresholds, if
   set correctly, can enable us to differentiate between random and
   non-random variation.  The simultaneous display of multiple project
   state values and how they change over the same time periods allow
   opportunistic analyses---the emergent knowledge that one state variable
   appears to co-vary with another in the context of the current project.

\end{enumerate}

We have identified several forms of software project telemetry.  {\em
Developer telemetry} is data gathered by observing the behavior of
developers, and includes information about the files they edit, the time
they spend, the changes they make to code, the sequences of tool or command
invocations, and so forth. Developer telemetry can be gathered by attaching
sensors to editors, such as Eclipse or Emacs, to configuration management
tools such as CVS, to code review tools, and so forth.  {\em Build
telemetry} is data gathered by observing the results of tools invoked to
compile, link, and test the system. Build telemetry can be gathered from
build tools like Ant or Make, testing tools like JUnit, and so forth.  {\em
Execution telemetry} is data gathered by observing the behavior of the
system as it executes. Execution telemetry can be gathered by instrumenting
the run-time environment of the system to collect data about its internal
state (heap size, occurrence of exceptions, etc.) as well as by tools that
perform load or stress testing of the system, such as JMeter.





\Section{Software Project Telemetry in Action}

Examples of Software Project Telemetry.

\Section{Hackystat: Infrastructure support for Software Telemetry}

Explanation of hackystat, issues that need to be solved, etc.

\Section{Future directions}

Does software project telemetry provide a ``silver bullet'' that solves the
problem of using metrics for software project management? Of course not.
While software project telemetry does address certain problems inherent in
traditional measurement, and provide new support for a more local,
in-process level of decision-making, it raises some new barriers to
adoption that future research must address.

First, the decision-making value of telemetry data is only as good as the
quality and diversity of data that can be obtained by sensors.  If the
project does not have a defect tracking database, then the utility of the
remaining telemetry data is reduced.  If the project also does not do
automated daily (or continuous) builds, then the utility of telemetry is
reduced further.   


   1. What are useful kinds of software telemetry data? Utility involves
   two dimensions: the effectiveness of the data in supporting software
   development, and its generality.  Effectiveness is measured by the
   ability of the data to help managers and developers identify positive
   changes in development. Generality is measured by the constraints that
   the type of data places on the development environment and approach. For
   example, a telemetry stream that monitors the success or failure of a
   daily build imposes the constraint that the development project do
   builds on a daily basis.
       
   2. How can software telemetry data be made usable? One thing is
   abundantly clear about software telemetry data: it is easy to collect an
   overwhelming amount of it.  To make it usable, we must determine
   appropriate ways to abstract raw telemetry data into higher-level
   representations, including synthetic measures that combine multiple raw
   data streams. We must also develop methods (perhaps based upon control
   charts) to separate normal from unusual telemetry data values. In
   addition, we must determine the appropriate grain-size for different
   telemetry streams.  Some telemetry analyses may be most appropriate when
   sampled on a daily basis, others might require a grain-size of a week or
   a month in order to effectively distinguish signal from noise. Usability
   of telemetry data also includes the design of presentation and
   notification strategies.  What form of visual presentation enables
   software developers and management to interpret software telemetry data
   appropriately?  When is it appropriate for the system to generate a
   notification about telemetry data, and to whom should the notification
   be sent?

Availability, future work.


\bibliographystyle{/export/home/csdl/tex/icse2003/latex8}
\bibliography{/export/home/csdl/techreports/04-11/04-11,/export/home/csdl/bib/csdl-trs,/export/home/csdl/bib/hackystat,/export/home/csdl/bib/psp}
\end{document}
 










