%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 04-14-intro.tex -- Thesis white paper - software inspections
%% Author          : Aaron A. Kagawa
%% Created On      : Mon Sep 23 11:52:28 2004
%% Last Modified By: Aaron Kagawa
%% Last Modified On: Sat Nov 13 15:28:32 2004
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 2004 Aaron A. Kagawa
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 


\Section{Introduction}
\label{sec:intro}
The use of software inspections has reported outstanding results in
improved productivity and quality. In fact, one study has found that if the
inspection process is followed correctly, then up to 95 percent of defects
can be removed before entering the testing phase \cite{Bush90}.
Inspections have been so successful that it is likely to be the closest
thing we have to a ``silver bullet'' for improving software quality.

%%Since the introduction of inspections by Michael Fagan, there have been
%%numerous views, descriptions, and process that have been proposed by
%%various authors and researchers. Some of these include; Fagan Inspections
%%\cite{Fagan76}, Software Inspections \cite{Gilb93}, High-Impact
%%Inspections, and Phased Inspections just to name a few. Throughout this
%%paper I will use the lower cased inspection to represent all inspection
%%techniques.

In another success story, the Jet Propulsion Laboratory adopted inspections
to identify defects and experienced a savings of 7.5 million dollars by
conducting 300 inspections \cite{Bush90a}. This statistic is very
impressive, however what is not usually emphasized is that each inspection
had an average cost of 28 hours. Using that average cost, the total cost
for JPL's inspection process was 8,400 hours or roughly 4 years of work.
This illustrates a fundamental problem with inspections; better results
come from greater investment \cite{Gilb93}.

Not all organizations have the time or the money to invest in full or
complete inspections. In most cases, organizations have limited funds or
resources that can be devoted to inspections. For example, a manager can
devote 200 hours of a project schedule for inspections. These organizations
must pick and choose what documents to use those precious resources on.
This realistic management of inspections directly contradicts the classical
inspection adage of ``when a document is ready you should inspect it''. The
bottom line is that most organizations cannot inspect every document.

The correct inspection process begins with the initiation phase, or
sometimes called the planning stage, in which authors volunteer their
documents for inspection \cite {Gilb93}. The inspection leader then checks
the document against entry criteria to determine if the document is ready
for inspection \cite{Ebenau94} \cite{Gilb93}. Again this process works very
well for organizations, like JPL, that have the resources to inspect every
document that is ready. However, I believe that this phase of inspection is
a major problem for organizations that do not have the necessary resources,
because the process does not consider that some documents are ``better'' to
inspect than others. A simple illustration of this fact is that 80 percent
of defects come from 20 percent of the modules \cite{Boehm01}.  Thus,
volunteering a document from that 20 percent will likely be ``more in need
of inspection'' than in any other module.

Furthermore, the current literature \cite{Ebenau94} \cite{Wiegers02}
\cite{Gilb93} on inspections does not provide any specific insights into
the trade offs between inspecting some documents and not inspecting others.
However, Tom Gilb provides two recommendations when inpsepction resources
are limited; sampling and emphasizing inspecting up-stream documents
\cite{Gilb93}. The use of sampling involves inspecting various areas of a
system to identify areas of interest. Up-Stream documents are documents
that define high-level requirements or designs. The idea is that at the
very least one should ensure that high-level documents are of high quality.
Although, these are very useful recommendations, they do not provide much
specific guidance of how best to use limited resources. At the end of the
day, an organization with limited inspection resources must select
documents to inspect.

The goal of this research is to optimize the selection of documents for
inspection. To do this I will create a Hackystat extension that will
determine what packages are in ``most need of inspection'' versus packages
that are in ``least need of inspection''. There are several research
questions that I must answer in order to make that determination. The most
important question is the operational definition of the general terms
``most need'' and ``least need''. What software attributes can quantifiably
distinguish between ``most'' and ``least'' need of inspection? In order to
create a definition we must understand the motivation for inspections.

Software inspection has two primary goals; increase quality and
productivity. For this research I am primarily concerned with increasing
quality. The successful inspection of a document has two main results:
finding defects which, once removed, increases software quality or not
finding defects thus indicating high software quality. Software quality is
vaguely defined as ``the degree to which software possesses a desired
combination of attributes'' \cite{IEEEGlossary83}. Some of the possible
attributes can include: portability, reliability, efficiency, usability,
testability, understandability, and modifiability \cite{Glass03}. Some
other widely accepted measures of quality include defect density and
complexity.  Whatever definition used for quality, inspections aim to
increase or validate the level of quality in software. Therefore, I would
claim that the same attributes defining software quality also provide good
indications of what code to inspect. For example, finding code that has low
portability, reliability, efficiency, usability, testability,
understandability, and modifiability would be a good indication of code
that would be beneficial to inspect.

My thesis claims are as follows: 
\begin{enumerate}
\item The attributes that define software quality provide good indications
  of what code to inspect.
\item Code that represents ``high'' software quality will have a low number
  of defects found in inspection.
\item Code that represents ``low'' software quality will have a high number
  of defects found in inspection.
\end{enumerate}


