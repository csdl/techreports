
The software industry and academia believe that software review,
specifically Formal Techinical Review (FTR), is a powerful method for
improving the quality of software. Computer support for FTR reduces the
overhead of conducting reviews for reviewers and managers.  This reduction
in overhead increases the likelyhood that software development
organizations will adopt FTR.  Computer support of FTR also allow for the
easy collection of empirical measurement of process and products of
software review.  These measurements allow researchers or reviewers to gain
valuable insights into the review process.  With these measurments
reviewers can also derive a simple measure of review efficiency.  A very
natural process improvment goal might be to improve the numerical value of
review efficiency over time.

My research group, the Collaborative Software Development Laboratory
(CSDL), developed a computer suported review system called Collaborative
Software Review System (CSRS)\cite{csdl-95-08}. CSRS is a computer
supported software review system implemented in UNIX with an Emacs
front-end.  It is fully instrumented and automatically records the effort
of the participants of the review.  The moderator of a review using CSRS
may define and implement their own review process.  Dr. Tjahjono and Dr.
Johnson used CSRS to investigate the effectiveness of group meetings in
FTR\cite{csdl-96-14}.  

After looking closely at review metrics we started to worry about {\em
  measurement dysfunction\cite{Austin96}} and reviews.  Measurement
dysfunction is a situation in which the act of measurement affects the
organization in a counter-productive fashion, which leads to results
directly counter to those intended by the organization for the measurement.
Some of the different types of measurement dysfunction that can occur in
software review are defect severity inflation/deflation, defect density
inflation/deflation, artificial defect closure, and reviewer preparation
time inflation/deflation.  Reviewers or project managers may feel pressure
to report good defect severity or density levels.  They can missreport the
severity or number of defects to affect the metric reported to management.
I have witnessed defect severity reduction in industry.  An organization's
policy was a project could not pass any milestone with any open defects of
critical severity.  The project manager talked to the developers and got
them to reclassify all the critical defects as high so the project could
pass the milestone on date.  Since all the critcal defects were now high
they did not get fixed first.  This reduced the quality of the overall
product.

How can we reduce the threat of measurement dysfunction in software review
without loosing the benefits of metrics collection?  Project LEAP,
(\url{http://csdl.ics.hawaii.edu/Research/LEAP/LEAP.html}), is our attempt
at to answer this question.  It investigates the design, implementation,
and evaluation of tools and methods for empirically-based, individualized
software developer improvement.  A major factor in developer improvement is
receiving external reviews of products. I developed the Leap Toolkit, a
Java based personal process improvement tool that supports distributed
review of documents.  It also support the generation of review and process
checklists and patterns.

The Leap toolkit gives the developer or reviewer full control on the
information they share with others.  In small groups they trust, developers
can share all their data.  In larger groups that the developer does not
trust they have three options: sharing the data they are comfortable with,
such as checklists and patterns, obfuscating their data, or falsifying
their data.  The Leap Toolkit give the developer complete control over the
data they share with others.  It will also provide an obfuscater.  Once the
data is obfuscated the identity of the developer or reviewer is removed.
The last option, falsifying data, is possible since Leap shows the user
exactly what will be sent and allows the user to change any entry.

We are evaluating project Leap in two ways.  First, we are introducing the
Leap Toolkit to industry and academia.  The adoption of the Leap toolkit in
industry will be an interesting case study.  The design features that
reduce measurement dysfunction allow the users to produce ``good'' answers
to management.  Our case study may show the difference between the real
answers and the answers that management sees.

Second, we are building the Leap data obfuscater and Web based shared data
repository.  The Leap data obfuscater removes all the identifying
information from Leap data files, yet retains the accuracy of the data.  We 
hope that Leap users will upload their obfuscated data files to our web
data repository and share their insights into review and development.  We
will have a repository of checklists and patterns for software development.