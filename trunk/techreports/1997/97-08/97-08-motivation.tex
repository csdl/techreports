%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 97-08-motivation.tex -- 
%% Author          : Philip Johnson
%% Created On      : Thu Nov  6 07:55:19 1997
%% Last Modified By: Philip Johnson
%% Last Modified On: Thu Apr 30 13:45:29 1998
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1997 Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 


\section{Motivation}

This proposal for Project LEAP arises from our experiences with research and
practice of formal technical review methods, affiliations with industry
software quality improvement groups, and research and practice of
individual improvement methods such as the Personal Software Process \cite{Humphrey95}.
These experiences lead us to the following observations:

\paragraph{FTR best practice is ``heavyweight'' and difficult to adopt.} 
Current industry best practice with respect to formal technical review
emphasizes management commitment, extensive training, detailed measurement,
structured data entry forms, prescribed roles for participants, and
precisely specified and constrained phases and constraints on interactions
\cite{Freedman90,Gilb93,Strauss94,Wheeler96}.  As a concrete example,
industry best practice suggests that review meetings should focus on issue
recording and not ``digress'' into issue resolution.  We term such precisely
specified and constrained methods ``heavyweight'', since they require a
large and ongoing investment of time, energy, and oversight of both
developers and managers to adopt as well as to maintain over time.

Our close affiliation with SQA leaders at many industrial organizations,
interviews with industry managers and developers, and first-hand
observation of industrial review practice have convinced us of two things:
(1) successful adoption of current review best practice does improve the
efficiency and effectiveness of review, and (2) current review best
practice is tremendously difficult to successfully adopt.  In addition to
our own observation, we conducted an informal poll using the internet
newsgroup comp.software-eng requesting information on review practice from
the respondant's organization.  Although the results are not statistically
meaningful, it is still interesting to note that approximately 80\% of the
respondants either did not use formal review practice or had tried and
abandoned it.


\paragraph{Software review is susceptible to measurement dysfunction.} 

Recent research by Robert Austin investigates the phenomenon in software
development organizations of ``measurement dysfunction'': a situation in
which the act of measurement affects the organization in a
counter-productive fashion, and which leads to results directly counter to
those intended by the organization for the measurement \cite{Austin96}.
Such dysfunction occurs because many process measures have two potential
applications: (1) to provide {\em information} about the organization and
(2) to support {\em performance evaluation} of individuals and groups.
It is impossible for an organization to guarantee that a
measure, once collected by the organization, will never be used for
performance evaluation. Thus, individuals in an organization may tend to
operate under the assumption that any measure may eventually be used for
performance evaluation, regardless of the stated intention of the
organization with respect to that measure at the time it is collected.
Austin's research provides examples of how this awareness can result in individual
and group behavior that produces good values of the measure for performance
evaluation, but undermines the measure's value for process information and
improvement.

Although most FTR training materials caution against the use of review
measures for individual performance evaluation
\cite{Gilb93,Freedman90,Strauss94}, Austin's research reveals that the {\em
  potential} for such use is sufficient to induce measurement dysfunction,
regardless of whether the organization ever actually uses the measures in
that fashion.  Our involvement with industrial development groups have
provided us with first-hand experiences of measurement dysfunction. For
example, at one company, defects discovered during design review are
routinely classified as ``enhancements'', because recording 
the defects as defects could cause them to miss an upcoming milestone,
with implications for subsequent performance evaluations.

Software review measurements are extremely susceptible to measurement
dysfunction.  We have catalogued many forms of FTR measurement dysfunction
induced to satisfy organizationally mandated goals \cite{csdl-96-16}.
Selected examples include: ``defect severity inflation'', where minor
defects are reclassified as major to improve review effectiveness measures;
``artificial defect closure'', where defects are either reclassified as
enhancements or reclassified with a lower severity level to improve defect
closure rates; ``preparation time inflation'', where reviewer preparation
time is inflated to improve review quality measures; ``defect discovery
rate inflation'', where preparation time is deflated to improve review
efficiency measures, and ``defect duplication inflation'', where reviewers
compare notes and submit duplicate defects to improve their personal defect
discovery efficiency.


\paragraph{Software review technology neglects downstream support.} 

A variety of review support tools have been developed, including:
ICICLE \cite{Brothers90,Sembugamoorthy90}, Scrutiny \cite{Gintell92,Gintell93,Gintell94}, ASSIST \cite{Macdonald97},
Hypercode \cite{Perry96,Perpich97}, INSPEQ \cite{Knight91,Knight93}, CAIS
\cite{Mashayekhi93,Mashayekhi94}, review tools based upon electronic
meeting rooms,  and our own tool CSRS. All of them,
including CSRS, focus on ``upstream'' review issues---issues involving
the conduct or disposition of a single review---such as definition of
review processes, hypertext annotation of the work product, voting on issue
disposition, and synchronous or asynchronous communication among developers
as they carry out preparation or review meeting activities.

None of these tools provide explicit support for ``downstream'' review
issues---issues involving the analysis of results from multiple reviews in
order to better understand the impact of reviews on the development process
and to discover potential process improvements. Although researchers may
argue that such analysis is more properly done as part of general process
improvement analysis and activities, our own industry experience leads us
to conclude that if review technology does not incorporate explicit, easy
to use mechanisms for analysis of review data, that analysis will simply
not be done. We know of one industry site where over five years of detailed
review data has never been touched by analysis.

\paragraph{Top-down process improvement is difficult to adopt.} 

Top-down process improvement initiatives are those resulting from a high
level management decision to invest time, resources, and training to
improve development practice through some combination of changes to the
organization's structure, staff, practices, and/or development
environments. Examples of top-down process improvement include Motorola's
``Six Sigma'' initiative and the Capability Maturity Model \cite{Paulk95}.  

While successes of these initiatives are widely publicized, problems with
their adoption are becoming more visible as well.  For example, Yourdon
exhorts software project managers to rebel against the ``Methodology'',
``Metrics'' and ``Process police'' in their organizations \cite{Yourdon97}.
Tom DeMarco likewise portrays top-down initiatives like the Capability
Maturity Model in an unfavorable light \cite{DeMarco97}.  

One structural problem with many top-down process improvement initiatives
is that the direct benefit to a given individual may be deferred, missing,
or even negative. For example, design staff might be required by management
to insert formal technical reviews into their schedule because reviews have
been shown to decrease test time, yet the design staff is not provided
extra schedule time or resources for review (because time-to-market is
critical and management wants to see coding begin as soon as possible). As
a result, individual designers incur new direct costs while the major
direct benefits accrue to the managers and testers. As another example,
top-down initiatives may require staff to fill out additional forms so
that the organization can assess process compliance, effectiveness, and
efficiency. By the time this data is analyzed, the staff who collected the
data have been reorganized and no longer a part of the group affected by
the analysis. Indeed, given the current level of churning in the industry, 
many developers can reasonably expect to have changed employers before
process improvements take effect.  Finally, top-down process improvement
data, like review data, is subject to measurement dysfunction.

\paragraph{Bottom-up process improvement has many benefits.}

An attractive alternative to top-down process improvement is bottom-up
process improvement, best exemplified by the Personal Software Process
developed by Watts Humphrey \cite{Humphrey95}.  In the Personal Software
Process, developers carefully collect data on the defects that they make
and the time they spend in each phase of development.  Over time,
developers build a ``personal database'' of measurements that they can use
to support design, planning, coding, testing, and process improvement
activities.  Humphrey's text provides an incremental introduction to
personal software process improvement that, with effort, can be
accomplished in one semester.  He has collected data from his
students at Carnegie-Mellon as well as from industry sites that appears to
show significant improvements in software quality and project planning
accuracy resulting from use of the PSP.

Bottom-up process improvement, such as the PSP, has many advantages
over top-down initiatives.. First, a ``PSP-enabled'' developer
does not need management approval or resources to maintain PSP data; it is
simply a part of her daily development practice.  Second, the investment
made by an individual into PSP data collection and analysis directly
benefits that individual.  Third, PSP data is ``portable'': a developer can
take PSP data with her as she moves within or across organizations. Fourth,
PSP data is not normally susceptible to measurement dysfunction since the
organization does not normally have access to PSP data.

We now have two years of direct experience using PSP both in a classroom
setting, where the principal investigator has taught PSP for three
semesters, and in a research setting, where PSP methods have been 
adapted to 
support not only our software development activities but student thesis
development as well.  Our results are mixed.  On the one hand, we
have found that bottom-up process improvement does have the clear advantages over
top-down initiatives noted above.  Evaluations of the course and the PSP
methods by students have been overwhelmingly positive. Our student data
appears, at first glance, to confirm all the claims made for the PSP.  On
the other hand, after gaining experience with teaching and use of the PSP,
we started to become suspicious about the quality of PSP data.
Did the data and analyses really reflect the practice?


\paragraph{Bottom-up process improvement can suffer from poor data quality.} 

To investigate this issue, we conducted a case study during a recent
semester while teaching PSP. Prior to the semester, and based upon our
experiences teaching PSP, we implemented three extensions to the standard PSP
curriculum in order to improve data quality. First, we added formal
technical reviews and checklists to each project. Prior to data submittal,
all PSP data was subject to a thirty minute group review. The
review followed a checklist (which was made available at the beginning of
the assignment) to check whether data and calculations were accurate.
Second, we distributed customized spreadsheets to each student to support
data analysis, and Java applications to calculate software size. Third, we 
added new forms to the process to clarify problem areas (such as selecting
the correct method for size estimation) found in previous semesters. 

During this semester we collected 90 PSP datasets from the students: 9
projects from 10 students. After the semester, we began a detailed
retrospective analysis. We implemented a database system to cross-check all
calculations and expose six classes of data defects. Next, all individual
data values from the 90 PSP datasets were entered into the system. Finally,
we ran our checks over the system.  Given our level of commitment to high
data quality during the semester, and the inclusion of additional
procedures designed specifically to support high data quality, we were
astounded by the results of this analysis.

\par


\begin{figure*}[h]
\begin{center}
\begin{tabular}{ll}
{\bf Frequency} & {\bf Type} \\
699 & Incorrect calculation \\
262 & Missing field \\
175 & Transfer of data between projects\\
146 & Entry error\\
100 & Transfer of data within a project\\
88  & Impossible values\\
\end{tabular} 
 \caption{{\em Summary statistics for error frequency and type in a PSP class}}
 \label{fig:psp-data}
\end{center}
\end{figure*}


\par

As Figure \ref{fig:psp-data} illustrates, we found that almost 1500 errors
in data collection or manipulation were made during the case study
semester.  (Our values are conservative, because there are other important
classes of errors, such as failing to log a defect on the form, which we
could not detect through our analyses.)  Furthermore, we did not see a
sharp drop-off in errors as the semester proceeded, which might indicate
that the errors were mostly an artifact of the learning process and would
not persist into the student's professional environment. The errors were
distributed between analysis and recording and across all students in the
class. 

Although this data is preliminary and our analyses are still ongoing, this
study clearly calls into question the quality of bottom-up process data, at
least when collected and analyzed in the manual fashion used by the PSP.
Although we are sure that engineers do exist who can flawlessly record,
analyze, and interpret their personal process data in a manual fashion, we
believe that the required time, energy, and attention to detail imposes
severe burdens on more typical engineering staff.  We conjecture that a
radical simplification of data collection and analysis must occur before
personal process data can be effectively and accurately manipulated by a
large segment of the development community.












