%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 97-08-proposed.tex -- 
%% Author          : Philip Johnson
%% Created On      : Thu Nov  6 12:29:19 1997
%% Last Modified By: Philip Johnson
%% Last Modified On: Thu Apr 30 13:46:32 1998
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1997 Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 



\section{Proposed Research}

As discussed above, our prior research raises concerns regarding
heavyweight review methods, the possibility of measurement dysfunction, the
lack of downstream analysis support, the need for extensive management
commitment to and investment in top-down process improvement, and the data
quality problems present in otherwise promising bottom-up process
improvement approaches like the PSP.  The four principal requirements
underlying Project LEAP's approach to software developer improvement derive
directly from these concerns.

\begin{itemize}
  
\item {\bf L}ightweight.  Project LEAP methods must be ``lightweight''.
  In other words, they must involve a minimum of process constraints, be
  easy to learn, easy to integrate with existing methods and tools, and
  require minimal investment and commitment from management. If a LEAP
  method imposes new overhead on the developer, then that effort must yield
  a short-term, positive return-on-investment to that same developer.
  
\item {\bf E}mpirical. Project LEAP must have a quantitative, as well as
  qualitative dimension.  A lightweight orientation cannot be gained at the
  expense of high quality collection and analysis of data. Developer
  improvement should be observable over time through measurements of
  effort, defects, size, and so forth.
  
\item {\bf A}nti-measurement dysfunction.   Measurement, while an
  integral part of Project LEAP, must be carefully designed to minimize
  dysfunction.  Yet the most simple solution to dysfunction, making all
  data totally private, is incompatible with the benefits to the
  organization of sharing certain kinds of data and process
improvements. Project LEAP is designed to find a
  suitable balance between totally public and totally private measurement data.
  
\item {\bf P}ortable. 
  Useful developer improvement support cannot be locked to a particular
  organization such that the developer must ``give up'' the data and tools
  when they leave the organization. Rather, this support should be akin to a
  developer's address book; a kind of ``personal information assistant''
  for their software engineering skill set that they can take with them
  across projects and companies.

\end{itemize}

These four criteria, when composed together, create additional
requirements. For example, we hypothesize that extensive automation is
required within any method that is simultaneously lightweight, empirical,
and anti-measurement dysfunction.  On the other hand, automation clearly
does not guarantee lightweight processes or meaningful empirical evidence
of improvement. As an example, a repeated criticism of the CSRS automated
review system was that its extensive measurement system would lead to
dysfunctional behavior in an industrial setting.

These four criteria, when composed together, preclude existing
approaches to bottom-up or top-down process and/or software quality
improvement. Approaches such as CMM or ISO 9000 are empirical, but not
lightweight or portable and they are prone to measurement dysfunction. The
PSP is portable and not susceptible to measurement dysfunction since there
is no necessity that PSP data be shared. (In fact, Humphrey recommends
against sharing PSP data, at least initially.)  As the results of our case
study revealed, however, PSP data is highly susceptible to low data
quality, and our own experience teaching and using it indicate that it is
most definitely not lightweight.  Finally, formal technical review is
empirical, but neither lightweight, anti-measurement dysfunction, nor
portable.

The above discussion may leave the impression that satisfying the LEAP
criteria is impossible. Actually, we conjecture that many LEAP-compliant
methods for software developer improvement can be designed, once these
requirements and their implications are understood. The next section
presents an overview of one such method and associated tool support for
defect entry and analysis under active development in our research group.
It represents an appealing synthesis of our previous research on Formal
Technical Review and the Personal Software Process.

\subsection*{A LEAP-compliant method and toolset for Defect Entry and Analysis}

A fundamental premise of both formal technical review and the Personal
Software Process is that collection and analysis of defect data can provide
useful insight into software and process quality.  In FTR, defect data is
generated through the combined efforts of a group of developers, using a
structured process to generate a composite defect report which the author
uses as the basis for rework activities.  In the PSP, defect data is
primarily recorded by the author as she discovers defects in her own work
products. In the past, we have collected defect data through FTR and PSP
and analyzed the data separately according to these two paradigms.
Currently, we are deploying an early release LEAP-compliant method and toolset
within our research group that supports both of these modes. This toolset
is written in Java using COTS components for tables and graphing and runs
on all major platforms.  The toolset supports a simple four stage method
for defect processing: (1) Entry, (2) Distribution, (3) Rework, and (4) Analysis.

In our example method, two tools are used to radically simplify most
aspects of defect collection, analysis, and their application to software
developer improvement, whether the user is participating in a group review or in
PSP-style individual defect collection.  The first tool is called LeapDet
(for LEAP Defect Entry Tool). Figure \ref{fig:leapdet} shows a screen image
from this tool.  The second tool is called LeapDat (for LEAP Defect
Analysis Tool), and is illustrated in Figure \ref{fig:leapdat}.\footnote{Although this toolset is still in an early stage of development,
  it is functional and has already been installed at one industrial
  site for evaluation. Though not yet mature enough for
  public release, interested readers of this proposal are welcome to
  examine the toolset, installation instructions, and user guide at
  ftp://ftp.ics.hawaii.edu/pub/csdl/leap.}

\begin{figure*}[t]
{\centerline {\psfig{figure=leapdet1.ps}}}
 \caption{{\em A LEAP-compliant defect entry tool. The system enables
 reviewers to efficiently raise issues about work products, distribute
 the results to other members of the review team, and save the data for
 later empirical analysis using the Leap Defect Analysis Tool.}}
 \label{fig:leapdet}
\end{figure*}

LeapDet supports review of arbitrary document types by focusing on high
efficiency entry and distribution of structured defect data between members
of a group.  To support as wide a range of formal and informal review
methods as possible, LeapDet makes no assumptions about the actual
review process in use in the organization. All fields are optional, and the
tool can be easily configured to provide the defect entry data fields
required by a given organization. 

Rather than focusing on process representation as is done in other review
tools, this toolset concentrates on support for defect representation.
Upon invocation, both tools read configuration files that declare the
projects, document types, defect types, and document IDs currently defined
within the organization.  Figure \ref{fig:leapdef} shows examples of the
four major declarations provided in this toolset. (Our sample leap
definition file comes with example declarations for six project types, over
100 defect types, and several Project and Document IDs.)

\begin{figure*}[t]
\small
\begin{verbatim}
!        NAME           COMMENT
Project: "CSDL"         "CSDL research projects"
!        NAME           COMMENT
DocType: "Java Source"  "Java source code"
!        DOCTYPE        NAME                    COMMENT         
DefType: "Java Source"  "10: Syntax"            "General syntax errors"
!        NAME               SIZE   COMMENT              DOCTYPE         PROJECT
DocID:  "FileCheck 1.0.2"   658   "File Syntax Checker" "Java Source"   "CSDL"
\end{verbatim}
\normalsize
 \caption{{\em Example toolset declarations. Lines beginning with '!' are
 comments and are included as documentation for the fields in the next
 line's declaration. These declarations are used to improve the efficiency
and effectiveness of both the Leap Defect Entry tool and the Leap Defect Analysis Tool. }}
 \label{fig:leapdef}
\end{figure*}

While using the toolset, individuals and development groups
incrementally build and refine this representation of the set of projects
they have worked on, the types of documents they have produced, the types
of defects that have occurred for each document type, and the work product
instances for which defects have been detected.

Explicit declaration of projects, defect types, document types, and
document IDs facilitate review and process improvement in three important
ways.  First, it enables each reviewer to work more efficiently.
Information about the document under review (the top line in Figure
\ref{fig:leapdet}) is automatically specified via a common .config file
provided at the start of review. In addition, pop-up menus allow users to
mouse-select structured information such as defect types rather than type
them by hand, although users are always free to invent new defect types on
the fly.  The goal is to minimize or eliminate manual entry of information
whenever possible.  Second, explicit representation of
project, document type, and defect type information facilitates
consistent use of this information among developers. A
development group will typically maintain one or more public Leapdef files with common
or group-wide definitions of projects, document types, and defect types,
while still allowing individual to add personal extensions through
additional private Leapdef files. Third, the declarations support downstream
analysis. Effective defect data analysis requires the ability to categorize
defects based upon the document type and defect type.  Explicit declaration
enables users to effectively analyze data across projects and from
different reviewers.

As groups and users evolve, so too will the set of projects, defect types,
document types, and document IDs. Clearly, the addition of new declarations
poses no problem to the toolset. However, users will also need to modify
existing declarations, such as splitting one document type (``Java
Source'') into subtypes (``Java GUI Source'', ``Java Network Source'',
``Java File I/O Source'', etc.).  We have found that the set of defect types is
particularly volatile when a user first begins collecting this information,
and major reorganizations occur frequently.  The toolset is designed
to accommodate such evolution.  While the declared types facilitate
organization and analysis, it is not syntactically or semantically illegal
for non-declared types to occur within data files. In addition, filtering
functions allow users to find those defects with obsolete type information
and update them to the current set of declarations.  More sophisticated
support for evolution is possible, but we have not yet found it necessary.


\begin{figure*}[t]
{\centerline {\psfig{figure=leapdat1.ps}}}
 \caption{{\em A LEAP-compliant defect analysis tool. This tool allows
 a work product author to display, sort, and annotate review comments sent via email from
 other reviewers. Analysis options allow developers to enter defect data
 gathered over multiple projects and perform various analyses to support
 software developer improvement.}}
 \label{fig:leapdat}
\end{figure*}

Efficient entry of defect information is the first step toward LEAP
compliance. The second step is efficient distribution.  In LeapDet, users
can quickly email defect commentary to others at any time via a button
which brings up a dialog box requesting the To: field and a brief comment.
The defects entered are then sent to all the recipients as a MIME-encoded
file which can be decoded by any standard mailer.  By default, LeapDet
automatically assigns a file name consisting of the document ID followed by
an eight character time-stamp (DDHHMMSS).  The time-stamped filename
minimizes the possibility of duplicate file names, while still keeping them
together within a directory.  So that users can easily distribute an
updated version that is intended to replace a previous version, this
time-stamped filename is generated the first time a set of defect data is
emailed or saved to a file and then persists through all subsequent
updates.

The design of this mechanism was influenced by our previous experience in
CSRS, where we implemented a much more sophisticated, heavyweight,
object-oriented persistent store with socket-based notification services.
We found that the sophistication of data storage and distribution posed a
barrier to industrial adoption of this technology. We expect that 
the current  file and email-based approach to storage and
distribution will remove this barrier to adoption.



After defects have been entered and distributed, the immediate next step is
to collate together all of the individual defect files, examine the
resulting aggregate set of defects, and use the combined data to improve
the work product.  The LeapDat (LEAP Defect Analysis Tool) supports this
process.  LeapDat allows the user to load multiple defect files and sort
them according to location, severity, number of occurrences, severity, and
type.  Defect data can be added, deleted, or modified at this time. New
fields are provided that enable the author to add information about the
validity and fix time associated with each defect, if desired.  

Figure \ref{fig:leapdat} illustrates LeapDat after the user has loaded in
three files containing defect data sent by reviewers.  The LeapDat
interface is divided into three sections. The top section provides a
listing of all currently declared projects, document types, and document
IDs. It also lists all of the files containing defect data currently loaded
into the system. Since the amount of defect data maintained by a developer
can be large, this top section allows the user to filter the defect data
displayed by selecting one or more projects, document types, and document
IDs to display.  The middle section contains a table-based interface to
individual defect entries. The filtered defect data appearing in this table
can be sorted by type, severity, fix time, location, or other attributes.

This toolset is designed to improve the efficiency and effectiveness
of both review methods with meetings and those without meetings. If a
review meeting is used, then LeapDat can be used to interactively display
the results of preparation using a projector. The moderator can sort the 
defects collected prior to the meeting by importance or by location and
guide the meeting discussion appropriately.  The scribe can enter changes
or new defects discovered during the meeting directly into LeapDat. The 
outcome of the meeting can be distributed either electronically or as an
HTML document generated automatically via LeapDat. If a review meeting is
not used, then the author alone would use LeapDat to organize the review
results. 

\begin{figure*}[t]
{\centerline {\psfig{figure=freqchart2.ps}}}
 \caption{{\em A defect data analysis, showing defect frequencies for
                one document type (Java Source), categorized by defect
                type. Two defect types, File I/O and String, account for
                the vast majority of this user's defects.}}
 \label{fig:freqchart}
\end{figure*}

The bottom section of the LeapDat interface provides a set of built-in
analysis mechanisms for historical defect data. To our knowledge, no other
review tool exists with this explicit support for analyzing historical
data.  While we intend this toolset to add value to conventional review
processes by improving the efficiency of defect data entry, distribution,
and rework, a more fundamental benefit from the standpoint of this research
is this support for analysis of defect data, both in aggregate and as
trends over time.

The current toolset is designed to analyze three basic measures:
defect density, defect frequency, and fix time and provide data about these
measures over a series of projects, stratified by defect type, and as
trends over time.  Figure \ref{fig:freqchart} shows the analysis obtained
by pressing the LeapDat button marked ``Frequency'' in the ``per Defect
Type'' column for one user.  This chart shows that two defect types, File
I/O and String, account for most of the errors found in documents of type
``Java Source'' by this user.  Armed with this knowledge, the user employs the
filtering functions of LeapDat to display only those defect entries and
look for process improvements that can reduce the frequency of these
defects in the future. For example, if further examination revealed to the
user that end-of-file errors occur frequently, then a possible remedy would
be the use of a checklist that focuses her attention on this issue.
Perhaps the user discovers that the erroneous use of the ``=='' with string
objects causes many errors. In this case, the introduction of a lint-type
tool might improve the developer's efficiency.

One final LeapDat analysis feature is worth noting. We encourage users to
distribute work product defect data not only to the work product authors,
but also to the other reviewers.  If this is done, then reviewers can
compare the defect data generated by their own review to the defect data
generated by other reviewers, and determine what defects they missed during
the review. Such feedback can help reviewers improve their own reviewing
capability by exposing ``blind spots''. The feedback also indicates likely
defects the reviewer might make when they create work products of that
type, and thus support developer as well as reviewer improvement.  No other
review technology includes this empirical support for reviewer improvement
based upon comparative analysis of defect data. 

To conclude this example of LEAP-compliant support, we return to the four
principles and show how this example toolset and method satisfy them:

\begin{itemize}
  
\item {\bf L}ightweight.  This method and toolset is designed to be easily
  adopted by any individuals or groups who have an interest in defect
  entry, distribution, rework, or analysis. On-line, context sensitive help
  enables ``just-in-time'' learning for most of the tool's capabilities,
  and initial training and installation is minimal. The tools and methods
  complement existing approaches to review and defect collection, so users
  can leverage any prior training in review or PSP, though this training is
  not a prerequisite to effective use of the tool. Finally, extensive
  buy-in from either management or even fellow development team members is
  not necessary for individuals to obtain value from the tool. In the 
  worst case, a developer could generate defect data through self-review.

  
\item {\bf E}mpirical. This method and toolset is clearly empirical in
  nature, collecting data on defect types, their frequency, and other
  properties and providing built-in analyses.  As analysis is entirely
  automated, the quality of data should be reasonably high. Of course, the
  quality of analysis ultimately depends upon the completeness of the
  data. This motivates the toolset's focus on simplifying defect entry,
  storage, and retrieval.

  
\item {\bf A}nti-measurement dysfunction.  This method and toolset explores
  a middle ground between PSP data, which is typically completely private
  (and thus not prone to measurement dysfunction), and FTR data, which is
  typically collected and analyzed by management (and is thus quite prone to
  measurement dysfunction \cite{csdl-96-16}). In this toolset, all defect
  data is distributed {\em anonymously}: once the file is decoded and
  separated from its email wrapper, no more identifying information about
  its author exists.\foot{This safeguard is not perfect. It is still 
  possible for management to decode the file once obtained, go into LeapDat, and add an annotation
  containing the author name to
  the description field of every single defect. By repeating this for every
  single defect, one could eventually build a profile of individual
  developer defects. However, this
  effort is large enough that virtually any manager would simply ask
  developers for copies of their analyses. At that point, if necessary,
developers could
  even generate a ``cooked book'' with dysfunctional defect data to
  satisfy management while still retaining an accurate copy for their own
  records.}
  
  As an additional precaution, although LeapDet enables reviewers to provide
  information about review time, this time-related information is omitted
  from the emailed data file. We have found that effort data is so susceptible
  to dysfunction that even sending anonymous effort data is too risky.
  
\item {\bf P}ortable.  This method and toolset is intended to be portable
  on several levels. First, it provides portability through its choice of
  Java as the implementation language, so it runs on all platforms. More
  interestingly, the use of declarations and data types allow users to take
  data with them as they move across organizations within a company or
  across companies. Finally, this feature enables the methods and
  tools to be useful for an individual working at multiple companies at the
  same time under a contractual or consultant arrangement. 

\end{itemize}


\section{Anticipated Results}

The previous section illustrated that LEAP-compliant tools and methods can
be designed by presenting a description of an early version of a toolset and
method for defect analysis.  In this section, we detail the research
directions we will pursue during the course of Project LEAP to gain greater
insight into the costs and benefits of the approach. We expect Project LEAP
to contribute to the current state of knowledge through the following five
major results:

\subsection*{Result 1: Validation of PSP data quality findings}

Our current data strongly indicates that manual approaches to PSP data
collection lead to serious data quality problems.  Our preliminary analysis
also indicates that appropriate automated support, similar to the Leap
toolset, could dramatically improve data quality. 

The first result of Project LEAP will be a thorough experimental
investigation of data quality issues in the PSP. To accomplish this, we
will conduct a controlled experiment over four semesters, where our current
data counts for semester 1. During Spring 1998, we will teach the PSP in an
advanced software engineering class and replicate the current study's
format, data collection, and analysis.  During Fall of 1998 and Spring of
1999, we will teach the class twice more, this time using appropriate
designed LEAP tool support for defect and time data collection and
analysis. We will then analyze our four datasets for differences in data
quality over the four semesters and for differences when the semesters are
combined into ``manual'' and ``automated'' treatments. Of course, we will
also be attentive to unexpected side-effects of the introduction of
automated technology.  

As we progress with this research, we will make the experimental design,
instruments, and protocols for the experiment available to our ISERN
(Internation Software Engineering Research Network) partners and to the
entire software engineering community. This could lead to additional
replication of the study, improving the generality of the results.


\subsection*{Result 2: Industry case studies of LEAP adoption}

A primary finding from our ongoing industrial collaborations is that
software review best practice and software review technology are
quite difficult to adopt.  Project LEAP is our response to this problem. We
hypothesize that lightweight, empirical, anti-measurement dysfunction, 
and portable methods should be substantially easier to deploy within
an organization.  

We will test this hypothesis through case studies at various industrial
sites. Currently, we have commitments from development groups at Tektronix,
Inc. and Digital Equipment Corporation to evaluate this technology.
(Indeed, a preliminary version of the toolset was already installed and
evaluated at Tektronix in early November, 1997.)

Clearly, the design of this case study must be carefully chosen so as to
not re-introduce the very measurement dysfunction we have tried so hard to
prevent!  Therefore, although it is tempting, we cannot ask developers to
provide us with their LEAP data files. We will, however, conduct structured
interviews with developers at regular intervals during the study to
evaluate the use of the tool.  With the developers' permission, we may be
able to obtain general empirical data, such as the number of LEAP data
files they have saved. However, we will rely primarily on qualitative
data for insights on the strengths and weaknesses of the LEAP methodology
and to enable us to partially test our hypothesis.

\subsection*{Result 3: Component-based infrastructure for LEAP developer improvement}

The current toolset is implemented as a suite of Java applications.  Over
the course of the study, we will continue to evolve and improve the initial
set of functionality demonstrated in this proposal. We will also extend the
toolset to support collection and analysis of time-related data.

As the toolset functionality begins to stabilize, we will first investigate
transition away from an application-based implementation and toward
a browser-based implementation.  The browser-based implementation
can simplify installation of the tool and support training
activities at the educational web site, to be discussed further below.

Our second investigation will be the decomposition of LEAP tools into
building blocks that can be integrated into other systems using component
technology such as Java Beans. The ultimate goal is to provide
infrastructure that enables integration of LEAP technology with any other
element of the software development environment, from code editors to
verification tools to GUI interface builders to change control systems.

As we did with the Egret and CSRS systems, we will make high quality,
well-documented releases of all of these technologies freely available to
the software engineering community through the Internet.

\subsection*{Result 4: Distance learning resources}

Although the LEAP tools are designed to be easy to adopt, they do not
eliminate all of the complexity associated with empirical software process
improvement.  Once developers begin to collect data, they will start to
have questions such as, ``What kinds of valid inferences can I make based
upon this data?'', or ``How should I collect future data in such a way as
to test the hypothesis I have generated from analysis of the current
data?''  To support developers in this process of empirically guided
self-discovery, we will develop a distance learning program that
will discuss these concepts.  We will employ browser-based versions
of the LEAP tools as an integral part of the learning process. For
example, we can provide the user with a browser interface to the
Leap defect analysis tool that is pre-loaded with defect data and 
use this data set to guide them through various empirical analyses.

We do not expect all of the information transmission to be one-way; rather we
expect the LEAP user community to contribute to this on-line resource
as well. For example, users can contribute defect type declaration
sets for new or existing document types. Users can also contribute
checklists, review methods integrating LEAP technology, or other 
process improvements they have found useful. 

\subsection*{Result 5: Standards for defect representation and analysis}

Finally, the LEAP toolset embodies a representation for defect and other
process representation and a set of associated analyses. We intend to work
with other groups to help develop a standard representation for this
information so that all tools manipulating this data can freely
interoperate.



      

