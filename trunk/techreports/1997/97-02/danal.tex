\subsection{Data}

Two sets of data are important to our study:  
the Individual Fault Summaries and the Team 
Fault Summaries. 


An individual fault summary shows whether a reviewer discovered
a particular fault. This data is gathered from the fault report forms
the reviewers completed during fault detection.

A team fault summary shows whether a team discovered a particular 
fault. For the PI and DC methods this data is gathered from the fault report 
forms filled out at the collection meetings. For the DD method the team 
summary is just the set union of faults recorded by all reviewers. This data
is used to assess the effectiveness of each fault detection method. 


One problem with the team summaries is that the DC and PI methods 
involve meetings, but the DD method doesn't. Consequently, 
any meeting losses -- faults discovered by individuals before 
the meeting that don't appear in the team fault report --
will lower the fault detection rate for these two methods
but not, of course, for the DD method.  However, 
since meeting losses average only
about 5\% for the DC method (we can't measure them for the PI method) 
we consider them to be insignificant.
One team's individual and team fault summaries are represented 
in Figure~\ref{fig:dvec}.
% and Figure~\ref{fig:dcru}. 

We also compare the benefits of meetings (DC method)
with the benefits of additional individual analysis (DD method).
For the DC method we examine the individual and the team summaries
to calculate meeting gains. For the DD method we examine the
individual summaries to determine whether faults are originally 
discovered during the first or the second detection activity.


%
%\begin{figure*}[tbp]
%\hrule
%\vskip 0.5em
%\centerline{\psfig{figure=cruise.data.eps,height=1.5in,width=5in}}
%\vskip 0.5em
%\caption{{\bf Data Collection for each CRUISE inspection.}
%This figure shows the data collected from one team's CRUISE inspection (using the DD inspection method). The data is identical 
%to that of the WLMS inspections except that the CRUISE has fewer faults --  
%26 for the CRUISE versus 42 for the WLMS.}
%\label{fig:dcru}
%\vskip 0.5em
%     \hrule
%\end{figure*}
%%

Our analysis is done in three steps: (1) We compared the 
team fault detection rates to ascertain whether the 
inspection methods have the same effectiveness. (2) We separately compared
the first and second round performance of individuals and teams 
to see how different treatments performed. (3) We 
analyzed the individual fault summaries determine whether different treatments 
found different faults.

\input{analysis1table}
\subsection{Analysis of Team Performance}

If our hypothesis that inspections without meetings are
no less than effective inspection with meetings
is true, we shouldn't find that the DD method's 
performances are significantly lower than 
those of the other two methods.
Our analysis strategy is to determine whether various 
threats to the experiment's internal validity can be seen.
And then to compare performances.

First we analyze each independent 
variable's contribution to the observed inspection performance data. 
Table \ref{anal1table} and Figure~\ref{fig:d1.all} show that 
Inspection Method and Specification 
are significant, but the Round, Order, and Team Composition are not.

Next, we analyzed the combined Instrumentation and 
Treatment effects. Table~\ref{tbl:analin} shows the 
input to this analysis. Six of the cells contain the average 
detection rate for teams using each inspection method and specification 
(3 detection methods applied to 2 specifications).  The results indicate that
the interaction between Method and Specification is not significant.
%
\begin{table*}[tbp]
%\hrule
%\vskip 0.5em
\centering
\begin{tabular}{|l|l|l|l|}
\cline{1-4}
\multicolumn{1}{|c|}{{\bf Specification}} & \multicolumn{3}{|c|}{\bf Inspection Method}\\
\cline{2-4}
  &\multicolumn{1}{|c|}{{\protect\bf PI}}   &\multicolumn{1}{|c|}{\protect\bf DC}    &\multicolumn{1}{|c|}{\protect\bf DD}\\
\cline{1-4}
\multicolumn{1}{|c|}{{\bf WLMS}}        &\multicolumn{1}{|c|}{ .38} &\multicolumn{1}{|c|}{ .29}     &\multicolumn{1}{|c|}{.62 .29 .43 .55}\\
\cline{1-4}
\multicolumn{1}{|c|}{(average)}&\multicolumn{1}{|c|}{.38}       &\multicolumn{1}{|c|}{.29}      &\multicolumn{1}{|c|}{.47}\\
\cline{1-4}
\multicolumn{1}{|c|}{{\bf Cruise}}&\multicolumn{1}{|c|}{.19 .12 .23 .077}       &\multicolumn{1}{|c|}{ .27 .19}       &\multicolumn{1}{|c|}{ .42}\\
\cline{1-4}
\multicolumn{1}{|c|}{(average)} &\multicolumn{1}{|c|}{.15}      &\multicolumn{1}{|c|}{.23}      &\multicolumn{1}{|c|}{.42}\\
\cline{1-4}
\end{tabular}
     \vskip 0.5em
\caption{{\bf Team Fault Detection Rate Data.} This table shows the
nominal and 
average fault detection rates for all 7 teams. There are only 13 
observations, however, since one team dropped out because of 
a team member's illness.}
\label{tbl:analin}
%\vskip 0.5em
%\hrule
\end{table*}

%\begin{table*}[tbp]
%\hrule
%\vskip 0.5em
%\centering
%\begin{tabular}{|l|c|c|c|c|c|c|}
%\hline
%{\hspace{.5 in} \bf Effect}       &$SS_{T}$       & $ \nu_{T} $
%&$SS_{R}$       &  $ \nu_{R} $  &$ ( SS_{T} / \nu_{T} ) ( \nu_{R} / SS_{R} )$& Significance \\
        %&       &       &       &       &       & Level \\
%\hline
%{\bf Detection Method} & .200  &2      & .212  &26     & 12.235 &$<.01$ \\
%\hline
%{\bf Specification} &.143  &1      &.212   &26     &17.556 &$<.01$ \\
%\hline
%{\bf Meth$\times$Spec} &.004  &2      &.212   &26     &.217 &.806 \\
%\hline
%\end{tabular}
%\caption{{\bf Analysis of Variance of Detection Method and Specification.}
%This table displays the results of an analysis of the variance of
%the average detection rates given in Table~\protect\ref{tbl:analin}.
%}
%\label{tbl:anvar}
%\vskip 0.5em
%\hrule
%\end{table*}


Finally, we compared the performance of each method. 
Table~\ref{tbl:analin} summarizes this data.  As depicted, 
the {DD} inspection method resulted in the
highest fault detection rates (46\%), followed by the {DC}
inspection method (23\%), and finally by the PI detection method (19\%).


%\begin{figure*}[tbp]
%\hrule
%\vskip 0.5em
%\centerline{\psfig{figure=allData.ps,height=3.5in,angle=-90}}
%\vskip 0.5em
%\caption{{\bf Fault detection by inspection method.} The observed
%fault detection rates are displayed above. The unfilled symbols indicate
%observations from Round 1; the filled symbols those from round 2. The
%vertical line through each point indicates one standard deviation in
%the rate's estimate (modeling fault detection as Bernoulli trials). The
%dashed (dotted) lines display the average detection rates for Round 1
%(Round 2).}
%\label{allData}
%\vskip 0.5em
%\hrule
%\end{figure*}

\subsection{Analysis of Second Phase Performance}

The previous analysis shows that in the current experiment, inspections
without meetings seem more effective than inspections with meetings.
In this section we examine whether the current data support 
our original hypothesis that meetingless inspections would
be at least as effective because the benefits of having a meeting wouldn't
outweigh the benefits of additional individual analysis. 

Our analysis strategy is to isolate first and second phase 
performances to see how well they explain differences in total inspection
performance. If the hypothesis is true we should see little difference
between the first phase performances of the DC and DD methods, but 
significant difference in the second phase performances.

\begin{figure*}[tbp]
%\hrule
%\vskip 0.5em
\centerline{\psfig{figure=individualPerf.ps,height=2.5in,angle=-90}}
\vskip 0.5em
\caption{{\bf Individual Fault Detection Rate (Phase 1).}
This figure shows the number faults discovered by each reviewer 
during the first phase of a DD or DC inspection.} 
\label{indPerf}
%\vskip 0.5em
%\hrule
\end{figure*}
%


Figure \ref{indPerf} contains a boxplot showing the
number of faults found by each reviewer during the 
first phase of the DD and DC inspections. The rates 
for both the WLMS and the CRUISE appear to be similar.

\begin{figure*}[tbp]
\hrule
\vskip 0.5em
\centerline{\psfig{figure=individualWOverlapPerf.ps,height=2.5in,angle=-90}}
\vskip 0.5em
\caption{{\bf Team Fault Detection Rate (Phase 1).}
This figure shows the number faults discovered by each team
during the first phase of a DD or DC inspection.}
\label{indWOverlap}
\vskip 0.5em
\hrule
\end{figure*}
%



Figure \ref{indWOverlap} shows the number of unique faults 
discovered by each team during the first phase of a DD or DC
inspection. This is the set union of 
faults found by all team members. For the WLMS the rates again 
appear to be similar, but for the CRUISE the data is inconclusive.



Figure \ref{secondPhase} shows the number of unique faults
discovered by each team during the second phase of a DC or DD 
inspection. For the WLMS the rates may be higher for the DD
method than for the  DC method, but the rates for the CRUISE
are again inconclusive.
 

\subsection{Analysis of Detection Rates for Specific Faults}

Even if inspections with meetings are less effective than 
inspections without meetings, team meetings might still be beneficial if
they promote detection of classes of faults that individuals seldom 
or never find.
This section analyzes the individual and team fault summaries
to determine whether the current data supports the hypothesis that
this is indeed the case. 

If this hypothesis is true, then there should be a subset of 
faults for which the probability of their being found by 
the DC and PI methods is greater than by the  DD method. 

For example, 9 faults in the WLMS had a higher detection probability  
with meetings than without. There were 3 such faults in the CRUISE.
We want to know whether a subset of these faults can be considered
``meeting sensitive''. 

If we knew which faults should be meeting sensitive, we could test 
whether they are so in this experiment. 
For example, if we had  
a classification scheme that could distinguish meeting sensitive 
from meeting insensitive faults we could 
separate each specification's faults into two 
populations and then compare the detection 
probabilities of each population using each inspection method.
The faults suspected of being meeting sensitive should have a higher
probability of being found at a meeting.

Since at present we don't have any such classification scheme
we can't do this and therefore our approach is to look
for ``markers'' or ``indicators'' that are associated with the
presence or absence of meeting sensitive faults.
For example, if the detection probabilities of meeting sensitive 
faults are statistically distinguishable from
those of meeting insensitive faults, then we can identify 
those faults which are very likely or very unlikely to be
meeting sensitive.  

To estimate these detection probabilities we conducted a
Monte Carlo simulation of inspections under a variety of 
different conditions (e.g., different proportions of 
total faults that are meeting sensitive). The outcome of each 
simulation run is the expected detection probabilities for meeting
sensitive and insensitive faults when the inspection is conducted 
with and without meetings.  

This information allows us to answer three questions.

\begin{enumerate}

\item If none of the faults are meeting sensitive, what is the 
inter-treatment difference in detection probabilities?

\item As the proportion of meeting sensitive faults grows, how
does this affect the inter-treatment difference in detection probabilities?

\item At what magnitude does the inter-treatment difference 
in detection probabilities become statistically significant?

\end{enumerate}

\input{secondPhase}

Each run simulates using two treatments, $T_a$ and $T_b$,
to inspect either the WLMS or CRUISE specification. 
The fault detection probabilities for each treatment 
are $p_a$ and $p_b$ and are based on the probabilities
$p_{DD}$ and $p_{DC,PI}$ observed during the experiment.   

The simulation manipulates five independent variables:

\begin{enumerate}

\item the specification (WLMS or CRUISE),

\item the detection rate for $T_a$. ($p_a = p_{DD}$ + \{ -4, -2, 0, 2, or 4 \}
$\times factor$, where $factor = \frac{1}{42} = 0.023 $ for the WLMS and 
$factor = \frac{1}{26} = 0.038$ for the CRUISE),

\item the detection rate for $T_b$. ($p_b = p_{DC,PI}$ + \{ -4, -2, 0, 2, or 4 \} 
$\times factor$), 

\item the proportion of faults that are meeting sensitive (0, .1, .2, .25, .4,
.5, .6, .75, .9 or 1 \}),

\item the detection rate for meeting sensitive defects ( $p_{sens} = p_a 
\times $ \{1.0, 1.1, 1.2, 1.25, 1.4, 1.5, 1.6, 1.75, 1.9, or 2\}. Note that 
the detection
probability for insensitive faults, $p_{insens}$, is now constrained by the 
values of $p_b$ and $p_{sens}$.),

\end{enumerate}

For each combination of independent variables we simulate 50 inspections of 
a single specification. Half of the inspections use treatment $T_a$; 
the other half use treatment $T_b$. 
For $T_a$, each fault's detection probability is estimated by 
the proportion of successes in a random draw from a Bernoulli 
distribution with parameters 25 and $p_a$. A similar approach is 
used for $T_b$, except that meeting sensitive faults 
use a Bernoulli distribution with parameters 25 and $p_{sens}$
and the insensitive faults use parameters  25 and $p_{insens}$.
The difference between the detection probability for a given fault
when using $T_a$ and when using $T_b$ is called the inter-treatment 
difference:

For each run we calculate two dependent variables:

\begin{enumerate}

\item the average inter-treatment difference for meeting sensitive faults, 
\item the average inter-treatment difference for meeting insensitive faults.

\end{enumerate}


Figure \ref{sim.wlms} 
%and \ref{sim.cruise} 
show some of the simulation results (WLMS only), 
about which we make several observations. 
(1) The larger the difference between $p_a$ and $p_b$,
the smaller the proportion of meeting sensitive faults can be,
but the easier it will be to differentiate sensitive faults from 
insensitive ones. 
(2) The meeting sensitivity will have to be roughly 
$p_a \times 1.5$ for a statistically significant difference
to be detectable.
(3) In many cases the two populations of meeting sensitive and insensitive 
faults will be statistically
indistinguishable from a population having no meeting sensitive faults. 

