%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% uh-experiment.tex -- 
%% Author          : Philip Johnson
%% Created On      : Tue Oct 29 11:08:28 1996
%% Last Modified By: Philip Johnson
%% Last Modified On: Thu Oct 31 09:56:14 1996
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1996 Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 


\section{Experiment 1: University of Hawaii}

\subsection{Motivation}

This experiment compared the performance of real group and nominal group
reviews.  A ``real'' group is one in which the participants meet
face-to-face and interact with each other to accomplish the group task.
In this experiment, real groups model the standard, meeting-based review method.  A
``nominal'' group is one in which the participants work individually
without interacting with each other, and their individual results are
pooled together to accomplish the group task. By comparing the 
performance of real groups with nominal groups, the experiment attempts
to tease out the effects that the meeting alone has on 
overall review performance.   Although there is prior
research on real vs. nominal group performance, these studies have focussed
on idea generation, not software review \cite{Diehl87,Mullen91}.

\subsection{Hypotheses}

The main research question was, ``Are there differences in detection
effectiveness (the number of program defects detected) and detection cost
(the amount of effort/time to find a defect) when subjects review source
code using a synchronous, same-place same-time interaction (real groups)
versus an asynchronous, same-place same-time interaction (nominal
groups)?''  The hypothesis was that there would be significant
differences in both detection effectiveness and detection cost. If Fagan's
and other meeting advocate's assumptions underlying the group meeting held,
then it was expected that real groups would find significantly more defects
due to the advantages (synergy, etc.) of a group meeting. If these
assumptions did not hold, then it was hypothesized that nominal groups
would outperform the real groups with respect to defect detection.  In
either case, it was expected that real groups would cost significantly 
more (i.e. require significantly more effort) that nominal groups.


Besides this primary hypothesis concerning cost and effectiveness, the
experiment pursued research questions concerning the ability of the two
methods to detect certain classes of defects and to detect ``false
positives'' (issues raised that are not actual defects).  
Space constraints preclude a complete discussion
of all the research questions and hypotheses considered in this experiment;
for complete details, see the 
dissertation by Danu Tjahjono \cite{csdl-95-08}.


\subsection{Experimental Design}


\subsubsection{Subjects}

The subjects were 27 undergraduate students enrolled in ICS-313
(Programming Language Theory) and 45 undergraduate students enrolled in
ICS-411 (System Programming) classes at the University of Hawaii in the
Spring of 1995. The subjects were assigned to groups of size 3, for a total
of 24 different groups.  Each group performed two reviews, once using a
real group review method and once using a nominal group review method.


\subsubsection{Design}

The experimental design involved one factor (group interaction) with two
treatments: real group interaction and nominal group
interaction.  The experimental design was a  balanced design in
which each group reviewed two sets of source code using two different group
interactions. Both the source code and type of group interaction were assigned 
to the groups randomly.  

The experiment was carried out in two rounds, the first round using the
ICS-313 students and the second round using the ICS-411 students. The
source code reviewed by the students was based upon recently completed
exercises in the two classes, so that the students were familiar with
the ideas underlying the review materials. The ICS-313 groups reviewed two
portions of an Employee database application written in C++, and the
ICS-411 groups reviewed two portions of a two-pass assembler written in C.
All groups used CSRS, a computer-mediated software review system, to both
analyze their review products and record their review findings.

Figure \ref{design} shows the experimental design 
for each of the two rounds. 

\begin{figure}[htb]
    \small
  \begin{center}
    \begin{tabular} {|l|p{1.2in}|p{1.2in}|}
      \multicolumn{3}{c}{{\bf Round 1: ICS-313 Groups}}\\
      \hline
      & {\bf Employee1} & {\bf Employee2}\\
      \hline
      & & \\
      {\bf EGSM} & G1$^1$, G6$^1$, G8$^1$, G9$^1$ & G2$^2$, G3$^2$, G4$^2$, G5$^2$, G7$^2$ \\
      & & \\
      \hline
      & & \\
      {\bf EIAM} & G2$^1$, G3$^1$, G4$^1$, G5$^1$, G7$^1$ & G1$^2$, G6$^2$, G8$^2$, G9$^2$ \\
      & &  \\
      \hline
      \multicolumn{3}{c}{{\bf Round 2: ICS-411 Groups}}\\
      \hline
      & {\bf Pass1} & {\bf Pass2}\\
      \hline
      & & \\
      {\bf EGSM} & G3$^2$,G4$^2$,G9$^2$,G10$^2$, G11$^1$,G12$^2$,G13$^1$ & G1$^2$,G2$^2$,G5$^2$,G6$^1$, G7$^2$,G8$^1$,G14$^1$,G15$^2$\\
      & & \\
      \hline
      & & \\
      {\bf EIAM} & G1$^1$,G2$^1$,G5$^1$,G6$^2$,
      G7$^1$,G8$^2$,G14$^2$,G15$^1$ & G3$^1$,G4$^1$,G9$^1$,G10$^1$, G11$^2$,G12$^1$,G13$^2$ \\
      & &  \\
      \hline
     \end{tabular}
  \end{center}
  \caption{Source code and group assignments for the two rounds. ``G''
  indicates a group. The
  superscript indicates the order in which the source code was reviewed.}
  \label{design}
  \normalsize
\end{figure}

\subsubsection{Variables}

The experiment manipulated the independent variable, {\em group interaction},
with two treatments, real group and nominal group. 

For the main experimental question, the experiment manipulated two
dependent variables, or review measures: {\em defects}, the total number of
distinct, valid defects detected by the group, and {\em effort}, the total
review time spent by the group.  Other research questions 
required several additional dependent variables, including the review
measures: {\em false positives}, the number of invalid defects recorded by
the group; {\em duplicates}, the number of duplicate defects found during
nominal group review; and {\em synergy}, the number of defects found
through interaction of two or more people during real group review.

\subsubsection{Threats}

Threats to internal validity are those factors that may affect the values
of the dependent variables apart from the independent variables.

To minimize selection effects in the ICS-313 round, each
individual's skill was rated as low, medium, or high, based upon their grades in
prior assignments. A member was then selected at random from each category
to form groups of three. To minimize selection effects in the ICS-411
round, individuals were chosen at random to form groups of three.

The order in which the two review methods were presented to groups was
randomized in order to minimize training effects. These effects were also
reduced through a training session prior to the experiment in which
subjects practiced the use of CSRS and the software review methods.

Finally, differences between the two documents inspected
were minimized in both rounds by ensuring that the two documents had approximately the
same numbers of defects of the same types. 
Instrumentation effects were also minimized by having all groups inspect both documents.

Threats to external validity are those factors that limit the applicability
of the experimental results to industry practice. Such threats include: the
student reviewers may not be representative of professional programmers,
the software reviewed may not be representative of professional software,
and the inspection process may not be representative of industrial
practice.

These threats are real.  Overcoming the first two threats is best
accomplished by replication of this study using industrial programmers with
real work products. To support this replication, our experimental materials
and apparatus are freely available via the 
Internet~\cite{csdl-www-csrs}.   To minimize the
third threat, the experimental review methods were based on descriptions of 
industrial practice of software review. 

\subsubsection{Analysis Strategy}

Most of the research questions were tested using the Wilcoxon signed rank
test \cite{Ferguson89}. This non-parametric test of significance does not
make any assumption regarding the underlying distribution of the data.  It
is based on the rank of differences between each pair of observations in
the dataset.

The data analysis proceeds in the following way. Assume that the data are a
set of N paired observations on X and Y. The difference, d, between each
pair is calculated.  If the two observations in a pair are the same, then d
= 0 and the pair is deleted from the analysis. The remaining d's are then ranked
without regard to sign; that is, the absolute values $|X_{i} - Y_{i}|$ are
ranked. A rank of 1 is assigned to the smallest d, of 2 to the next
smallest, and so on. The sign of the difference d is then attached to each
rank. Denote the sum of the positive ranks by $W_{+}$ and the sum of the
negative ranks by $W_{-}$. The normal deviate z (z-value) is given by

\small
 \( z = \frac{W - \frac{N(N+1)/4}{4}}{\sqrt{\frac{N(N+1)(2N+1)}{24}}} \),
where $W = W_{+}$ if $W_{+} \leq W_{-}$ else $W_{-}$.

\normalsize

The p-value of z is then used to test the null hypothesis concerning X and
Y, that is, that there is no significant differences between X and Y.  If
the p-value is less than the significance level $\alpha$ = 0.05, then we
reject the null hypothesis, and can conclude that there is a significant
difference between X and Y.

\subsubsection{Experimental Instrumentation}

Two basic instruments were developed for this experiment: the source
code materials reviewed by the subjects, and the experimental apparatus
using CSRS. 

\paragraph{Source code review materials.} 

The experimental review materials were based on programs recently
implemented by the students themselves.  Sets of source code with
approximately the same size were selected.  The code was combined, 
re-edited and re-compiled to ensure that it had no syntax errors.  
Natural language specifications for each procedure or function in 
the source code were provided.

In both rounds, the defects were mostly logic, computation, and data
handling problems, such as missing or incorrect condition tests, forgotten
cases or steps, and incorrect data access. Some of these defects were
specific to the C/C++ languages, such as memory leaks.  None of the defects,
however, involved an incorrect specification. In fact, the participants
were told beforehand that when the code did not conform to the
specification, then the specification should be assumed correct, and the
code was therefore incorrect.

For the ICS-313 round, the programs implemented an Employee database using
the C++ programming language. One program used an array implementation of
the database, and the other used a linked-list implementation.  The source
code was seeded with natural defects, in other words, defects made by the students
themselves. To obtain these defects, the students were asked to submit the
programs right after first successful compilation.  Twenty defects were
seeded in
each of the two programs for the ICS-313 round, but by the end of the
experiment, three additional defects were found in the array implementation and 
five were found in the linked list implementation.

For the ICS-411 round, the programs implemented a two-pass assembler using
the C programming language. The two programs corresponded to Pass-1 and
Pass-2 of the assembler. As in the ICS-313 round, the two
programs had approximately equal numbers of defects and types of defects.
Unlike the ICS-313 round, the defects were seeded in the same relative
location.  For example, when a defect of type uninitialized variable was
seeded in the beginning of the function Pass-1, the same type of defect was
also seeded in the beginning of the function Pass-2.  Nineteen defects
were seeded in each of the two programs for the ICS-411 round, but by the end of the
experiment, one additional defect was documented in the Pass-1 source code.



\paragraph{Experimental Apparatus.}

To help ensure that all groups carried out review the same way, and to 
facilitate data collection, the CSRS computer-mediated software
review environment was used as the experimental apparatus for this study.  The
data and process modeling languages of CSRS were used to implement two
different review methods, EGSM and EIAM,  that differed only with respect 
to group interaction. EGSM stands for "Experimental Group Synchronous Method",
and EIAM stands for "Experimental Individual Asynchronous Method."

Figure \ref{fig:screen} shows a screen image from the EGSM review from the
Pass2 assembler source.  In both methods, CSRS presented subjects with this
three window user interface, where the set of functions/procedures to be
reviewed are shown in the upper right screen, the function or procedure
currently reviewed is shown in the left screen, and defects raised by
reviewers are entered in a commentary window in the lower right screen.


\begin{figure*}[htb]
{\centerline {\psfig{figure=egsm.ps}}}
 \caption{An EGSM screen image from the ICS-411 round.}
 \label{fig:screen}
\end{figure*}


The EIAM interface differs only slightly from that shown in Figure
\ref{fig:screen}.  All issues in EIAM are private to each reviewer, but
public in EGSM among all reviewers of a given group. Similarly, the
criticality field value is private to each reviewer in EIAM, but public
(all votes are shown) in EGSM. EGSM also includes a field called
``Suggested-by'' that allows each reviewer to indicate who suggested the
issue, and is used to measure group synergy. This field is not included in
EIAM, since synergy is not present by definition.



\subsubsection{Experimental Procedures}

\paragraph{Training.}

All subjects attended a set of lectures on formal technical review. This
lecture explained the goals of formal technical review and the specific
procedures to be used in this study.  The training was based upon software
review tutorial materials used by one of the authors (Philip Johnson) for industry. 

The subjects were then assembled into three person teams according to the
procedures specified above. They next attended a two hour training
session to familiarize themselves with the CSRS review environment and the
EGSM and EIAM review methods. During this session, they practiced review on
sample source code implementing a ``BigInteger'' data abstraction. They
practiced the use of paraphrasing as a mechanism to analyze software and
discover defects.

\paragraph{General Review Procedures.}

Both EGSM and EIAM consisted of a single review phase, whose objective was
defect detection.  Subjects were told not to determine how to correct any
defects they discovered, but merely to note their presence.  

Since all subjects had recently completed the implementation of a program
quite similar to the review materials, there was no need for a
``preparation'' phase with the objective of comprehension, or to mix
comprehension with defect discovery. The subjects were presumed to be 
already familiar with the requirements, specifications, and design of the 
software under review.

Both methods used the paraphrasing method from Fagan Inspection as the
analysis technique.  For EGSM, one of the three subjects in each group was
assigned to the role of Presenter, and he/she orally summarized the
source code in a line-by-line fashion.  The presenter also acted as a
reviewer and was free to discover defects. For EIAM, subjects paraphrased the
source code silently to themselves.

In EGSM, the subjects collaborated fully with each other. As the Presenter
paraphrased the code aloud, any of the review team members were free to interrupt
with suggestions of potential defects. Others would then confirm or reject
the suggestion.  If disagreement continued, the team would vote on whether
or not to include the issue as a defect.  The Presenter was the only one
who could record issues, and all review screens were kept synchronized. This
prevented reviewers from ``wandering off'' into the code and kept the
reviewers together.

In EIAM, subjects worked entirely independently, raising issues and noting
them by themselves. For administrative purposes, each EIAM team did meet in
the same room at the same time, but all interaction between members was
prevented.

An ``external moderator'' participated in all review sessions. 
His role was simply to assure correct execution of the process. 
For example, he ensured that paraphrasing was used in EGSM, that 
discussion did not wander, and that any questions about the CSRS 
user interface could be answered quickly and correctly. 

The review time for every session was specified as a maximum of three hours. 
However, no review team or reviewer used more than 2.5 hours to complete
the review of each program. 

\paragraph{Round 1: ICS-313.}

For the ICS-313 round, 27 students participated and were split into 9
groups. Four of the groups were randomly chosen to use the EGSM method to
review the array implementation of the employee database, while the remaining
five groups used EIAM to review the same source materials.  All groups then
switched methods and reviewed the linked list version of the employee
database. The round was completed within two weeks. 

\paragraph{Round 2: ICS-411.}

For the ICS-411 round, 45 students participated and were split into 15
groups. Seven of the groups were randomly chosen to use the EGSM method
first, while the other eight groups used the EIAM method first.  The 
review material encountered first was also randomly assigned, with seven
groups encountered the Pass1 source code for their first review, while the
other eight groups encountered the Pass2 source code first.  All groups
then switched both review method and source material for their second
review session. This round was also completed within two weeks. 

\subsubsection{Data Collection}

Data was collected through two mechanisms: CSRS and questionnaires filled
out by all subjects at the end of each review session.  CSRS stored
all defects recorded by both real groups (using EGSM) and nominal groups
(using EIAM) in an internal database for later analysis.

For each real group, the value of the dependent variable {\em defects} was
calculated as the total number of defects entered into CSRS by the group,
minus those manually determined to be {\em false positives}.

For each nominal group, the value of {\em defects} was calculated by summing
all of the errors found by the individuals in a particular group, then
subtracting both those defects manually determined to be {\em false
  positives} as well as any defects determined to be {\em duplicates},
i.e. found by more than one member of the group. 

For all groups, the value of {\em effort} was calculated
as the sum of the total time spent on review by each member of the group. 
CSRS automatically recorded the time spent by each reviewer logged in to
the system. 

For each real group, the value of {\em synergy} was determined by analysis
of the value of the ``Suggested-by'' field for each recorded defect. The
Suggested-by field had four possible values: ``Me'', ``Me, but inspired by
others'', ``Other but also occurred to me'', and ``Other and had not
occurred to me''.  If one or more of the reviewers recorded ``Me'' as the
value of this field, then synergy was defined as not occurring during the
discovery of this particular defect. The value of {\em synergy} for 
a real group was calculated as the total number of defects found, minus
those for which synergy did not occur. 

Finally, each subject filled out three questionnaires during the study.  A
questionnaire evaluating the subject's attitudes towards the EGSM method
and the EGSM review group experience was administered after the EGSM
review. A similar questionnaire on EIAM was administered after the EIAM
review. A final questionnaire evaluating subject preference for EIAM or
EGSM, and their satisfaction with CSRS was administered at the end of the
study.  Most of the questions required subjects to respond by circling
one number on a five point scale, although a few questions were open ended
and asked for explicit commentary. 


\subsection{Experimental Results}

Figure \ref{fig:wilcoxon-results} summarizes the results of comparing the
performance of real groups (EGSM) and nominal groups (EIAM) for certain
review measures using the Wilcoxon signed rank test.  The results
from analyzing the data for each round separately and when grouped
together are shown.

A ``-'' in a column indicates that a significant difference between real
and nominal groups could not be detected for the review measure. A ``$>$''
indicates that real group performance using EGSM was significantly higher
(p$<$.05) than nominal groups using EIAM for the corresponding review
metric, while a ``$<$'' indicates that real group performance was
significantly lower (p$<$.05) than nominal group performance.


\begin{figure}[ht]
  \small
  \begin{center}
  \begin{tabular}{|l|c|c|c|}
  \multicolumn{4}{c}{EGSM vs. EIAM}\\
   \hline
Review Measure    & ICS-313 & ICS-411 & All \\
   \hline
{\em Defects}         & -       & -       & - \\
{\em Cost/Defect}     & $>$     & -       & $>$ \\  
{\em Effort}          & $>$     & -       & $>$ \\
{\em Issues}          & $<$     & $<$     & $<$ \\
{\em False positives} & $<$     & $<$     & $<$ \\
  \hline
   \end{tabular}
  \end{center}
 \caption{Selected Wilcoxon signed rank test results}
 \label{fig:wilcoxon-results}
\normalsize
\end{figure}


As shown in Figure \ref{fig:wilcoxon-results}, the data does not show a
significant difference between real and nominal groups with respect to the
number of valid defects discovered. In other words, this study was unable
to demonstrate that review meetings for the purpose of defect discovery
outperformed individuals working independently.  Real groups found an
average of 42.8\% of all known defects, while nominal groups found an
average of 46.4\% of all known defects.

On the other hand, the data shows that real groups using meetings were
significantly more costly than individuals working independently.  The
average effort required per defect was 41 minutes for real groups and 34
minutes for nominal groups, and the average overall effort for a review
session was 5:57 hours for real groups and 5:11 hours for nominal groups.

The experiment also found that individuals working independently in nominal groups
raised significantly more total issues, an average of 14 issues per nominal
group session, than real groups which raised an average of 9 issues per
session.  However, nominal groups had a significantly greater percentage of
false positives (22\% of all raised issues, on average) than those working
in real groups (5.3\% of all raised issues, on average). 

Figure \ref{fig:individual-results} summarizes some of the major results from
analysis of measures that apply only to one of the two review methods.
{\em Synergy} indicates the percentage of defects in which
synergy played a role for the set of EGSM review sessions. 
Synergy participated in the process of defect discovery about a third
of the time overall. 

{\em Duplicates} indicates the average percentage of defects that were
discovered by more than one reviewer in a given EIAM group for the set of
review sessions. Again, about a third of the defects were discovered by 
more than one reviewer during individual review. 

\begin{figure}[ht]
\small
  \begin{center}
  \begin{tabular}{|l|l|c|c|c|}
   \hline
Data                       & ICS-313 & ICS-411 & All \\
   \hline
{\em Synergy (EGSM only)}         & 42\%    & 21\%     & 29\% \\
{\em Duplicates (EIAM only)}      & 29\%    & 31\%     & 30\% \\
  \hline
   \end{tabular}
  \end{center}
 \caption{Selected method-specific measurements}
 \label{fig:individual-results}
\normalsize
\end{figure}

