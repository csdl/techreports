
To evaluate these hypotheses 
we designed and conducted a controlled 
experiment. The goals of this experiment were twofold: 
to characterize the behavior of existing approaches
and to assess the potential benefits of 
meetingless inspections. We ran the experiment
in the spring of 1995 with 21 subjects 
-- students taking a graduate course in software engineering --
who acted as reviewers. Each complete run consisted of (1)
a training phase in which the subjects were taught
inspection methods and the experimental procedures, and 
in which they inspected a sample SRS; and (2) 
an experimental phase in which the subjects conducted 
two inspections.

\subsection{Experimental Design}
\label{design}

\subsubsection{Variables}
\label{variables}
The experiment manipulates four independent variables:

\begin{enumerate}
\item the inspection method used by each reviewer (PI, DC, or DD);
\item the inspection round (each reviewer participating in two inspections 
during the experiment);
\item the specification to be inspected (two are used during the 
experiment);
\item the order in which the specifications are inspected (Either 
specification can be inspected first.)
\end{enumerate}


The inspection method is our treatment variable. The other 
variables allow us to assess several potential threats to 
the experiment's internal validity. For each inspection 
we measure three dependent variables:

\begin{enumerate}
\item the Individual Fault Detection Rate, 
\item  the Team Fault Detection Rate,
\footnote{The Team and the Individual Fault Detection Rates are the 
number of faults detected by a team or individual divided by the 
total number of faults known to be in the specification.  
The closer these values are to 1, the more effective the detection method. 
No faults were intentionally seeded into the specifications. All 
faults are naturally occurring.}
\item the Gain Rate, i.e., the percentage of faults 
initially identified during the 
second phase of the inspection.(For the PI and DC methods
the second phase is the team meeting; for the DD method it is the
second individual detection activity.)
\end{enumerate}

These calculations are explained in Section \ref{danal}.  

\subsubsection{Design}

The purpose of this experiment is to compare the 
PI, DC, and DD methods for inspecting software 
requirements specifications.  To keep the duration 
of the experiment short,
we chose a partial factorial design in
which each team participates in two inspections, 
using some combination of the three inspection methods.
Table~\ref{indepvar} shows the settings of the independent variables.

\subsubsection{Threats to Internal Validity}\label{sec:teamselec}

A potential problem in any experiment is that some factor may affect 
the 
dependent variable without the researcher's knowledge. This 
possibility must be minimized. We considered four such threats:
(1) selection effects, (2) maturation effects, (3) instrumentation 
effects, and (4) presentation effects.

Selection effects are caused by natural variation in human performance.
For example, random assignment of subjects may accidentally create 
an elite team. Therefore, the difference in this team's natural ability 
will mask differences in the performances of the detection methods.

Our strategy is to assign teams and inspection methods on a completely 
random basis.  This approach attempts to spread differences in natural ability 
across the inspection methods in an unbiased fashion.  However, since each
team uses only two of the three inspection methods, differences in the methods
can't be completely separated from differences in ability.

Maturation effects are caused by subjects learning 
as the experiment proceeds. 
We have manipulated the inspection method used and 
the order in which the documents are inspected so that 
the presence of this effect can be discovered and 
taken into account. 

Presentation effects can occur if inspecting one 
specification first makes it easier to inspect the
remaining one. We control for this possibility by
having half the teams inspect the documents in 
each of the two possible orders.

Finally, instrumentation effects may result from 
differences in the specification documents.
Such variation is impossible to avoid, 
but we controlled for it by having each team 
inspect both documents.

As we will show in Section~\ref{danal}, 
variation in the fault detection rate is 
not explained by selection, maturation, or
presentation effects.


\subsubsection{Threats to External Validity}
\label{external}

Threats to external validity limit our ability to 
generalize the results of our experiment to industrial
practice.  We identified three such threats:

\begin{enumerate}
\item The subjects in our experiment may not be 
representative of software programming professionals. Although more
than half of the subjects have 2 or more years of industrial
experience, they are graduate students, not software professionals.
Furthermore, as students they may have different motivations for
participating in the experiment.

\item The specification documents we used may not be representative of real 
programming problems. Our experimental specifications are atypical
of industrial SRS in two ways. First, most of the experimental 
specification is written in a formal requirements notation (see 
Section \ref{instruments}). Although several groups at AT\&T
and elsewhere are experimenting with formal 
notations \cite{ardis.1994,GCR.formal}, it is
not the industry's standard practice. Second, the specifications
used are considerably shorter than industrial specifications.

\item The inspection process in our experimental design may not be 
representative of software development practice. We have modeled our
experiment's inspection process after the ones used in many
development organizations, although each organization may 
adapt the process to fit its specific needs.
Another difference is that the SRS authors are not present at our inspections,
although in practice they normally would be.  Finally, industrial reviewers may 
bring more domain knowledge to an inspection than our student subjects did.
\end{enumerate}

To surmount these threats we will need to replicate our experiment
using software professionals to inspect industrial work products.
Nevertheless, laboratory experimentation is a necessary first step
because it greatly reduces the risk of transferring immature
technology and it is far less costly than using professional
subjects while we refine our experimental design.

\subsubsection{Analysis Strategy}\label{sec:danalstrat}
\label{analysis}

Our analysis strategy has several steps.
The first step is to find those independent variables 
that individually explain a significant amount
of the variation in the team detection rate.
The second step is to evaluate the combined effect 
of the variables shown to be significant 
in the initial analysis.  Both analyses use 
standard analysis of variance methods 
(see \cite{BHH:Statistics}, pp. {165\it ff} and {210\it ff} or
\cite{Heiberger.1989}).
Once these relationships were discovered and their magnitude
estimated, we examined other data, such as the gain rates,
that would confirm or reject (if possible) a causal relationship 
between the inspection methods and inspection performance. 

%\input{form}

\subsection{Experiment Instrumentation}  
\label{instruments}
We used several instruments for this experiment: three small
software requirements specifications (SRS), instructions 
for each inspection method, and a data collection form. 
(These specifications were originally developed 
for another experiment. See Porter et al.\cite{PVB.TSE94}.)

\subsubsection{Software Requirements Specifications}
\label{srs}
The SRSs we used describe three event-driven process control 
systems: 
an elevator control system (ELEVATOR), 
a water level monitoring system (WLMS),
and an automobile cruise control system (CRUISE). Each specification has four 
sections:
Overview, Specific Functional Requirements, External Interfaces, and 
a Glossary. The overview is written in  natural language, while 
the other three sections are specified using the SCR
tabular requirements notation~\cite{heninger80}.

For this experiment, all three documents were adapted 
to adhere to the IEEE suggested format~\cite{ieee.1989}. 
All faults present in these SRS appear in 
the original documents or were generated 
during adaptation; 
no faults were intentionally seeded into the document. 
The authors discovered 42 faults in the WLMS SRS 
and 26 in the CRUISE SRS. The authors did not 
inspect the ELEVATOR SRS since it was used only
for training exercises. 

\input{wlmsCollect}

\paragraph{Elevator Control System}~\cite{Wood:ELEV}
describes the functional and performance 
requirements of a system for monitoring the operation of
a bank of elevators (16 pages).

\paragraph{Water Level Monitoring System}~\cite{vanSchouwen:WLMS}
describes the functional and performance 
requirements of a system for monitoring the operation of
a steam generating system (24 pages).

\paragraph{Automobile Cruise Control System}~\cite{Kirby:Cruise}
describes the functional and performance requirements for 
an automobile cruise  control system (31 pages).

\subsubsection{Fault Reporting Forms}
\label{forms} 

We also developed a Fault Report Form. Whenever a 
potential fault was discovered -- during either
the fault detection or the collection activities 
--  an entry was made on this form.  The entry 
included four kinds of information:  
Inspection Activity (Detection or Collection),
Fault Location (Page and Line Numbers),
Fault Disposition (Faults can be True Faults or False Positives), 
and Fault Description (in prose).
%A small sample of a Fault Report appears in
%Figure~\ref{fig:formsEx}.


\subsection{Experiment Preparation}
\label{preparation}

The participants were given two lectures of 75 minutes each on  
software requirements specifications,
the SCR tabular requirements notation,
inspection procedures, the fault classification scheme,
and the filling out of data collection forms. The references
for these lectures were Fagan~\cite{fagan.1976}, 
Parnas~\cite{parnas.1985},
and the IEEE Guide to Software Requirements 
Specifications~\cite{ieee.1984}.
The participants were then divided into 
three-person teams -- (see Section~\ref{sec:teamselec} 
for details.) Within each team, members were randomly assigned 
to act as the moderator, the recorder, or the reader during the
collection meeting.

\subsection{Conducting the Experiment}
\label{conducting}

\subsubsection{Training}

For the training exercise, each team inspected  
the ELEVATOR SRS. Individual team members read the specification 
and 
recorded all faults they found on a Fault Report Form. 
Their efforts were restricted to two hours. Later we met with 
the participants and answered questions about the experimental
procedures. The ELEVATOR SRS was not used in the remainder of the experiment.

\input{contrast}

\subsubsection{Experimental Phase}
\label{sec:experm_results}
The experimental phase involved two inspection rounds. The instruments used 
were the WLMS and CRUISE specifications discussed in Section 
\ref{srs}, and the Fault Report Form. 

During the first Round, three of the seven teams were asked to inspect 
the CRUISE specification; the remaining four teams inspected the 
WLMS specification. Each inspection involved two phases that  
differed according to the method used.  The inspection methods 
used by each team are shown in Table \ref{indepvar}.

The first phase for the Detection-Collection and Detection-Detection 
methods lasted up to 2.5 
hours, and all potential faults were reported on the Fault Report Form. 
The first phase for the Preparation-Inspection
method took the same amount of time, but 
the reviewers were not allowed to report faults or take any notes. 
After the first phase all materials were collected.\footnote{
For each round, we set aside 14 2-hour time slots during which 
inspection tasks could be done.  Participants performed each task 
within a single two-hour session and were not allowed to work at 
other times.}

Once a team's members had finished the first phase, the team 
moderator arranged for the second phase which was also
limited to 2.5 hours. This second phase
involved a team meeting for the PI and DC methods
during which the reader paraphrased each requirement,
and the reviewers brought
up any issues they had found earlier or had just discovered. 
The team  recorder  maintained the team's master Fault Report Form. 
The DD team did not hold a second meeting. Their second phase was 
exactly the same as the first -- individual fault detection.  
The entire first Round was completed in one week. 
The second Round was similar to the first except that teams who
had inspected the WLMS during Round 1 inspected the CRUISE
in Round 2 and vice versa. 




