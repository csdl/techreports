  *************************************************************************** 
     Reviewer A:
     
     My summary: Porter and Johnson jointly reanalyze the results each achieved 
     in separate controlled experiments in the light of work done by the other.  
     Their "meta-analysis" indicates that the two studies agree on the questions 
     the studies attempted to answer about the use of meetings as part of 
     review/inspection techniques, namely whether meetings yield greater number 
     of detected problems (no) and whether meetings result in greater costs 
     (yes).
     
     Praise: The paper is in general nicely written, presents the problems and 
     solutions of meta-analysis clearly, plainly discloses limitations in 
     extrapolating the findings, and does not overstate conclusions. 
     
     The following comments list some points that require repair.
     
     Content
     
     1.  Length: It is characteristic of empirical studies in software 
     engineering that an enormous amount of material must be supplied to reach 
     the three-word conclusion of "our results agree."  This is my way of 
     saying that I found the paper overly long for the conclusion. The only 
     constructive suggestion I can offer for chopping it down 
     is to remove as many repetitive statements as possible. 
     

	A: No change.

     2.  Terminology: Johnson uses "review" but Porter writes "inspection." The 
     structure of the descriptions of the two experiments was nicely parallel, 
     but the terminology should be made parallel as well.  Or 
     is the different usage meant to highlight some important difference?

	A: Changed inspection to review throughout.
     
     3.  Abstract: The casual reader would think that an enormous controversy 
     surrounds the use/non-use of meetings in conjunction with inspections, 
     reviews, or whatever we're going to call these defect-detection activities. 
      This might overstate the case.  I think few have ever seen Porter's and 
     Votta's work that challenges the assumption.

	A: No change.
     
     4.  Abstract: After reading this, it was unclear whether requirements, 
     design, code, or some other artifacts are the targets.  Might be 
     appropriate to state this up front.

	A: One study looked at code, the other at requirements. We felt that explaining 
	all the differences would just be confusing.
     
     5.  Intro, 1st par: what makes Fagan's method "canonical" ?

	A: removed the word canonical.
     
     6. Sec. 2.0, p. 2, bullets: When/how does an overview occur?
     Is a meeting held for this??  Might it be helpful to mention the roles of 
     reader, moderator and scribe?
    
	A: We felt that this wasn't critical to our study and thus we left it out.
 
     7. Sec. 3.2, re dissertation by Danu Tjahjono: Could this be made available 
     on the web to permit casual browsing?
     
	A: We put in a pointer to web version of thesis.

     8. Sec. 3.3.2: Here we learn for the first time that source code was 
     reviewed.  Also: the independent variable "order" was also manipulated, but 
     this is not mentioned.
    
	A: This seems to be a style issue between the two studies. Order isn't 
	technically an independent variable. It's really a blocking factor.

     9. Figure 1:  I would label this a table, but more importantly, "EGSM" and 
     "EIAM" were not defined previously.


    	A: done.
 
     10. Sec. 3.3.4, p. 5: Why was matching based on scores done for one group, 
     but not for the other group?  Some explanation seems missing here.  Might 
     also check Campbell&Stanley66 re the dangers on the use of performance on 
     testing instruments for building groups.

	A: I agree with the reviewer. I never do this. Johnson chose to.  However, 
	even while Campbell and Stanley argue against this practice, they do give 
	arguments in favor of it. It's not a solved problem. 

     11. Sec. 3.3.4, p. 6: Where are work products available on the 'net?

    	A: fixed. 

     12. Sec. 3.3.5: An appropriate reference might be more appropriate than a 
     complete definition of Wilcoxon rank tests, even if coding the equation in 
     latex is lots of fun.
    
	A: Again I agree, but Philip sent hours on the LaTeX. :-)

     13. Sec. 3.3.6: This par implies that the instruments were written by the 
     same student(s) who participated in the experiment.  But the author of a 
     document would have an advantage and/or bias about reviewing his or her own 
     material (who wants to criticize their own work?).  How was this problem 
     handled?
    
	A: The instruments were an _amalgam_ of student work, so there
	wasn't this problem. We changed the wording slightly to make
	this more clear.  

     14. Sec. 3.3.7: The total lack of preparation time put in by the subjects 
     before participating in defect-detection activities seems to throw a monkey 
     wrench into the whole business.  Obviously the authors feel differently.  
     Some discussion here would be appropriate.

	A: We feel this is adequately handled.
     
     15. Sec. 3.3.7, last par: Was the limit of 3 hours stated up front to the 
     subjects?

	A: Yes. The text now reflects this fact.
     
     16. Sec. 3.4: Figure 3 looks like a table to me, and the use of less-than 
     and greater-than signs is uniquely difficult to read, but there's a bigger 
     question: what about differences due to the instruments and order (the 
     other independent variables)?
    
	A:  Johnson randomized the order and instruments, but doesn't analyze 
	their effects.  No change.

     17. Sec. 4.3.2, 2nd sen.: How did each team use *some combination* of 
     inspection methods?  I thought that each method was used as described?

	A: awkward phrasing has been fixed.
     
     18. Sec. 4.3.3: Why is a percentage labeled a rate?  I believe this 
     misleads the reader into believing that some component of time was 
     incorporated into the result.
    
	A: Changed rate to ratio throughout paper.

     19. Sec. 4.3.4, p. 11: I've never before heard of a presentation effect. 
     From the description, it sounds like a maturation effect.

	A: There's no definitive list of threats to validity. Intuitively,
	we're asking about whether the order in which the documents are 
	reviewed, A then B vs. B then A makes a difference. Maturation is 
	round 1 vs. round 2. 

     20. Sec. 5.1, item 1: What does the removal of data and regrouping of 
     independent variables imply?  How does this change things?  This seems 
     like a major step but the discussion is brief.


     21. Sec. 5.2., 2nd par: The mapping of UM data to UH-nominal groups and 
     real groups is another major step that deserves discussion.
    
	A: The last two comments are related. I can't give a crisp answer to 
	this. The mapping is straightforward based on the tasks the reviewers
	were performing. This does not account for motivation or other non-task-
	related factors, but I don't have any measure of them.  I'm assuming that 
	the effect of meeting vs. no-meeting outweighs other non-task-related effects. 

     22. Sec. 5.6.3: A supporting reference for capture-recapture techniques 
     might be helpful.

	A: fixed.

     23. Sec. 5.6.4: Could it be that the meeting-sensitive defects were 
     especially difficult to find, or would be especially costly if not found?  
     I.e., is there some difference in the severity of the defects that might 
     justify the use of meetings?

	A: As we discuss in the paper, we could find no data to support this 
	conclusion, but we do say that this should be investigated in the 
	future. 

     24. Sec. 6.1: The goals of the component review tasks (prepare only, 
     prepare and find defects, only find defects) seem critical and the 
     discussion in the introduction nicely separated the issues.  But here in 
     the conclusion it appears that these different goals are neglected, or at 
     least not mentioned, which struck me as problematic.  Has the 
     meta-analysis really merged the issues sufficiently?

	A: Essentially, we mapped the UM study onto the UH study. The UH study
	only has preparation with the goal of familiarization. Johnson's claim
	was that other work already showed that meetings weren't effective if 
	preparation was oriented to finding defects.

Presentation
     
     25.  Capitalization: Words that are not proper nouns (e.g., "Inspection," 
     "Preparation," "Active Design Review," etc.) are frequently capitalized in 
     the text.  I found this practice unhelpful. 

	A: Our style. No change. 
     
     26.  Abstract, 2nd to last sentence: "strengthens" should be "strengthen"
   
	A: fixed
  
     27.  Intro, p. 2: suggest "refuted" instead of "disconfirmed"
	A: Fixed
     
     28.  Intro, p. 2, 2nd-to-last par, last sen: extraneous words "the both"
     
	A: Fixed

     29. Intro, p. 2, last par: The use of "the following" is confusing, why not 
     just use section numbers?
    
	A: Fixed

     30. Sec. 2.1, p. 2, 1st par, 2nd sen: Missing word "of"
	A: Fixed
  
     31. Sec. 3.3.4, p. 6: "CSRS" is not defined previously.
    
	A: fixed
     32. Sec. 3.3.7: Danu Tjahjono is not listed as an author on this paper.

	A: fixed
     
     33. Sec. 4.3.2, 2nd par.: "SRS" not defined previously.
	A: Fixed

     34. Sec. 4.3.4, p. 12: Why the forward ref to Sec. 4.3.9 with the results?  
     Seems like unnecessary repetition.


. Sec. 4.3.4, p. 12: Probably AT&T should be written Lucent.
	A: Fixed
     
     36. Sec. 4.3.8, p. 13: Couldn't find Table 5.  Was Figure 5 meant?
	A: Fixed
     
     37. Sec. 4.4.2, last sen.: Capitalize "based"
	A: Fixed
     
     38. Figs. 10, 11, 12: Seem overly large for the information content.

 	A: Fixed
    
     39. Sec. 6.3, 3rd par.: "increases" should be singular.
     
	A: Fixed 
  ___________________________________________________________________________
     
     Reviewer B:
     
     The experiments described in this paper are very good and are 
     well-presented.  However, the paper reads as if two technical reports were 
     cut and pasted together, with little smoothing done.  There are
     many places where more explanation is needed.  For instance, the acronyms 
     for the two UH techniques (EGSM and EIAM) are never spelled out
Philip

, one of Johnson's students is referred to as an author, 

	A: Fixed

	there are extra words and missing words and mismatched subjects and verbs -- 
	this is too sloppy 
     to have been submitted to TSE for refereeing.  The authors should also 
     learn when to use "less" and when to use "fewer."  The good news, though, 
     is that these are good experiments, and the design, validity issues, 
     variables, etc. are nicely described.  The authors should spend more time 
     at the end of the paper, describing what the results mean for 
     practitioners.
     
     The authors should also look at work on meeting loss that has been done by 
     Ross Jeffrey and his colleagues at the University of New South Wales, and 
     on inspection team size by Ed Weller at Bull.  The results in these papers 
     should be mentioned at least in the front matter of this paper, if not in 
     the summary to put all the inspection results in context.
     
	A: I don't think Ross has done any work on meeting losses and Weller's
	work doesn't appear relevant (we're not looking at team size).

     The authors may also want to mention that practitioners are adopting 
     inspections without giving much thought to what really works.  For 
     instance, the US Defense Department is recommending inspection (through 
     its Software Project Managers' Network -- see www.spmn.com) on all 
     projects, and the Software Engineering Institute has been pushing 
     inspections, too.  The authors should stress that their work, and the 
     empirical work of others, is showing that a great deal of fine-tuning 
     should be done to make inspections work well.
    
	A: We considered this, but would rather not take advocacy positions. 
     ___________________________________________________________________________
     
     Reviewer C:
     
     This is an important paper for several reasons: it reports on experiments, 
     and its method of combination of experimental results is probably unique in 
     computing literature. Also, it pushes the envelope of understanding about 
     whether (though perhaps not why) inspections meetings are important.
     
     I think the paper is brave because it combines two studies that, on the 
     surface, have too little in common to consider such a tactic. The one 
     saving aspect of the experiments, from my point of view, was that they did 
     not have to re-scored by subjective rating, the way other meta-analyses 
     do. Incidentally, I thought I would mention my favorite source on 
     meta-analysis because I did not see it in the bib: Handbook of Research 
     Synthesis, by Cooper & Hedges. Also see, Rikard Larsson, Case survey 
     methodology ..., Academy of Management Journal, v36 n6, Dec 1993, starting 
     on p. 1515.

	A: Thanks. We'll look up the reference.
     
     Several of the parts I really liked are: (1) attention to threats to 
     validity, because we see experiments published only rarely in TSE and then 
     they don't often have a "Threats" section, so I think these authors are 
     continuing to lead the charge of solid experimental reporting, and (2) some 
     display of raw data, such as the way Fig. 8 depicts fault detection 
     probabilities by fault. It is thought-provoking.
     
     One possible weakness - though the authors did not promise the reader they 
     would attempt this - is the lack of a link to theory. What explains the 
     pattern of results? Where would we look for such an explanation? Isn't it 
     too weak to say that neither study found that real groups discovered fewer 
     defects than nominal groups when Fig. 11 shows as much? Isn't a potential 
     theory that meetings interfere with or discourage defect finding or 
     reporting?
     
	A: Theory is important, but we don't really have an adequate one at 
	this point. Our strategy is to get more hard data and work from there. 

     One final comment. I have probably read everything these two authors have 
     ever written. I was struck by this manuscript because of the high 
     occurrence of typographic and logical order errors. This is 
     uncharacteristic of either authors' previous work and demonstrated 
     standard of care, and I think the next version should be read by a senior 
     editor just to assure that it contains no typos or sight errors.
     
     Also, if the editors of TSE do not accept this paper, the authors should 
     consider submitting it to Empirical Software Engineering or the 
     International Journal of Man-Machine Studies.



	A: fixed.



------- end -------
