\section{EXPERIMENT 2: UNIVERSITY OF MARYLAND} 
\label{um-experiment}

\subsection{Motivation}

Like the UH experiment, this study also addressed the questions of 
review meeting effectiveness. However, this experiment focused on 
the argument that (1) many faults or many faults of 
specific types are found during meetings, and therefore they justify their 
cost; and (2) without these meetings the faults would not be found.
Thus, a group of reviewers is likely to be more effective 
working together than working separately.

To test this argument, this experiment evaluated the 
performances of the following three review methods.

{\bf Preparation-Inspection (PI).}  Each reviewer individually analyzes the
artifact in order to become familiar with it.  The goal is not to
discover faults but only to prepare for the review meeting. Reviewers are
prohibited from taking any notes during Preparation.  After all reviewers have
completed Preparation, the team holds an Inspection meeting to
find as many faults as possible.

{\bf Detection-Collection (DC).} Each reviewer individually analyzes the
artifact with the goal of Detecting as many faults as possible. The 
team then meets during the Collection phase to review the document. 
The meeting results will contain faults found during Detection, as
well as new defects found during the meeting itself.

{\bf Detection-Detection (DD).}  Each reviewer individually analyzes the
artifact with the goal of Detecting as many faults as possible. After all
reviewers complete the Detection phase, they are asked to repeat
Detection a second time, again individually, and again with the goal of
detecting as many faults as possible.  This approach does not involve 
a meeting. Instead the time is used by the reviewers to continue working individually.

These methods were chosen in order to answer two questions. The first is
``Will more faults be found in reviews whose meetings have the primary responsibility 
for discovering faults, or will more be found in a review that involves a meeting, but 
in which faults may be found before the meeting starts?''
The second question is, ``How would review performance be affected if the 
time devoted to meetings were used instead for additional individual analysis?'' 


\begin{figure*}
\centering
\begin{tabular}{l|l|l|l|l|l|}
\multicolumn{2}{c}{} & \multicolumn{4}{c}{\large \bf Round/Specification}\\
 \cline{2-6} & & \multicolumn{2}{|c|}{Round 1} &\multicolumn{2}{|c|}{Round 2} \\
\cline{2-6} &               &    WLMS   &       CRUISE       & WLMS         & CRUISE \\
\cline{2-6} {\large \bf Review }    & PI	& 1G, 2A, 2B, 2C  	& 1B, D  & 	&  1E,  1F\\
\cline{2-6} {\large \bf Method}     	& DC 	& 1F, 2D, 2E, 2F     &     	& 1C   	& 1A\\
\cline{2-6} 				& DD    & 1A, 1E, 2G, 2H, 2I  &   1C   & 1D     & 1G \\
\cline{2-6}
\end{tabular}
\vskip 0.5em
\caption{ This Figure shows the settings of the independent variables. The student teams
are denoted 1A--1G. The professional teams are denoted 2A--2I.
The student teams reviewed two documents, the WLMS and CRUISE, one per round, using one
of the three review methods. The professionals only reviewed the WLMS.}
\label{indepvar}
     \vskip 0.5em
     \hrule
\end{figure*}


\subsection{Hypotheses}

One hypothesis of this experiment is that there is no difference in 
the performance of the two review methods that involve meetings.
A second hypothesis is that review methods that eliminate meetings
(the DD method) are at least as cost-effective as methods that rely heavily 
on them (the PI and DC methods) and probably more so. Moreover, 
this should be because the benefit of additional individual analysis 
(as provided by the DD method) is greater than or equal to holding 
review meetings.

\subsection{Experimental Design}
\label{design2}

\subsubsection{Subjects}

The experiment was conducted twice, once in the Spring of 1995 with 
21 graduate students in computer science as subjects, and once 
more in the Fall of 1996 with 27 professional software developers 
as subjects. The subjects were assigned to groups of size 3, for a total
of 16 different groups.

\subsubsection{Design}

This experiment compared the PI, DC, and DD methods 
for reviewing software requirements specifications (SRS).  To limit 
its duration, the experiment used a partial factorial design in
which each team participated in at most two reviews, using one 
of the three reviews methods in each round.  Figure~\ref{indepvar} shows 
the settings of the independent variables.

Each complete run consisted of (1) a training phase in which 
the subjects were taught reviews methods and the experimental 
procedures, and in which they reviewed a sample SRS; and (2) 
an experimental phase in which the subjects conducted 
reviews which were monitored. During the experimental phase the graduate 
students conducted two reviews and the professionals conducted 
one. 


\subsubsection{Variables}
\label{variables}
The experiment manipulated four independent variables:
(1) the review method used by each reviewer (PI, DC, or DD);
(2) the review round (each reviewer participated in two reviews 
during the experiment);
(3) the specification to be reviewed (two are used during the experiment); and
(4) the order in which the specifications are reviewed (either 
specification can be reviewed first.)

The review method is the treatment variable. The other 
variables were used to assess several potential threats to 
the experiment's internal validity. 

Due to time constraints, the professional population conducted only one 
review. Therefore, the only independent variable for that population
is the review Method. For each review several dependent variables
were measured:
(1) the Individual Fault Detection Ratio, 
(2)  the Team Fault Detection Ratio\footnote{The Team and the Individual 
Fault Detection Ratios are the 
number of faults detected by a team or individual divided by the 
total number of faults known to be in the specification.  
The closer these values are to 1, the more effective the detection method. 
No faults were intentionally seeded into the specifications. All 
faults are naturally occurring.}, and 
(3) the Gain Ratio, i.e., the percentage of faults 
initially identified during the 
second phase of the review.(For the PI and DC methods
the second phase is the team meeting; for the DD method it is the
second individual detection activity.)
These calculations are explained in detail in Section \ref{danal}.  

\subsubsection{Threats}
\label{sec:teamselec}

Four threats to internal validity were considered:  
selection effects, maturation effects, instrumentation 
effects, and  presentation effects.

To minimize selection effects in both the graduate student and professional population,
the experimental design composed teams and assigned review methods on a completely 
random basis.  This approach attempts to spread differences in natural ability 
across the review methods in an unbiased fashion.  However, since each
team uses only one or two of the three review methods, differences in 
the methods can't be completely separated from differences in reviewer ability. 

Maturation effects are caused by subjects learning as the experiment proceeds. 
The review method used and the order in which the documents 
are reviewed were manipulated in order to assess this effect. 
These effects do not exist in the professional population since
they only reviewed one document.

Instrumentation effects result from differences in the specification documents.
Such variation is impossible to avoid, but the experiments controlled for it by having each team 
review both documents. Again, these effects are not present in the professional population
since they only reviewed one document.

Finally, presentation effects can occur if reviewing one specification first makes 
it easier to review the remaining one. This possibility was controlled for 
by having half the teams review the documents in each of the two possible orders. 
Once again these effects are not present in the professional population
since they only reviewed one document.

Section~\ref{danal} shows that the variation in the fault detection ratio is not explained by 
selection, maturation, or presentation effects.

The experiment considered three threats to external validity: subject representativeness, 
instrumentation representativeness, and process representativeness.

The graduate student subjects in our experiment may not be 
representative of software programming professionals. Although more
than half of the subjects have 2 or more years of industrial
experience, they are graduate students, not software professionals.
Furthermore, as students they may have different motivations for
participating in the experiment.

The specification documents may not be representative of real 
programming problems. The experimental specifications are atypical
of industrial SRS in two ways. First, most of the experimental 
specification is written in a formal requirements notation (see 
Section \ref{instruments}). Although some industrial groups 
are experimenting with formal notations \cite{ardis.1994,GCR.formal}, it is
not the industry's standard practice. Second, the specifications
used are considerably shorter than typical industrial specifications.

Finally, the review process in our experimental design may not be 
representative of software development practice. We have modeled our
experiment's review process after the ones used in many
development organizations, although each organization may 
adapt the process to fit its specific needs.
Another difference is that the SRS authors are not present at our reviews,
although in practice they normally would be.  Finally, industrial reviewers may 
bring more domain knowledge to a review than our student subjects did.

\subsubsection{Analysis Strategy}\label{sec:danalstrat}
\label{analysis}

The analysis strategy had several steps.
The first step was to find those independent variables 
that individually explained a significant amount
of the variation in the team detection ratio.
The second step was to evaluate the combined effect 
of the variables shown to be significant 
in the initial analysis.  Both analyses used 
standard nonparametric analysis methods 
(see \cite{BHH:Statistics} or \cite{Heiberger.1989}).
Once these relationships were discovered and their magnitude
estimated, other data, such as the gain ratios, was analyzed
to confirm or reject (if possible) a causal relationship 
between the review methods and review performance. 


\subsubsection{Experimental Instrumentation}  
\label{instruments}
Several instruments were used in this experiment: three small
software requirements specifications (SRS), instructions 
for each review method, and a data collection form. 

\paragraph{Software Requirements Specification Documents.}
\label{srs}
The SRS documents describe three event-driven process control 
systems: an elevator control system (ELEVATOR), 
a water level monitoring system (WLMS),
and an automobile cruise control system (CRUISE). 
All faults present in these documents appear in 
the original documents or were generated 
during adaptation; no faults were intentionally seeded into the document. 
The experiments discovered 42 faults in the WLMS SRS 
and 26 in the CRUISE SRS. The number of faults in the  
ELEVATOR SRS is unknown since it was used only for training exercises. 
(These specifications were originally developed 
for another experiment. See Porter et al.\cite{PVB:TSE94}
for details.)

\subsubsection{Fault Reporting Forms}
\label{forms} 

The experiment also required a Fault Report Form. Whenever a 
potential fault was discovered -- during either
the fault detection or the collection activities 
--  an entry was made on this form.  The entry 
included four kinds of information:  
Review Phase (First or Second),
Fault Location (Page and Line Numbers),
Fault Disposition (Faults can be True Faults or False Positives), 
and Fault Description (in prose).

\subsubsection{Experimental Procedures}
\label{preparation}

The participants were given two lectures of 75 minutes each on  
software requirements specifications~\cite{ieee.1984},
the SCR tabular requirements notation~\cite{heninger80},
review procedures~\cite{Fagan76,parnas.1985}, and on how 
to fill out the data collection forms. 
The participants were then divided into 
three-person teams -- (see Section~\ref{sec:teamselec} 
for details.) Within each team, members were randomly assigned 
to act as the moderator, the recorder, or the reader during the
collection meeting.

\label{conducting}

\paragraph{Training Phase.}

For the training exercise, each team reviewed  
the ELEVATOR SRS. Individual team members read the specification 
and recorded all faults they found on a Fault Report Form. 
Their efforts were restricted to two hours. Later the experimenters
met with the participants and answered questions about the experimental
procedures. The ELEVATOR SRS was not used in the remainder of the experiment.


\begin{figure*}[t]
\hrule
\vskip 0.5em
\centerline{\psfig{figure=contrast.prn,height=3.5in,width=6in}}
\vskip 0.5em
\caption{{\bf Fault Detection Ratios by Independent Variable.}
The dashes in the far left column of each panel shows each team's 
fault detection ratio. The horizontal line is the average fault 
detection ratio. The plot demonstrates the ability of each variable 
to explain variation in the fault detection ratios. For example,
for the Method variable, the vertical location of DD symbol 
is determined by averaging the fault detection ratios for all teams 
reviewing with the DD method. The vertical bracket, ], to the right of 
the variable shows one standard error of the difference between 
two settings of the variable. }
\vskip 0.5em
\hrule
\label{fig:d1.all}
\end{figure*}

\paragraph{the Experimental Phase.}

\label{sec:experm_results}
The experimental phase involved two review rounds for the graduate 
students and one for the professionals. The instruments used 
were the WLMS and CRUISE specifications discussed in Section 
\ref{srs}, and the Fault Report Form. 

During the first Round, three of the seven graduate student teams 
were asked to review the CRUISE specification; the remaining four 
teams reviewed the WLMS specification. Each review involved 
two phases that differed according to the method used.  
All the professional teams reviewed the WLMS in the first Round.  
The review methods used by each team are shown in Figure~\ref{indepvar}.

The first phase for the Detection-Collection and Detection-Detection 
methods lasted up to 2.5 hours, and all potential faults were reported 
on the Fault Report Form. The first phase for the Preparation-Inspection
method took the same amount of time, but 
the reviewers were not allowed to report faults or take any notes. 
After the first phase all materials were collected.
For the student population a total of 28, 2-hour time slots were set aside 
during which review tasks could be done.  Participants performed each task 
within a single two-hour session and were not allowed to work at 
other times. For the professional population, the review was conducted as part of 
a class session with each phase limited to 2 hours.

Once the team finished the first phase, the moderator arranged for the 
second phase which was also limited to 2.5 hours. This second phase
involved a team meeting for the PI and DC methods
during which the reader paraphrased each requirement,
and the reviewers brought up any issues they had found earlier or 
had just discovered. The team  recorder  maintained the team's master 
Fault Report Form. The DD team did not hold a second meeting. Their second 
phase was exactly the same as the first -- individual fault detection.  
The entire first Round was completed in one week. The second Round for 
the graduate students was similar to the first except that teams who 
had reviewed the WLMS during Round 1 reviewed the CRUISE in Round 2 and 
vice versa. 


\subsubsection{Data Collection}
\label{danal}

Two sets of data are important to our study:  
the Individual Fault Summaries and the Team 
Fault Summaries. 

An individual fault summary shows whether a reviewer discovered
a particular fault. These data are gathered from the Fault Report 
Forms the reviewers completed during fault detection.

A team fault summary shows whether a team discovered a particular 
fault. For the PI and DC methods these data are gathered from the Fault Report 
Forms filled out at the collection meetings. For the DD method the team 
summary is the set union of faults recorded by all reviewers. These data
are used to assess the effectiveness of each fault detection method. 


One problem with the team summaries is that the DC and PI methods 
involve meetings, but the DD method doesn't. Consequently, 
any meeting losses -- faults discovered by individuals before 
the meeting that don't appear in the team fault report --
will lower the fault detection ratio for these two methods
but not, of course, for the DD method.  However, 
since meeting losses average only
about 5\% for the DC method (meeting losses can't be measured 
for the PI method) they were considered insignificant.

This experiment also compared the benefits of meetings (DC method)
with the benefits of additional individual analysis (DD method).
For the DC method meetings gains are calculated from the 
individual and the team summaries. For the DD method meeting 
gains are calculated by determining whether each fault was 
originally discovered during the first or the second detection activity.


\subsection{Experimental Results}
The analysis was done in three steps: (1) The team fault detection ratios 
were compared to ascertain whether the review methods had the same 
effectiveness. (2) The first and second round performance of individuals and teams 
were then compared to see how the different treatments performed. (3) Finally, 
individual fault summaries were analyzed to determine whether different treatments 
found different faults.

\subsubsection{Analysis of Team Performance}

If the hypothesis that reviews without meetings are
no less effective than reviews with meetings
is true, then the performances of the DD method
should not be significantly lower than those of the other two methods.  
The analysis strategy was first to determine whether various 
threats to the experiment's internal validity could be seen and 
second to test this hypothesis.

The first analysis measured each independent 
variable's contribution to review performance. 
The Wilcoxon test showed that for the student population,
Review Method and Specification are significant, but the Round, 
Order, and Team Composition are not.  Review Method was
also significant for the professional population. 

Figure~\ref{tbl:analin} shows the input to this analysis. Six of the cells 
contain the average detection ratio for teams using each review method 
and specification (3 detection methods applied to 2 specifications).  

%
\begin{figure*}[tbp]
\hrule
\vskip 0.5em
\centering
\begin{tabular}{|l|l|l|l|}
\cline{1-4}
\multicolumn{1}{|c|}{{\bf Specification}} & \multicolumn{3}{|c|}{\bf Review Method}\\
\cline{2-4}
  &\multicolumn{1}{|c|}{{\protect\bf PI}}   &\multicolumn{1}{|c|}{\protect\bf DC}    &\multicolumn{1}{|c|}{\protect\bf DD}\\
\cline{1-4}
\multicolumn{1}{|c|}{{\bf WLMS}} &\multicolumn{1}{|c|}{ (.21) (.24) (.29) .36} &\multicolumn{1}{|c|}{(.14) (.21) (.21) .24}     &\multicolumn{1}{|c|}{.33 .4 (.4) (.43) (.45) .55 .69}\\
\cline{1-4}
\multicolumn{1}{|c|}{(average)}&\multicolumn{1}{|c|}{(.24) .36}       &\multicolumn{1}{|c|}{(.21) .24}      &\multicolumn{1}{|c|}{(.43) .46}\\
\cline{1-4}
\multicolumn{1}{|c|}{{\bf Cruise}}&\multicolumn{1}{|c|}{.08 .12 .19 .23} &\multicolumn{1}{|c|}{.19 .23}  &\multicolumn{1}{|c|}{ .46}\\
\cline{1-4}
\multicolumn{1}{|c|}{(average)} &\multicolumn{1}{|c|}{.15}   &\multicolumn{1}{|c|}{.22}      &\multicolumn{1}{|c|}{.46}\\
\cline{1-4}
\end{tabular}
     \vskip 0.5em
\caption{{\bf Team Fault Detection Ratio.} This table shows the
nominal and average fault detection ratios for all 16 teams (7 graduate student and 
9 professional teams). There are only 22 observations, because one student team 
dropped out due to a team member's illness. Data from the professional population is
enclosed in parentheses.}
\label{tbl:analin}
\vskip 0.5em
\hrule
\end{figure*}


Next, the performances of each method were compared. 
Figures  ~\ref{fig:d1.all} and ~\ref{tbl:analin} summarize 
the defect detection data.  As depicted, the {DD} review method resulted 
in the highest fault detection ratios (49\% for graduate students, 43\% for professionals). 
No difference could be detected between the {DC} review method (22\% for 
graduate students, 19\% for professionals), and the PI detection method 
(19\% for graduate students, 24\% for professionals). These differences 
are consistent across both the graduate student and professional populations. 


\subsubsection{Analysis of Detection Ratios for Specific Faults}
\label{specfaultanal}

Even if reviews with meetings are less effective than
reviews without meetings, team meetings might still be beneficial if
they promote detection of classes of faults that individuals seldom
or never find.  If this hypothesis is true, then there should be a subset of
faults for which the probability of their being found by
the DC and PI methods is greater than by the  DD method.

Figure~\ref{specFaults} compares probability of detecting each 
WLMS defect when reviewing with meetings (DC and PI) and 
without (DD).  The probability of detecting a defect with the DD 
method is defined  as the number of teams that used the DD method 
and discovered the defect divided by the total number of teams 
that used the DD method.  The detection probabilities for the 
other two methods are calculated similarly. 

For each defect the following test is used to determine whether
its detection probability is significantly higher when inspecting 
with a meeting than when inspecting without one. 

Assuming that the detection of each defect follows a binomial
distribution, we first estimate the the detection probability 
for reviews without meetings (i.e., p in the binomial distribution). 
The estimate is the percentage of DD groups that found the defect. 

Next, the actual number of teams that detected the defect during 
a meeting-based review is calculated. Finally, a standard significance 
test is used to determine whether the number of teams that found the
defect during a meeting-based review is significantly
greater than that expected given p.

In this study only 5 faults in the WLMS had a higher detection 
probability with meetings than without. Of these, three were rarely detected 
with either method, one was frequently detected with both, and one (defect 17) 
was found with substantially greater frequency by reviews with meetings. 
Based on this data there appears to be insufficient evidence to support the 
hypothesis that ``meeting sensitive'' faults are common in review.


\begin{figure*}[t]
\hrule
\vskip 0.5em
\centerline{\psfig{figure=spec.prn,height=3.5in,width=6in}}
\vskip 0.5em
\caption{{\bf Individual Defect Detection Probabilities for Reviews with and 
without Meetings.} Each defect is represented by an index along the horizontal 
axis. The with-meeting and without-meeting detection probabilities are represented
by an arrow. That arrow's head indicates the with-meeting probability and its tail 
indicates the without-meeting probability. When a defect is found more often with 
a meeting than without, the arrow point up.  The longer the arrow, the greater the 
difference between the two probabilities. }
\vskip 0.5em
\hrule
\label{specFaults}
\end{figure*}


\subsubsection{Analysis of Second Phase Performance}

The previous analysis showed that in the current experiment, reviews
without meetings appear to be more effective than reviews with meetings.
This section examines whether the current data support 
the original hypothesis that meetingless reviews would
be at least as effective because the benefits of having a meeting wouldn't
outweigh the benefits of additional individual analysis. 

If this hypothesis is true, then there should be little difference
between the first phase performances of the DC and DD methods, but 
significant differences between the second phase performances.
Therefore, the analysis strategy was to isolate the first and second phase 
performances to see how well they explain differences in total review
performance. 

This analysis is restricted to WLMS reviews only,
and includes data from 14 WLMS reviews taken from two earlier 
review experiments \cite{PV:TR}. These reviews involved both 
graduate student and professional populations and all used the DC method. 
This was done to increase our sample of DC reviews. 
This new data appears to be similar to that of the current experiment
because the average defect detection ratio of the new reviews is 
36\%, nearly identical (34\%) to the average for the current study. 
Furthermore, a Wilcoxon test was unable to reject the hypothesis 
that these two sets of data were drawn from the same distributions (p=.63).  

Figure~\ref{indWOverlap} contains a boxplot showing the
number of faults found by each reviewer during the 
first phase of the DD and DC reviews. The ratios 
for both methods are statistically indistinguishable.

\begin{figure*}[tbp]
\hrule
\vskip 0.5em
\centerline{\psfig{figure=team.prn,height=2.5in,width=6in}}
\vskip 0.5em
\caption{{\bf Team and Individual Fault Detection Densities.}
This figure shows the percentage of faults discovered by each 
individual during the first phase and by each team during the 
first and second phases of DD and DC review.}
\label{indWOverlap}
\vskip 0.5em
\hrule
\end{figure*}
%

Figure \ref{indWOverlap} shows the number of unique faults 
discovered by each team during the first phase of a DD or DC
review. This is the set union of faults found by all team 
members. Again the ratios appear to be similar (p=.61). 

However, Figure \ref{indWOverlap} shows that the number of 
unique faults discovered during the second phase of a DD 
review are higher that those discovered during DC review
(p=$<.001$). 
 
Based on this analysis it appears that, in terms of number of
defects found, the benefit of having a review meeting is less
than that of performing additional individual analysis. 

