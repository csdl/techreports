\section{Meeting-Sensitive Defects}

If we knew which faults should be meeting sensitive, we could test 
whether they are so in this experiment. For example, if we had  
a classification scheme that could distinguish meeting sensitive 
from meeting insensitive faults we could 
separate each specification's faults into two 
populations and then compare the detection 
probabilities of each population using each inspection method.
The faults suspected of being meeting sensitive should have a higher
probability of being found at a meeting.

Since at present we don't have any such classification scheme
we can't do this and therefore our approach is to look
for ``markers'' or ``indicators'' that are associated with the
presence or absence of meeting sensitive faults.
For example, if the detection probabilities of meeting sensitive 
faults are statistically distinguishable from
those of meeting insensitive faults, then we can identify 
those faults which are very likely or very unlikely to be
meeting sensitive.  

To estimate these detection probabilities we conducted a
Monte Carlo simulation of inspections under a variety of 
different conditions (e.g., different proportions of 
total faults that are meeting sensitive). The outcome of each 
simulation run is the expected detection probabilities for meeting
sensitive and insensitive faults when the inspection is conducted 
with and without meetings.  

This information allows us to answer three questions.

\begin{enumerate}

\item If none of the faults are meeting sensitive, what is the 
inter-treatment difference in detection probabilities?

\item As the proportion of meeting sensitive faults grows, how
does this affect the inter-treatment difference in detection probabilities?

\item At what magnitude does the inter-treatment difference 
in detection probabilities become statistically significant?

\end{enumerate}

%\begin{figure*}[tbp]
%\hrule
%\vskip 0.5em
%\centerline{\psfig{figure=secondPhasePerf.ps,height=2.5in,angle=-90}}
%\vskip 0.5em
%\caption{{\bf Team Fault Detection Rate (Phase 2).}
%This figure shows the number of faults discovered by each team
%during the second phase of a DD or DC inspection.}
%\label{secondPhase}
%\vskip 0.5em
%\hrule
%\end{figure*}
%%

Each run simulates using two treatments, $T_a$ and $T_b$,
to inspect either the WLMS or CRUISE specification. 
The fault detection probabilities for each treatment 
are $p_a$ and $p_b$ and are based on the probabilities
$p_{DD}$ and $p_{DC,PI}$ observed during the experiment.   

The simulation manipulates five independent variables:

\begin{enumerate}

\item the specification (WLMS or CRUISE),

\item the detection rate for $T_a$. ($p_a = p_{DD}$ + \{ -4, -2, 0, 2, or 4 \}
$\times factor$, where $factor = \frac{1}{42} = 0.023 $ for the WLMS and 
$factor = \frac{1}{26} = 0.038$ for the CRUISE),

\item the detection rate for $T_b$. ($p_b = p_{DC,PI}$ + \{ -4, -2, 0, 2, or 4 \} 
$\times factor$), 

\item the proportion of faults that are meeting sensitive (0, .1, .2, .25, .4,
.5, .6, .75, .9 or 1 \}),

\item the detection rate for meeting sensitive defects ( $p_{sens} = p_a 
\times $ \{1.0, 1.1, 1.2, 1.25, 1.4, 1.5, 1.6, 1.75, 1.9, or 2\}. Note that 
the detection
probability for insensitive faults, $p_{insens}$, is now constrained by the 
values of $p_b$ and $p_{sens}$.),

\end{enumerate}

For each combination of independent variables we simulate 50 inspections of 
a single specification. Half of the inspections use treatment $T_a$; 
the other half use treatment $T_b$. 
For $T_a$, each fault's detection probability is estimated by 
the proportion of successes in a random draw from a Bernoulli 
distribution with parameters 25 and $p_a$. A similar approach is 
used for $T_b$, except that meeting sensitive faults 
use a Bernoulli distribution with parameters 25 and $p_{sens}$
and the insensitive faults use parameters  25 and $p_{insens}$.
The difference between the detection probability for a given fault
when using $T_a$ and when using $T_b$ is called the inter-treatment 
difference:

For each run we calculate two dependent variables:

\begin{enumerate}

\item the average inter-treatment difference for meeting sensitive faults, 
\item the average inter-treatment difference for meeting insensitive faults.

\end{enumerate}


Figure \ref{sim.wlms} 
%and \ref{sim.cruise} 
shows some of the simulation results (WLMS only), 
about which we make several observations. 
(1) The larger the difference between $p_a$ and $p_b$,
the smaller the proportion of meeting sensitive faults can be,
but the easier it will be to differentiate sensitive faults from 
insensitive ones. 
(2) The meeting sensitivity will have to be roughly 
$p_a \times 1.5$ for a statistically significant difference
to be detectable.
(3) In many cases the two populations of meeting sensitive and insensitive 
faults will be statistically
indistinguishable from a population having no meeting sensitive faults. 

