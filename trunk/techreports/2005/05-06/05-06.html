<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html>
  <head>
    <title>Empirical Readings for Software Engineering Researchers</title>
  </head>
<body text="#330066">
<font face="verdana">

    <h1 align="center">Readings in Empirical Evaluation for&nbsp;<br>
    Budding Software Engineering Researchers</h1>
    <p align="center"><a href="http://csdl.ics.hawaii.edu/~johnson/">Philip
    Johnson</a><br>
    <a href="http://csdl.ics.hawaii.edu/">Collaborative Software Development
    Laboratory</a><br>
    <a href="http://www.hawaii.edu/">University of Hawaii&nbsp;</a></p>


    <p align="center">Last Update: <!--webbot bot="Timestamp" startspan
    S-Type="EDITED" S-Format="%m/%d/%Y %I:%M %p" -->08/03/2005 11:47 AM<!--webbot
    bot="Timestamp" i-CheckSum="26548" endspan -->
    </p>


    <h3>Motivation</h3>

    <p>In CSDL, students usually write a thesis describing research in software
    engineering.&nbsp; In the CSDL research culture, this typically means (a)
    designing and implementing a new kind of technology, (b) performing an
    empirical study using the technology, and (c) writing up the results. </p>

    <p>In many cases, the hardest part is coming up with an appropriate and effective empirical
    study.&nbsp; In some circumstances, it appears to me that
    students look at old theses, find a survey that was given out in a
    classroom, and modify it slightly.&nbsp; While learning from examples is not
    a bad thing to do, it is important to develop a deeper understanding of how
    to do empirical studies in order to make sure that the example you are
    leveraging is appropriate to your situation. </p>

    <p>To help all of us become more sophisticated in our approach to empirical
    studies in the context of software engineering research, this technical report provides
    pointers to a set of readings.&nbsp;&nbsp; </p>

    <p>As you proceed with your research, you are attempting to create a
    &quot;tight&quot; research project: one that contains (a) a motivation in
    terms of a 'big' problem in software engineering whose solution would be
    important; (b) a set of research questions regarding some concrete aspect of
    the 'big' problem, (c) a set of testable hypotheses corresponding to these
    research questions, (d) an evaluation methodology that yields evidence
    either for or against the hypotheses, (e) the data you collected, and (f)
    your interpretation of the results.&nbsp; I find that in the initial phases
    of research, some students come up with interesting technological ideas
    without good research questions; in other words, they have a solution
    looking for a problem . That's not good. Others come up with interesting
    hypotheses that aren't actually tested by the data they intend to
    collect.&nbsp; That's not good either.&nbsp;&nbsp; A good research project
    &quot;hangs together&quot; as a whole. </p>

    <p>This is much, much harder than it might seem at first glance.&nbsp;
    Empirical design is a skill that requires practice to become good at.&nbsp;
    Just as you don't read a book on software design with the expectation of
    finding the exact design required for your software system, you shouldn't
    read the following papers with the expectation of finding the exact
    evaluation design appropriate for your research.&nbsp; Similarly, just as
    you wouldn't expect to become a good software designer just by reading a
    book or two, you can't become a good experimentalist just by reading through
    the following links (although that's a good first step).&nbsp; To be a good
    designer of either software or empirical evaluation takes practice and
    experience.&nbsp;&nbsp; But you have to start somewhere, so this document
    contains
    citations that I hope will help you get some traction.&nbsp; </p>

    <h3>CSDL Empirical Evaluation Strategies</h3>
    <p>As I noted above, almost all CSDL theses involve the development of a
    novel software-based technology.&nbsp; This greatly influences the approach
    to evaluation. (In contrast, theses in &quot;information technology&quot;
    might study a pre-existing technology, and thus their whole research process
    can be devoted to evaluation.&nbsp; In CSDL, a substantial amount of the
    overall research effort involves the design and implementation of the new
    technology.)</p>
    <p>If you look at prior CSDL theses, the evaluation tends to address one (or
    more) of the following issues:</p>
    <p><i>(1) What happens when users employ my technology?&nbsp;</i> This is
    the most basic form of evaluation, and is essentially a usability
    evaluation. The common pitfall when developing this kind of evaluation in
    the context of CSDL is to focus too much on superficial surface
    characteristics of the technology, such as fonts, colors, menu items, etc.
    (This is because much of the usability literature seems to focus on these
    issues.)&nbsp; Although some evaluation of the &quot;look and feel&quot; is
    important, your usability evaluation should focus primarily on ways to
    discover whether the technology worked correctly, whether users used your
    technology correctly, whether they feel they benefited from the use of your
    technology, what obstacles they encountered while using it, and so forth. In
    general, you want to find out what you would want to know if you were going
    to embark on the 2.0 version of your technology.&nbsp;&nbsp; By reading the
    literature on usability evaluation, you will find that there are many
    different ways to accomplish this task, from questionnaires, to video-taped
    observations, to interviews.&nbsp; All of these have been used in CSDL in
    the past; you must determine what approach is best suited to your
    situation.&nbsp;</p>
    <p><i>(2) Is my technology effective?</i> Unlike the first form of
    evaluation, which is essentially descriptive in nature, determining whether
    your technology is effective involves some form of <i>comparison</i>.
    Basically, you want to see if your technology produces some kind of change,
    or effect.&nbsp; For this to work, you need to gather at least two different
    groups of data and compare the values you obtained.&nbsp; How you create
    these groups really depends upon the nature of your technology and research
    questions.&nbsp; For example, Danu Tjahjono's evaluation involved splitting
    up two classes of students into multiple groups to test different inspection
    approaches.&nbsp; Aaron Kagawa's evaluation involved splitting up Hackystat
    packages into two groups: one group being packages that should be less in
    need of inspection and the other group being packages that should be more in
    need of inspection. Cedric Zhang's evaluation involves a comparison over
    time: first a set of baseline measures are obtained, then the technology is
    introduced, and the new values for the measures are compared to the original
    ones.&nbsp;</p>
    <p><i>(3) What broader issue(s) in software engineering can be investigated
    using my technology?&nbsp;</i> In most cases, CSDL research results in
    technology that not only provides automated support for some kind of
    practice, it also provides infrastructure that helps address more
    fundamental questions in software engineering.&nbsp; In other words, it can
    serve as experimental infrastructure.&nbsp;
    As an example, Danu Tjahjono's research on CSRS not only resulted in
    technology support for code inspection, it also provided infrastructure that
    allowed him to investigate whether the group meeting phase of inspection was
    actually cost-effective for defect removal.&nbsp;</p>
    <p>In general, B.S. theses tend to address (1) and perhaps (2), M.S. theses
    tend to address (1) and (2), and Ph.D. theses tend to address (1), (2), and
    (3).&nbsp;&nbsp;</p>

    <h3>General Introductions to Empirical Research Design&nbsp; </h3>

    <p>One question you should be prepared to answer at your thesis defense: why
    did you _not_ choose another empirical evaluation approach (i.e. a
    controlled experiment, or an ethnographic study, etc.)&nbsp; In other words,
    why is the design that you chose the best one for your research situation?
    In order to answer that question, you need to first understand what the
    alternatives are.&nbsp; This section provides links to two sites:
    allpsych.com and socialresearchmethods.net, both of which have well-written
    overviews of the various empirical design methods.</p>

    <p><i>AllPsych.com:</i></p>

    <p><a href="http://allpsych.com/researchmethods/singlesubjectdesign.html">Research
    Methods, Chapter 4: Single Subject Design. </a> This chapter shows an
    experimental design method for comparing treatment effects on a single
    subject or a group of single subjects.&nbsp; The basic idea is to begin with
    a pre-test, or collection of baseline information, then introduce the
    treatment, and see if the baselines change.&nbsp; </p>
    <p><a href="http://allpsych.com/researchmethods/experimentaldesign.html">Research
    Methods, Chapter 5: Experimental Design. </a> This chapter introduces three
    basic experimental designs: (1) pre-experimental design, (2)
    quasi-experimental design, and (3) true experimental design.&nbsp; </p>
    <p><a href="http://allpsych.com/researchmethods/otherresearchdesigns.html">Research
    Methods: Chapter 6: Historical, Developmental, and Qualitative Research
    Design.</a>&nbsp; These are 'non-experimental', or qualitative research
    designs. If properly designed and executed, these designs aren't any
    &quot;less&quot; valid than experimental designs, they are just
    &quot;differently&quot; valid. </p>
    <p><i>SocialResearchMethods.net:</i></p>
    <p><a href="http://www.socialresearchmethods.net/kb/design.htm">Experimental
    Design.</a>&nbsp; This chapter provides another overview of experimental
    (and quasi-experimental) design, and discusses the trade-offs between them. </p>
    <p><i>Books and other offline resources:</i> </p>
    <p>Experimental and Quasi-Experimental Designs for Research, Donald Campbell
    and Julian Stanley, Houghton-Mifflin, 1963. (Recommended by Victor Basili) </p>
    <p>Research Methods in Social Relations, Charles Judd, Eliot Smith, Louise
    Kidder, Harcourt Brace Jovanovich, 1991. (Recommended by Larry Votta). </p>
    <p>Case Study Research, Robert Yin, Sage Publications, 1994. </p>
    <h3>Guidelines for Empirical Research</h3>
    <p>Although most of the papers cited on this page present guidelines of one
    sort or another, the following articles provide an excellent overview.&nbsp;
    </p>
    <p><a href="doc/Kitchenham2002.pdf">Preliminary guidelines for empirical
    research in software engineering, Barbara Kitchenham et al. </a>Once you
    have decided on a possible evaluation approach for your research, this paper
    can help you identify the key pieces of information that you must specify to
    carry out the evaluation appropriately. </p>
    <p><a href="doc/Shaw2003.pdf">Writing good software engineering research
    papers, Mary Shaw. </a> Discusses research paradigms present in typical
    software engineering conference paper submissions, the concerns of program
    committee members, and suggestions on how to design research and present
    results for optimal acceptance. </p>
    <p><a href="doc/Singer2002.pdf">Ethical issues in empirical studies of
    software engineering, Singer et al. </a>Introduces ethical issues that arise
    in software engineering research and how to best address them. </p>
    <h3>Perspectives on the state of empirical research in software engineering</h3>
    <p>These papers combine practical guidance on empirical research with a
    discussion of the state of the discipline.&nbsp; </p>
    <p><a href="doc/Basili1986.pdf">Experimentation in Software Engineering,
    Victor Basili et al. </a>This paper provides a framework for classification
    of experimental research in software engineering, and recommendations for
    future experimental research.</p>
    <p><a href="doc/Tichy1998.pdf">Should computer scientists experiment more?,
    Walter Tichy. </a> An article describing (and refuting) common
    misperceptions regarding empirical evaluation in software engineering.</p>
    <p><a href="doc/Shaw2002.pdf">What makes good research in software
    engineering?, Mary Shaw. </a> Provides an overview of research paradigms in
    software engineering. </p>
    <p><a href="doc/Perry2000.pdf">Empirical Studies of Software Engineering,
    Perry at al. </a> Discusses why we need empirical studies, common problems
    in empirical research in software engineering, and guidelines to support
    useful software engineering empirical research.</p>
    <p><a href="doc/Zelkowitz1997.pdf">Experimental Validation in Software
    Engineering</a>, Marvin Zelkowitz and Dolores Wallace. Discusses a 12 model
    classification scheme for experimental software engineering research and
    uses it to evaluate how software engineering research validates its theories
    and how software engineering compares to other scientific disciplines.&nbsp;</p>
    <h3>Qualitative Research</h3>
    <p>Most CSDL research has a qualitative component.&nbsp; These links provide useful insight into how to do effective qualitative research.</p>
    <p><a href="doc/Kitchenham1995.pdf">Case Studies for Method and Tool
    Evaluation, Barbara Kitchenham et al. </a> Provides a nice introduction to
    case studies, indicating how they differ from experimental studies, and what
    guidelines to follow to help improve the usefulness of the results. </p>
    <p><a href="doc/Seaman1999.pdf">Qualitative Methods in Empirical Studies of
    Software Engineering, Seaman et al.&nbsp; </a>Discusses qualitative
    techniques for gathering information about the human aspects of software
    engineering and how they can be integrated with quantitative techniques. </p>
    <p><a href="doc/Carver2004.pdf">Using Qualitative Methods in Software
    Engineering</a>, Jeff Carver, Carolyn Seaman, Ross Jeffrey.&nbsp; These are
    slides from the 2004 International Advanced School of Empirical Software
    Engineering. Provides insights on effective interviewing, participant
    observation, surveys, coding, along with case studies of qualitative
    software engineering research. </p>

    <p><a href="http://www.scu.edu.au/schools/gcm/ar/arp/grounded.html">Grounded theory:
a thumbnail sketch, Bob Dick</a>. Provides a very nice overview of Grounded Theory, a
data-driven, qualitative, emergent approach to theory building. Includes references and 
online links to further information. </p>

    <h3>Specialized issues in software engineering empirical research</h3>
    <p>This section provides more detailed information about specific kinds of
    empirical research issues.&nbsp; They will be more or less relevant to you
    depending upon the empirical design you choose. In many cases, these links
    are only a starting point; you will have to do more research to learn how to
    apply the techniques introduced below.&nbsp; </p>
    <p><i>Usability Evaluation:</i> </p>
    <p><a href="http://www.pages.drexel.edu/~zwz22/UsabilityHome.html">Usability
    Evaluation Methods, Zhijun Zhang.</a>&nbsp; Provides an overview of the
    three types of usability methods: Testing, Inspection, and&nbsp;
    Inquiry.&nbsp; </p>
    <p><a href="doc/Axup2005.html">Comparison of Usability Evaluation Methods,
    Jeff Axup.</a>&nbsp; Provides a taxonomy of 10 usability evaluation methods
    with comparisons and contrasts.&nbsp; </p>
    <p><i>Surveys and questionnaires:</i></p>
    <p><a href="doc/Kirakowski2000.html">Questionnaires in Usability
    Engineering, Jurek Kirakowski. </a>Provides a good introduction to the
    design of questionnaires, including concepts of reliability and validity. </p>
    <p><a href="http://csdl.ics.hawaii.edu/techreports/05-06/doc/Stasko.html">Questionnaire
    Design, John Stasko.</a>&nbsp; Provides an introduction to questionnaire
    design, including how to formulate objectives, when to use questionnaires,
    and how to write questionnaire questions.&nbsp; </p>
    <p><a href="doc/Burkey2003.pdf">Web-Based Surveys for Corporate Information
    Gathering: A Bias-Reducing Design Framework, Jake Burkey at al. </a> This
    paper reviews literature on web-based surveying and discusses how to design
    statistically useful web-based surveys. </p>
    <p><i>Use of students:</i></p>
    <p><a href="http://www.cs.ua.edu/~carver/Papers/Journal/2010/EMSE_15_1_AuthorVersion.pdf">A checklist for integrating student empirical studies with research and teaching goals, Jeff Carver et al. </a> As the
    title suggests, this paper provides guidance for the use of students in empirical studies. </p>
    <p><i>Theory:</i></p>
    <p><a href="doc/Sutton1995.pdf">What Theory is NOT, Robert Sutton et
    al.&nbsp; </a>An interesting paper about the importance of grounding
    empirical research in an underlying theory. </p>
    <p><a href="doc/Dimaggio1995.pdf">Comments on What a Theory is Not, Paul
    DiMaggio. </a> Feedback on the article, with additional perspectives on
    theory. </p>
    <p><a href="doc/Weber2003.pdf">Theoretically Speaking, Ron Weber.</a>&nbsp;
    A perspective on the nature of theories, what makes a good theory,&nbsp; and
    how to make theories useful. </p>
    <p><i>Metric validation:</i></p>
    <p><a href="doc/ElEmam2000.pdf">A methodology for validating software
    product metrics, Khaled El Emam.</a>&nbsp; If you are proposing a new kind
    of measure in your research, you must consider the issue of its validation,
    or how to assess what the meaning of the numbers produced by the measure are
    (if any).</p>
    <p><i>Field studies:</i></p>
    <p><a href="doc/Klein1999.pdf">A set of principles for conducting and
    evaluating interpretive field studies in information systems, Klein et al. </a>
    If your research involves the study of an external organization, this paper
    can help you understand how to best collect and evaluate your data. </p>
    <p><i>Technology adoption:</i></p>
    <p><a href="doc/Venkatesh2003.pdf">User acceptance of information
    technology: Toward a unified view, Viswanath Venkatesh. </a> If your
    evaluation focuses on the adoption of your technology, this article
    provides a good overview of some of the issues. </p>
    <p><i>Statistical analysis:</i></p>
    <p><a href="http://www.ruf.rice.edu/~lane/case_studies/index.html">Case
    study examples from the Rice Virtual Lab in Statistics.</a> A very cool site
    that shows various kinds of analyses on real case study data. Gives you a
    nice, practical introduction to statistical analyses appropriate to various
    experimental designs. </p>
    <p><i>Model generation:</i> </p>
    <p><a href="doc/Boehm2003.pdf">Building Parametric Models</a>, Barry Boehm.
    These are slides from the 2003 International Advanced School of Empirical
    Software Engineering. Provides insights on how to build parametric models,
    including an 8 step model development process. Uses examples from the COCOMO
    model family. </p>
    <h3>Help improve this technical report</h3>
    <p>I am sure there are useful articles I have missed in this
    collection.&nbsp; Please email me with suggestions on how to improve the
    content and structure of this technical report.</p>
    <p>&nbsp;</p>
    <p>&nbsp;</p>

    <p>&nbsp;</p>

      </font>
<pre>
</pre>
<p>&nbsp;</p>
<p>&nbsp;</p>
</body>
</html>
