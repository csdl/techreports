<!DOCTYPE HTML PUBLIC "-//IETF/DTD HTML 3.2//EN">
<HTML>
<HEAD>
   <META http-equiv="Keywords" content="questionnaire HCI Human Factors usability computers testing">
   <META NAME="Author" CONTENT="Jurek Kirakowski">
   <TITLE>Questionnaires in Usability Engineering FAQ</TITLE>
</HEAD>

<BODY>



<A NAME="top"></A><IMG SRC="../hfrglogo.gif" ALT="HFRG" HEIGHT=96 WIDTH=96 ALIGN=LEFT>
<CENTER>
<H1>Questionnaires in Usability Engineering</H1>
<H2>A List of Frequently Asked Questions (3rd Ed.)</H2>
</CENTER>
<H3>Compiled by: Jurek Kirakowski,<BR>
Human Factors Research Group, Cork, Ireland.<BR>
This edition: 2nd June, 2000.</H3>
<IMG SRC="../bar13.gif" ALT="Line"  HEIGHT=3 WIDTH=100%>
<DIV ALIGN=right><A HREF="index.html"><IMG SRC="../arrowleft.gif" ALT="Resources Page" BORDER=0 HEIGHT=20 WIDTH=20 ALIGN=CENTER></A></DIV>

<blockquote>
<P><em>Over the years, I have seen many questions asked about
the use of questionnaires in usability engineering. The list on this page
is a compilation of the questions I have heard most often and the answers
I gave, should have given, or would have given if I had thought of it first.</P>
<P>A number of folk have given me feedback on this document, and they are
gratefully acknowledged <A HREF="#acknowledgements">below.</a></P>
<P>There is a mailto: box at the <A HREF="#anycomments">bottom of the page:</A>
I will be delighted to receive more comments, questions, or corrections.</em></P>
<BR>
</blockquote>
<IMG SRC="../bar13.gif" ALT="Line" HEIGHT=3 WIDTH=100%>

<blockquote>
<blockquote>
<H3>Index of questions on this page<em>(use the 'back' button on your browser to get back to this index)</em></H3>

<UL>
<LI><a href="#whatisaquestionnaire">What is a questionnaire?</a></li>

<li><a href="#aretheredifferentkindsof">Are there different kinds of questions?</a></li>

<LI><A HREF="#Whataretheadvantages">What are the advantages of using questionnaires in usability research?</A></LI>

<LI><A HREF="#whatarethedisadvantages">What are the disadvantages?</A></LI>

<LI><A HREF="#howdoquestionnaires">How do questionnaires fit in with other HCI evaluation methods?</a></li>

<li><a href="#whatisreliability">What is meant by <em>reliability?</em></a></li>

<li><a href="#whatisvalidity">What is meant by <em>validity?</em></a></li>

<LI><A HREF="#shouldidevelopmy own">Should I develop my own questionnaire?</A></LI>

<LI><A HREF="#whatswrongwithputting">What's wrong with putting a quick-and-dirty
questionnaire together?</A></LI>

<LI><A HREF="#factualtypequestionnaires">Factual-type of questionnaires are
easy to do, though, aren't they?</A></LI>

<LI><A HREF="#whatstheidfferencebetween">What's the difference between a questionnaire
which gives you numbers and one that gives you free text comments?</A></LI>

<LI><A HREF="#canyoumixfactual">Can you mix factual and opinion questions,
closed and open ended questions?</A></LI>

<LI><A HREF="#howdoyouanalyse">How do you analyse open-ended questionnaires?</A></LI>

<LI><A HREF="#whatisaLikertstyle">What is a Likert-style questionnaire?
One with five response choices to each statement, right?</A></LI>

<LI><A HREF="#howcanItellif">How can I tell if a question belongs to a Likert scale or not?</A></LI>

<li><a href="#howmanyresponse"> How many response options should there be in a numeric questionnaire?</a></li>

<li><a href="#howmanyanchors">How many anchors should a questionnaire have?</a></li>

<li><a href="#myrespondentsare">My respondents are continually complaining about my questionnaire items.  What can I do?</a></li>

<LI><A HREF="#whatotherkindsof">What other kinds of questionnaires are there?</A></LI>

<LI><A HREF="#somequestionnairesseemto">Should favourable responses always
be be checked on the left (or right) hand side of the scale? </a></li>

<LI><A HREF="#isalongquestionnaires">Is a long questionnaire better than a
short one? How short can a questionnaire be?</A></LI>

<LI><A HREF="#sohighstatistical">Is high statistical reliability the
'gold standard' to aim for?</A></LI>

<LI><A HREF="#whatsthemionimum">What's the minimum and maximum figure for reliability?</A></LI>

<LI><A HREF="#canyoutellif">Can you tell if a respondent is lying?</A></LI>

<LI><A HREF="#somequestionnairesseemto">Why do some questionnaires have 
sub-scales?</A></LI>

<LI><A HREF="#howdoyougo">How do you go about identifying component sub-scales?</A></LI>

<LI><A HREF="#howmuchcanIchange">How much can I change wordings by in a standardised
opinion questionnaire?</A></LI>

<LI><A HREF="#whatstyhedifferencebetween">What's the difference between a questionnaire
and a checklist?</A></LI>

<LI><A HREF="#wherecanifindoutmore">Where can I find out more about questionnaires?</A></LI>

</ul>

</blockquote>

<P><A HREF="#anycomments">Any comments? How are we doing, so far?</A>
<P><A HREF="#acknowledgements">Acknowledgements.</A>
</blockquote>
<IMG SRC="../bar13.gif" ALT="Line" HEIGHT=3 WIDTH=100%>

<blockquote>
<blockquote>

<DL>

<dt><h3><a name="whatisaquestionnaire">What is a questionnaire?</a>
</h3><dd>
A questionnaire is a <i>method</i> for the <i>elicitation,</i> and <i>recording,</i> and <i>collecting</i>of information. The four italicised words in this definition summarise the essence of what questionnaires are about.  I can give a 50-minute lecture explaining this definition with examples and anecdotes, but the notes below summarise the gist of it.
<UL>
<LI>Method: This means that a questionnaire is a tool to be used rather than an end in itself or a work of modern art.  Before you start even thinking of using a questionnaire, a useful question to ask yourself is: 'what do I need to know and how best can I find this out?'  Some kinds of information are not very reliably gathered using questionnaires (eg how often people do things, or self-reports about aspects of life where status is involved.) And it is also very useful at the start to ask yourself 'how will I summarise the information I am seeking to give me a true picture of what I want to know?' 
<LI>Elicitation: A questionnaire may bring out information from the respondent or it may start the respondent thinking or even doing some work on their own in order to supply the requested information.  In any case, a questionnaire is a device that starts off a process of discovery in the respondent's mind.
<LI>Recording: The answers the respondent makes are somehow recorded onto a permanent medium which can be re-played and brought into analysis.  Usually by writing, but also possibly by recording voice or video.
<LI>Collecting: People who use questionnaires are collectors. Given the amount of effort involved in creating a questionnaire, if you only ever needed to use it for one respondent, chances are you'd find some more efficient method of getting the information. However, unless you intend to leave piles of questionnaire mouldering in your filing cabinet, you must also consider what you are going to do with the information you have amassed.  Which brings one neatly back to the first point that a questionnaire is a method.
</UL>
<P>Questionnaires are made up of items to which the user supplies answers or reactions.  
<P>Answering a questionnaire focuses the respondent's mind to a particular topic and almost by definition, to a certain way of approaching the topic.  We try hard to avoid bias when we construct questionnaires;  when a respondent has to react to very tightly focussed questions (so-called <em>closed-ended</em> questionnaires) bias is a real problem.  When a respondent has to react to a more loose set of questions (so-called <em>open-ended</em>), bias is still there, but it's most probably more deeply hidden.

<dt><h3><a name="aretheredifferentkindsof">Are there different kinds of questions?</a>
</h3><dd>

There are three basic types of questions:
<dl>
<dt><h4>Factual-type questionnaires </h4>
<dd>Such questions ask about <em>public, observable</em> information that it would be tedious or inconvenient to get any other way.  For instance, number of years that a respondent has been working with computers, or what kind of education did the respondent get.  Or, how many times did the computer break down in a two-hour session, or how quickly did a user complete a certain task. If you are going to include such questions you should spend time and effort to ensure that the information you are collecting is accurate, or at least to determine the amount of bias in the answers you are getting.

<dt><h4>Opinion-type questions </h4>
<dd>These ask the respondent what they think about something or someone.  There's no right or wrong answer, all we have to do is give the strength of our feeling: do we like it or not, or
which do we prefer?  Will we vote for Mr A or Mr B?  An opinion survey does not concern itself with subtleties of thought in the respondent, it is concerned with finding out how popular someone or something is. Opinion questions direct the thought of the respondent outwards, towards people or artefacts in the world out there.  Responses to opinion questions can be checked against actual behaviour of people, usually, in retrospect ('Wow!  It turned out that those soft, flexible keyboards were a lot less popular than we imagined they would be!')

<dt><h4>Attitude questions</h4>
<dd> Attitude questions focus the respondent's attention to inside themselves, to their internal response to events and situations in their lives.  There are a lot of questionnaires consisting of attitude questions about experiences with Information Technology, the Internet, Multi-media and so on.  These tend to be of interest to the student of social science.  Of more use to the HCI practitioner are questionnaires that ask the respondent what their attitudes are to working with a particular product the respondents have had some experience of.  These are generally called <i>satisfaction questionnaires.</i>

<P>In our research, we have found that user's attitudes to working with a particular computer system can be divided up into attitudes concerning:
<ul>
<li>The user's feeling of being efficient
<li>The degree to which the user likes the system
<li>How helpful the user feels the system is
<li>To what extent the user feels in control of the interactions
<li>Does the user feel they can learn more about the system by using it.
</ul>
<P>We can't directly cross-check attitude results against behaviours in the way we can with factual and opinion type questions.  However, we can check whether attitude results are <i>internally consistent</i> and this is an important consideration when developing attitude questionnaires.
</dl>



<p><dt><h3><A NAME="Whataretheadvantages"></A>What are the advantages of using questionnaires in usability research?
</h3><dd>
<UL>
<LI>The biggest single advantage is that a usability questionnaire gives you<em>
feedback from the point of view of the user</em>. If the questionnaire
is reliable, and you have used it according to the instructions, then this
feedback is a trustworthy sample of what you (will) get from your whole
user population.</LI>

<LI>Another big advantage is that measures gained from a questionnaire are
to a large extent, independent of the system, users, or tasks to which
the questionnaire was applied. You could therefore compare</LI>

<UL>
<LI>the perceived usability of a word processor with an electronic mailing
system,</LI>

<LI>the ease of use of a database as seen by a novice and an expert user,</LI>

<LI>the ease with which you can do graphs and statistical computations on a
spreadsheet.</LI>
</UL>

<LI>Additional advantages are that questionnaires are usually quick and therefore cost effective to administer and to score and that you can gather
a lot of data using questionnaires as surveys.  And of course, questionnaire data can be used as a reliable basis for comparison or for demonstrating that
quantitative targets in usability have been met.</LI>
</UL>

<p><dt><h3><A NAME="whatarethedisadvantages"></A>What are the disadvantages?
</h3><dd>
<UL>
<LI>
The biggest single disadvantage is that a questionnaire tells you only
the user's reaction as the user perceives the situation. Thus some
kinds of questions, <em>for instance, to do with time measurement or frequency
of event occurrence, </em>are not usually reliably answered in questionnaires.
On the whole it is useful to distinguish between subjective measures (which
is what questionnaires are good for) and performance measures (which are
publicly-observable facts and are more reliably gathered using direct event
and time recording techniques).</LI>

<LI>
There is an additional smaller disadvantage. A questionnaire is usually
designed to fit a number of different situations (because of the costs
involved). Thus a questionnaire cannot tell you in detail what is
going right or wrong with the application you are testing. But a
well-designed questionnaire can get your near to the issues, and an open-ended
questionnaire can be designed to deliver specific information if properly
worded.</LI>

<LI>Those who have worked with questionnaires for a long time in industry will also be aware of the seductive power of the printed number.  Getting hard, quantitative data about user attitudes or opinions is good, but this is not the whole story.  If the aim of the investigation is to analyse the overall usability of a piece of software, then the subjective data must be enhanced with performance, mental effort, and effectiveness data.  In addition, one should also ask, <em>why?</em>  This means talking to the users and observing them. 
</UL>


<p><dt><h3><A name="howdoquestionnaires"></a>How do questionnaires fit in with other HCI evaluation methods?

</h3><dd>
The ISO 9241 standard, part 12, defines usability in terms of <em>effectiveness, efficiency,</em> and <em>satisfaction.</em>  If you are going to do a usability laboratory type of study, then you will most probably be recording user behaviour on a video or at least timing and counting events such as errors. This is known as <em>performance</em>or <em>efficiency analysis</em>.

<P>You will also most probably be assessing the quality of the outputs that the end user generates with the aid of the system you are evaluating.  Although this is harder to do, and more subjective, this is known as <em>effectiveness analysis.</em>

<P>But these two together don't add up to a complete picture of usability.  You want to know what the user feels about the way they interacted with the software.  In many situations, this may be the single most important item arising from an evaluation!  Enter the <em>user satisfaction questionnaire.</em>

<P>It is important to remember that these three items (effectiveness, efficiency, and satisfaction) don't always give the same answers: a system may be effective and efficient to use, but users may hate it.  Or the other way round.

<P>Questionnaires of a factual variety are also used very frequently in evaluation work to keep track of data about users such as their age, experience, and what their expectations are about the system that will be evaluated.


<p><dt><h3><a name="whatisreliability"></a>What is meant by <em>reliability?</em>
</h3><dd>The reliability of a questionnaire is the ability of the questionnaire to give the same results when filled out by like-minded people in similar circumstances.  Reliability is usually expressed on a numerical scale from zero (very unreliable) to one (extremely reliable.)  

<p><dt><h3><a name="whatisvalidity"></a>What is meant by <em>validity?</em>
</h3><dd>The validity of a questionnaire is the degree to which the questionnaire is actually measuring or collecting data about what you think it should be measuring or collecting data about.  Note that not only do opinion surveys have validity issues;  factual questionnaires may have very serious validity issues if for instance, respondents interpret the questions in different ways.

<p><dt><h3><A NAME="shouldidevelopmy own"></A>Should I develop my own questionnaire?

</h3><dd>If you have a lot of time, patience, and resources, then go right ahead.
You are well advised to do a course in psychological measurement, including
a heavy dose of statistics beforehand, and to gain experience with administering
and interpreting questionnaires that have already been devised, for purposes
outside usability engineering as well as for purposes within. You
should ensure that your questionnaire has adequate reliability and validity
and that you have an idea of what the expected values are. If this list
of qualifications sounds ominous to you, then take the sensible option:
use a questionnaire that has already been developed and standardised by
someone else.

<p><dt><h3><A NAME="whatswrongwithputting"></A>What's wrong with putting a quick-and-dirty questionnaire together?

</h3><dd>The problem with a quick-and-dirty questionnaire is that you
usually have no notion of how reliable or valid the questionnaire is.
You may be lucky and have developed a very good questionnaire you
may be unlucky. However, until you put your questionnaire through
the intensive statistical and methodological procedure involved in creating
a questionnaire, you just won't know.
<P>A poor questionnaire will be insensitive to differences between versions
of software, releases, etc. and will not show significant differences.
You are then left in a quandary: does the questionnaire fail to show
differences because they do not actually exist, or is it simply because
your questionnaire is insensitive and unreliable? If your questionnaire
does show differences, is this because it is biased, or is it because one
version is actually better?
<P>The crux of the matter is: you can't tell unless the questionnaire
has been through the standard development and test process.</P>

<p><dt><h3><A NAME="factualtypequestionnaires"></A>Factual-type questionnaires are
easy to do, though, aren't they?

</h3><dd>A factual, or <em>'survey'</em> questionnaire is one that asks for relatively
straightforward information and does not need personal interpretation to
answer. Answers to factual questions can be proven right or wrong.
An <em>opinion based </em>questionnaire is one that asks the respondent what
they think of something. An answer to an opinion question cannot
be proven right or wrong: it is simply the opinion of the respondent and
is inaccessible to independent verification.
<P>Although it is important to check that the respondents understand the
questions of both kinds of questionnaires clearly, the burden of checking
is much greater with opinion style questionnaires because we cannot sanity
check the answers against reality.</P>

<p><dt><h3><A NAME="whatstheidfferencebetween"></A>What's the difference between a
questionnaire which gives you numbers and one that gives you free text
comments?

</h3><dd>A <em>closed-ended questionnaire</em> is one that leaves no room for
individual comments from the respondent. The respondent replies
to a set of questions in terms of pre-set responses for each question.
These responses can then be coded as numbers. An <em>open-ended questionnaire</em>
requests the respondent to reply to the questions in their own words, maybe
even to suggest topics to which replies may be given. The ultimate
open-ended questionnaire is a <em>'critical incident'</em> type of questionnaire
in which respondents explain several good or bad experiences, and the circumstances
which led up to them, and what happened after, all in their own words.</P>

<UL>
<LI>
Closed-ended questionnaires are good if you are going to be processing
massive quantities of data, or if your questionnaire is appropriately scaled
to yield meaningful numeric data.  If you are using a closed-ended questionnaire, however, encourage the respondents to leave their comments either in a special space provided on the page, or in the margins.  You'll be surprised what this gives you.</LI>

<LI>
Open ended questionnaires are good if you are in an exploratory phase of
your research or you are looking for some very specific comments or answers
that can't be summarised in a numeric code.</LI>
</UL>

<p><dt><h3><A NAME="canyoumixfactual"></A>Can you mix factual and opinion questions,
closed and open ended questions?

</h3><dd>It doesn't do to be too purist about this. It's a good idea to
mix some open-ended questions in a closed-ended opinion questionnaire
and it's also not a bad thing to have some factual questions at the start
of an opinion questionnaire to find out who the respondents are, what they
do, and so on. Some of your factual questions may need to be open-ended,
for instance if you are asking respondents for the name of the hardware
they are using.
<P>This also means you can construct your own questionnaire booklets by
putting together a reliable opinion questionnaire, for instance, and then
add some factual questions at the front and maybe some open ended opinion
questions at the end.</P>

<p><dt><h3><A NAME="howdoyouanalyse"></A>How do you analyse open-ended questionnaires?

</h3><dd>The standard method is called <em>'content analysis' </em>and is a subject
all of its own. Content analysis usually lets you boil down responses
into categories, and then you can count the frequency of occurrence of
different categories of response.</P>

<p><dt><h3><A NAME="whatisaLikertstyle"></A>What is a Likert-style questionnaire?
One with five response choices to each statement, right?

</h3><dd>No indeed not. A <em>Likert-style</em> questionnaire is one in which
you have been able to prove that each item of the questionnaire has a similar
psychological <em>'weight' </em>in the respondent's mind, and that each item
is making a statement about the same construct. Likert scaling is
quite tricky to get right, but when you do have it right, you are able
to sum the scores on the individual items to yield a questionnaire score
that you can interpret as differentiating between shades of opinion from
<em>'completely against'</em> to <em>'completely for'</em> the construct you
are measuring.
<P>It is possible to find questionnaires which seem to display Likert-style
properties in which many of the items are simply re-wordings of other
items. Such questionnaires may show some fantastic reliability
data, but basically they're a cheat because you're just adding in extra
items that bulk up the statistics without telling you anything really
new.
<P>And of course there are plenty of questionnaires around which are masquerading
as Likert-style questionnaires but which have never had their items tested for
any of the required Likert properties.  Summing item scores of such
questionnaires is just nonsense.  Treat such questionnaires as checklists (see below)
until you are able to do some psychometric validation on them.</P>

<p><dt><h3><A NAME="howcanItellif"></A>How can I tell if a question belongs to a Likert scale or not?

</h3><dd>
The essence of a Likert scale is that the scale items, like a shoal of  tropical fish, are all of approximately the same size, and are going in the same direction.

<P>People who design Likert scales are concerned about developing a batch of items that all have approximately the same level of importance (size) to the respondent, and are all more or less talking about the same concept (direction), which concept the scale is trying to measure.  Designers use various statistical criteria to quantify these two ideas.

<P>To start with, we have to get a bunch of people to fill out the first draft of the questionnaire we are trying to design.  We should ideally have about 100 respondents with varied views on the topic we are trying to measure, and certainly, more respondents than questions.  We then compute various statistical summaries of this data.

<P>Do the items all have the same level of importance to the respondent?  To measure this we look at the reliability coefficient of the questionnaire.  If the reliability coefficient is low (near to zero) this means that some of the items may be more important to the respondents than others.  If the reliability coefficient is high (near to one) then the items are most probably all of the same psychological 'size.'

<P>Are the items all more or less talking about the same concept?  To measure this we look at the statistical correlation between each item and the sum of the rest of the items.  This is sometimes called the item-whole correlation.  Items which don't correlate well are clearly not part of the scale (going in a different 'direction') and should be thrown out or amended. 

<P>It's fascinating to use an interactive statistical package and to watch how reliabilities and item-whole correlations change as you take items in and out of the questionnaire.

<P>A very real risk a developer runs when constructing a scale is that they start to 'model the data.'  That is, they take items in and out and they compute their statistics, but their conclusions are only applicable to the sample that evaluated the questionnaire.  What the developer must do next is to try the new questionnaire on a fresh sample, and re-compute all the above statistics again.  If the statistics hold on the fresh sample, then well and good.  If not, then it's back to the drawing board.

<P>Warning: one sometimes sees some very good-looking statistics reported on the basis of analysis of the original sample, without any check on a fresh sample.  Take these with a large pinch of salt.  The statistics will most probably be a lot less impressive when re-sampled.

<P>In general, in answer to the question:  is this a real Likert scale or not,   the onus is on the person who created the scale to tell you to what extent  the above criteria have been met.  If you are not getting this level of re-assurance from the scale designer, then it really is a fishy business. A scale item which may work very nicely  in one questionnaire may be totally out of place in another.

<p><dt><h3>
<a name="howmanyresponse"></a>How many response options should there be in a numeric questionnaire?
</h3><dd>
There are two sets of issues here.  One is, should we have an odd or even number of response options.  The general answer to give here is that, if there is a possibility of having a 'neutral' response to a set of questions, then you should have an odd number of questions with the central point being the neutral place.  On the other hand, if it is a question of whether something is good/bad, male/female (bi-polar) then basically, you are looking at two response options.  You may wish to assess the <em>strength</em> of the polarity; you are actually asking two questions in one:  firstly, is to good or bad, and secondly, is it really very good or very bad.  This leads you to an even number of response options.
<p>Some people use even numbers of response options to 'force' the respondents to go one way or another.  What happens in practice is that respondents end up giving random responses between the two middle items.  Not very useful.
<p>The other set of issues is how wide should the response options be.  A scale of 1 to 3, 1 to 5, or even 1 to 12?  The usual answer is that it depends on how accurately can the majority of respondents distinguish between flavours of meaning in the questions?  If you suspect that the majority of respondents are going to be fairly uninformed about the topic, then stick with a small number of response options.  If you are going to be dealing with experts, then you can use a much larger set of response options.
<p>A sure way of telling if you are using too many response options is to listen to the respondents talking after they have done the questionnaire.  When people have to differentiate between fine shades of meaning that may be beyond their ability, they will complain that the questionnaire was 'long' and 'hard.'

<p><dt><h3><a name="howmanyanchors"></a>How many anchors should a questionnaire have?
</h3><dd>The little verbal comments above the numbers ('strongly agree', <em>etc.</em>) are what we call anchors.  In survey work, where the questions are factual, it is considered a good idea to have anchors above all the response options, and this will give you accurate results.  In opinion or attitude work, you are asking a respondent to express their position on a scale of feeling from strong agreement to strong disagreement, for instance.  Although it would be helpful to indicate the central (neutral) point if it is meaningful to do so, having numerous anchors may not be so important.  Indeed, some questionnaires on attitudes have been proposed with a continuous line and two end anchors for each statement.  The respondent has to place a mark on the line indicating the amount of agreement or disagreement they wish to express.  Such methods are still relatively new.
<p>A related question is, should I include a 'no answer' option for each item.  This depends on what kind of questionnaire you are developing.  A factual style questionnaire should most probably not have a 'no answer' option unless issues of privacy are involved.  If in an opinion questionnaire, many of your respondents complain about items 'not being applicable' to the situation, you should consider carefully whether these items should be changed or re-worded.
<p>In general, I tend to distrust 'not applicable' boxes in questionnaires.  If the item is really not applicable, it shouldn't be there in the first place.  If it is applicable, then you are simply cutting down on the amount of data you are going to get.  But this is a personal opinion.

<p><dt><h3>
<a name="myrespondentsare"></a>My respondents are continually complaining about my questionnaire items.  What can I do?
</h3><dd>
People always complain.  It's a fact of life.  And everybody thinks of themselves as a 'questionnaire expert.'  If you get the odd grumble from your respondents, this usually means that the person doing the grumble has something extra they want to tell you, beyond the questionnaire.  So listen to them.  
<p>If you get a lot of grumbles, this may mean that you have badly miscalculated and it's time to go back to the drawing board.  When you listen to people complaining about a questionnaire, listen carefully:  are they unhappy about what the questionnaire is attempting to measure, or are they unhappy about the wordings of some of your items?


<p><dt><h3>
<A NAME="whatotherkindsof"></A>What other kinds of questionnaires are there?

</h3><dd>You mean, what other kinds of techniques can you employ to
construct a questionnaire? There are two main other varieties:

<OL>
<LI>
Semantic differential type questionnaires in which the user is asked to
say where their opinion lies between two anchor points which have been shown 
to represent some kind of <em>polar opposition</em> in the respondent's mind</LI>

<LI>
Guttman scaling type questionnaires which are a collection of statements
which gradually get more extreme, and you calculate at what statement the
respondent begins to answer negatively rather than positively.</LI>
</OL>


<P>Of the two, <em>semantic differential scales</em> are more frequently
encountered in practice, although they are not used as much as <em>Likert
scales</em>, and professionals seem to have relegated Thurstone and Guttman
scaling techniques into the research area.

<p><dt><h3>
<A NAME="somequestionnairesseemto"></A>Should favourable responses always
be be checked on the left (or right) hand side of the scale?

</h3><dd>Usually no. The reason for not constructing a questionnaire in
this manner is because response bias can come into play. A respondent
can simply check off all the <em>'agrees'</em> without having to consider
each statement carefully, so you have no guarantee that they've actually
responded to your statements -- they could be working on <em>'auto-pilot'</em>.
Of course, such questionnaires will also produce fairly impressive statistical
reliabilities, but again, that could be a cheat.</P>

<p><dt><h3>
<A NAME="isalongquestionnaires"></A>Is a long questionnaire better than
a short one? How short can a questionnaire be?

</h3><dd>You have to ensure that you have enough statements which cover the
most common shades of opinion about the construct being rated. But
this has to be balanced against the need for conciseness: you can
produce a long questionnaire that has fantastic reliabilities and validities
when tested under controlled conditions with well-motivated respondents,
but ordinary respondents may just switch off and respond at random after
a while. In general, because of statistical artefacts, long questionnaires
will tend to produce good reliabilities with well-motivated respondents,
and shorter questionnaires will produce less impressive reliabilities but
short questionnaires may be a better test of overall opinion in practice.</P>
<P>A questionnaire should not be judged by its statistical reliability alone.  Because of the nature of statistics, especially the so-called <em>law of large numbers,</em> we will find that what was only a trend with a small sample becomes statistically significant with a large sample.  Statistical 'significance' is a technical term with a precise mathematical meaning.  Significance in the everyday sense of the word is a much broader concept.

<p><dt><h3>
<A NAME="sohighstatistical"></A>So high statistical reliability is not
the 'gold standard' to aim for?

</h3><dd>If a short (say 8 - 10 items) questionnaire exhibits high reliabilities
(above 0.85, as a rule of thumb) then you should look at the items carefully
and examine them for spurious repetitions. Longer questionnaires
(12 - 20 items) if well constructed should yield reliability values of 0.70 or more.
<P>I stress these are rules of thumb:  there is nothing absolute about them.</P>

<p><dt><h3>
<A NAME="whatsthemionimum"></A>What's the minimum and maximum figure for
reliability?

</h3><dd>Theoretically, the minimum is 0.00 and the maximum is 1.0. Suspect
a questionnaire whose reliability falls below 0.50 unless it is very short
(3-4 items) and there is a sound reason to adopt it.
<P>The problem with questionnaires of low reliability is that you simply don't
know whether they are telling you the truth about what you are trying to measure
or not.  It's the lack of assurance that's the problem.</P>

<p><dt><h3>
<A NAME="canyoutellif"></A>Can you tell if a respondent is lying?

</h3><dd>The polite way of saying this, is, can you tell if the respondent is
giving you <em>'socially desirable'</em> answers. You can, but the
development of a social desirability scale within your questionnaire (so-called
<em>'lie scale'</em>) is a topic all of its own. <em>'Lie scales'</em>
work on the principle that if someone is trying to make themselves look
good, they will also strongly agree to an inordinate number of statements
that ask about impossible behaviours, such as
<UL>
<LI><TT>'I have never been late for an appointment in my life.'</TT></LI>
<LI><TT>'I always tell the truth no matter what the cost.'</TT></LI>
</UL>
Now, some respondents may strongly agree with some of these items but they'd
have to be a saint to be able to honestly agree to all of them.
<P><em>'Lie scales'</em> generally bulk up a questionnaire and are generally
not used in HCI. If you are really concerned with your respondents
giving you socially desirable answers, you could always put a social desirability
questionnaire into the test booklet and look hard at those respondents who
give you high scores on social desirability.</P>

<p><dt><h3>
<A NAME="somequestionnairesseem"></A>Why do some questionnaires have 
sub-scales?

</h3><dd>Suppose that the overall construct you are getting the respondents
to rate is complex: there are different components to the construct.
Thus for instance, overall user satisfaction is a complex construct that
can be broken down into a number of separate components, like <em>'attractiveness',
'helpfulness', 'feelings of efficiency' </em>and so on. If you can
identify these components, it makes sense to create a number of sub-scales
in your questionnaire, each of which is a <em>'mini questionnaire'</em> in
its own right, measuring one component, but which also contributes to the overall construct.

<p><dt><h3>
<A NAME="howdoyougo"></A>How do you go about identifying component sub-scales?

</h3><dd>The soundest way of doing this is to carry out a statistical procedure
called 'factor analysis' on a large set of questions, to find out how many
underlying (latent) factors the respondents are operating with but often,
received opinion or expert analysis of the overall construct may be used
instead. The crucial questions are:

<OL>
<LI>
Are these factors truly independent? That is, if they are, we would
expect items that make up the factors to be more highly correlated with
each other than with items from other factor scales.</LI>

<LI>
What use can the analyst make of the different factors? Extracting
a bunch of factors that actually contributes little to our understanding
of what is going on is pseudo-science. On the other hand, separating
factors which are fairly highly inter-correlated but which make sense to
separate out practically makes for a more usable questionnaire. For
instance, <em>'screen layout'</em> and <em>'menu structure'</em> are two factors
which may be fairly strongly inter-correlated in a statistical sense but
separately they may give the analyst useful information about these two
aspects of an interface.</LI>
</OL>

<p><dt><h3>
<A NAME="howmuchcanIchange"></A>How much can I change wordings by in a
standardised opinion questionnaire?

</h3><dd>In general, if a questionnaire has been through the standardisation
process the danger in changing, deleting, or adding items is that you undo
the statistical basis for the questionnaire: you set yourself back by unknown
amounts. You are generally advised not to do this unless you have
all the background statistical data and have access to user samples on
which you can re-validate your amended version.
<P>There is one general exception. If statements in the questionnaire
refer to something like <TT>'this system'</TT> or <TT>'this software'</TT>
you can usually change these words to refer explicitly to the system you
are evaluating without making too much damage to the questionnaire.
For instance:
<UL>
<LI><TT>(1)  'Using this system gives me a headache.'</TT></LI>
<LI><TT>(2)  'Using Word-Mate gives me a headache'.</TT></LI>
</UL>
Changing (1) to (2) is called <em>'focussing'</em> the questionnaire and is usually no problem.
<P>You may be able to do a more radical change of focus, without affecting the statistical properties too much
if for instance you were to change all occurrences of (3) to (4):
<UL>
<LI><TT>(3) 'using this system...'</TT></LI>
<LI><TT>(4) 'configuring this system...'</TT></LI>
</UL>
...but you should examine
the result very carefully to check that you are not introducing shifts
of meaning by doing so.  If the questionnaire you are intending to change has an associated
database of reference values, then changing the focus in this way is most
probably not a good idea if you want to still use the database of reference
values.

<p><dt><h3>
<A NAME="whatstyhedifferencebetween"></A>What's the difference between
a questionnaire and a checklist?

</h3><dd>A <em>checklist </em>is simply a list of statements or features that
it may be desirable or undesirable to have. It is not usually a scale in
the psychometric sense of the term. A checklist is not amenable to
Likert scaling, for instance, so summing the items of a checklist does
not make sense. As an example, consider a checklist for landing a
plane. You may have 95% of the items checked, but if you haven't
checked that the wheels are down, your landing may be a disaster.
But if you haven't checked that the passengers have put their safety belts
on, the consequences may not be nearly as grave.
<P>Individual items within a checklist may be averaged across users, so
you can get a percentage strength of agreement on each item (thus you are
trying to establish truth by consensus) but even then, an expert's opinion may outweigh an averaged
opinion of a group of less well informed users (a class of 30 children
may decide by vote that a hamster is female, for instance, but an expert
may have to over-ride that opinion after a detailed inspection).

<p><dt><h3>
<A NAME="wherecanifindoutmore"></A>Where can I find out more about questionnaires?

</h3><dd>Please don't take seriously those books which devote a chapter to Likert
scaling and then urge you to go out and try doing a questionnaire yourself.
<em>These authors are doing everyone a disservice.</em> Here is a minimalist
list of reference sources for questionnaire construction that I have found
useful as teaching material.

<blockquote>
<P>Aiken, Lewis R., 1996, <em>Rating Scales and Checklists.</em> Wiley.
ISBN 0-471-12787-6. Good general introduction including discussions of
personality and achievement questionnaires.

<P>Czaja, Ronald, and Johnny Blair, 1996, <em>Designing Surveys.</em>
Pine Forge Press. ISBN 0-8039-9056-1. A useful resource for factual-style
surveys, including material on interviews as well as mail surveys.

<P>DeVellis, Robert F., 1991, <em>Scale Development, Theory and Applications.</em>
Sage Publications, Applied Social Research Methods Series vol. 26.
ISBN 0-8039-3776-8. Somewhat theoretical, but important information if
you want to take questionnaire development seriously.

<P>Ghiselli, Edwin E., John P. Campbell, and Sheldon Zedeck, 1981, <em>Measurement
Theory for the Behavioural Sciences.</em> WH Freeman &amp; Co.
ISBN 0-7167-1252-0. A useful reference for statistical issues. Considered
'very readable' by some.

<P>Kline, Paul, 1986, <em>A Handbook of Test Construction.</em> Methuen.
ISBN 0-416-39430-2. Practically-orientated, with a lot of good, helpful
advice for all stages of questionnaire construction and testing.
Some people find it tough going but it is a classic.

<P>Stecher, Brian M. and W. Alan Davis, 1987, <em>How to Focus an Evaluation.</em>
Sage Publications. ISBN 0-803903127-1. About more than just questionnaires,
but it serves to remind the reader that questionnaires are always part
of a broader set of concerns when carrying out an evaluation.</UL>
</blockquote>

</dl>
</blockquote>
<H3>
<A NAME="anycomments"></A>Any comments? How are we doing, so far?</H3>

<blockquote>
<P>Please don't copy this page since I hope it's going to change over time, but you are very welcome to create a link to it from your site.  Reciprocal links would be very nice, please mail me
if you'd like a reciprocal link from the HFRG site. Excerpts may be made from reasonable portions of this page and included in information material so long as my authorship is acknowledged.</P>
<P>If you have any comments on the FAQ, or want to suggest some extra questions or resources, please contact me: <A HREF="mailto:jzk@ucc.ie">jzk@ucc.ie.</A><P>
</blockquote>
<h3>
<a name="acknowledgements"></a>Acknowledgements</h3>

<blockquote>
<P>As always thanks to Dr Murray Porteous for keeping me straight.  Dick Miller, Owen Daly-Jones, Cynthia Toryu, 
Julianne Chatelain, Anne-Mari Flemming and Carolyn Snyder have all commented and stimulated.  Thanks, folks!</P>
</blockquote>
</blockquote>
<IMG SRC="../bar13.gif" ALT="Line" HEIGHT=3 WIDTH=100%>
<DIV ALIGN=right><A HREF="index.html"><IMG SRC="../arrowleft.gif" ALT="Resources Page" BORDER=0 HEIGHT=20 WIDTH=20></A><A HREF="#top"><IMG SRC="../arrowup.gif" ALT="Top of page" BORDER=0 HEIGHT=20 WIDTH=20></A></DIV>

</BODY>
</HTML>
