%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% project-overview.tex -- 
%% Author          : Philip Johnson
%% Created On      : Thu Oct  4 08:05:31 2001
%% Last Modified By: Philip M. Johnson
%% Last Modified On: Fri May 13 09:05:34 2005
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 2001 Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%\nocite{*}

\section{Overview}

\subsection{Motivation}

The NSF Next Generation Cybertools program has the ambitious goal of
producing technologies that ``not only change ways in which social and
behavioral scientists research the behavior of organizations and
individuals, but also serve sciences more broadly.'' This goal is particularly salient because the increased automation and “digitization” of work creates a sea of information about organizations and their processes.  The availability of data creates the potential to revolutionize the way we understand, design, and manage organizations.  To gain insight from this sea of data (rather than being drowned by it), we need ways to find patterns, interpret them and generalize appropriately.  

In private organizations, availability of digitized data creates new opportunities in every core business area, such as new product development, customer support, supply chain management, and even basic accounting. In addition to competitive pressures for process control and improvement, which date back to the early days of scientific management \cite(pentland03b), private organizations are facing increased demands for compliance monitoring and internal controls. \cite(Hunton04).  New technologies, such as Enterprise Resource Planning systems, and continuous assurance systems create a virtual tidal wave of quantitative accounting data, but organizations lack effective ways to integrate the qualitative data needed to interpret it \cite (Hunton04) \cite(Varsarhelyi04). 

Many analogous opportunities exist in government and defense, as well.  For example, military training and operations generates enormous amounts of detailed operational data that must be analyzed and interpreted \cite(Carolan04 ). Like private organizations, military operations include multiple, distributed participants, multiple hierarchical layers, and qualitative and quantitative data from many sources. Current technology for doing interpreting this data (e.g., Distributed Battlefield Exercise Simulation and Debriefing) focuses on one exercise at a time \cite(Johnston04 ). As with private organizations, the military faces significant challenges in gaining insights from qualitative and quantitative data generated by diverse sources \cite(Carolan04 ) \cite(Johnston04 ).   

\subsection{ Example: High Productivity Computing Systems }
To make the research issues related to this proposal more concrete, and to frame our approach to addressing them, we begin by describing an
organization that exemplifies the core challenges for Next Generation Cybertools:  the Defense Advanced Research Projects Agency (DARPA) High
Productivity Computing Systems (HPCS) program \cite{hpcs}.

The mission of the HPCS program involves the development of next
generation, peta-scale high performance computing platforms for commercial
availability by 2010.  In a radical break with past high performance
computing initiatives, the focus of this program is not just on the
development of new and faster hardware. In addition, an explicit objective
of this program is to radically decrease the cost and time required by
organizations to perform their science and engineering activities that
require these high performance computing environments.  For example, the
development of a new climate model might currently require a team of dozens
of scientists and engineers several years to implement.  Next generation
HPC environments should simultaneously halve the size of the team and the
time required to implement such a system. DARPA is currently funding
research and development by IBM, Sun Microsystems, and Cray to better
understand the hardware, software, and organizational requirements to
achieve up to 10x productivity improvements.  

%% The HPCS ``organization'' thus
%% consists the program management group at DARPA, research and development
%% groups at IBM, Sun, and Cray, end-user organizations such as Lawrence
%% Livermore Laboratory and Los Alamos National Laboratory, and affiliated
%% academic research organizations at universities such as the University of
%% Maryland and the University of Hawaii.

Two of the principal investigators on this proposal have been associated
with the HPCS program as academic researchers. This has given us insight
into the enormous challenges associated with understanding, assessing, and
improving organizational behavior in the largely unstudied domain of high
performance computing system application development.  While still in a
very early stage, research by the vendors and affiliated researchers has
begun to generate a body of quantitative and qualitative data
concerning the behavior of developers and others in HPC organizations.

For example, pilot studies have been performed in a classroom setting with
students developing simple high performance systems, resulting in
quantitative data on the tools they used, the times at which they
invoked the tools and the results, and properties (such as the size) of the
software they produced \cite{Funk05}. Examples of qualitative data range from interviews
with administrative staff of high performance computing centers to journals
kept by professional developers as they work on HPC software \cite{Votta05}.

Initial analyses of the raw data have included formal models, such as Timed
Markov Models fit to classroom data \cite{Smith05}. Timed Markov Models are
used to characterize workflows and the effort associated with workflow
steps.  Other case study data has been used to generate semi-formal models,
such as ``telemetry'' based analyses \cite{csdl2-04-11}.  Still other kinds
of data, such as the qualitative journal data, has been best suited to
qualitative encoding techniques \cite{Votta05}.  Research has also led to
proposals for new ways to assess high performance productivity, such as
Purpose-Based Benchmarks \cite{Gustafson04}.

So far, dissemination of research data and results have been via HPCS program
meetings \cite{hpcs-meeting} academic workshops \cite{pphec05,sehpcs05}, and 
themed journal issues \cite{ijhpca04}.

As the HPCS program builds momentum, a variety of organizational research
challenges are appearing.

First, the HPCS program is revealing the need for primary research on
organizations using high performance computing environments. Basic
questions need to be answered: How are high performance computing system
applications developed and maintained?  Where are the productivity
bottlenecks? What are the organizational constraints on innovation in
technology or methods? What is the most appropriate research methodology,
or combination of methodologies, for gaining insight into these questions?

Second, the answers to these basic question must support the design of new
technologies and organizational procedures that will yield an order of
magnitude productivity improvement in high performance computing
applications.  This requires, of course, an operational definition of
``productivity'' that can be measured in both current and future
environments.  Interestingly, no such measure has yet been agreed upon by
this community, even though its definition has profound implications for
the evaluation of the technologies under development and the future
processes and products of the end-user organizations.

Third, the HPCS program serves as an umbrella over many different types of
organizations, generating substantial challenges regarding the publication
and/or protection of information.  The three HPCS vendor awardees, Sun,
IBM, and Cray, are motivated to openly publish certain types of research
results regarding productivity in order to (for example) influence the
ultimate definition of the productivity measure used to evaluate their
systems. On the other hand, each organization also generates research
results that constitute proprietary information. The ultimate end-users of
these systems (government and military laboratories, automobile companies,
financial service institutions, etc.)  form another set of organizations.
The academic and corporate researchers form a third set of
organizations. Each of these organizational layers have privacy issues
related to the information they collect, manage, and disseminate to others.

Fourth, the HPCS program is distributed geographically and involves a large
number of constituent organizations and concurrent research activities.  A
major challenge to the program involves ensuring alignment among the many
approaches to qualitative and quantitative data gathering and research
methods.  A true ``alignment'' will enable replication, in which data
gathered to test a hypothesis at one site can be gathered in a similar
manner at another site in order to see if the hypothesis is similarly
supported.  Alignment will also enable meta-analysis, in which data from
multiple sites can be validly composed together into a larger dataset for
the purpose of certain analyses.

Having set the stage, we now present the fundamental objective of this
proposal, followed by an overview of the information infrastructure
we will use to achieve it.

\subsection{Objective}

The objective of our proposed research is to produce an open source
information infrastructure architecture and data management policies that
support scalable, collaborative, distributed, integrated, qualitative and
quantitative organizational research data collection, analysis,
dissemination, and archiving.

By ``open source'', we mean not only source code released under a license
that allows access and modification by others, but also the creation of a
community of developers willing and able to maintain and enhance this 
infrastructure beyond the period of this grant. 

By ``information infrastructure architecture'', we mean the creation of a
software framework that allows integration and interoperability of tools
developed by us and by others.  

By ``data management policies'', we mean procedures and mechanisms that
support context-sensitive publication or protection of raw or processed
qualitative or quantitative data.  The management policies will not only
address privacy issues, but also ``lifecycle'' issues related to data
repositories. 

By ``scalable, collaborative, distributed, integrated, qualitative and
quantitative organizational research data'', we mean an infrastructure that
can support hundreds to thousands of concurrent data collection and
analysis activities, allowing analysis and annotation of data by many
researchers across many institutions, combining both qualitative and
quantitative data.

Finally, by ``collection, analysis, dissemination, and archiving'', we mean
an  infrastucture that  can  support data  management  policies across  the
entire lifecycle of qualitative and quantitative data.

Our approach is intended to address the requirements for Testbed I (Organizations).

\subsection{GarageLab/Datalla}

Our information infrastructure consists of two fundamental components:
GarageLab, a front-end system to support the display and analysis of
qualitative and quantitative information, and Datalla, a back-end
peer-to-peer network to support controlled dissemination of the collected
data.

The basic function of GarageLab is to support display and analysis of data.
First, it enables the researcher to visualize multiple streams of raw
qualitative and quantitative data by organizing each as ``tracks'' along a
timeline.  Similar to multi-track editors for music (such as GarageBand
\cite{GarageBand}), GarageLab allows the user to ``zoom in'' or ``zoom
out'' of the chosen data streams, and ``cut and paste'' data streams from
one timeline to another.  GarageLab will also allow annotation of timelines
with additional information, such as for encoding episodes with
classifiers. Finally, GarageLab will allow plug-ins to support
``processing'' of the raw data in various ways.  For example, one plug-in
might produce a timed markov model, while another might produce a network
representation.

Datalla, the back-end system, provides several services.  First, each
Datalla server provides storage for raw qualitative and quantitative
organizational data.  A user with an account on a Datalla server can login
via GarageLab to access data on that Datalla server.  Second, each Datalla
server can receive qualitative and quantitative data from Datalla
``sensors'', which are small software programs that can be used to collect
and send raw data to a server.  Finally, each Datalla server can
communicate with other Datalla servers, forming a peer-to-peer network.
The kinds of data that can be communicated to other servers is controlled
by the privacy policy in effect.


\subsection{Research Approach}

Achieving our objective using the GarageLab/Datalla infrastructure will require us to 
carry out the following research and development activities. 

(1) {\em Infrastructure technology research and development.} Our prior
experience with Hackystat \cite{Hackystat} provides us with expertise in
open source development of client-server systems for automated collection
and analysis of quantitative data. We will leverage this experience in the
development of the GarageLab and Datalla software infrastructure. Research
challenges include successful application of the GarageBand multitrack
metaphor to display and manipulation of qualitative and quantitative
organizational data, and the development of suitable APIs to allow
'plug-ins' with appropriate access to internal data.

(2) {\em Research on and development of policies and procedures for data
  privacy and dissemination.} While the infrastructure can make
  context-dependent privacy policies possible, we must perform research to
  understand what appropriate privacy policies would be. Such policies will
  influence the design of publication/protection mechanisms within the
  infrastructure.  Research challenges include an appropriate means to
  classify data with respect to its privacy policy, appropriate safeguards
  to prevent unauthorized dissemination, and evaluation mechanisms to
  determine the effectiveness of a privacy policy once in place. 

(3) {\em Research on and development of models and mechanisms for
  integrating qualitative and quantitative information.}  Our basic
infrastructure can ``integrate'' qualitative and quantitative information
only in a fairly superficial sense: the data can be stored in together in a
repository, and the raw data can be displayed together along a timeline.
True integration goes much deeper: how do the qualitative and quantitative
data come together to tell us something new about the organization that we
could not have known from either kind of data by itself?  We will pursue
network models \cite{Pentland05} as one approach to this ``deeper''
integration.  Research challenges include the dependencies between the
integration models and the raw data required to successfully connect the
two types of data.

(4) {\em Case study evaluation.}  To test the validity of our approach, we
  will perform a case study of infrastructure deployment with selected
  partners in the HPCS domain. Through this case study, we will evaluate
  how well we have accomplished each component of the objective stated
  above. Note that there is a ``meta'' level in this case study. At one
  level, HPCS researchers will be collecting, integrating, analyzing,
  disseminating, and archiving qualitative and quantitative data about high
  performance computing.  At the meta level, we will be collecting
  qualitative and quantitative data about this usage of the GarageLab
  infrastructure and policies in order to evaluate our approach.  Research
  challenges include ensuring that the technology is robust enough for use
  in a live environment, and gaining buy-in from the case study
  participants necessary to evaluate the deployment effectively.


