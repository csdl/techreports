%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% thesis-body.tex --
%% Author          : Robert Brewer
%% Created On      : Fri Sep  5 13:50:18 1997
%% Last Modified By: 
%% Last Modified On: Thu Mar 06 11:07:04 2003
%% RCS: $Id: thesis-body.tex,v 1.4 2000/03/17 21:28:10 rbrewer Exp $
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1998 Robert Brewer
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%

\chapter{Introduction}

Software testing is a crucial element of Software Engineering.  It can
consume approximately half of the labor required to produce a working
product \cite{Beizer:1990}.  Without proper planning and effective tests,
quality costs could increase during software development as unexpected
failures are discovered.  Removing the errors that cause the failures, or
bugs, have been found in many cases to be the single largest cost in
software development \cite{Beizer:1990}.  Removal requires detection,
correction, tests designed to detect them, and the execution of those
tests.

A common lesson taught to Computer Science students at the beginning of
their college careers is that the longer a bug remains undiscovered, the
costlier it is to fix.  In other words, as the lifetime of a bug increases,
the more time and effort it requires to be removed.  For example, it would
cost more to fix a requirements bug found during testing than it would cost
to fix a coding bug found during testing because fixing a requirements bug
would require fixing the requirements, design, implementation, and
re-testing to ensure the bug is fixed.  Fixing a coding bug, on the other
hand, would only require fixing the implementation and re-testing to ensure
the bug is fixed \cite{Humphrey:1995}.  Therefore, the sooner a bug is
fixed, the cheaper it is to resolve, and testing can help fix bugs sooner.

There are two categories of testing techniques used to find bugs:
functional testing and structural testing \cite{Beizer:1990}. Functional
testing, also known as black box testing, is the verification of a system's
functionality and features as specified by its requirements.
Implementation details are irrelevant as all testing is done from a user's
perspective.  On the other hand, structural testing, also known as white
box testing, glass box testing, or path testing \cite{Cornett}
\cite{Kaner:1999}, is based upon a system's implementation.  By using the
structure of a program's source code to create test cases, the tester is
able to compare the behavior of test cases to the intended behavior of the
source code.

The two testing techniques can be found in the three levels of testing
commonly applied during software development: unit testing, system testing,
and acceptance testing \cite{Hetzel:1984}.  Unit testing, also known as
module testing or element testing \cite{Kaner:1999}, is the exercising of
``a single program module, usually in an isolated environment (i.e.,
isolated from all other modules)'' \cite{Myers:1976} in various way so as
``to show that it does not satisfy its functional specification and/or that
its implemented structure does not match the intended design structure''
\cite{Beizer:1990}.  This is a structural testing technique.  System
testing, a functional testing technique, attempts to uncover
inconsistencies between a system as a whole and its requirements
\cite{Myers:1976}.  Acceptance testing, another functional testing
technique, is conducted to assure the end user that the software is stable
and ready for deployment \cite{Hetzel:1984}.  The technique that discovers
bugs earliest in the development cycle is unit testing.

\section{The Problem with Unit Testing}
Unit testing can be applied to the software development process as soon as
the core functionality of a program is implemented.  After this first phase
of coding, programmers will have access to source code with which they can
begin testing \cite{Kaner:1999}.  The three main motivations for unit
testing are: 1) unit testing improves management of the individual units,
or ``modules'', or combinations of modules before they are combined to form
the entire system, 2) unit testing makes finding and correcting bugs, or
debug, easier since the test is already exercising the module in which the
bug originates, thus a unit test eliminates wasted time searching for the
guilty module containing the bug, and 3) unit testing allows multiple
modules to be tested in parallel \cite{Myers:1979}.

Without any unit tests, one common problem experienced in traditional
development is wasted time analyzing bugs found during system testing
\cite{Crispin:2002}.  At this point in development, trying to find the
cause of the bug could be very expensive with respect to the time and
effort needed to isolate the cause.  Therefore, fixing these bugs takes
effort away from other planned testing phases, like acceptance testing.  If
enough bugs are discovered, the delivery date of the system will be pushed
back.

In traditional development, some code is developed prior to (or in parallel
with) the development of the test code.  Testers need to have access to a
module's specifications and source code to design proper test cases.
First, white-box testing techniques, are applied to the source code to
verify its logic.  Second, black-box testing techniques are derived from
the specifications and then applied \cite{Myers:1979}.  To best ensure that
intimate association with a module does not influence testing, the tester
is often recommended to be different from the programmer.  However, in the
case of unit testing, the programmer and the tester is typically the same
person \cite{Beizer:1990} \cite{Dalal:1993}, reducing the cost of deciding
whether bugs are due to errors in the module or the test case
\cite{Kaner:1999}.

In Extreme Programming (XP), the test code is actually developed prior to
the system source code!  It is only after some code is written that it can
be exercised by the unit tests.  Furthermore, one hundred percent of the
unit tests must pass before development can proceed.  This process is then
repeated throughout the software development life cycle as each feature is
coded.  Some authors claim that this test-first design (TFD) actually
improves the quality of testing and the resulting system.  In a nutshell,
using TFD can have the following influences on software development
\cite{Langr:2001}: 1) the code developed is easier to test since it is
being implemented specifically to satisfy a test, 2) the more difficult
task, designing tests, is completed prior to the easier task, coding, 3)
the size of the code is smaller since no effort is spent on extra features,
and 4) the overall development process is done in shorter increments,
allowing for easier modification/adaption to changes.

In environments that evolves rapidly, like the Internet, XP provides
guidelines that helps programmers adapt quickly to new demands
\cite{Highsmith:2000}.  By creating unit tests first, focus is placed on
current requirements.  No time is wasted on implementing anticipated future
features that may never be needed.  In the meantime, programmers are also
expected to refactor their code continuously so that it may become more
flexible and adaptive during the coding.  The existing unit tests can help
programmers during refactoring by ensuring that modifications do not break
the system and still produces the correct functionality.

However, it is during refactoring in which segments of code that are no
longer needed may emerge and accidentally remain in the system due to a
slight oversight by the programmer.  Unit testing cannot detect these
stagnant lines of code because they would no longer be invoked.  One could
imagine that as the size of the system increases with each iteration, the
existence of these segments could also increase.  Having enough stagnant
code could result in increased cognitive complexity and the amount of time
needed to implement subsequent features.

One approach to reducing this problem is measuring test case coverage,
i.e., a metric that measures the amount of code that is exercised by test
cases.  With a coverage measurement, programmers will always know how much
and which pieces of their code are invoked during testing.  Then they can
redesign or design new tests to thoroughly exercise the untested code.
With this extra effort, the possibility of the unexercised code containing
errors reduces, increasing confidence in the correctness of the program
\cite{Dalal:1993}.  Furthermore, when all tests pass and coverage is not
100\%, programmers will also be able to easily locate unneeded code and
promptly remove it, reducing the size and cognitive complexity of their
code.

Boris Beizer claims that during unit testing 100\% coverage is necessary
\cite{Beizer:1990}.  However, this level of coverage usually drops as
modules are combined or as testing is done on huge systems of approximately
10 million lines of code.  On the other hand, Brian Marick conducted a
study where he examined the different granularities of coverage, which will
be discussed further in Chapter 2. From his study, he claimed that 100\% of
``feasible coverage'' is an acceptable level of coverage to achieve.

\section{The Extreme Coverage Approach}
To further investigate feasible levels of coverage and unit testing, I have
designed a method called ``extreme coverage''.  It is an approach to unit
testing that requires 100\% method-level coverage, but applies a flexible
set of rules that eliminates untestable or trivial methods from coverage.
In other words, all testable, non-trivial methods need to be invoked at
least once.

Of the different coverage granularities that are discussed in Chapter 2,
the focus of extreme coverage is method-level coverage because it can be
calculated quickly and efficiently, yet still provide useful feedback on
the quality of test cases.  For example, during highly volatile periods of
software development where a system's source code is continuously evolving
and refactored at a relatively quick pace, an uninvoked method serves as a
warning sign to the programmer.  It can signal that another test case is
needed to invoke the method, that an error in coding exists if the method
was supposed to be invoked, i.e., the wrong method was called or the name
of the method is misspelled, or that the method is no longer needed.

This is the case in XP, where a system's code is developed at a relatively
quick pace.  Interestingly, there is no rule in XP stating that every
method written needs to be executed during testing. This norm is most
likely implicitly implied since source code should only be written to
satisfy a test case, but is impossible to guarantee as programmers move
further into development, regardless of creating test cases prior to coding
in an attempt to increase the level of test case coverage
\cite{Langr:2001}.

In its ``purest'' form, 100\% method-level coverage requires invoking every
method during testing. However, it is not clear whether this approach to
coverage is a practical goal. Although method-level coverage is cheap for
achieving high levels of coverage, test cases solely aimed at exercising
methods with only one line of code can be expensive to create and maintain,
yet contribute little to improving software quality since the method can
(in most cases) be visually verified.  For example, a typical one line
method in Java has the following form:

\begin{alltt}
{\small{}public class Foo \{

  private int foo;

  /** Sets new value of Foo instance to foo. */
  public Foo(int foo) \{
    this.foo = foo;
  \}

  /** Returns the value foo. */
  public int getFoo() \{
    return this.foo;
  \}
\}
}
\end{alltt}
Obviously, the accessor method, or ``getter'', is correct by inspection.
If this method required its own test case, the simplest implementation of a
test case using the JUnit \cite{JUnit} testing framework would look
something like:

\begin{alltt}
{\small{}public class TestFoo extends TestCase \{
  ...
  public void testGetFoo() \{
    int foo = (new Foo(3)).getFoo();
    assertEquals(``Checking value of foo'', foo, 3);
  \}
  ...
\}
}
\end{alltt}
First, an instance of the Foo class needs to be initialized with an integer
value.  Then the {\tt getFoo} method is invoked.  Finally, a test is
conducted on whether the integer value retrieved is correct.  These three
steps are essential for the test to run successfully.  In the worst case
scenario, creating a test case for each additional method similar to {\tt
getFoo} increases the amount of work by 4 LOC and approximately 30 seconds
for design and implementation per test case.  Therefore, in a system like
Hackystat2 \cite{Hackystat}, the second version of a metrics software
validation package for measuring individual software developers' behaviors
developed in CSDL, that contains 510 one line methods, the size of test
code would increase by approximately 510 * 4 = 2040 LOC and the amount of
time to implement test cases would increase by approximately 510 * 30 =
1530 seconds, or 25.5 minutes.  The fact that contents of the test case can
be reduced to one line of code is irrelevant.

From this example, more work is clearly needed to design and implement the
test case than is needed to inspect the content of the method.  From
experience, Java classes usually contain getters and setters (modification
methods that usually contains a single line of code) in pairs.  If each of
these methods required their own test cases, the overhead of achieving
100\% method-level coverage for these one line methods suddenly increases
linearly with each pair in a class.  Therefore, unless the single line of
code contains a complicated logical expression, for example, inspection is
probably better for verifying its correctness.

In addition, some methods are untestable. For example, abstract methods in
Java can never be invoked.  Thus, ``pure'' method-level coverage is not only
impractical, but can also be impossible to achieve. However, method-level
coverage of all ``non-trivial'' methods, i.e., methods that contain more than
one line of code, or are not abstract, is not impractical.

\section{JBlanket: A System for Measuring Extreme Coverage}
JBlanket \cite{JBlanket} is a method-level coverage tool I developed in the
Collaborative Software Development Laboratory (CSDL) at the
University of Hawai'i (UH).  As with other applications that are used with
Java programs, the ``J'' in JBlanket stands for Java, which is also the
programming language the system is written in.  ``Blanket'', a large piece
of fabric used to cover a bed (as described by the Merriam-Webster
Dictionary), represents the method-level coverage this system provides.
Coverage can be measured for both stand-alone systems and client-server
systems implementing unit tests with JUnit.  The only server used so far is
Apache Tomcat \cite{Tomcat}, or Tomcat.

The JBlanket system has three major components: one that counts the total
methods, one that modifies byte code, and one that collects and reports
coverage data.  Once a system's byte code is modified, execution of its
JUnit tests produces output from each method invoked in XML format.  Then
the coverage reporting transforms the XML output into HTML pages similar to
those created by JUnit. (See Figure \ref{fig:hackystat2.jblanket.summary} and
Figure \ref{fig:hackystat2.junit.index}) From the reports, users can see
how much of their system's methods were invoked by their test cases, and
then drill down to either the package level (Figure
\ref{fig:hackystat2.jblanket.package}) or class level (Figure
\ref{fig:hackystat2.jblanket.class}) for more detailed feedback.  The
JBlanket report user interface is designed to feel ``intuitive'' to users
familiar with the JUnit report interface.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figs/hackystat2.junit.index3.eps}
  \caption{JUnit report for Hackystat2}
  \label{fig:hackystat2.junit.index}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figs/fig.hackystat2.jblanket.summary.eps}
  \caption{Summary View of JBlanket report for Hackystat2}
  \label{fig:hackystat2.jblanket.summary}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figs/fig.hackystat2.jblanket.package.eps}
  \caption{Package View of JBlanket report for Hackystat2}
  \label{fig:hackystat2.jblanket.package}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figs/fig.hackystat2.jblanket.class.eps}
  \caption{Class View of JBlanket report for Hackystat2}
  \label{fig:hackystat2.jblanket.class}
\end{figure}

For example, JBlanket was integrated into Hackystat2.  By using Apache Ant
\cite{Ant}, or Ant, for development, the Hackystat2 build process was
modified in two major steps: one that counted the total methods in
Hackystat and modified its byte code after it was compiled and one that
collected and reported the coverage data.  A summary from running JBlanket
with Hackystat2 that was output to the screen looked like:

\begin{alltt}
{\small{}[jblanketreport] *******************************************************
[jblanketreport] Method-level Coverage:
[jblanketreport] All methods: \{total=1249\}
[jblanketreport] One line methods: \{total=510\}
[jblanketreport] Non-one line methods: \{total=739\}
[jblanketreport] Tested methods: \{total=514, percent=70\%\}
[jblanketreport] Untested methods: \{total=225,  percent=30\%\}
[jblanketreport] *******************************************************
}
\end{alltt}
With this summary, programmers can easily see that there are 225 methods
that can still be tested.  They can then peruse the HTML report for further
details about which methods were and were not invoked during testing.

By using JBlanket to calculate extreme coverage, programmers will know at
all times the number of unit tests that pass as well as the extreme
coverage of their systems.  With knowledge of which methods are executed
during testing, programmers will also be able to reduce the size of their
systems by verifying if untested methods include unneeded code and improve
testing by improving design and implementation of test cases.

\section{Evaluation of Extreme Coverage and JBlanket}
Undergraduate students enrolled in a second semester introductory Software
Engineering course assisted with the evaluation of this research.  The
class consisted of 13 students that implemented 8 web services using Java,
JSP, and a common CVS repository.  They participated in three separate
activities: a Pre-Use Questionnaire, use of JBlanket, and a Post-Use
Questionnaire.

After ten weeks of development, the students filled out the Pre-Use
Questionnaire to assess their comfort and confidence with their unit
testing skills.  I then presented a brief introduction on how to invoke
JBlanket on their own projects.  Before class, with the permission of the
professor, I integrated JBlanket into the build processes of each service.
They were then able to use the system for approximately five weeks.  During
this time, I downloaded the projects from the common CVS repository and
invoked JBlanket on the services once every three days.  At the end of the
semester, the students filled out the Post-Use Questionnaire to once again
assess their comfort and confidence with their unit testing skills as well
as their opinion on the usefulness and improvements for JBlanket.

The results from their questionnaire responses were compared and analyzed
to find out how coverage information influenced their unit testing.
Coverage results were aggregated and plotted on line graphs and bar charts
for analysis.

\section{Thesis Statement}
This research investigates the concept of extreme coverage and gathers
empirical data in order to asses the following hypotheses:
\begin{itemize}
\item The effort required to achieve and maintain extreme coverage is
reasonable.
\item Knowledge of method-level coverage can help with system design and implementation.
\item Extreme coverage is feasible in software development.
\end{itemize}
Due to the coarse granularity of the coverage measured as well as the
additional rules in extreme coverage, the amount of effort required to
achieve and maintain 100\% coverage should be appropriate for the benefits
obtained.  Furthermore, it should take less effort to maintain complete
coverage than to achieve complete coverage.

The second claim concerns a chain reaction of events.  When students know
the test case coverage of their systems, if coverage is not 100\%, they
will need to either write new test cases or modify existing test cases.
Either way, they will search for a way to invoke more methods.  From trying
to increase coverage, students should discover better ways to implement
their services such that they will be easier to test.

Finally, the feasibility of further extreme programming studies comes into
question.\linebreak Whether it is found to be feasible or not, this
approach can be improved or investigated further.

\section{Structure of the Proposal}
The remainder of this thesis is as follows. Chapter 2 discusses previous
studies that influenced this research and a selection of coverage tools
that influenced the implementation of JBlanket. Chapter 3 describes the
functionality and architecture of the JBlanket system. The evaluation
procedures are described in Chapter 4 and results of the above hypotheses
are discussed in Chapter 5. Finally, Chapter 6 contains the conclusions and
possible future directions of this research.

\chapter{Related Work}
Test code coverage has been measured for at least three decades
\cite{Beizer:1990}.  During this time, numerous white papers, tools, and
experiments have been published, touching upon the different granularities
of coverage.  In this chapter common granularities of coverage, previous
research, existing tools, and caveats of using test case coverage (all of which
guided the design of this study) will be described.

\section{Variations of Coverage Criteria}
Coverage criteria refers to a specific group of paths through a program
that can be executed during testing \cite{Kaner:1999}.  Beizer claims that
``Path-testing techniques are the oldest of all structural test
techniques'' \cite{Beizer:1990}.  He discovered that IBM has records of its
use for over two decades.  Since that earliest known reference to a
coverage analyzer, many variations of coverage criteria have evolved.
Among the most common in use (listed in decreasing granularity) are
statement coverage, branch coverage, and condition coverage
\cite{Kaner:1999}.

\subsection{Statement Coverage}
Statement coverage records which statements are executed during testing.
Also known as line coverage and segment coverage and basic block coverage
\cite{Cornett}, this criteria does not require the presence of source code.
Instead, line numbers can be inserted directly into the compiled code for
calculating coverage.  For example, compiling Java programs with debug
turned ``on''.  However, it can be considered as the weakest granularity of
coverage \cite{Kaner:1999} \cite{Myers:1979} due to its insensitivity to
any condition or multiple condition statement.

For example, consider the following Java statements:
\begin{alltt}
{\small{}line 1:  if (a > b) \{
line 2:    b = a + b;
line 3:  \}
line 4:  a / b;
}
\end{alltt}
Regardless of the value of a or b, line 1 needs to be executed only once to
be counted towards coverage measurement.  If $a=10$ and $b=9$, this test
case will cause lines 1-4 to be executed successfully, yielding 100\%
coverage.  However, further testing with the case where $a=10$ and $b=-10$,
values that will cause line 4 to fail, will never be considered.

\subsection{Branch Coverage}
Branch coverage checks for both true and false evaluations of all boolean
expressions.  Also known as decision coverage, all-edge coverage, and basis
path coverage \cite{Cornett}, this criteria considers multiple decision
boolean expressions separated by logical-and or logical-or as a single
boolean expression.

Consider this modified version of the above statement coverage example:
\begin{alltt}
{\small{}line 1:  if ((a > b) && ((b > 0) || (b == -a))) \{
line 2:    b = a + b;
line 3:  \}
line 4:  a / b;
}
\end{alltt}
Problems arise in programming languages that use short-circuit operators.
In Java, lines 1-3 will be executed as long as $a>b$ and $b>0$.  However,
they will not be executed if $a<=b$.  The expression $b==-a$ will never be
executed, so the tester will never know that the expression should instead
be $b !=a$ until values like $a=10$ and $b=-10$ appear.

\subsection{Condition Coverage}
Condition coverage is a more thorough case of branch coverage.  Instead of
treating multiple decision boolean expressions separated by local-and or
logical-or as a single boolean expression, each sub-expression combination
is considered as separate tests.  From the branch coverage example, there
would be $2^3$ combinations of tests since each sub-expression has either a
true or false value and there are three such sub-expressions.  Therefore,
the number of test cases per multiple decision boolean expressions
increases or decreases by a factor of 2.

\subsection{Method-level coverage}
The coverage criteria that will be investigated in this research is
method-level coverage.  Also known as function coverage, call coverage,
method coverage \cite{GlassJar}, this criteria uses methods to form paths
through the system and measures which percentage of methods were invoked
during testing.  Method-level coverage is particularly useful during the
beginning stages of testing since it has a much broader scope than the
previous criteria mentioned and is therefore cheaper to implement and less
expensive to measure.  Furthermore, at least every method in a system can
be exercised at least once during testing, increasing confidence in the
system's correctness \cite{Dalal:1993} before moving on to more specific
testing techniques.

In addition, it obviously requires less effort for programmers to achieve
higher levels of method-level coverage than statement coverage, one of the
simplest measurements to calculate \cite{Marick:1997} \cite{Kaner:1995},
since the only way to exercise every statement is to exercise every method
that contains those lines of code. (The exception, of course, being
abstract methods in Java).  Moreover, there is no proof that the time spent
trying to increase levels of statement coverage yields substantially
higher benefits than spending less time trying to increase levels of a coarser
grained coverage like method-level coverage.

\section{Code Coverage Studies}
Various studies have been conducted on large and small scales with the
different coverage granularities to discover the ideal level for test code
coverages and the possible impacts they have on the quality of software.
However, only a limited number of them have included method-level coverage.
The three experiments described show that method-level coverage is a
useful, albeit limited, criteria that can be used during initial stages of
software testing.

\subsection{Test Case Prioritization}
In \cite{Elbaum:2002}, Elbaum et. al presented a study comparing the
effectiveness of using either statement coverage or method-level coverage
for prioritizing test cases during regression testing. Each coverage type
was measured in four different ways: total coverage, additional elements
invoked, total fault-exposing potential (FEP), and additional FEP
potential. These eight types of coverages were executed on eight C programs
with sizes ranging from 138 to 6218 lines of code (LOC), seven of which
were under 520 LOC.

They found that while statement coverage performed better than method-level
coverage, there were several cases in which the difference between
coverages were not significant, and two cases in which a method-level
measurement performed better than its statement-level counterpart.
Furthermore, on the average, the various method-level coverage measurements
performed similarly to statement coverage measurements. The ranking for
both types were: 1) additional FEP potential, 2) total FEP, 3) total
coverage, and 4) additional elements invoked. The authors also noted that,
while some loss of effectiveness can be expected due to its coarser
granularity, their findings suggest benefits of method-level coverage
should be further investigated since it is the ``less costly and less
intrusive'' approach \cite{Elbaum:2002}.

This study relates to the usefulness of method-level coverage.  If it can
perform similarly to a finer granularity during regression testing, perhaps
it can be used during unit testing to obtain helpful data about test cases
and the amount of a system being exercised.

\subsection{Experimenting with Different Coverage Goals}
An experiment conducted by Marick \cite{Marick:1991} suggested that high
levels of coverage are acceptable goals with various granularities of test
case coverage. He measured the cost of reaching near 100\% coverage with
branch coverage, loop coverage, multi-condition coverage, and weak-mutation
coverage. Cost was determined in terms of the amount of coverage attained,
the number of test cases documented, the amount of time needed to design
the test cases, and the number of conditions argued to not be feasible to
test. Infeasible conditions included conditions which are either impossible
to test or are not worthwhile testing.

The results of this single person experiment showed that after two tries,
branch coverage reached 95\% using black-box testing techniques, a level
noted to be higher than those reached in previous studies. In addition,
when both loop and multi-condition coverage results were combined, their
total reached 92\%. To exercise the remaining 8\% would have required ``3\%
of total time, 2\% of total test conditions, and 3\% of the total test
cases'' \cite{Marick:1991}. By using these various granularities of
coverage, the experimenter concluded that ``100\% feasible coverage is a
reasonable testing goal for unit testing'' \cite{Marick:1991}.

However, this experiment was conducted on an extremely small scale. The
experimenter was the only person conducting the experiment (i.e., creating
missing specifications, designing test cases, calculating the amount of
time used designing the test cases, etc.). The systems measured were C
programs consisting of 30 to 272 LOC. Results from such small experiments
cannot be generalized to include larger systems \cite{Glass:1981} or be
generalized to other granularities of coverage since each coverage type has
different weaknesses \cite{Cornett}.

Therefore, these findings cannot be generalized to method-level
coverage. So, how much effort is needed to reach 100\% method-level
coverage remains unknown. In this research, effort will be measured in
terms of the total LOC, total test LOC, and the amount of coverage obtained
for the system measured. To ensure only ``feasible coverage'' is measured,
rules pertaining to the types of methods included in coverage will be
applied.

\subsection{Measuring Coverage with Function Testing}
Piwowarski et. al studied the benefits of statement coverage on a large
scale software system at IBM \cite{Piwowarski:1993}. They measured
statement coverage during unit testing, function testing, and system
testing. Initially, they observed that testers overestimated their coverage
when they did not know their actual coverage. For example, some estimated
achieving coverage of 90\% or above, but actually reached only 50\% to
60\%. However, after measuring coverage, they found problems such as
unreachable code or unexecutable code prevented 100\% coverage. For
example, code managing unlikely errors during normal execution is not
reached under normal circumstances, or special hardware commands cannot be
executed during testing \cite{Piwowarski:1993}.

The authors concluded that ``70\% statement coverage is the critical point
for our function test'', ``50\% statement coverage was generally
insufficient'', and ``beyond a certain range (70\%-80\%), increasing
statement coverage becomes difficult and is not cost effective''
\cite{Piwowarski:1993}. They also found that with coverage information,
test cases could be improved to increase coverage by 10\%. Furthermore, while
100\% statement coverage is not feasible during function testing, it is
feasible during unit testing.

From their experiment, it is clear that knowledge of statement coverage
influenced the implementation of test cases while trying to increase
coverage. This is probably the case with method-level coverage
also. However, in what ways are the test cases modified? For example, does
it require significantly more code, or minor adjustments to current test
cases to increase coverage?

These three case studies have influenced the evaluation of the usefulness
of extreme coverage. The next section describes the influences that guided
the design and implementation of JBlanket, the system used to gather data
for this research.

\section{Coverage Tools}
Numerous commercial and non-commercial tools currently available include
more than one type of coverage measurement and reporting functionality.
All of them instrument a system's code in different ways.  However, none of
them were considered appropriate for this research.  Although the tools may
or may not have offered method-level coverage, the main reasons behind this
decision are that majority were Closed Source projects and/or would have
been prohibitively expensive to obtain and deploy.

With Closed Source projects, they either did or did not offer method-level
coverage. When method-level coverage was not included, the tool could not
be extended to include the needed coverage measurement. When the tool
included method-level coverage, the options were to either spend over a
hundred dollars to purchase a single license, or use trial versions for at
most 30-days. Since undergraduate college students were the evaluators,
requiring them to purchase the licenses for this research or be constrained
by the lifespan of trial versions did not seem feasible. Either action
would most likely have frustrated the evaluator population and have
negative influences on the research results.

Therefore, the coverage measurement tool used in this research needed to be
accessible and available for use under any situation. Hence, to avoid
re-downloading expired trial versions, the obvious choice was to try to use
an Open Source Project.

The coverage tools reviewed here appear to be among the most popular (i.e.,
appeared higher up in the Google \cite{Google} ranking) for Java programs.

\subsection{Clover}
Clover \cite{Clover} is an impressive code coverage tool that determines
which sections of code are not executed during testing. The current version
of Clover, version 0.6b, comes with two JAR files and can measure
method-level, statement, and branch coverage. After running this tool with
Ant, class files are produced that include both the original
program and Clover's methods to record trace data. This automatic addition
ensures that the user does not need to manually alter their source
code. Clover's output can be viewed as either XML, HTML, or through a Swing
GUI. Any unexecuted code is highlighted for quick identification.

Users need to have access to the source code of the system being tested
because Clover recompiles the entire system to include its ``coverage
gathering mechanism''. While this restricts the tool from being used on
systems in which only byte code is available, it allows users to include or
exclude specific chunks of code from coverage by adding Clover specific
commands to the source code.

In addition, this is a Closed Source system and it is not clear whether it
can be used with client-server systems. The projects used for this
evaluation uses Tomcat as the server.

\subsection{JCover \texttrademark}
With JCover \cite{JCover}, users can work with a program's source code,
class files, or both to calculate statement, branch, method, class, file,
or package coverage. It can conduct client and server-side testing with any
``standards-compliant JVM''. An additional Java API is included that allows
the user to ``programmatically control JCover's coverage agent at
runtime'' \cite{JCover}. This API must be integrated into the user's test
framework. All coverage data is archived for future analysis. The data
collected can also aid in optimizing tests by including whether coverages
overlap or are disjoint. The reports are formatted in HTML, XML, CSV, and
MDB.

JCover is not an Open Source project, but a fully functional 15-day
evaluation copy can be downloaded. This tool's web page does not clearly
state what is the process of data collection or what servers it can be used
with.

\subsection{Optimizeit Code Coverage}
Optimizeit Code Coverage \cite{Optimizeit} is a part of Borland's
Optimizeit Suite, which also contains two other tools, Optimizeit Profiler
and Optimizeit Thread Debugger. It measures class, method, and statement
coverage. Depending upon the type of measurement, it calculates the number
of times a class, method, or line of code is executed in real-time. A GUI
is also available for quick identification of results. The source code is
not required for this coverage tool. Class and jar files are sufficient to
receive an accurate measurement. It also works with application servers.

While this is not an Open Source project, it also offers a 15-day trial
version. In addition, Optimizeit Code Coverage seems to show coverage for
every class in an application. The user does not appear to have the option
to focus on a specific subset of classes

\subsection{JUnit-Quilt}
JUnit-Quilt \cite{Quilt}, or Quilt, is an Open Source project created by David
Dixon-Peugh and Tom Copeland. Currently it offers statement, branch, and
path coverage. Through byte code instrumentation, classes are loaded into a
ClassLoader specifically designed for Quilt before they are loaded into the
JVM. Statistics are kept, from which coverage is calculated. Results can be
displayed in HTML or XML using its reporting functionality.

Quilt is released under the Apache License
\cite{license:Apache}. Therefore, someone other than the authors can
extend the system to include method-level coverage. However, while their
licensing makes Quilt available for use free of charge, I was unable to
modify it to include method-level coverage and integrate it with Tomcat.

\subsection{Conclusion of Tools}
From the coverage tools reviewed, both Clover and Quilt were considered as
possible candidates in this research. However, its price as well as the
length of its trial version hindered access to Clover. It would be
detrimental to this study if the evaluators were required to obtain new
trial versions after the old trial versions expired. Furthermore, if they
would not be able to run Clover with Tomcat, the evaluators would not be
able to use it to measure their systems. (see Chapter 4).

With respect to Quilt, while it is an Open Source system that can be
modified to include the method-level coverage measurement, the use of a
ClassLoader was found to inhibit integration of Quilt with
Tomcat. Therefore, the decision was made to create JBlanket.

\begin{table}[htbp]
  \begin{center}
    \caption{Coverage tools summary}
    \begin{tabular}{|l|l|l|} \hline
      {\bf Tool} & {\bf Coverage(s)} & {\bf Source}\\ \hline
      Clover & method, statement, branch & source code\\ \hline
      JCover & file, class, method, statement, branch & source code, byte code, both\\ \hline
      Optimizeit Code Coverage & class, method, statement & byte code\\ \hline
      Quilt & statement, branch, path & byte code\\ \hline
      JBlanket & ? & ?\\ \hline
    \end{tabular}
  \end{center}
\end{table}

However, finding the right tool to use was not enough.  There are many
caveats to using test code coverage to measure the quality of testing.
These include known misconceptions and misuses of code coverage.
 
\section{Code Coverage Misconceptions and Misuses}
Ensuring software quality is an important task, a small portion of which is
measuring test code coverage.  Companies that produce software are
responsible under negligence law to ensure that the products they release
do not ``pose an unreasonable risk of personal injury or property damage''
\cite{Kaner:1996:2}.  Therefore, the level of testing intensity a system
endures will vary depending upon the nature of the system.  Misinterpreting
and misusing various testing results can potentially lead to hazardous
conditions.

\subsection{Misconceptions}
When test code coverage is used during the beginning stages of software
implementation, using phrases like ``complete coverage'' or ``100\%
coverage'' to describe testing results can be misleading
\cite{Kaner:1996:2}.  Inexperienced testers might infer that testing, in
general, was thorough and complete.  They may not immediately understand
that the phrases only describe the completion of a particular coverage
criteria.

Furthermore, knowing a coverage measurement does not imply that the test
cases were distributed uniformly \cite{Marick:1997}.  With respect to
method-level coverage, a total system coverage of 70\%, for example, does
not indicate that all classes or packages were tested equally.  However, by
breaking down the total coverage according to packages, testers will be
able to make up the deficiencies in packages with lower levels of coverage
by either creating new test cases or modifying existing test cases.

Coverage measurements cannot be extended to include the other
variations of coverage criteria or any other testing techniques.
Obviously it is incorrect to conclude that reaching 100\% statement
coverage implies simultaneously achieving 100\% condition coverage.  On the
other hand, it may not be so obvious that 100\% condition coverage does not
mean all boundary conditions have been tested.

Tools used to measure coverage cannot detect faults of omission
\cite{Marick:1997} and are subject to programming language rules.  In the
statement coverage example, coverage results would not report that the
condition $b!=-a$ is needed to make the if-statement condition valid.  From
the branch coverage example, its coverage results will not report that
$b==-a$ is wrong, and will in fact, erroneously cause line 2 to execute.

\subsection{Misuses}
Most misuses of coverage data are based upon pressure perceived by testers
that are either self-imposed or imposed by higher management.  When
specific coverage criteria is required to reach a certain level before the
development of the software can proceed, tendencies may emerge to create
simple tests to make up any deficiencies \cite{Marick:1997}.

Furthermore, ``designing [the] initial test suite to achieve 100\% coverage
is an even worse idea'' \cite{Marick:1997}.  These types of tests corrupt
the testing process because they are no longer created to find errors in
the program.  Instead, valuable time is wasted with their implementation
and execution.

Coverage measurements can also be misused by management.  When a software's
quality is measured by its test code coverage alone, workers may choose
to implement the easiest and most obvious tests \cite{Marick:1997}.
Clearly this approach to testing will result in problems later in the
development process where more obscure faults can emerge.  Even worse, an
obscure fault can be detected before the software is distributed, or
another more obscure fault that caused the first fault is detected after
distribution.

\chapter{JBlanket System Architecture and Design}
I created JBlanket to gather ``extreme coverage'' data for this
research. It uses JUnit test cases to calculate the percent of methods in a
system invoked during testing. Based upon my research on various coverage
tools that were previously described, I designed JBlanket to combine
desirable features from the different systems so that it would be a
feasible tool for research. This means that from a research standpoint, it
is readily available, easy to use, and easy to understand.

Creating a readily available tool allows others to gain access to the tool
and be able to integrate it into their own research. With ease of use,
people will not be discouraged from using the tool inside and outside of
their research. With coverage results presented in a comprehensible manner,
people will be able to easily understand how to apply the results.  At this
time, both the JBlanket source code and byte code have been released to the
public under the GNU General Public License \cite{license:GNU} so that
users will be able to use the system and modify it to suit their needs.

In this chapter I will discuss its goals, its functions, its architecture,
its design, implementation history, and conclude with its user scenarios.

\section{System Functionality}
JBlanket is able to measure coverage of both stand-alone and client-server
systems. Currently, it has only been applied to client-server systems that
use Tomcat as the web server. To calculate coverage, users will need to
have access to a system's byte code. With this source of input, four sets
of data are created and stored: (1) the total methods measured in the
system (total), (2) the methods that cannot be invoked during testing
(untestable), (3) the methods invoked during testing (tested), and (4) the
remaining methods that are not invoked during testing (untested). Coverage
is measured with the following formula:

\begin{alltt}
\% coverage = tested / subtotal
\end{alltt}
where
\begin{alltt}
subtotal = total - untestable
\end{alltt}
To measure extreme coverage, users have the option of excluding methods
that contain one line of code. These ``one-line methods'' form an optional
fifth output set (one-line). The percent coverage for this approach is
calculated with

\begin{alltt}
\% coverage = (tested - one-line) / subtotal
\end{alltt}
where
\begin{alltt}
subtotal = (total - untestable) - one-line
\end{alltt}
To further improve the versatility of JBlanket, specific class files can
either be excluded from or included in coverage data. This feature allows
combinations of multiple sub-packages to be measured separately, which is
useful for targeting parts of a system.

Coverage results are reported in an HTML format similar to that of JUnit
reports. Since users are required to implement JUnit test cases prior to
running JBlanket, mimicking JUnit's reports would increase the degree of
familiarity with JBlanket reports by first-time users by reducing the
amount of time they would need to understand and interpret the results and
learn to navigate between reports.

\section{Architecture}
JBlanket uses three main steps to calculate extreme coverage. (See Figure
\ref{fig:architecture}) The first step is counting the total methods to
include in the coverage measurement, counting the untestable methods, and
counting the methods that contain one line of code (optional).  The second
step is modifying a system's byte code.  The last step is creating an HTML
formatted report from all of the output that the user can review with any
web browser.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figs/jblanket.architecture3.eps}
  \caption{JBlanket architecture}
  \label{fig:architecture}
\end{figure}

\section{Design}
JBlanket was implemented using Sun's Java 2 Platform, Standard Edition
(J2SE) (which includes the Java 2 Standard Development Kit version 1.3)).
By using Java, I was able to use existing packages like the Apache Byte
Code Engineering Library (BCEL) \cite{BCEL} for modifying byte code, Xerces
\cite{Xerces} for creating reports with XSL Transformations (XSLT) \cite{XSLT}, and DOM
\cite{DOM} and JDOM \cite{JDOM} for manipulating XML files.  I was also
able to use the existing JUnit framework for unit testing.  Final reports
are in HTML format to ease navigation between reports.

To implement the three main steps mentioned in the previous section, the
system was split into five packages: {\tt csdl.jblanket.modify}, {\tt
csdl.jblanket.methodset},\linebreak {\tt csdl.jblanket.report}, {\tt
csdl.jblanket.ant}, and {\tt csdl.jblanket.util}.

\subsection{Package {\tt csdl.jblanket.modify}}
This package contains the classes used in the first and second main steps.
Methods are modified to include a static method call to the {\tt
MethodCollector.storeMethodTypeSig-} {\tt nature} method.  With this
modification, a method's type signature (the fully qualified name of the
class it belongs to, the name of the method, and its fully qualified
parameter types) can be recorded in ``intermediate'' JBlanket files the
first time it is invoked by a JUnit test case.  The names of these
intermediate files are ``COVER-*.xml'', where * is the name of the test
class that invoked the method.  (However, when Tomcat is used, * is the
name of the first modified class that was invoked on Tomcat.)

In addition to modifying the byte code, a collection of all the included
methods in the system and a collection of all the untestable methods that
will never be modified by JBlanket are stored.  Untestable methods are
either abstract (methods that have no contents and cannot be invoked) or
native (methods whose contents are of a programming language different from
Java).  An optional third collection of methods whose content is one line
of source code is also stored here.  These resulting output files are
referred to as ``essential'' JBlanket files.

\subsection{Package {\tt csdl.jblanket.methodset}}
This package contains the classes used to manage method data collected by
JBlanket.  The main class, {\tt MethodSet}, is implemented using the
Singleton design pattern and synchronization.  This is to ensure that every
method type signature is stored in the correct output XML file and that no
output file is over-written during JUnit test executions.

\subsection{Package {\tt csdl.jblanket.report}}
This package is used in the third main step, creating the final report for
JBlanket.  The {\tt AggregateTransformer} class mimics the behavior of the
Ant {\tt AggregateTransformer} class -- combining all of the intermediate
files into one aggregate file, ``COVER-MethodSets.xml'' and transforms the
XML file into an HTML report.  The methods in the aggregate file are sorted
by the class they belong to and their classification (tested, untested,
one-line).  The {\tt JBlanketReport} class calculates and stores the
methods invoked during testing and the methods that were not invoked during
testing.  These XML files are also considered ``essential'' JBlanket files.

\subsection{Package {\tt csdl.jblanket.report}}
This package contained the Ant task definitions for the jblanket ({\tt
JBlanketModifyTask}) and jblanketreport ({\tt JBlanketReportTask}) Ant
tasks.

\subsection{Package {\tt csdl.jblanket.util}}
This package contains the {\tt SysInfo} class, which provides version
information about this system.  It also contains the {\tt JarFactory}
class, which includes the {\tt jar} and {\tt unjar} methods used for
packaging or extracting JAR files.  This utility is used to extend the
application of JBlanket to include systems that rely heavily on JAR files
to contain their functionality.  The {\tt JBlanketConstants} class contains
constant values that are used throughout the system.

\section{Building a Coverage Tool}
A tool to collect coverage data can be build in three ways \cite{Quilt}:

\begin{itemize}
\item Source Code Instrumentation - Change the source code before the
software is compiled.
\item Byte Code Instrumentation - Change the binary before it is run.
\item Profiler Monitoring - Monitor a profiler and report based upon its
results.
\end{itemize}
The decision to use any particular approach depends upon a developer's
preference.

Prior to its release to the ICS 414 class during evaluation, JBlanket
evolved through two previous implementations, both using profiler
monitoring, before settling on its current implementation using byte code
instrumentation.  Source code implementation was never considered.

\subsection{Version 1.0}
The initial design of JBlanket utilized the Java Debug Interface (JDI)
provided by Sun and was first designed for stand-alone systems.  A new Java
Virtual Machine (JVM) was launched every time JUnit testing was conducted.
A second JVM was then launched to run the test suite.  The additional JVM
traced through a test case during execution and recorded the type
signatures of each method called, including default methods that were not
implemented by the programmer like default constructors, etc.  The results
from testing were stored in an XML file.  LOCC \cite{LOCC}, a line of code
counter tool for Java developed in CSDL, was then run over the entire
system to record every method's type signature and stored them in a second
XML file.  Then the two files were compared and the difference recorded in
a third XML file.  With the 3 XML files, coverage statistics could easily
be calculated as the percent of methods tested.  Users needed to look
through the XML files to find out which methods were tested and which
methods were not.

Executing this version on a 500 MHz Pentium III processor took approximately 2
minutes to run on a program that contained 20 lines of code with 4 methods.
Believing that the length of execution time for stand-alone programs could
be overlooked, the system was then extended to include client-server
systems.  Suddenly the execution time could no longer be overlooked.

To extend the system to client-server systems, Tomcat was required to be
launched in a separate JVM running the JDI.  Without the separate JVM,
i.e., normal launching of Tomcat with the ``startup'' script, methods
executed on Tomcat could not be recorded.  For example, methods called by
JSP pages during HTTPUnit testing were not recorded.

During normal execution on a 512 MHz dual processor machine running Windows
2000, preparing Tomcat for testing took an average of 7 seconds.  However,
preparing Tomcat using JDI took approximately 7 minutes, 10 times the
normal execution time!  Furthermore, the speed of running the tests after
the initialization appeared to have the same relative run-time as compared
to testing with the stand-alone system.

Methods invoked by Tomcat were recorded in a fourth file.  These results
were combined with the first file to calculate the percentage of methods
tested.  Overall, a client-server program with 330 lines of code and 33
methods took approximately 28 minutes to test on the 512 MHz dual processor
machine.

Due to the limitations introduced by the execution time of JBlanket, a
different approach was required.  As the size and complexity of systems
grew, the increase in execution time was no longer acceptable.  So instead
of using the JDI, loading the byte code through more normal means appeared
to be a faster solution.  In theory, the next execution of the test cases
should take approximately the same amount of time as a normal execution of
the test cases.  Therefore, the search began for an Open Source project
that either already contained method-level coverage, or could be extended
to include method-level coverage.

\subsection{Version 2.0}
By searching the Internet with Google, SourceForge.net was the web site
targeted because it contains a diverse collection of Open Source projects.
This is where JUnit-Quilt (Quilt) was found.  As discussed in the previous
chapter under related works, Quilt needed to be extended to include
method-level coverage.  An extension of the Java ClassLoader,
JBlanketClassLoader, was implemented to accomplish this goal.  With the
JBlanketClassLoader, every time a method was invoked during testing, it's
type signature could be accessed and stored.  With this profiling-like
capability, the number of times a method was invoked could also be counted.

Unfortunately, using the ClassLoader was limited.  It could be used with
simple and complex stand-alone systems, but could not be used with Tomcat.
From my limited knowledge of Java, I was not able to use my
JBlanketClassLoader to load the system's classes on the server-side instead
of Tomcat's ClassLoader.  Faced with this handicap, another approach was
needed.

\subsection{Version 3.0}
The current approach of JBlanket uses byte code instrumentation.  It was
designed to be executed from either the command-line or integrated with
Ant. There are three main steps that need to be executed to calculate
coverage. The first step generates a set containing the total methods
included in the coverage measurement, a set containing the total untestable
methods, and a set containing all methods with one line of code.  These
sets are stored to essential XML files.  Users see this step performed
simultaneously with the second step.

The second step in JBlanket modifies the byte code created by compiling the
system's source code. BCEL is used to alter each method such that when one
of the modified methods is executed for the first time during the execution
of a unit test case, its type-signature will be stored.

Before each method is modified, it is checked against two separate
conditions. The first condition is if the method can be invoked or should
be included in coverage. If a method falls into either category it is not
modified and placed in the second output set. The other condition relies on
the number of lines of code the method contains. If methods containing a
single line of code are to be excluded from coverage, then one line methods
are not modified and recorded in the optional one line output set. The second
output set is immediately removed from the first output set, creating a
modified first output set, and so not included in the coverage measurement.

The last step to perform is preluded by executing the JUnit test
cases. Prerequisite setup steps may be needed depending upon how the system
is tested. For example, the modified class files can be packaged into JAR
files before running the JUnit tests. A WAR file can
be created and copied to Tomcat or the modified class files can be copied
to one of the ``classes'' subdirectories in the Tomcat directory. Then Tomcat
can be launched for client-server systems. This prelude to the last step
outputs the intermediate JBlanket output files.

After the JUnit tests are executed, the final step can be performed. This
is the report step that interprets all of the accumulated results. First a
fourth tested methods output set is created from the combination of the
intermediate files.  Then the fifth untested methods set is created from
the difference of the modified first set, the fourth set, and the optional
one line set. Then these sets of raw coverage data are aggregated into one
XML file, where each method is stored according to the fully qualified name
of its class. Each fully qualified class contains at most three different
method classifications (tested, untested, one-line) under which the
corresponding type-signatures are stored. This file is then transformed
into HTML through XSLT.

Before JBlanket can be used, the javac ``debug'' option must be turned on
when compiling the source code. The debug option ensures that line numbers
from the source code are included in the byte code. Without line numbers,
several JBlanket steps employing BCEL will fail.  The first set of total
methods will be inaccurate because line numbers are used to decide when
constructors are implemented in the source code or are default
constructors.  Furthermore, the lines of code in a method, used in the
second step, cannot be calculated for methods with one line of code.

In addition, to ensure that the Java ClassLoader loads the correct class, I
recommend that only one copy of the class files be referenced in the
classpath.  This means excluding JAR files which may contain unmodified
versions of the class files. If multiple references to these files exist,
there is no guarantee that the ClassLoader will find the correct, modified
class files. No data can be collected from invoking methods from unmodified
classes.

Finally, for coverage data to be reliable, all test cases need to pass. It
is possible for JBlanket to calculate coverage when some test cases succeed
and others fail. However, this measurement will not reflect the true
coverage of the system.

\section{User Scenario}
Sally is a student in Computer Science working on a class assignment.  The
problem is implementing a stack that is accessible over the Internet.  The
requirements specify use of Java v1.4, JSP, JBlanket, Ant, and Tomcat as
the web server.  Being a conscientious student, Sally beings working on the
assignment right away.  She is working on a PC running Windows 2000. All of
the tools have been previously installed, except for JBlanket.

After completing her first attempt at the program, she decides to create
unit tests using JUnit and HTTPUnit to ensure that her program works
correctly.  She designs and implements 3 test cases, one per stack function
(push, pop, and clear), and places them in one class.

After the first run of her tests, Sally finds that her clear function test
does not pass.  She views the JUnit report to find out what happened.  The
stack did not clear.  There was one element remaining in the stack.  So she
checks her code for clearing the stack and finds that she does not pop off
the last element.  Feeling relieved to have found the error so quickly, she
adds a command to pop off another element and runs her tests again.

100\% success!

Knowing that all her tests must pass before any meaningful data can be
gained from coverage, Sally is now able to integrate JBlanket into her
project.  She downloads the jblanket.zip file off of the CSDL web site and
reads the README.html file.  Under the ``Invocation'' section, there are
instructions on how to add JBlanket to a project.\footnote{The process of
integrating JBlanket into a system has evolved from the previous process
used with CREST.}

Opening her build file, she finds the Ant target that compiles her program.
As stated in the directions, she modifies to her build file to look like
the following:

\begin{alltt}
{\small{}<taskdef name="jblanket"
         classname="csdl.jblanket.ant.JBlanketModifyTask"/>
  <target name="compile"
    description="Compiles code and run JBlanket over byte code.">

    <javac srcdir="\$\{basedir\}/src/edu/hawaii/stack"
           debug="on"/>

    <mkdir dir="\$\{jblanket_dir\}"/>

    <!-- Run JBlanket over class files. -->
    <jblanket testgrammar="Test*.class"
              enable="true"
              totalfile="totalMethods.xml"
              onelinefile="onelineMethods.xml"
              untestablefile="untestableMethods.xml">
      <fileset dir="\$\{basedir\}/src">
        <include name="**/stack/**/*.class"/>
      </fileset>
    </jblanket>
  </target>
}
\end{alltt}
Then she scans through the build file to find the Ant target that runs her
unit tests.  To create the report for JBlanket, Sally changes her build
file to:

\begin{alltt}
{\small{}<taskdef name="junit"
 classname="org.apache.tools.ant.taskdefs.optional.junit.JUnitTask" />
  <taskdef name="junitreport" 
   classname="org.apache.tools.ant.taskdefs.optional.junit.
              XMLResultAggregator" />
  <taskdef name="jblanketreport"
   classname="csdl.jblanket.ant.JBlanketReportTask"/>
  <target name="test" depends="init"
    description="Run JUnit tests and generate reports.">
    <mkdir dir="\$\{basedir\}/test_output"/>

    <!-- Run the tests, all classes whose name starts with 'Test'. -->
    <junit printsummary="withOutAndErr" fork="yes">
      <sysproperty key="jblanket_dir" value="\$\{jblanket_dir\}"/>
      <sysproperty key="test_host" value="http://localhost:8080/"/>
      <classpath>
        <pathelement path="\$\{java.class.path\}"/>
        <pathelement path="\$\{basedir\}/src"/>
        <fileset dir="\$\{lib.dir\}">
          <include name="*.jar"/>
        </fileset>
      </classpath>
      <formatter type="xml" />
      <batchtest todir="\$\{basedir\}/test_output">
        <fileset dir="\$\{basedir\}/src">
          <include name="**/stack/**/Test*.java" />
        </fileset>
      </batchtest>
    </junit>

    <!-- Generate JUnit report on the results. -->
    <junitreport todir="\$\{basedir\}/test_output">
      <fileset dir="\$\{basedir\}/test_output">
        <include name="TEST-*.xml"/>
      </fileset>
      <report format="frames" todir="\$\{basedir\}/test_output" />
    </junitreport>
    <echo message="JUnit results in \$\{basedir\}/test_output/index.html" />

    <!-- Generate JBlanket report on the results. -->
    <jblanketreport totalfile="totalMethods.xml"
                    testfile="testMethods.xml"
                    difffile="diffMethods.xml"
                    onelinefile="onelineMethods.xml"
                    reportformat="frames"
                    enable="true">
    </jblanketreport>
    <echo message="JBlanket results in \$\{jblanket_dir\}/index.html" />
  </target>
}
\end{alltt}
Finally, she crease the JBLANKET\_DIR environment variable and copies the
setEnv.bat file to her project.

Anxious to try out the new tool, Sally opens another command prompt window
and navigates to her project directory.  She sets the environment with the
batch file, and immediately re-builds her project and runs the test cases.
The following output is sent to the screen:

\begin{alltt}
{\small{}[jblanketreport] *******************************************************
[jblanketreport] Method-level Coverage:
[jblanketreport] All methods: \{total=18\}
[jblanketreport] One line methods: \{total=10\}
[jblanketreport] Non-one line methods: \{total=8\}
[jblanketreport] Tested methods: \{total=8, percent=100\%\}
[jblanketreport] Untested methods: \{total=0,  percent=0\%\}
[jblanketreport] *******************************************************
}
\end{alltt}
Happy that she received 100\% coverage on the first try, Sally decides to
attempt the extra credit -- implementing a command that doubles the
contents of a stack -- since there's still lots of time before the due
date.  Before proceeding any further, she turns JBlanket off before
implementing the new feature, changing the ``enable'' attributes to
``false''.

After several tries, Sally is able to play with the stack on her Mozilla
web browser.  Now she has to create the test case.  Within 2 minutes she's
implemented the test case.  Sally is amazed that after the first run, the
test case passes.

Remembering to check her coverage again, she turns JBlanket back on by
changing the ``enable'' attributes back to ``true''.  She then re-builds
her assignment and runs the test cases.  JBlanket produces the following
output:

\begin{alltt}
{\small{}[jblanketreport] *******************************************************
[jblanketreport] Method-level Coverage:
[jblanketreport] All methods: \{total=20\}
[jblanketreport] One line methods: \{total=10\}
[jblanketreport] Non-one line methods: \{total=10\}
[jblanketreport] Tested methods: \{total=8, percent=80\%\}
[jblanketreport] Untested methods: \{total=2,  percent=20\%\}
[jblanketreport] *******************************************************
}
\end{alltt}
Sally is shocked to find out that she did not get 100\% coverage.  So she
immediately opens the JBlanket report in Mozilla and searches for the
methods that were not invoked.  First she sees that the double method she
wrote was not invoked.  Confused, she looks for the other method that
wasn't invoked.

It turns out that the tstDouble method was not invoked.  Remembering that
JUnit requires test methods to begin with ``test'', she quickly corrects
the name of the method and runs her test cases again.

This time it's 100\% coverage.  Satisfied with her progress, Sally quits
for the day.

\chapter{Evaluation of Extreme Coverage}
The evaluation of the hypotheses of this research took place in an academic
environment by undergraduates in a senior-level, second-semester Software
Engineering course (ICS 414) at the University of Hawai'i.  There were 13
students in ICS 414, all of whom participated in developing 8 separate web
services (using Java 1.4 and JSP) that will be deployed on the Information
and Computer Science (ICS) home page (http://www.ics.hawaii.edu) in Spring
2003. The combination of the web services is called CREST.\footnote{This
project has been renamed to CLEW.  However, in this research it will be
referred to by its original name, CREST} Due to the nature of the projects,
each student was assumed to have either enrolled in the previous semester's
Software Engineering course, or have adequate knowledge of Java, JSP,
Apache Tomcat, CVS, JUnit, HTTPUnit, and Apache Ant.

\section{Qualitative Data Gathering Process}
Students were given time to accustom themselves to the course and their
projects for the first 10 weeks of the semester.  Teams of 2-3 people were
assigned to six of the projects.  Two students, who were also members in
the aforementioned teams, individually implemented the remaining 2
projects.

At the end of the 10th week, I integrated JBlanket into the Ant build files
the students used to build their projects.  By doing this, I hoped to
remove two possible obstacles:

\begin{enumerate}
\item The effort needed to include JBlanket into the build processes.

When the students were first given access to JBlanket, it required a degree
of familiarity with the system.  I was concerned that students might be
discouraged from using the tool if they found it difficult to install.
However, since then, the installation process has been improved.

\item Inconsistent use of JBlanket.

To gather accurate data, everyone needed to have access to JBlanket
simultaneously.  It was then up to the students to use the system.  If
students integrated the system themselves, the addition would have been
done at their convenience.  Threfore, making any coverage behavior
observations impossible if some students were using the tool and others
were not.
\end{enumerate}

In the middle of the 11th week, the professor handed out the Pre-Use
Questionnaires (``pre'') in Figure \ref{fig:questionnaire.pre} to the
students so that I could judge their current practices and beliefs towards
unit testing.  Each copy was marked with a letter of the alphabet.  The
professor kept a list that identified each student with the letter that
appeared on their copy.  I waited outside the classroom so that I would not
know which questionnaire each student completed.

After the professor collected the completed questionnaires and placed them
in a closed envelope with the list, I entered the classroom and presented a
20-minute introduction to JBlanket - a description of the system, how to
run it with their JUnit test cases, and how to use the output to increase
their coverage.  The professor followed the introductory presentation with
instructions that each service was required to reach 100\% extreme coverage
by the end of the semester. To increase the likelihood of discovering
whether it is difficult to reach the total coverage and the amount of work
it would take to maintain such a high level this requirement was enforced
with an assigned grade.

At the end of the semester, the 16th week, the students were given the
Post-Use Questionnaire (``post'') in Figure \ref{fig:questionnaire.post} to
find out their reactions to extreme coverage and if their practices and
beliefs toward unit testing changed.  Each sheet was once again marked with
a letter of the alphabet. Students were given the sheet with the same
letter as on their ``pre'' questionnaire (as recorded on the identification
list).  To ensure that responses were sincere, students were assured that
their answers would not affect their final grade by including the
questionnaires with the course evaluation forms.\footnote{Course evaluation
forms are student evaluations of the content and suggestions for
improvements of a class.  These forms are not turned over to the professor
until after the grades are officially turned in.}  I was not present for
this phase either and was never allowed to see the identification list.

With the metrics collected from the student projects and comparisons
between the ``pre'' and ``post'' questionnaires, the feasibility or
infeasibility of extreme coverage will be determined.

\section{Quantitative Data Gathering Process}
To gather metrics for measuring effort, the eight student projects were
checked out from a common CVS repository.  For the first two weeks of data
gathering, I downloaded the projects daily at approximately the same time.
Twelve o'clock noon was chosen as the download time with the following 3
assumptions:

\begin{enumerate}
\item Most classes are during the day, so the projects would most likely
      not be modified during this time.
\item Most students will work on their projects at night, when I assumed
      they would have the most continuous amount of time available.
\item I, too, am a college student, and so should be able to get up by at
      least noon to checkout the projects on weekdays and weekends.
      (Actually, being a graduate student makes it even harder to get up
      early.)
\end{enumerate}

Results from these initial checkouts were used to determine the best
schedule reflecting students' effort and changes in their projects'
coverage.  As can be seen in Figure \ref{fig:crest.daily.coverage} and
Figure \ref{fig:crest.daily.loc}, checking the projects every day did not
result in finding many significant changes.\footnote{Missing data is due to
inability to calculate coverage.  This happens when a project does not
compile.  However, coverage was recorded even though errors or failures
occurred during unit testing.}  While the repository of at least one
project was modified every day, none was consistently modified every day.
Furthermore, changes that did occur were not present in any particular
pattern.  For example, the coverage of the Poll service appeared to change
in spurts while the coverage of Techreports service increased for about 4
days before remaining at a steady level for 4 days before increasing 0.1\%.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figs/fig.crest.daily.coverage5.eps}
  \caption{Daily extreme coverage of CREST services}
  \label{fig:crest.daily.coverage}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figs/fig.crest.daily.loc4.eps}
  \caption{Daily LOC of CREST services}
  \label{fig:crest.daily.loc}
\end{figure}

Therefore, from the data gathered thus far, I decided that the projects
should be checked out once every three days.  Within the 3 days, average
coverage change was 8.3\% instead of the 2.2\% average daily coverage
change and average LOC change is 107.6 LOC instead of the 42.6 LOC average
daily LOC change.

\begin{table}[htbp]
  \centering
  \caption{Daily extreme coverage of CREST services}
  \includegraphics[width=1.0\textwidth]{figs/table.crest.daily.coverage2.eps}
  \label{table:crest.daily.coverage}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Daily total LOC of CREST services}
  \includegraphics[width=1.0\textwidth]{figs/table.crest.daily.loc.total2.eps}
  \label{table:crest.daily.total}
\end{table}

\section{Measurements}
In this section I will discuss the significance of the data collected by
both the questionnaires and JBlanket.

\subsection{Questionnaires}
This section describes my motivation for including each question in the
questionnaire.  The Pre-Use Questionnaire contains 5 questions, 4
close-ended questions and 1 open-ended question.  The Post-Use
Questionnaire contains 8 questions, 5 close-ended questions and 3
open-ended questions.  Four of the 5 close-ended questions and 1 of the 3
open-ended questions were taken from the Pre-Use Questionnaire.  Repeated
questions provided feedback on any changes, or lack thereof, in each
student's opinions.
\begin{enumerate}
\item Unit tests are very important for creating correctly functioning
software.  (``pre'', ``post'')

This question was included to uncover students' opinions about the value of
unit testing.  It gives an indication of how enthusiastic they are towards
the design and implementation of unit tests, which could be connected to
the quality of their test cases.  For example, a student who views unit
tests as important will most likely put more effort into creating useful
and meaningful tests than one who does not.

\item Designing unit tests to support correctly functioning software is
hard. (``pre'', ``post'')

This question indicates the amount of effort students believe is needed to
implement useful tests.  Effort can be thought of in terms of time, LOC,
etc.  The answers in the ``post'' questionnaire may also be related to how
much effort was needed to sustain 100\% coverage.

\item My current set of unit tests does a good job of ensuring that my
software functions correctly. (``pre'', ``post'')

This question indicates the students' confidence in the design of their
unit tests.

\item JBlanket helps me to write unit tests that ensure the correct
functioning of my software. (``post'')

This question directly asks how useful the students felt the tool was
during development.

\item To the nearest 25\%, what \% of the methods in your software are
currently invoked by your unit tests? (``pre'', ``post'')

As indicated in \cite{Piwowarski:1993}, most estimations of coverage are
higher than their actual of coverage measurement.  In the ``pre''
questionnaire, this question reflects how much confidence students have
with respect to their testing abilities.  In the ``post'' questionnaire, it
reflects what they were able to achieve.

\item Please briefly describe one or two of the most significant problems
you've encountered while designing unit tests.  (Do not include the problem
of learning how to use unit testing facilities such as JUnit or HTTPUnit.)
(``pre'', ``post'')

This open-ended question, was intended to find out what hinders the
implementation of quality test cases for students, and if extreme coverage
would be able to help rectify the more basic problems, like implementation,
and move focus to a more complex set of problems, like design.

\item Briefly describe how access to JBlanket has influenced the way you
write unit tests. (``post'')

This question addresses the second hypothesis, which states that extreme
coverage influences the design and implementation of testing.  It is
important to find out what influences JBlanket had and if these influences
can be compared to the problems expressed in the ``pre'' questionnaire
question \#6.

\item What would you suggest we do to improve the usefulness of JBlanket?
(``post'')

This question was added for two reasons.  JBlanket is one approach to
measuring extreme coverage.  If any improvements are suggested, it may be
directly applicable to improving extreme coverage.  On the other hand,
suggestions could also be targeted towards the usability of the tool, as
improvements are always needed.
\end{enumerate}

\subsection{Extreme Coverage}
For every set of metrics collected from the CVS checkouts, the following
measurements were recorded from the normal set of results provided by the
JBlanket and LOCC tools and additional calculations:

\begin{enumerate}
  \item JBlanket measurements: total methods, total one-line methods, total
        multi-line methods, total tested multi-line methods, total untested
        multi-line methods, and percent coverage
  \item LOCC measurements: total methods, total test methods, total
        non-test methods, percent test methods, total LOC, total test LOC,
        total non-test LOC, and percent test LOC
\end{enumerate}
The most important JBlanket coverage measurement was the percent coverage.
With a history of this measurement over a period of time, possible trends
can be observed.  In addition, it can be matched against various LOCC
measurements to estimate the effort to reach and maintain 100\%
coverage.

While I cannot list the one-line methods, including this metric signals how
many more methods could have required exercising.  It also indicates the
maximum number of additional tests that were avoided.  The total methods
gives a rough estimate of each service's size.  The total multi-lined
methods represents how many methods need to be tested, and the total tested
and untested multi-lined methods represent exactly how difficult it is to
reach and maintain coverage.  For example, the case when 200 methods are
multi-lined and only 2 were not invoked during testing could suggest that
achieving total coverage could be more challenging than expected.

From LOCC, the main measurements were the uncommented total LOC, or total
LOC, and the uncommented test LOC, or test LOC.  With these measurements,
changes in coding activity could be detected.  This is most interesting when
activities are detected but coverage remains the same.  Moreover, with a
measurement like the total methods or total test methods changes in the
coverage measurement, or lack thereof, could have many implications with
respect to maintaining coverage.

The percent of test methods and test LOC were calculated to simplify
comparison against coverage because of the same unit of measurement would
be used.  The total non-test methods and total non-test LOC are
measurements calculated from their respective total and test counterparts.
These were recorded to complete the set of observations.

\section{Duration}
The evaluation period began on November 6, 2002, when students filled out
the Pre-Use questionnaire.  This took at most 10 minutes for all 13
students to complete.

The coverage data collection period, on the other hand, lasted for 5 weeks.
A preliminary sample was taken the day before the questionnaires were given
to the students.  The sample was solely for the presentation to inform the
students of the level of extreme coverage of their services.  It was not
included in the resulting data set.  The first official data collection day
was November 8.  The last day of collection was December 11.

On December 11, the Post-Use Questionnaire was handed out to the students.
Because they were included with the course evaluation forms, I did not
receive them until mid-January 2003, along with the ``pre'' questionnaires
so that I would not be swayed by any suggestions during this evaluation
period.

\chapter{Results}
The metrics collected from the CREST services will be discussed in stages.
First, possible trends will be investigated using extreme coverage and then
verified with the LOC and method metrics collected.  Then LOC and method
metrics will be analyzed in an attempt to quantify the results from extreme
coverage.  Next, data from the questionnaires will also be discussed in
stages, beginning with a comparison between the Pre-Use Questionnaire and
Post-Use Questionnaire answers to the closed-ended questions whose
responses ranged from ``Strongly disagree'' to ``Strongly agree''.  Then
the responses to the open-ended questions will be introduced and their
applicability to extreme coverage will be discussed.

Since the first two hypotheses influenced the evaluation of the feasibility
of extreme coverage, the collected JBlanket and LOCC metrics and the
questionnaire answers will be used to evaluate both the first and second
hypotheses.  Therefore, the third hypothesis will be discussed last.

\section{Extreme Coverage}
By itself, coverage is a rather thought provoking metric to observe.  Only
5 of the 8 services finished with 100\% coverage.  The remaining 3 services
were within 6\%.  From the graphs, it appears that no two services
exhibited the exact same behavior.  Instead, most services were similar
towards the end of the evaluation period, when they were close to or
obtained 100\% coverage.  At first glance it seemed as if after a service
reached a threshold value that may not have been 100\%, it's coverage did
not deviate very far from that level.  (This is especially the case with
Textbooks, which remained at 100\% throughout the entire evaluation.)

For example, in the case of Newsbulletin's graph, coverage appeared to be
quite unstable until it reached 100\% on December 2.  It never left that
level thereafter.  The unstable coverage measurements suggests that focus
was not always on increasing coverage.  Instead, either unit testing was
done periodically or Newsbulletin increased its functional testing.  The
stable coverage measurement suggests either maintaining such a high level
of coverage is not difficult, students neared the completion of
implementing the service, so little activity was occurring, or the effort
required to reach 100\% was so great that the students did not want to
alter their code in fear of losing total coverage.

Upon closer inspection of the LOC behaviors, they suggest that majority of
the changes in Newsbulletin's total LOC was due to an increase in the
number of test LOC. From the data collected, it is unclear whether all of
the testing efforts were focused on only increasing coverage, but it is
clear that coverage did increase every time the test LOC increased.
However, before reaching 100\%, at least 2 occasions (November 17 and 23)
existed in which a lot of changes were not apparent with respect to
testing, i.e., no visible changes in test LOC or test methods.

The final effort required by Newsbulletin to reach 100\% included an
increase of 5.9\% in coverage and an increase of 31 LOC in total LOC, most
of which looks to be from an increase in test LOC. It is difficult to
conclude if any other factors were involved in this increase.  For example,
it is not obvious if the increase was due to the refactoring of either test
or non-test code because there was no observable change in the number of
test methods, but an observable increase in the number of non-test methods.

After reaching 100\%, Newsbulletin did not appear to evolve very much.
There was little observable activity with respect to the change in the LOC,
but there was no observable activity with respect to the change in the
number of methods.  So the only conclusion at this time is that perhaps the
students were satisfied with what they achieved so far, and so spent most
of their time cleaning up the code.

On the other hand, Poll, whose threshold value appeared to be between
94-95\%, showed a dramatic decrease in coverage of 21.5\% on November 29,
and then seemed to quickly recover at the next check on December 2.  This
sudden decrease in coverage only happens when a large number of methods are
not tested as compared to the number of total methods.  It turns out that
on November 29, 4 failures were recorded from Poll's test cases.  Since
JUnit failures throw an exception when they occur, the remainder of the
methods in which they are invoked are never executed, resulting in a
possible loss of methods from coverage.

The fact that Poll missed 100\% coverage by at least 4.7\% is crucial.  By
itself this behavior suggests that removing methods with only one line of
code may not enough.  Perhaps there are other untestable categories of
methods beside abstract and native methods.  On the other hand, perhaps
those methods that were not invoked during testing should have been
implemented differently.  For example, if TFD were applied, the methods
should have been more testable.  (While Resume was also not able to reach
total coverage due to 1 method, it is harder to conclude if this is due to
the difficulty of achieving total coverage or if the students were not
aware this method was untested.  Console output lists the coverage
measurement rounded to the nearest percent in addition to the number of
methods that were not invoked.)

Overall, every service, except for Textbooks, experienced at least one
decrease in coverage that appeared at no particular point in development.
From Table \ref{table:crest.change}, the drops ranged from 0.1\% to 21.5\%.
The coverage measurements of FAQ, Login, and Techreports decreased around
1\%.  Newsbulletin showed decreases less than 4\%.  Poll had one big
decrease of about 21\% and other smaller dips less than 2\%.  The remaining
2 services, Resume and Tutor, each experienced decreases less than 15\%.
The presence of these drops in coverage suggest that maintaining coverage
is not effortless and that during those periods in development, focus may
not have been totally on increasing coverage.  For example, in FAQ and
Login, decreases in coverage appeared when little or no evidence of change
to the observed test LOC or test methods existed.  However, in Techreports,
there appeared to be half as much change in the total test LOC than
non-test LOC and an increase in the total number of non-test methods as
well as test LOC.

Another more interesting observation is that several services experienced
one drop in coverage that was significantly larger than the others, while
the others were smaller drops around 1\%.  For example, the Tutor service
showed a 14.3\% drop at the beginning while its other 2 drops were only
1.5\% and 0.2\%.  Similarly, Resume's biggest drop is 8.7\%, and its other
drop is 0.9\%.  The large drops occur during times in which there was no
observable change in the total test LOC, but changes in the total non-test
LOC.  These two services further support the assumption that maintaining
coverage is not easy and requires some effort.  (Poll's big drop of 21.5\%
was previously explained and is also in favor of the difficulty of
maintaining coverage.)

My last coverage observation was derived from the amount of effort that
might be needed to reach 100\%.  From FAQ, Login, Newsbulletin,
Techreports, and Tutor, an average of 2.8\% increase in coverage was
calculated as the last increase before reaching total coverage.
(Technically, Resume did not reach 100\%, so it is not fair to assume that
the students could have invoked the 1 remaining method.)  Because the
remaining uncovered percentage is so low, covering the remaining methods
would probably require some effort on the part of the programmers.

\section{LOC and Method Metrics}
In the previous section, I was able to conclude that some effort is
required for reaching 100\% coverage and that some effort is needed to
maintain that level of coverage.  In this section, I will attempt to
discover how much effort is required in both cases, and if that effort is
reasonable.  Since 6 of the 8 services (FAQ, Login, Newsbulletin,
Techreports, Textbooks, and Tutor) were able to reach and/or maintain total
coverage, they will be used in this analysis.  In addition, even though
Poll was not able to achieve total coverage, it was able to reach a
threshold value and more or less maintain it.  Therefore, Poll will also be
included in this analysis.

\subsection{Reaching 100\% Extreme Coverage}
As mentioned in the previous section, an average of 2.8\% increase in
coverage was needed before a service reached their threshold value that may
or may not have been 100\%.  Since Textbooks started off with total
coverage, it cannot be included in this portion of the analysis because it
is impossible to calculate how much effort was used to reach that level.
It could very well be that no additional effort was needed to invoke every
method at least once during testing, but on the other hand, the complexity
of this service could have been low enough such that it allowed for easier
testing methods.

Every service showed an increase in their total number of test LOC except
for Techreports.  In fact, Techreports decreased its total LOC (-101 LOC),
which resulted in a total loss of 2 one-line methods and 4 non-test methods
for an increase of 1\% in coverage.  In addition, the single test failure
that occurred in the previous check was fixed.  This decrease in LOC and
methods suggests the programmer(s) may have restructured the non-test code
to simplify the service's testing and improved the tests.

Most services showed more change in their test LOC than their non-test LOC
except for FAQ.  While there was no obvious change in the number of
methods, FAQ increased its total non-test LOC (+136 LOC) more than test LOC
(+4 LOC).  This behavior provides further evidence that non-test code was
restructured and test code was improved.

The number of one-line methods changed either positively or negatively
except for Newsbulletin, whose one-line methods did not change.
Newsbulletin had the second highest increase in coverage of 5.9\% before
reaching total coverage.  In addition to its small total LOC increase (+31
LOC), its total number of methods decreased by 1 while its test LOC
increased by 22 LOC.  This strongly suggests that focus was placed on
testing and increasing coverage.  On the other extreme, Login increased its
number of one-line methods to 34, which contributed to the improvement of
its coverage by 29.4\% to reach 100\% the first time.  Other noticeable
changes included a total increase of 279 test LOC, 21 test methods, 6
non-test methods, and the removal of 57 LOC from the total non-test LOC.
The programmer for Login clearly took advantage of the exclusion of
one-line methods.  He also could have dramatically improved the quality of
the test cases since there is barely any evidence of change in test LOC
from the previous checkout or the subsequent checkout.

At one point, a student revealed to me that inside of every one of their
JUnit test classes are main methods that looked like the following:

\begin{alltt}
{\small{}public static void main(String[] argv) \{
  //Runs all no-arg methods starting with "test".
  System.out.println(``Running testclass TestFoo.'');
  TestRunner.run(new TestSuite(TestFoo.class));
\}
}
\end{alltt}
These methods are included to ensure that individual tests classes could be
executed from the command line instead of executing all the test classes in
the system through the Ant build.xml file.  However, the implementation of
test cases to invoke this main method is not feasible because this method
is used to invoke the test class.  Upon further reflection, one student
realized that by commenting out, or removing the {\tt System.out.println}
method call, the {\tt main} method would be reduced to one line of code,
and therefore be exempt from extreme coverage.

From the above observations, it seems as though in most cases programmers
can try to reduce multi-line methods to one-line methods by removing
non-essential code or modify non-test LOC so as to make it more testable.
With extreme coverage, test cases are also required to be updated whenever
changes are made to systems, increasing the chances of higher coverage
measurements.  Overall, these are positive changes that should always be
performed regardless of whether coverage is measured or not.  However, the
fact that no service showed changes in their total LOC exceeding 300 LOC is
cause for concern.  Aside from previously mentioned possibilities, it is
also plausible that the small amount of noticeable change is due entirely to
the fact that many of the test and non-test methods and classes were
refactored and re-designed in such a way that the result contained slightly
more code than the original code.  Therefore, it is only fair to conclude
that while reaching 100\% extreme coverage does not appear to require an
unreasonable amount of work, more research will be needed to backup this
claim.

\subsection{Maintaining Extreme Coverage}
Now the focus is shifted to determining if extreme coverage can be
maintained with a reasonable amount of effort.  Textbooks can be included
in this portion of analysis since it already had 100\% coverage from the
first checkout and maintained that level of coverage throughout the
evaluation period.  However, Tutor cannot be included in this analysis
because it reached total coverage on the last day of the evaluation.  So no
data exists regarding any activities, or lack thereof, for maintaining its
coverage.

Actually, Textbook's behavior provides some evidence to how much effort is
needed to maintain extreme coverage.  While coverage never varied from
100\%, only a minimal amount of activity was observed.  For example, the
only day in which activity clearly could have influenced the
maintainability of coverage is November 17, when the total methods
increased by 3 methods.  One assumption is that some additional
modifications were implemented and then its tweaking carried over to the
next checkout.  Aside from this day, only 3 other days of activities can be
seen, each one altering the total LOC found by less than 30 LOC.  It is
possible the implementation of this service's features were completed by
the time coverage was measured, so the only task remaining was to clean up
the code and improve unit testing.  Therefore, my conclusion that coverage
maintainability requires a reasonable amount of effort is due mainly to the
data collected on November 17.

On the other hand, FAQ displayed a lot of activity in the four checkouts
after reaching 100\%.  Total LOC appears to change by 309 LOC (+181, -128)
and total methods appears to change by 18 methods (-10, +8).  This is the
second most observable trauma a service is put through.  Interestingly, the
total one-line methods drops by 4 during this time.  The number of test
methods and non-test methods does not seem to always increase nor always
decrease.  Instead, there was a decrease in total methods, then an increase
in total methods, then a decrease again.  This could be due to the removal
of unneeded methods, then the creation of more methods and more test
methods to test the new methods.  If this is the case, then maintaining
extreme coverage would probably require some effort since the increase in
test LOC is greater than the increase in non-test LOC.

The most traumatized service after reaching 100\% coverage appears to be
Login.  This service first achieved total coverage on November 17.  Then
dropped to 99.2\% for the next two checkouts, missing 1 method.  It then
regained 100\% coverage until the end, when once again a method was not
invoked, dropping to 99.2\%.  The bouncing between two values suggests that
maintaining coverage requires a lot of work since the observable changes in
LOC is relatively small compared to the final size of the service.  More
activities were shown during the second visit to 100\%.  But the most
activities seem to have occurred between December 8 to 11, when total LOC
increased by 207 LOC, total methods increased by 17 methods, and one-line
methods increased by 16 methods.  This last change looks suspiciously like
a new class was created.  If so, then it would imply that code could have
been refactored.  Interestingly, these increases did not include any
observable increases in either the test LOC or test methods.

Poll's coverage reached approximately 96\% at best, but it's coverage
measurements also bounced around while the amount of noticeable activity was
minimal.  The coverage measurement alone hints that coverage is not easy to
maintain.

The behavior of Newsbulletin is somewhat similar to Textbooks -- no
observable changes in number of methods, but observable changes in LOC.

Techreports displayed unusual behavior throughout the evaluation period.
It showed a steady increase in test LOC before reaching 100\% coverage, and
then a steady decrease in test LOC after reaching 100\%.  As a whole,
maintenance behavior suggests that non-test and test code may have been
cleaned up.  Therefore, in this case, maintaining coverage once again did
not appear to require an unreasonable amount of effort.

Overall, the amount of effort needed to maintain extreme coverage is not
clear because different services produced different results.  From the
bouncing between values by Login and Poll that suggest perhaps the amount
of effort is unreasonable to the steady coverage by the remainder of the
services that suggest perhaps the amount of effort is reasonable, the one
clear conclusion is that more investigation is required.

\section{Questionnaires}
Two different types of questions appeared on both the Pre-Use Questionnaire
\ref{fig:questionnaire.pre} (``pre'') and Post-Use Questionnaire
\ref{fig:questionnaire.post} (``post''): close-ended questions and
open-ended questions.  The close-ended questions asked students to rate
their responses as one of the following: strongly disagree, disagree, no
opinion, agree, or strongly agree.  The rating for their current coverage
is: 0\%, 25\%, 50\%. 75\%, 100\%.  All answers are compared using ``pre''
responses versus ``post'' responses either per student or analyzed as a
whole.

\subsection{Unit Tests are Very Important}
This close-ended question was meant to discover how much value student's
placed on unit testing in general.  In ``pre'', most students answered
either agreed or strongly agreed with one student that had no opinion.  In
``post'', most students either agreed or strongly agreed.  Every answer
either remained the same or increased.  Clearly students agreed that unit
tests are important to correctly functioning software.

\subsection{Designing Unit Tests is Hard}
This close-ended question gauges the amount of effort students need to
implement useful unit tests.  In ``pre'', most students answered agreed,
but answers ranged from disagree to strongly agree.  In ``post'', most
students once again answered agreed, with answers ranging from disagree to
strongly agreed.  There was no particular direction in change between
questionnaires.  Some opinions increased in favor of difficulty, some
decreased in favor of simplicity, and others stayed the same.

From these answers alone, it is difficult to decide if changes in opinions
were influenced by the introduction to extreme coverage and JBlanket or
the complexity of the 8 services.

\subsection{My Unit Tests are Good}
This close-ended question directly measures the confidence students have in
their current unit testing abilities.  In ``pre'', answers ranged from
strongly disagree to agree, with agree being the more popular, though not
by much.  In ``post'', answers were slightly more positive, ranging from
disagree to strongly agree, with the consensus at agree.  There were some
opinions that decreased in confidence or remained the same, but the
confidence of most students increased.

While it is possible that the exposure to extreme coverage and JBlanket was
the cause of the increase in level of confidence, this may not be the case.
Instead, it is most likely the exposure to a method of measuring the
usefulness of unit tests has increased the student's confidence in their
unit test capabilities.

\subsection{JBlanket is helpful}
This close-ended ``post'' question directly measures the usefulness of
JBlanket.  The answers ranged from disagree to strongly agree, with
majority of the students agreeing that JBlanket was helpful, i.e., knowing
their extreme coverage was helpful.

\subsection{My Method-Level Coverage is ...}
This close-ended question is to verify the realistic-ness of students'
expectations of their coverage.  In ``post'' every student estimated their
coverage at 100\%.  However, in ``pre'', a surprising amount of 6 students
ranked their coverage as approximately 25\%, and only 1 student
approximating 100\%.  The average coverage approximation is about 46\%.
The actual average coverage from the first group of data is 38.7\%, proving
once again that people over-estimate their coverages when they are not
measuring it.

\subsection{Unit Test Problems}
This open-ended question should hint to whether extreme coverage succeeded
in addressing some of the problems with designing and implementing unit
tests.  The ``pre'' concerns came in a wide variety from not having enough
time to implement adequate tests to frustration from not being able to
implement tests correctly to the inconvenience of changing unit tests in
response to changes in source code.  For example, one student claimed that
outside inspiration was needed to provide motivation for implementing
tests.  Another student claimed that updating unit tests was ``very tedious
and cumbersome.''  Some of his classmates agreed in that unit tests were
fragile and required immediate attention whenever source code is changed.
(Because the services were always built as a whole, one failed test would
``break'' the build.)  Finally, 3 students commented on having problems
determining what test cases to write and how to ensure that they included
all combinations of valid and invalid values.

Having a goal of 100\% extreme coverage can provide a degree of motivation
towards writing test cases more than not having any direction except for
instructions that specify code needs to be tested. However, if a programmer
will not create test cases unless a severe punishment is threatened, then
applying extreme coverage will never be enough motivation.

With regards to updating unit tests, extreme coverage cannot help students
predict which methods will change and advise them on how to implement tests
that will not need modification the next time some code is changed.  If a
student finds this problem occurring regularly, it may signal that perhaps
the implementation of the test cases are too tightly coupled to the
implementation of the system being tested.  On the other hand, perhaps the
problem is not the test cases, but the design of the system.  The student
may need to re-think the current design before proceeding with testing so
that the system will be as robust and easy to test as possible.

As to the design of test cases, JBlanket creates reports of methods that
were not included in testing.  This list of untested methods can serve as
a starting point for students to decide what to test next.  By creating
these tests, other test cases that use specific boundary points or
conditions may be realized.

In ``post'', answers once again covered a variety of problems.  However,
the types of the problems shifted from personal issues with testing, like
finding the motivation to create them, to the act of designing,
implementing, and executing tests.  For example, students are no longer
clueless on what to test.  Instead, they now know about method-level
coverage and can move beyond it to ``trying to figure out how to thoroughly
test the system.''  At least one student realized that ``Reaching 100\%
method coverage does not mean that the software is fault free.  If you make
that assumption you are worse off then not having 100\%.''

Other problems included testing linked pages, lack of a testing tool for
Javascript, testing void or file I/O methods, and NullPointerExceptions.
The problem of updating tests was still an issue for one student.  Issues
related to getting test cases to execute correctly and reliably were still
present as well.

One problem experienced by at least one student concerned the execution of
tests.  Running only the service's tests produced different output than
running all of the tests in CREST together.  While the cause of this is
still unknown, the one difference between the student's development
environment and my devlopement environment was our versions of Tomcat.
After Tomcat was fixed to one specific version, this problem was not
brought up again until ``post''.  Therefore, it is unclear whether this
comment refers to the previous problem or the problem was still present at
the end of the semester.

\subsection{JBlanket Influenced My Unit Tests}
This open-ended question in ``post'' directly asks what types of changes
the students noticed with respect to their unit testing.  It can also be
interpreted as how knowledge of extreme coverage influenced unit testing.

A handful of students claimed that they wrote more unit tests.  A couple of
them said that they wrote less test code to increase their coverage by
invoking the ``bigger'' methods that called other ``smaller'' methods
instead of writing individual tests for the ``smaller'' methods.  One
student said that he was able to write tests quicker.  Interestingly,
another student wrote that using JBlanket improved his confidence in his
unit tests.

However, JBlanket and extreme coverage also influenced some undesirable
behavior.  Some students mentioned that they ended up focusing so much on
reaching 100\% coverage that they did not think too much about including
other types of testing, like conditionals or boundary conditions.

\subsection{JBlanket Improvements}
Since JBlanket is closely associated to extreme coverage, answers to this
open-ended question could be interpreted as suggestions to improve either
extreme coverage, JBlanket, or both.  For example, suggested improvements
on the speed, clarity of output, and inclusion of a method calling tree are
specific to JBlanket.  The speed of the earlier version of JBlanket that
was given to the ICS 414 class for evaluation was relatively slow.  As the
size and complexity of a system increased, the longer it would take to
measure coverage because a clean build of the system was always needed.
This problem has been addressed in the current version.  With respect to
the output, as mentioned previously, the summary printed to the console
rounded numbers to the nearest percent.  So it was deceiving to students
when coverage was listed at 100\%, but 1 method was not invoked.

Including a method calling tree is actually a planned future improvement to
the JBlanket system.  This tree would represent a map of all the methods in
a system and the methods they call.  After results from one run of JBlanket
are calculated, the nodes in the tree would either indicate if a method was
invoked or not, visually identifying the next possible test case that would
increase coverage.

A couple of students requested changes to both extreme coverage and
JBlanket.  For example, someone said that all methods, regardless of size,
should be included in coverage.  Another students claimed that other
granularities of coverage should also be included, like statement coverage
or branch coverage.

One comment included in the previous question that actually applies to this
question is the covering of empty methods, i.e., methods that do not
contain any lines of code in its body.  While these empty methods were
included in coverage, they were included with the assumption that
eventually the methods would contain code.  If not, then they would be
implemented as abstract methods.  However, the case where empty methods act
as place holders such that it either restricts a user from invoking itself,
like private constructors, or provide implementation of an abstract method
was not considered.  This issue will need to be further investigated.

\section{Summary}
Armed with the observations made from the evaluation results, the follow
things can be summarized about the three hypotheses:
\begin{itemize}
  \item The effort required to achieve and maintain extreme coverage is
        reasonable.

        Evidence did suggest that some effort was needed to achieve extreme
        coverage, and some effort was also needed to maintain extreme
        coverage.  Conflicting observations between the behaviors of
        different services suggestes that extreme coverage was relatively
        easy to maintain and that extreme coverage was relatively hard to
        maintain.  However, due to the limitations of the LOCC collected
        metrics, it is unclear exactly how much effort is needed.

  \item Knowledge of method-level coverage can help with system design and implementation.

        From the questionnaires, majority of the students agreed that using
        JBlanket to measure their coverage was helpful with writing useful
        unit tests.  It encouraged students to write tests more often and
        helped them by reducing the amount of test code implemented while
        exercising more methods.  However, some students claimed that
        concentration of achieving 100\% coverage drew attention away from
        designing and implementing other test cases that checked boundary
        values and conditionals.

  \item Extreme coverage is feasible in software development.

        Although the exact amount of effort required to achieve and
        maintain extreme coverage is unknown, students found that awareness
        of extreme coverage was helpful.  Unfortunately, it also drew
        attention away from exercising cases for boundary conditions, which
        is not good.  Therefore, extreme coverage may be feasible during
        unit testing if it is also accompanied by other means of ensuring
        software quality.  However, further research is required.
\end{itemize}

\section{Limitations}
After presenting the results of this study, several limitations on the
generality and applicability of this research need to be mentioned.

First, generalizing the results of this study to a bigger population is
impossible.  The hypotheses of this research were evaluated using a small
group of 13 undergraduate students.  The range of their ``experience'' is
unknown as they were only expected to have some working knowledge of Java,
Tomcat, Ant, etc.  Moreover, the students' behaviors and the behaviors of
the services they implemented are specific to their cases.  As can be seen
from the results, no one service acted exactly the same as another one.

In addition, the metrics obtained from both JBlanket and LOCC reflect
snapshots of the state of each service at periodic intervals for 5 weeks.
The data does not represent all of the activities that occurred during the
intervals.  For example, while the change in total LOC may appear to be 0,
the truth of the matter is that 20 non-test LOC could have been removed and
replaced by 20 other non-test LOC that are more effective at increasing
coverage.  Furthermore, it is impossible to detect big changes to a system
when only small amounts of changes are detected.  For example, a new class
could have been created to replace a group of related methods and the total
number of methods would not have changed.  Poll is an example of this
possibility -- while no visible changes could be detected with respect to
LOC or the number of methods, extreme coverage activity was still present.

Finally, all coverage measurements should be taken with a grain of salt.
Unfortunately, not all test cases passed 100\% of the time.  For example,
in Poll, the coverage decreased at one point during the checks.  However,
other services with test failures did not reflect such big differences in
their coverage.

\section{Additional Observations}
Aside from proving or disproving the hypotheses, another interesting
observation was made from the results of JBlanket and LOCC that were not
directly connected to the objectives of this study.

At the end of the evaluation, the percentage of test LOC and test methods
for each service on each date was calculated.  Then the difference in the
percentages between November 8 and December 11 was found.  The results are
shown in Figure \ref{fig:crest.results.change} and Table
\ref{table:crest.results.change}.

Interestingly, the service that had the most increase in coverage, Resume
(69.2\%), also had the most increase in percent of test LOC (31.0\%) and
percent of test methods (22.0\%).  Conversely, the service that had the
least increase in coverage, Textbooks (0.0\%), did not have the least
increase in percent of test LOC (3.3\%), but did have the least increase in
percent of test methods (1.5\%).  The service that had the least increase
in percent of test LOC (-0.1\%) was Tutor.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=1.0\textwidth]{figs/fig.crest.results.change.eps}
    \caption{Percent change in metrics of CREST services}
    \label{fig:crest.results.change}
  \end{center}
\end{figure}

\begin{table}[htbp]
  \begin{center}
    \caption{Percent change in metrics of CREST services}
    \includegraphics{figs/table.crest.results.change.eps}
    \label{table:crest.results.change}
    \end{center}
\end{table}

\chapter{Conclusions and Future Directions}

Unit testing is a useful software testing technique that can reduce the
cost of software development by revealing defects sooner and can increase
the likelihood of producing quality software.  In this research, a
variation of method-level coverage called ``extreme coverage'' was combined
with unit testing in an attempt to reduce the cost of implementing unit
tests but increase their quality.  Results showed that knowledge of extreme
coverage influenced the frequency of the creation of unit tests and helped
to increase the confidence in them.  However, knowledge of extreme coverage
also influenced the reduction of conditional and boundary value testing.

\section{Evaluation Improvements}
During the evaluation period with the ICS 414 students, several unforeseen
problems and issues arose.  Some of them were due to problems with JBlanket
and some were due to the development environment used by the students.  No
matter the cause of the problem, coverage measurements were not
recalculated because it was important to view the results as they were
viewed by the students.

\subsection{JBlanket}
A couple of minor bugs were found while the CREST web services were
developed.  The first bug was the missing translation of the boolean
primative type from the way it was represented in the Contant Pool.
Without this translation, methods that included a boolean argument type or
return type would not be recorded as an untested method eventhough it could
have been either a tested or one-line method.

The second bug was due to the incorrect implementation of the Singleton
design pattern.  It was needed for the execution of multiple unit tests due
to forking in Ant's junit task.  Furthermore, the synchronization keyword
was missing from those methods that access the same data structures.  These
two bugs led to results being overwritten.

The third bug was the speed with which JBlanket was run.  This was due in
part to a design flaw in JBlanket as well as the development process.  The
design flaw (that has been fixed in the current version) was not modifying
methods only once.  For example, suppose JBlanket is used to calculate the
coverage of a system Foo.  Inside Foo is a class Bar.  If Bar is never
changed, and thus never recompiled, the methods in Bar would continually be
modified.  Not only would the size of Bar.class steadily increase, but many
CPU cycles would be wasted on calls to store the method type signatures, of
which only the first attempt would trigger the actual storage.

\subsection{Development Environment}
However, the third bug related to JBlanket could also be attributed to
CREST's build process.  Each service had its own build.xml file to build
themselves.  However, the whole system needed to be compiled as a whole
first, packaged and copied over to Tomcat, and launched on Tomcat before a
service's test cases could be run.  Depending upon the speed of the
processor, the entire process would last anywhere from 5 minutes to 30
minutes.

A related problem occurred when forking was turned off in an attempt to
decrease the amount of time needed to run all test cases.  After forking
was no longer turned on, the {\tt Sys-} {\tt tem.out.println} statements
added by the students began appearing on the screen instead of mysteriously
disappearing into the output XML files.  However, {\tt System.err.println}
statements also started appearing on the screen that were never displayed
in the JUnit reports.  What some students found out was that they did not
fully comprehend the client-server model.

The third problem in the development process was the different versions of
Tomcat used by the students that ranged from 4.0.1 to 4.1.12.  JBlanket was
developed using version 4.0.1 and some of the students that enrolled in the
previous semester's ICS 413 class were also using the same version.  Other
students were using version 4.0.3, which is not as lenient with the Tag
Libraries.  Interestingly, this version problem was not discovered until
after the students tried using JBlanket.

\subsection{Data Collection Process}
The data collection and recording process was done manually.  The CREST
module was checked out once every three days.  I then ran the test cases of
each service and sent the output to different files.  Each output contained
a measurement from LOCC and JBlanket.  I would then scan the output files
and enter the data into one column of a Microsoft Excel spreadsheet.  Then
the source code and output files would be compressed with WinZip and
archived on a separate storage location.

While I tried to be extremely careful entering the data, checking the
numbers twice, 2 errors were found during the analysis phase of this
research.  One error used a '3' instead of a '2' to describe the number of
test LOC, skewing the results.  The second error was the result of
switching two numbers, '41' instead of '14', which did not affect the
results as much as the first error.

\section{JBlanket Improvments}
Changes to the JBlanket system design have been implemented continuously
since the evaluation period ended three months ago to improve its ability
to gather extreme coverage measurements.

\subsection{Modifying Methods}
For example, as previously mentioned, the unconditional modification of
methods was a problem.  This simple approach could cause dramatic increases
in .class files if a class was never recompiled during a 6 month lifespan
of a project.  Therefore, a simple check of whether the Constant Pool
already contained a reference to the {\tt storeMethodTypeSignature} method
turned the simple modification into a smart modification.  This fix also
reduced the amount of time needed to run JBlanket over a system since
systems would no longer need a clean copy before calculating coverage.

However, with every fix emerges another issue.  The next problem that will
be fix is removing the method modification.

\subsection{Modifying More Than .class Files}
Integrating JBlanket into Hackystat3, the third version of Hackystat,
proved to be impractical.  The architecture of Hackystat was split between
a kernel with basic functionality and plug-in extensions for each feature.
Therefore, the kernel and its extensions were developed separately, and
packaged into separate JAR files that were transferred between each other.
Because all testing is done through the kernel, and the extensions were
packaged in a JAR file, it was not reasonable to unjar the extensions,
modify them with JBlanket, and then re-jar them through Ant every time
coverage was measured.  Instead, a JAR utility was added to JBlanket to
perform this service.  Therefore, whenever extensions were included in
test, their method type signatures could also be recorded.

Furthermore, modifying all files in a JAR file was not enough.  It turns
out that some of the files previously belonged to other JAR files created
by outside sources, like JDOM, etc.  Not only did these files not contain
line numbers (which is what brought attention to this problem), but
including them in the coverage measurement was undesirable.  Therefore,
another improvement was implemented to specify which package prefixes would
be modified.

\section{Future Directions}
The results of this study are intended as a foundation for future studies
on the feasibility of including method-level coverage in the software
development process and the extent of its applicability.

\subsection{Gathering Informative Data Samples}
The amount of effort needed to achieve and maintain extreme coverage still
remains a mystery.  One of the problems experienced during the analysis
phase was the difficulty of arriving at any plausible conclusions due to
lack of sufficient data to support those conclusions.  The metrics
collected from both JBlanket and LOCC reflected only a snapshot of the
actual development processes.  For example, it was difficult to conclude
whether a change of 20 LOC in total LOC was because strictly 20 LOC was
added to the service or 300 LOC was added and 280 LOC was removed.

Perhaps shortening the intervals between gathering measurements will
increase the\linebreak chances of detecting the cause of previously subtle
changes.  For example, a good unit testing practice is to execute unit
tests after a major change is made in a system to ensure the change did not
break the system.  When the unit test is run, data could be collected for
the different metrics, including coverage (useful only when all tests
pass).  In this manner, changes will be recorded more often, and can be
compared with eachother, like total LOC and test LOC, test LOC and test
methods, etc.

One approach to obtaining this data is through Hackystat.  With sensors
that can be designed to gather specific metrics, data can be accumulated
every time a unit test is executed.  At this time, sensors for JUnit,
JBlanket, and CK (Chidamber-Kemermer) metrics exist.  All that remains is
the implementation of analyses that group these data together in a
meaningful manner so that possible behaviors can be detected.  By
automating the data gathering process, the amount of human error introduced
into the analysis is also reduced.

To address the issue of calculating actual effort, increasing the length of
the evaluation period would produce more data on the students' behaviors.
For example, the Textbooks service, one of the services that reached and
maintained 100\% coverage, reached 100\% before the evaluation period
began.  By extending the evaluation period to the point at which core
functionality and the first unit test was implemented, coverage can be
measured from the beginning of software implementation.

\subsection{Expanding Extreme Coverage Rules}
From the Poll service's behavior, the possibility that the rules applied by
extreme coverage is not complete.  A next step in this direction would
include recording which methods were not invoked during unit testing, and
then finding out if they can be categorized by type signatures.  For
example, in Hackystat3, testing of the JBlanket sensor package is almost
100\%.  The only method not covered is the {\tt execute} method, the method
invoked by Ant whenever the JBlanket sensor Ant task is invoked.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figs/fig.hackystat3.sensor.jblanket.eps}
  \caption{JBlanket results of JBlanket Sensor in Hackystat3}
  \label{fig:hackystat3.sensor.jblanket}
\end{figure}

\subsection{Comparison Against Similar Coverages}
As previously stated, method-level coverage is a coarser granularity
coverage measurement than statement coverage, branch coverage, etc.
However, statement coverage presents some behaviors similar to method-level
coverage.  For example, consider the following if-else statement:

\begin{alltt}
{\small{}
if (condition) \{
...
\}
else \{
...
\}
}
\end{alltt}
If within the bodies of the if-statement and else-statement lay method
calls, statement coverage will reach 100\% if both the bodies of the
if-statement and else-statement are tested.  Similarly, method-level
coverage will reach 100\% if both bodies are tested.

On the other hand, if there are no method calls within the bodies of the
if-else statement, statement coverage will not reach 100\% until both
bodies are tested while method-level coverage will.  However, this case can
also be measured by another type of coverage like branch coverage.
Therefore, it is not clear at this time how significant the difference is
between statement coverage and method-level coverage that cannot be
measured by another type of coverage.  Furthermore, the study conducted by
Elbaum et. al \cite{Elbaum:2002} suggests similiar pursuits, i.e.,
investigating the benefits of applying method-level coverage.

\subsection{Measuring Coverage From the Beginning}
In 1994 Horgan et. al presented two case studies in which they measured
dataflow coverage using ATAC (Automated Test Analysis for $C^3$) on C
programs \cite{Horgan:1994}.  ATAC measures block coverage, decision
coverage, c-use (computational expression), p-use (predicate), and all-uses
(either c-use or p-use).

One case study conducted in the large attempted to find a relationship
between coverage measurement and the total faults by inspecting one of
Bellcore's production software.  These results were inconclusive.

The other case study involved an autopilot system that was developed by 40
students divided into 15 teams at the University of Iowa and the
Rockwell/Collins Avionics Division.  Each team created their own system
that ranged from 900 to 4,000 lines of code.  They discovered the following
interesting outcomes:
\begin{itemize}
\item With every test execution, the quality of tests improved while
the range of coverages decreased.
\item The first test execution tested large amounts of the systems
with overall coverages increasing monotonically with respect to the
amount of test cases.  In each subsequent execution, the differences
between coverages decreased until eventually leveling out.
\item Reaching above 80 percent coverage was an important step toward
software quality and reliability.
\item There did not seem be a strong correlation between ``the total
faults detected in the program versions and their coverage measures
during various testing conditions'' \cite{Horgan:1994}.
\end{itemize}

Although the study conducted by Horgan et. al did not measure method-level
coverage, a study similar to this one could be conducted using extreme
coverage.  With results from this study, extreme coverage measurements were
found to not always reach 100\% even with the exclusion of one-line
methods.  Coverage measurements showed signs of both subtle and volatile
changes in measurements.  However, the quality of the test cases themselves
nor the quality of the resulting system was ever considered.  Nor could any
conclusions be made about the first execution with respect to coverage
since the test cases existed a couple of weeks before coverage measurements
were taken.

\section{Final Thoughts}
From the results of this research, extreme coverage was deemed to be useful
tool by undergraduates in a senior-level second semester Software
Engineering class.  However, as a couple of students realized, it is not
meant to be the only technique applied during unit testing, but to assist
with unit testing.  If the amount of effort needed by extreme coverage can
be determined, its feasibility can be determined.  If the quality of the
resulting software can also be determined, its usefullness and
applicability will also be determined.
