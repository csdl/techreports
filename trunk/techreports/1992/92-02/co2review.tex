\documentstyle [11pt, /home/3/johnson/Tex/definemargins,           
                /home/3/johnson/Tex/lmacros,                      
          /home/3/johnson/Tex/functiondoc]{article} 

\begin{document}

\begin {center} 
{\large\bf Collaborative Code Review}
\bigskip

Danu Tjahjono\\ 
Department of Information and Computer Science\\ 
University of Hawaii at Manoa\\ 
{\sf dat@uhics.ics.hawaii.edu}   
  
\medskip 
February 7, 1992
\end {center} 

\ls{1.2}

\section {Introduction}
 
The quality of a software product has been the
concern of software engineers since the invention of the first
computer.  Many software projects failed and needed to be scrapped
due to the poor quality of the code which in turn causes the
system to run unpredictably.

There are many factors that
determine the  quality of software.
One of them is the absence of errors
in the program code.  Unfortunately, the presence of errors in code 
is often unavoidable.  They  are caused by human
errors when the code is written. 

Over the past 20 years, people have invented 
various software engineering techniques  to detect
the presence of these errors, but no single one
has proven to be the best.  They generally solve one aspect
of the problems at the expense of another.
Some techniques are complementary to each other in
solving  common problems, but they are usually difficult to apply
or introduce unnecessary overhead.

One particular technique that was invented in 1976
and  has regained its popularity
recently is code review.  This technique has proven  to be
a cost-effective mechanism for detecting software errors.

Two the most prominent approaches to code review are code
 walkthrough and code inspection.
They are both review techniques applied to a software product, and
both are conducted by a team of software developers with the objective of
detecting the presence of errors in the program.  

In walkthrough, the author of the code walks the team through the
code in a face-to-face meeting. Team members are free to
interrupt at any moments and raise questions whenever they do not
understand the code or when they suspect any problems.  Through this
interaction  the group may uncover errors in the program.  
The discussion is usually not limited to solely
detecting errors but includes group education and
discussion on how to solve the problems.

Code inspection was first published by Fagan in 1976[].  It was
an attempt to create a more structure, systematic, 
productive and  efficient form of software review. The technique
 consists of six well-defined sequential phases: planning, overview, 
preparation,
inspection, rework, and follow-up, and five well-defined roles of
the participants: author, inspector, reader, scribe, and tester.
During inspection, participants (inspectors) use a checklist of
items to check against the product being reviewed.  Participants are
strongly discouraged to discuss design alternatives or solution
to the problems.  The data resulting from this inspection can
be used to improve future inspection processes and in general
to control the overall development process.


\section {Research problems}

Many studies have shown that inspection is  superior to
walkthrough in error detection efficiency and work productivity.  
However, in practice these
two techniques are equally popular and people often use them
interchangably by ignoring or adding some procedures as
they like.

The main differences between these two techniques can be
described as follows:

Code inspection is a formal review process. It has strict and 
well-defined rules and procedures which are enforced throughout the entire 
review process. Walkthrough, on the other hand, is more informal and 
flexible.  People often use walkthrough as a forum for learning
and communication as the author of the code presents his design
rationale to the group of technical experts and receives
comments and critiques in return.
Because of this flexibility, walkthrough is often less productive
in using meeting time. 
Participants often spend their time arguing about designs instead
of focusing their energy on finding software defects.  Code inspection
does not have this problem since participants are not allowed to discuss
any issues during the meeting except those related to defects finding.
However, the strength of code inspection is also its weaknesses.
People often need to exchange ideas or suggestions to resolve
problems found during the inspection. Furthermore, since participants
have little chance to go through the code with the author 
in order to understand the code (i.e., only during overview [Fagan, 1976])
they need to have relatively the same level of understanding about the code
being reviewed as the author in order to participate effectively.

Unfortunately, both review techniques in current practice are lacking
the following aspects:
\begin {itemize}
\item Computational supports. Only a
handful of review systems provide computer based support
supports[Brothers et al., 1990].  
Many reviews in  practice are still done manually where 
reviewers flip through a stack paper printout of the code back
and forth in order to find errors and to understand the logic
of a particular section of the code.

\item Groupware supports. A code review is fundamentally a 
group activity.  Each member of the group contributes a portion
of his knowledge and energy to the accomplishment of a common goal.  
Current groupware supports are
either missing  or limited to face-to-face meeting.

\item Effective use of knowledge acquisition.  As the group
completes the entire review sessions, they have accumulated 
additional knowledge about the semantic of the code (what 
a specific section of the code does, why and how),
coding styles, error prone coding style, error
types, how the errors are introduced into the code and how to avoid them, 
and other knowledge related to the  process review itself (better 
classification of error types, effective use of the tools, etc.)
This knowledge will boost work productivity of the participants
(as reviewers or producers) in the future.

In traditional code review this large amount of knowledge 
often goes unrecorded, or
resides in the memory of each individual team member.
This limits the effectiveness of future reviews, especially
if the review group changes.

\item Support for reinspection.
There are many reasons why reinspection is necessary.
Many studies have shown that producers often introduce new
errors when fixing errors in the code.  Thus, it is necessary
to reinspect the changes in the code.
For management, reinspection gives better assurance of the
reliability  and the quality of the code. And since  participants
found the errors in the first place, they can
effectively reinspect these errors. Furthermore
this may serve  educational purposes as the participants learn from
the producer how to fix the errors.

Traditional review process pays little attention to reinspection. 
This responsibility is often delegated to the moderator to avoid
high cost of reinspection. In other words, reinspection is 
often perceived as a redundant review  where the entire process
is reiterated.

\item Support for evolutionary review. 
It is a well-recognized fact in the computer field that software does
evolve. The same piece of code is repeatedly modified over time
until the desired outcome is achieved.

Traditional code review on this type of code will result in 
high cost overhead as exactly the same piece of the code is
repeatedly reviewed and the logic of relatively similar pieces
of the code is repeatedly untangled.

\end {itemize}

\section {Research proposal}

As described above
both code inspection and walkthrough have some benefits and
shortcomings depending on the stated
objectives of the review.  If
the objective is solely to find errors then inspection is
better compared to walkthrough as shown empirically by Fagan [Fagan, 1976].

In practice, however, the objectives of code review are also
education and communication for the project team.  
In fact, the overview phase of the formal
inspection [Fagan, 1976], can be considered
as a limited form of walkthrough.

For that reason, our process of conducting
code review  will incorporate both
the formal inspection and the semi-formal walkthrough
techniques. We will be enforcing inspection entry criteria and
inspection checklist as in formal inspection,
and educate participants
as in walkthrough.  Our objective is to apply code review as both
an error detection mechanism and as a forum for learning and communication.
For this latter objective, the review process will be structured in such a way
as to maximize the efficient use of meeting time.

Group interaction will take place both in a regular face-to-face
meeting and a non face-to-face or computer mediated meeting in which
participants can work at different time, and in different places,
but access a common database.
We claim that the latter meeting mode  will raise
error detection effectiveness and productivity as individual 
participant can keep 
his own pace of reviewing the code.  

Many studies have also shown 
that there is no differences in code inspection result between 3, 4
or 5 persons inspection team 
[Bisant and Lyle, 1989] [Myers, 1978][Martin and Tsai, 1990]. 
In fact,  Martin and Tsai showed that
integrating results from N numbers of small groups is more effective than
having a single large group with N participants.
Our computer mediated meeting  is  to have a participant working on the 
review individually, and then later the result is integrated and 
consolidated with other participants.

The basic rule during our non face-to-face review
meeting is similar to the code inspection:
participants are allowed to
raise questions, issues, seek clarification, but 
prohibited to discuss
design alternatives or solution to the problems.

Our review meeting will also accumulate 
knowledge about code comprehension in written form as the group members
electronically interact with each others untangling the logic of the code. 

And finally, we will be providing a set of computational services to  support
each of the review processes.


\noindent The review process will proceed as follows:

\noindent {\bf Phase 1: Planning}

At a first step, the producer and his or her manager will decide that
the product is ready for review. Then, they identify review team,
select a moderator who will lead the entire review process, and
invite the team to the planning session.

During the session, the manager and the moderator
will explain the objectives of the review and the review process
in general and other administrative details. They then assign
different review roles to different participants.
Finally, the team plans and schedules the next meetings time and place
(if it is a regular face-to-face meeting)

We define four inspection roles: the moderator (who leads the review),
the author or producer (who writes the code), reviewers (who
inspect the code for coding errors), and the recorder or scribe
(who records the significant review results).
We do not see the need for separate reader's role other than
the author himself as in formal code inspection, because, 
we have no face-to-face meeting where the group does active
errors hunting.


\noindent {\bf Phase 2: Preparation (Producer and Moderator)}

The next phase after planning is preparation for the computer mediated
inspection meeting.  This task 
is accomplished by the producer and the moderator.

The main objectives of this phase are to validate entry criteria and
to  generate the  inspection package.
Unlike traditional code inspection, our review system will rely strongly
on computer supports. Reviewers will use workstations,
reviewing the code and annotating it (reporting defects, raising issues,
posting questions and answers). 
This new mode of communication will require significant amount of preparation
for both the author and the moderator.
This preparation is not only critical for the success of review process
but also gives the author a last minute chance to perform 
inspection on his own code.

\noindent This phase proceeds as follows:

First, the moderator will deliver the entry criteria to the author.
Two well-recognized entry criteria are clean compilation and design 
specification consistency [Fagan, 1976][Humphrey, 1990].
The author will copy the code from his workspace to the common database
and perform recompilation. Related documents including the design
specification will be copied as well.

Next, the author will read through the design specification and
create cross-referencing between items
in the specification and their  implementation in the code.
In the future, we may provide an automatic tool to handle this task.

\noindent There are many benefits to having  cross-referencing between design
specification and the code:
\begin {itemize}
\item The inspection entry criteria requires that the design specification
and the code is consistent.
\item The author is compelled to inspect his code against the design
in a more careful way, potentially revealing new errors.
\item Reviewers can easily refer to the specification while reviewing
code.
\item The impact of future changes in the specification or in the code
can easily be traced and analyzed.
\end {itemize}

Next, the author will generate the inspection package which includes:
\begin {enumerate}
\item Copying the code to the common database (see above discussion).
\item Printing out the  hardcopy of the code
to be distributed to the reviewers later.  This paper listing of the code
is given  for supplementary purposes and may be removed in the future as
reviewers become more familiar with the system.
All comments, issues, concerns and the results of the review must be entered
into the common database.
\item Creating the {\it travel-map} of the code.  
A travel-map is a guide to traverse the code in some meaningful manners.

In traditional paper based code inspection, this map is simply the
ordering of the pages of source listing according to the author's
rationale.
In our system, we may have various different types of travel-maps
corresponding to various different ways to browse and read the code.  

\noindent Some examples of travel-map include:
\begin {itemize}
\item Semantic map. This map  groups
or links semantically similar program modules.  It is used
to facilitate better comprehension of the modules.
\item Compiled map. This map groups program files
according to the order the compilation order.
\item Revision map. This map links the old part of the code to the new,
modified one.
\end {itemize}

The links in these maps are bidirectional, we can go back and forth
from one side of the link to the other.  Furthermore, the map may
include comments or annotation.  For example, in the semantic map, the author
may annotate the link  with an explanation of his rationale.
In the revision map, the link will be 
annotated with the result from the code inspection (accurately
describes the reason for the changes)

In summary, when conducting a review, reviewers are encouraged to follow
this map to increase their work productivities.

\item Preparing code presentation for the next phase.  This preparation
may involve creating yet another travel-map of the code.
The author may choose to use one of travel maps prepared
for the reviewers above.
\end {enumerate}

The moderator's tasks include:
\begin {enumerate}
\item Assist the author in fulfilling the entry criteria and preparing
the inspection package.
\item Prepare the inspection checklist and make it available online
(common database).
\item Ensure the author has included all related documentations in
the common database.
\item If the review is a follow-up review,
brings up related/historical information which may be useful for the 
reviewers, 
such as statistics of the number of errors found in certain modules
during previous review, error types, etc.
\end {enumerate}


\noindent {\bf Phase 3: Code Presentation}

This phase is a regular face-to-face meeting  held  after the 
author completed his preparation. All participants must attend the meeting.
The purpose of this meeting is to prepare the reviewers in
accomplishing  their tasks (i.e, finding coding errors).

First of all, the moderator will explain the details of the
review process, outline the reviewers' task, and then
show them  how to use various computational
supports, such as annotating the code,
browsing, recording and classifying concerns or issues, etc.
Each participant will sit down in front of a workstation experiencing
the tools while the moderator is explaining how to use them.
A  big viewing screen will be used to 
project the display of the moderator's workstation.

When the moderator has completed his presentation and demonstration,
the producer will take control the meeting.
Using his prepared travel-map, he is presenting the code to
the group.  For example, he will show the global structure of
the code as given in the map, its relevant design documentations,
and other essential information to understand the code. 
All of these information are available in the common database and 
accessible to the participants during their reviews.
Thus, the producer is basically running through his
prepared travel-map and explaining what information can be found.
He will not, however, go into the detail of the code;
only the structure of the code is presented.

Again, during
the presentation the participants follow the author traveling
through the map on their workstations.  The participants do not
only learn the materials but also get hands-on experience in
using the tools.

Participants are allowed to interrupt the producer any time during
his presentation, and raised questions to seek clarifications,
or questions to improve their understanding of the global structure
of the code.
Any other type of questions, for example,
details implementation are strongly prohibited at this time.
These questions should be written down  by the reviewers.

The producer will also take note on any concerns raised by the
participants, especially the information which the group
asks to be included into  the database or into the travel map.
This note will become the author's to-do list right after the
meeting.

\noindent {\bf Phase 4: Computer mediated review}

This is the core of our code review process.  Each participant
will work on his own workstation reviewing the code which resides
on the common database.

Unlike formal code inspection by Fagan [Fagan, 1976], we do not have 
a separate phase for individual preparation.  Or more precisely, individual
preparation is integrated within the review process itself.
We claim  that preparation (understanding the logic of the code)
is inseparable process from  review.  
Reviewers need to understand the code in order to effectively
inspect the code, and in the process of this understanding 
they discover errors. 

\noindent We also claim that there are four types of knowledge used during code 
review:
\begin {enumerate}
\item Knowledge about the code and its behavior (what the
code does and how).
\item Knowledge about the programmer's intention (why the
code is written the way it is).
\item Knowledge about the programming language 
(the data structure, typical algorithm, etc).
\item Knowledge about the environment where the code resides 
(operating system, shell, etc).
\item Knowledge about the group culture --the stylistic conventions
and assumed knowledge of readers and maintainers of the code.
\end {enumerate}

This knowledge will determine the error
detection effectiveness (the number of errors found per unit time), 
and the productivity of the work in general.
Reviewers having only one type of knowledge are not
so effective and productive in detecting errors than those who
have all of them.  However, mixing reviewers with
 different knowledge expertise may improve the overall
effectiveness.  Our conjecture is that a specific type of knowledge
tends to detect a specific type of errors.

A goal of our collaborative review system is to increase the participants
knowledge in each of these areas, and thus
increase their effectiveness in detecting errors.
For example, when a reviewer first joins the team, he will usually 
have the third and the fourth type of knowledge, but will lack  the first
and  second.  After going through the review, he
will gain not only the first and the second, but also
improve his third and the fourth knowledge. 

The first and the second type of knowledge are often referred 
as program understanding knowledge.
Letovsky[Soloway and Iyengar Ed., 1986] showed that 
the following cognitive processes take
place when one is reading and understanding a program:
\begin {enumerate}
\item The subject is reading along, and encounters some fact which prompts 
him to ask a question.
\item He conjectures answers to his question.
\item He attempts to find an answer, by searching through the code and/or
documentation for relevant information, or occasionally, by doing 
detailed reasoning about the program.
\item At some point he finds something which allows him to draw a
conclusion.  He then resumes his previous previous activity.
\end {enumerate}

In a collaborative environment, the third step above may contain
collaborative effort; the subject can ask questions to
his peers or even the author himself. We will show later that
our review process is equipped with various tools to support each
of the above cognitive processes.

The conclusion which the subject draws at the last step can
be either understand or failed
to understand the program.  In the latter case, we say a {\it suspected
error} is found.  However, as implied from the previous
paragraphs, not all errors are found this way. Other types of errors
such as those that violate certain programming language constructs are
more straight forward to find.

\noindent The details of the review process in this phase are as follows:
\begin {enumerate}
\item An individual reviewer starts his review by reading the code
and following the travel-map prepared by the author.
\item When suspicious errors or issues are found, he annotates
the code, describes the issues in detail and
classifies  them according to the prepared errors/issues list 
(we use the term {\it issues} to refer to the suspicious errors found by
individual reviewer).
This individual annotation is not accessible by other reviewers.

\item When finished with one module (or function), he marks the module 
{\it reviewed} and then moves on to the next module  in the
order given in the travel-map.
The system will automatically keep track which module or
function has or has not been reviewed.  He can also leave the module
unmarked, or unmark a marked module. 

\item At any time during the review, reviewers may collaborate with 
each other
(or even with the author) to discuss  problems and  the logic of
the code  by posting questions to the group.  All participants will be
notified by the system  and are encouraged to respond.

\item Questions and responses should be pursued only to the
extend of revealing ambiguity, seeking clarification or understanding
the code.  Participants are not allowed to vote on 
any conclusions at this time, or to conjecture alternative designs.  
This restriction 
is necessary so as not to drag participants into less productive 
discussions. The moderator will monitor and ensure that all participants
abide to this rule. In fact, our system will provide a structure 
to the question-answer session to avoid less productive discussion,
and at the same time be able to capture the essence of the conversation.
Individual reviewer should keep
his own conclusions and suggestions private.  Later in the next
phases the reviewers will be given the chance to vote on issues and
to voice their suggestions and concerns.

\item While reviewers perform the reviews, the author will
answer any questions and 
consolidate and analyze the  issues raised so far.
These issues will be presented and discussed in the next meeting
following this phase.  To prepare for the presentation,
the author will create yet another travel-map which links together
the consolidated issues.
\item The author also gives the reviewers a chance to look
at the consolidated issues  prior to the next meeting. 
\end {enumerate}

\noindent The following tools or  computational services will be available
during this phase of the review process:
\begin {itemize}
\item Browser.  This tool will enable participants to traverse
the code which has links information in it.  Travel-map is
an example of link information.

\item Cross-referencing.
Two types of cross-referencing tools are available: manual and automatic.
The manual cross-referencing tool is to create 
a link between two entities with similar semantic manually.
For example, reviewers may create
a link between one item in the code to the other for explaining
or reporting issues.

The automatic cross-referencing tool is to used to look up 
information about variables,
constants and function declarations and their relationships 
in the code.
Some useful relationships include calling hierarchy, and
uses and used hierarchy.  
Reviewers just move his cursor to the specified variable or function,
invoke the tool, and  the requested information
will be displayed on the screen.

\item Source level debugger. This tool is used to understand
the behavior of the code.

\item Knowledge understanding tool.  This tool is used
during question-answer session among participants in an attempt to
understand specific implementation of the code.
This tool will not only assist the participants in understanding the code,
but also guide the discussion (by providing a structure to the coversation).
It also captures, records and accumulates essential information
about the code for future reuse.

This  tool will be modeled after the gIBIS design rationale representation
and the cognitive
model of program understanding by Letovsky above.
Specifically, our model will use three entities:
question, conjecture and conclusion.
Participants first post questions about some facts, then
conjectures, followed by more questions
in response to these conjectures,
and finally  they confirm  a specific conjecture with a conclusion.

The questions will be of the following types:[Soloway and Iyengar, 1986]
\begin {enumerate}
\item Why questions: Questions which ask about the purpose of an action
or design choice.
\item How questions: Questions which ask about the way some goal of
the program is accomplished.
\item What questions: Questions which ask what a variable or function does.
\item Whether questions: Questions which ask whether the code behaves
in a certain way.
\item Discrepancy questions: questions which reflect confusion over
a perceived inconsistency.
\end {enumerate}

In fact, our tool can answer some of the questions above without
collaboration from the participants. For example,
some what questions can be easily satisfied by our automatic
cross-referencing tool; some why questions can be answered partly
by looking up relationships among variables or functions.
The how, whether and discrepancy questions are harder
to answer without deep understanding of the code. 

In any cases, when a reviewer fails to understand the logic of the code, 
he posts one or more the above typed questions.
The author or other participants then post conjectures
with the same types in response to the questions. 
The inquirer may also post questions and the conjectures
to the questions at the same time.  Finally, the group will
conclude on one most accepted conjecture by the participants.

\end {itemize}

\noindent {\bf Phase 5: Issues confirmation.}

This meeting will be held in a regular face-to-face meeting
and will be attended by all participants.
The purpose of the meeting is to discuss the issues raised during
the previous phase and to confirm the validity of the issues
as either errors or non errors.

Similar to the preparation phase, there will be a big viewing
screen and a workstation  for  each of the participants.
The big screen will project the displays of both the author's
and  the scribe's workstations.

The author will basically go through his prepared presentations
issue by issue, explaining whether it is a valid error or not.  
The scribe will record any decisions pertinent to the issue
which include error confirmation, error classification, and
recommended action.
The author will not be able to move to the next
issue until the scribe finishes recording the current one.  

While following the discussion, participants may examine his
own prepared list of questions on his workstation and 
interrupt the discussion any time.  He can also request that the big
screen is switched temporarily to his workstation to better
present his concerns.

As in the computer mediated meeting,
any discussions will be pursued only to the 
extend of confirming or rejecting the errors.  When the group
cannot reach consensus in deciding one issue, or a lot of
time will be needed to address the nature of the error,
the scribe with the moderator`s consent will mark the issue as
{\it postponed}, and the author will
continue with the next issue.  This decision is necessary
to avoid less productive discussions.

Rejecting an issue implies no action is elicited from the author.
Confirming an error, on the other hand, implies an action to fix
the error will be provided by the group to the author.

For simple errors such as misspelled names and comments, the action
is straight forward and need not be explicitly stated.
For relatively subtle issues such as missing condition statements,
the action will be recorded explicitly.
For very subtle errors, such as improper data structure or algorithm,
the action should not be pursued at this time, and the discussion will 
be postponed (the issue is marked postponed).

Based on our experience,
this approach should save  significant amounts of review time
since related issues tend to group  as we move along and the
action given to the previous issue may become obselete and
need to be refined as well.

At the conclusion the scribe will check for postponed issues
and reopen discussion for these issues.  We are hoping
that by this time (after covering all the cases), their solutions
have become trivial.
Any yet unresolved issues  will be passed to the next phase.


\noindent {\bf Phase 6: Discuss solutions}

This phase is optional (only when unresolved issues remain)
and very flexible. It can be held right
after the phase 5 above with a short break, or 
is scheduled for later time.

Not all participants
are required to attend this meeting (the author is mandatory to
attend). The moderator will formally
invite those participants with strong technical background or 
those who show interest to the resolution of the issues.

This meeting will take place in a rather informal 
face-to-face discussion.  
Participants are expected to prepare prior to the meeting.  They should 
come up with their solutions and present them  to the group.  
Any results from this meeting will be recorded by the scribe
similar to the previous meeting.

An alternative approach to this phase is 
to have the meeting conducted in a non
face-to-face or computer mediated meeting similar to phase 4,
where the entire session
can be effectively captured (eg., using the gIBIS).


\noindent {\bf Phase 7: Rework.}

Following the review, the author fixes the identified errors.
Since these errors are well recorded with specific recommended
action and also cross-referenced to
the line number or the section in the code, the author will
have no difficulty fixing them.


\noindent {\bf Phase 8: Follow-up review}

Following the rework, the author notifies the moderator for
follow-up review.  The moderator then instructs the author
to carry out the preparation phase (phase 2).

Next, the moderator will examine the revision-map prepared by
the author.  If the modification seems to be extensive, all
participants from the previous review session will be
invited to participate.  Otherwise, only a small number of
participants will be included.

In any cases, the follow-up review will proceed as before except
it is much faster and productive as reviewers can take advantage
of revision-map and other historical relevant information.


\end {document}
