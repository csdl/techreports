
From johnson@hawaii.edu Thu Apr 23 11:11:37 1998
Date: Thu, 23 Apr 1998 11:02:51 -1000
From: johnson@hawaii.edu
To: csdl@ics.Hawaii.Edu
Subject: forwarded message from William Hayes


Here is a much more interesting challenge to our methodology.
Anne, what do you think?

Philip


  [ Part 2: "Included Message" ]

Date: Thu, 23 Apr 1998 09:52:26 -1000
From: William Hayes <wh@sei.cmu.edu>
To: johnson@hawaii.edu
Cc: James Over <jwo@sei.cmu.edu>, watts@sei.cmu.edu
Subject: Re: Investigating Data Quality Issues in the PSP 

Gentlemen,

I would like to provide a small contribution to this discussion. I have been
silent thus far because my workload simply will not allow any protracted
involvement on my part, and there are MANY good topics for discussion
regarding data collection and data quality.

I wish to be clear from the start: I strenuously object to the conclusion "Our
results raise questions about the accuracy of published data on the PSP..." as
stated in the paper _Investigating Data Quality Problems in the PSP_ by Disney
and Johnson.

One of the most seemingly credible pieces of evidence sited for this claim
revolves around Figure 6, on page 7 of the paper. In the table provided, the
authors list values of "Original" and "Corrected" CPI. The inference for the
reader is that the "Corrected" values of CPI are in some way more accurate
than the original values. I see no evidence to support this inference, and, in
fact, my reading of the error correction rules listed on page 4 of the paper
suggests that there may be a systematic bias present in the "Corrected" CPI
values. Rule 5 states that missing values of estimated time in phase are to be
replaced with the actual value recorded for that phase.

While many ways of dealing with missing data are available, good judgement
dictates avoiding options that demonstrably distort the data in favor of the
research hypothesis. Replacing a missing estimate with the actual time will
change the interpretation of the CPI, by definition. This numerical artifact
is hardly worthy of attention in a paper on data quality. Furthermore, the
subtle implication that our work is based on analyses conducted without
concern for this type of missing data is offensive to me as a professional
research methodologist.

With respect to Figure 7 (showing original and corrected yields), I suspect
that there are differences between the teaching methods that may account for
at least some of the discrepancy implied by these results.

SEI-trained instructors of PSP are trained not to accept data collection forms
that contain blank fields. This is a VERY fundamental issue for the PSP. Data
collection and evaluation are at the heart of the training course as we are
all well aware. Furthermore, instructors use carefully designed checklists to
review each assignment for issues like the ones listed in Rule 3 on page 4 of
the paper. Fallibility of human perception and judgement aside, the process
that leads to the data we analyze and publish is nowhere near as sloppy as a
casual reading of this paper would imply.

Prior to implementing each programming assignment, PSP students are to review
their high level design and project plan (including all estimates and related
data) with the instructor. This provides an opportunity to validate the data
used in planning each assignment. Following the completion of each assignment,
the instructor employs a well-tested checklist to verify the data submitted to
him/her. This grading process changes during the training as new data items
are added, and by PSP3, the checklist is 4.5 pages long and requires 20-30
minutes to complete for each assignment package. These checklists also include
verifications that span multiple assignments, where transfer of data from one
project to another may result in errors.

Having said all of the above, I agree that the issue of what you term
"collection-stage" data quality cannot be rigorously tested in most research
data published today. I would point out, however, that such a risk exists for
all data collection involving human beings. Evaluation of PSP using data which
are 'external' to the PSP does not mitigate this risk, as these data are also
potentially subject to this type of distorting effect. In my experience
working with engineers using PSP in their professional work, I have observed
that their motivation for collecting the data derives from enlightened
self-interest. These engineers expend energy to get accurate and valid data
because they expect to make good decisions with them. On the other hand, data
reflecting organizational performance has a larger risk of distortion for the
sake of social acceptability and public perception. For these reasons, my work
requires a close interaction with members of the organizations who provide
data to us. This attention to detail and data quality requires more time and
effort than many R&D organizations are willing to invest.

These are challenging issues, and careful attention is required to do
competent research in our field. As an employee of the SEI, I constantly face
perceptions that our results are advertising rather than research. I fear that
this paper will bolster that perception, rather than fostering an
understanding of the important methodological issues on which we are focused.

Respectfully,

Will Hayes
Software Engineering Measurement and Analysis Initiative
Software Engineering Institute

