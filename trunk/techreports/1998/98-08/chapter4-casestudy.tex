\chapter{Case Study}

To investigate the issue of data quality in the PSP, Dr. Johnson taught a
modified version of the PSP curriculum over the course of a semester in
1996.  Ten students participated in this course, completing nine projects
each. (A final project for one student had so many incomplete values that I
excluded it from the study.) At the end of the course I collected these
projects and then examined them to uncover any errors the students may have
made in filling out the PSP forms.  This chapter will cover the case study
in more detail, describing how Dr. Johnson modified the standard PSP
curriculum, the method I used to record errors, the rules used to correct
the original data, and the comparison of the original and corrected data.


\section{\label{section:ModifiedPSP}The Modified PSP Curriculum}

In order to address data quality problems discovered in previous semesters,
Dr. Johnson made some changes to the standard PSP curriculum before
teaching the course:

{\bf Increased process repetition.} The standard PSP curriculum assigns ten
programs during the course, in addition to several midterm and final
reports.  Over the course of these ten programs, students practice seven
different PSP processes, which means that the development process used by
the students changes for seven out of ten programs.  After his initial
experience teaching the PSP, Dr. Johnson came to believe that the high
overhead of this almost constant ``process acquisition'' led to data errors
and had a significant impact upon the overall data values.  To ameliorate
this problem, the modified PSP curriculum included only five PSP processes,
enabling students to practice most processes at least twice before moving
on.  The modified curriculum also included only nine programs instead of
ten, providing additional time in each assignment for data collection and
analysis.

{\bf Increased process description.} In his initial experience teaching the
PSP, Dr. Johnson also found that students had a great deal of trouble
learning to do size and time estimation correctly.  For example, PSP time
estimation requires choosing between three alternative methods for
estimation depending upon the types of correlations that exist in the
historical data from prior programs.  To help resolve this and other
similar problems, he added four additional worksheets to the standard PSP
form set: (1) a Time Estimating Worksheet to provide a guide through the
various methods of time estimation; (2) a Conceptual Design Worksheet to
help in developing class names, method names, method parameters, and method
return values; (3) an Object Size Category Worksheet to help in size
estimation; and (4) a Size Estimating Template Appendix to provide a place
to record planned and actual size for prior projects (this proved
invaluable to me later on - it allowed me to determine whether errors in
size and time estimation occurred during data transfer between projects or
in the actual calculations).

{\bf Technical reviews.} At the completion of each project, students
divided into pairs and carried out an in-class technical review of each
other's work.  A two page checklist facilitated this process.  It included 
such questions as, ``Did the author follow the PSP Development Phases
correctly?'' and, ``Is the Projected LOC calculated correctly?''  A second
``Technical Review Defect Recording Log'' form included columns for number, 
document, severity, location, and description.  The review took about 60
minutes to complete.  The students then submitted the technical review
forms with the completed projects.  Dr. Johnson reviewed the projects a
second time for grading purposes, using the Technical Review Defect
Recording Log to record any additional errors.

{\bf Tool support.}  Finally, Dr. Johnson provided four spreadsheets to
support records of planned and actual data values.  In addition, students
had access to tools to count lines of code for Java programs and to compare 
two versions of a Java program and report lines of code added and deleted.
In the textbook PSP curriculum, a LOC counting tool is assigned as one of
the programs.  The modified curriculum included completely new program
assignments more suited to the Java language.

Dr. Johnson also emphasized data quality during the course.  For example,
he augmented the lecture notes in the Instructor's Guide with fully worked
out examples of the PSP process data for a fictitious student to show how
data is collected and analyzed for each assignment, and more importantly,
how data is accumulated over the course of the semester.  He dedicated
lectures to collection and analysis of data periodically throughout the
semester. He showed the class aggregate statistics on the entire class'
performance throughout the semester.  He met with students individually and
in groups throughout the semester to go over their assignments and PSP data
while they were in the midst of planning, design, code, compile, test,
and/or postmortem; but prior to project turn-in.  He uncovered and removed
many, many PSP data errors through these meetings which are not counted in
my results.  He did technical reviews of every assignment's PSP data.  He
habitually presented problems faced by one student to the entire class,
either by e-mail or in person, in order that students could profit from each
other's mistakes (of course, removing all identifying information from
the material to avoid public embarrassment).

\section{Data Entry}

At the end of the course, Dr. Johnson gave me the PSP forms for all 90
projects, as well as all the technical review checklists and the instructor
grading sheets for each project.  At that time I designed and implemented a
database system to store all of the PSP data from this course and to
subject it to further data quality analysis (see Chapter 5). Then I entered
each project into the PSP tool; recording primary data such as time,
defects, and program size.  I then compared the values calculated by the
tool with the corresponding values calculated by the students. Each time an
error was found, I reviewed the record sheets from the technical review
process and the grading sheets used by the instructor to determine which
reviewer first identified the error.

I did not check the Task Planning Template, the Schedule Planning Template, 
or the Object Size Category Worksheet for errors.

After entering the first few projects, it became harder and harder to find
errors by simply comparing a value calculated by the student with one
calculated by the PSP tool.  This was because it was possible for students
to do calculations correctly, but with incorrect data from prior projects.
This produced values that differed from the ones generated by the PSP tool,
but which could not be counted as errors (see section \ref{sec:error}).
This meant that I often had to hand-calculate certain values such as {\it
  Total LOC (T), to date} using the paper PSP forms for the current and
previous project.  For the more complicated calculations, such as linear
regression for time estimation, I modified the PSP tool to allow me to
update the inputs.  The values defaulted to the database values stored in
the tool, and then I overrode ones that varied from the ones actually used
by the student.

After getting about halfway through project 4, I also realized that some of
the primary data was showing inconsistencies.  For example, there were
overlapping time log entries and defect logs showing more fix time per
phase than the time log showed total for the phase.  Since it would be
interesting to see any possible clues about a collection stage problem, I
went back to the beginning and examined all the primary data for various
problems.  I recorded errors found under the general type of ``Impossible
Values'', and continued to do consistency checks as I entered the rest of
the projects.

Whenever I found an error, I recorded the following information in
another database:

\begin{itemize}
\item A code identifying the student responsible for the error.
\item A code identifying the person who first identified the error;
the instructor, another student, or myself.
\item The number of the assignment in which the error occurred.
\item The programming language used for the assignment.  (As it turned out, 
      all projects were done using Java.)
\item The PSP process that the student was using when the error was
      injected.
\item The PSP phase in which the student was working when the error was
      injected.
\item A code identifying the general error type.  For example: 
      \begin{tabbing}
      BF  \quad\= Blank Field\quad\\ 
      CI  \>Calculation done incorrectly
      \end{tabbing}
\item A code identifying the specific error. For example:
      \begin{tabbing}
      BLC \quad\= Base LOC, actual: incorrect\quad\\ 
      BC  \>Base LOC, plan:   different from Size Estimating Template
      \end{tabbing}
\item A code identifying the error severity. For example: 
      \begin{tabbing}
      0   \quad\= Error has no impact on PSP data\quad\\
      1   \>Results in a single bad value, single form
      \end{tabbing}
\item The age of the error.  This represents the number of assignments 
      since the introduction of the data field or PSP operation in which the
      error occurred.
\item The incorrect value (where applicable).
\item The value that should have been used (where applicable).
\end{itemize} 

\subsection{Error Counting Method} \label{sec:error}
Upon finding an incorrect value, I recorded an error for the field, but
from that point on I treated the incorrect data value as correct.  For
example, consider the first entry in the sample Time Recording Log shown in
Figure \ref{fig:timelog}.

\begin{figure} [h]
\begin{center} 
\begin{tabular} {|l|l|l|r|r|l|l|}\hline 
\multicolumn{7}{|c|}{\bf Time Recording Log}\\ \hline 
     &       &      & Interruption & Delta &       &          \\
Date & Start & Stop & Time         & Time  & Phase & Comments \\ \hline\hline 
12/01/97 & 10:40 & 11:20 & 0 & 30 & plan       & total minutes should be 40 \\ \hline 
12/01/97 & 11:20 & 11:30 & 0 & 10 & design     & \\ \hline 
12/01/97 & 12:30 & 12:50 & 0 & 20 & code       & \\ \hline 
12/01/97 & 12:50 & 12:55 & 0 &  5 & compile    & \\ \hline 
12/02/97 & 09:00 & 09:15 & 0 & 15 & test       & \\ \hline 
12/02/97 & 09:15 & 09:25 & 0 & 10 & postmortem & \\ \hline
\end{tabular} 
\caption{Example of Time Recording Log Error}\label{fig:timelog} 
\end{center} 
\end{figure}
 
Since it appears the user incorrectly subtracted {\it Stop} from {\it
  Start} when calculating the number of minutes spent in planning, an error
would be recorded for the {\it Delta Time} field.  But when looking at {\it
  Total Actual Minutes}, which is the sum of all the {\it Delta Time}
fields in the Time Recording Log, on the Project Plan Summary form (not
shown), 90 minutes would be treated as the correct value, even though the
time spent in planning was actually 40 minutes, making the true value of
{\it Total Actual Minutes} 100 minutes.

I do feel confident that every error that I recorded exists in the student
data - I checked and rechecked, sometimes five or six times, to be
absolutely sure.  On the other hand, I don't believe that I uncovered all
or even most of the errors present.  While the PSP tool did enable me to
determine the correctness or incorrectness of values generated during the
analysis stage, it provided only limited insight into collection stage
errors.  For example, in a Time Recording Log, it was possible to check
the {\it Delta Time} computation, but not the accuracy of {\it Date, Start, 
Stop}, or {\it Interruption Time}.  Of course, the tool could not, in
general, detect the absence of entries for work that was done but not
recorded.  Two other areas that created similar problems were the Defect
Recording Log and the measured and counted {\it Program Size} fields for
the Project Plan Summary.

\section{Data Correction}
It could be the case that although users of the PSP make many errors, these
errors are only ``noise'' and do not make a significant impact upon the
trends and conclusions reached from the method.  I attempted to look into
this more closely by computing the measures of yield, defect density, and
so forth for individual data and the aggregate results twice: once using
the original data supplied by the students, and once using correct versions
of the data produced by the PSP tool.

To do this, I copied the original database, which now contained all PSP
values for the 89 complete projects.  Using the second database, I then
went again through each phase of each project, attempting to correct the
primary data and then re-running the calculation steps.

As I attempted to correct the students' data, it soon became clear that
their errors fell into two classes.  Some errors I could easily correct
with reasonable confidence, such as a mismatch between the number of
defects entered in the Defect Recording Log and the total calculated for
the Project Plan Summary.  But in other cases, such as a blank {\it Phase
  Injected} for a defect, it was impossible to determine a correct value.

This can be illustrated with the Time Recording Log example used in Figure
\ref{fig:timelog} to demonstrate the error counting method.  It is clear
that there is an error in the first entry, and it seems obvious that it
occurred when calculating {\it Delta Time}.  However, we really can't tell
for sure which field was entered incorrectly.  Should {\it Start} be 10:50,
or should {\it Stop} be 11:10, or was there a ten-minute interruption that
was not recorded, or was {\it Delta Time} actually 40 minutes but
calculated incorrectly as 30 minutes?

\subsection{Correction Rules}
Despite the obvious impossibility of doing a perfect job in correcting the
student data, I formulated a set of rules that could be consistently
applied in attempting to make corrections.  Underlying all of these rules
is the basic assumption that even though primary data may be faulty, it is
probably more trustworthy than calculations performed upon it.  So, for the
Time Recording Log example above, the {\it Start}, {\it Stop}, and {\it
  Interruption Time} values would be considered correct, and the {\it Delta
  Time} value would be changed to 40 minutes.

These are the rules I used in data correction (some were only used once, or
a few times):

\begin{itemize}
\item {\bf Errors in Time Recording Log entries:} Assume that the
  start/stop/interruption times are correct and that the delta time is
  wrong, unless two Time Recording Log entries overlap.  In that case, use
  the preceding and following entries and the delta time for the current
  entry to formulate plausible start/stop times.  Generally this will mean
  starting the second entry where the first one stops.  This rule was used
  53 times.
  % 48 times for delta time problems, 5 times for overlapping entries
  
\item {\bf Missing Time Recording Log entries:} If a Time Recording Log is
  missing an entry for an entire phase, but the Project Plan Summary form
  contains a value for the phase under {\it Time in Phase (min.), Actual},
  formulate an appropriate Time Recording Log entry with fabricated date
  and time values. This rule was used 5 times.
  
\item {\bf Conflicts between Defect Recording Log and Project Plan
    Summary:} Assume that the number of defects and the phases recorded in
  the Defect Recording Log are correct and that the discrepancy occurred as
  a result of incorrectly adding up the numbers of defects injected/fixed
  per phase and/or incorrectly transferring these totals to the Project
  Plan Summary form.  This rule was used 28 times.
% 11 times for actual defects injected on PPS doesn't match defect log
% 17 times for actual defects removed  on PPS doesn't match defect log
  
\item {\bf Conflicts between Defect Recording Log and Time Recording Log:}
  If, for the Defect Recording Log, the total of all fix times for defects
  removed in a certain phase is more than the time recorded for that phase
  in the Time Recording Log, insert a Time Recording Log entry with start
  and stop times such that, combined with existing Time Recording Log
  entries for the phase, will produce a delta time of the total fix times
  plus one minute for each defect.  This will represent the minimum amount
  of time required to find and remove defects.  This rule was used 10
  times.

\item {\bf Post Mortem phase used in Defect Recording Log:} If the post
  mortem phase is used on a Defect Recording Log entry for {\it Phase
    Removed}, increment the count in the {\it Defects Removed, After
    Development, Actual} field on the Project Plan Summary form once for
  each such defect.  This rule was used 3 times.

\item {\bf Blank Injection Phase for Defect Recording Log:} If the {\it
    Inject} field is blank for a Defect Recording Log entry, but the {\it
    Fix Defect} field contains a phase name instead of a defect number, use
  the phase name to fill in the {\it Inject} field.  This rule was used 37
  times.
  
\item {\bf Blank {\it Time in Phase (min.), Plan} field on Project Plan
    Summary form:} Use the value for {\it Time in Phase (min.), Actual} for
  the same phase.  This rule was used only once.
  
\item {\bf Conflicts in {\it Program Size (LOC)} fields on Project Plan
    Summary form and Size Estimating Template:} Assume that {\it Base,
    Deleted, Modified, Added, and Reused} are correct and that errors are
  the result of incorrect calculations.  Note: this is not a truly
  satisfactory assumption because {\it Total LOC, Actual} should be the
  result of a measurement rather than a calculation and should therefore be
  relied upon.  However, given correct values for {\it Base, Deleted,
    Modified, Added, and Reused}; {\it Total LOC} can be calculated,
  whereas it is impossible to even guess at the correct values for the
  other fields.  Unfortunately errors in the {\it Program Size (LOC)}
  fields were some of the most common errors.  Combined with the importance
  of these fields in both size and time estimation and my inability to
  provide adequate corrections, estimates made with the "corrected" data
  were undoubtedly severely affected.  This rule was used over 60 times.
  %defect model codes SET/TLC/PPS4
\end{itemize}

\section{Data Comparison}
After I partially corrected the project data according to the rule set, I
decided which values to compare to best reveal the effects of errors.
Projects 8 and 9 had the most fields to compare since they were completed
using PSP2, and provided the best opportunities for observing the
cumulative effect of errors made in earlier projects.  Project 9 was the
best project for comparison because students had had the most practice in
PSP by the time this project was completed and because it provided more
time for cumulative effects to exhibit their true characteristics.
Unfortunately, one student did not fully complete this assignment,
resulting in fewer data points for the final project.

One of the more interesting areas for comparison would have been size and
time estimation.  This was not possible due to the difficulties in
adequately correcting the {\it Program Size (LOC)} fields.  Instead, I
selected a few fields from each of the other major sections of the Project
Plan Summary, including some fields that represented fairly simple
calculations but included to date values from all nine projects, and other
fields that were more local to the current project but were the result of
more difficult operations.

