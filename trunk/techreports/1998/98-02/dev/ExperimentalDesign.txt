Evaluation Design
1 Evaluating the Leap toolkit, an automated Reflective Software 
Engineering tool
1.1 Reflective Software Engineering implies improvement in Software 
Engineering skills
Reflective software engineering is a new term coined by Dr. Philip Johnson.  Reflective Software Engineering 
focuses on the individual software developer and tries to provide them with the insights they need to improve 
their development process.
 Reflective software engineering starts with the assumption that the developer is motivated to become a 
professional, highly skilled, perhaps even world class software engineer. They don't have to be one yet---they 
just have to have the desire and commitment to grow in that direction. Every concept in reflective software 
engineering is oriented around the developer’s personal growth as a technology professional. 
Reflective software engineering is based upon an extremely simple and obvious idea: people learn best from 
their own experience. To some extent, every professional software developer learns from experience. But not all 
software developers learn equally well or equally efficiently from experience. Some developers seem to 
"plateau" at a certain point in their development, while others take much longer than others to acquire basic 
proficiency in an application area, language, or environment. Reflective software engineering teaches a simple 
set of skills that facilitate the software developer’s ability to learn from their own experience.

1.2 Manual Reflective Software Engineering
The most familiar Reflective Software Engineering method is the Personal Software Process (PSP) developed by 
Watts Humphry and described in his 1995 book “A Discipline for Software Engineering”{Humphry95}.  The 
PSP is a self-improvement process for software developers.  Each developer conducts a longitudinal case study 
of their own software development process.  They collect data about the size of, the amount of effort for, and the 
defects made during each project.  The PSP provides powerful analyses on this data.  These analyses allow the 
developer to learn about their own software development practices and can aid current and future software 
projects.  Each software project provides additional data and possible insights into the developer’s own 
processes.  
The PSP has two primary improvement goals produce higher quality software more efficiently and to increase 
the developer’s size and effort estimation accuracy.  First, by recording the defects introduced into the work 
product the developers learn what types of problems they typically make an possibly areas to focus their quality 
efforts.  The higher PSP process also introduce software quality improvement phases such as design and code 
reviews.  These phases help detect and fix defects earlier in the development process, when it is less expensive to 
fix defects.  Second, the developer using PSP records their planned size and effort and the actual size and effort 
for each project.  The PSP describes a time estimation method that uses linear regression or averages to predict 
the effort of new projects based upon historical data.
To help insure successful improvement in the above two goals the PSP requires that the developer follow very 
strict rules for development, data collection and data analysis.  The PSP defines seven different development 
processes, PSP 0, PSP 0.1, PSP 1.0, PSP 1.1, PSP 2.0, PSP 2.1, and PSP 3.0.  Each process builds upon the 
previous process adding more steps, requirements, data collection and analyses.  The PSP processes all follow 
the classic waterfall model of  software development  There are scripts for the developer to follow at each step 
along the development process.  The scripts describe the tasks, entry and exit criteria, data collection and the 
analyses to perform.  
At the end of each project the developer performs a “Post Mortem” of the project. The Post Mortem allows the 
developer to automatically reflect on their past and current performance.  The reflections focus on the two 
primary goals of the PSP, work product quality and accurate estimation.  They show the developer what the 
quality of the project was with respect to the developer’s historical projects and also compares the plan and the 
actual size and  efforts.  During the Post Mortem any Process Improvement Proposals (PIPs) raised during the 
development or prior developments are evaluated based upon the data collected from the project and historical 
data. 
The reflection in the PSP is intended to improve either the quality, efficiency or prediction of the developer.  If 
these are not the goals of the developer then the Post Mortem reflection is of little use.  Also, since often this 
reflection is done by rote according to the PSP script, the developer may not realize the significance of the 
analyses.  They may miss some valuable insight in their data.
 During the PSP training course as described in the book “A Discipline for Software Engineering”, the PSP 
student also produces a defect analysis report called the R3 report.  This report analyses the cost to fix the defects 
based upon the phase of development that they were injected and the phase of development that they were 
removed.  This report helps motivate the developer to remove as many defects as possible as soon as possible.  
The R3 report also shows the developer what type of defects are the most common and which are the most 
expensive to fix.
Our experiences with the PSP, over the last two years has shown us several problems with manual Reflective 
Software Engineering as implemented in the PSP.  First, we started to question the quality of the data recorded. 
For example, we noticed that it is extremely difficult to accurately record every defect made during software 
development, in part because of the overhead of collection.  Anne Disney and Philip Johnson conducted a study 
to look at the data quality of PSP data.  They found that there are significant data quality issues with manual 
PSP. {Disney98}
Second, our experiences with industrial partners, management practices, and Robert Austin's book ``Measuring 
and Managing Performance in Organizations' {Austin96} made us think about the issues of measurement 
dysfunction in PSP and review data.  There are many subtle pressures on professionals to provide management 
with ``good'' results. While the PSP is a private process, we question whether management directed PSP training 
might not induce measurement dysfunction.  
Third, after four years, the results with long term adoption of PSP are mixed.  Pat Ferguson and others report 
excellent results with PSP adoption at Advanced Information Services, Motorola and Union Switch and Signal 
{Ferguson97}.  However, Barry Shostak and others report poor adoption of PSP in  industry 
{Shostak96,Emam96}. 
Fourth, we had significant problems with PSP's strict water fall process for software development and data 
collection.  We wanted the flexibility to define our own development processes and explore the possibilities of 
collecting different types of data on our processes.
Another of the major problems with any manual Reflective Software Engineering is the overhead of conducting 
analysis on historical data.  The files and files of raw data must be processed by hand and this greatly reduces the 
types of analysis that is possible.  It is very unreasonable to ask the developer to spend hours combing through 
their old PSP data files calculating their productivity based upon different size values.  More complex filtering 
and grouping is also very difficult to perform.  In general data manipulation is very difficult with manual paper 
data.  An automated approach to Reflective Software Engineering would greatly reduce the amount of effort the 
developer must expend.
1.3 Automated support for Reflective Software Engineering
1.3.1 University of Tennessee Automated PSP tool
Researchers at the University of Tennessee have developed a software tool that automates the PSP.  Their tool 
automates collection of time, size and defect data.  The tool follows the PSP processes exactly.  When the user 
starts a project they choose the PSP process that they are going to follow.  The tool then presents them with the 
correct scripts and asks them to provide all the data that is necessary to start the PSP process.  During the project 
the Tool keeps track of what phase the developer is in.  When the project is finished the tool performs the Post 
Mortem for the user and gives them the Post Mortem report.  
The UT automated PSP tool greatly reduces the overhead of using the PSP.  However, it still has the same 
problems as the manual PSP, fixed processes, fixed analyses, less reflection on the development process.  Since 
the tool accurately automates the PSP processes, all of them, the user cannot change the phases used or the 
analyses used. Purely automating the PSP does not resolve the data quality issue but it eliminates some of the 
data quality issues.  The automated tool does all the data manipulation and calculation.  It does not resolve the 
data collection problem.   
Regarding the issue of Measurement dysfunction the UT automated PSP uses a database to store the developer’s 
data.  The developer has control over the database since it is on their machine.  I do not think the database has 
any build in access control mechanisms. The UT PSP tool runs on Windows platforms so it could be widely 
adopted.  Since the tool strictly follows the PSP, developers using the tool must follow the Waterfall method of 
software development or their data will not accurately reflect their development.  This is a major problem with 
this tool and the PSP.  Very few developers follow a strict Waterfall software development process.
1.3.2 LEAP Toolkit
The above issues with the PSP started us thinking about how to design an automated, empirically based, 
reflective software engineering support tool to address these issues.  Our goals are to reduce the collection and 
analysis overhead for the engineer, to reduce the potential for measurement dysfunction in the collection process, 
and to allow the engineer to use their own development style.  We are also incorporating collaborative review 
support into our reflective software engineering support tool.  Adding review support allows the developer to 
gain insight from other developers.  This group input is an important feature lacking in the PSP.  These features 
are intended to improve the benefits to the engineer and the long term adoption of empirically based reflective 
software engineering.  To pursue this work, we initiated Project LEAP, 
{http://csdl.ics.hawaii.edu/Research/LEAP/LEAP.html},  and began developing the Leap Toolkit as a 
``reference implementation'' of the following design goals:
Light weight – The methods used must be “lightweight”.  In other words, they must involve a minimum 
of process constraints, be easy to learn, easy to integrate with existing methods and tools, and require 
minimal investment and commitment from management.  If a LEAP method imposes new overhead on 
the developer, then that effort must yield a short term, positive return-on-investment to that same 
developer.
Empirically based – The methods must have quantitative, as well as qualitative dimensions.  A 
lightweight orientation cannot be gained at the expense of high quality data collection and analysis.  
Developer improvement should be observable over time through measurements of effort, defects, size, 
and so forth.
Anti-measurement dysfunction – Measurement, while an integral part of LEAP, must be carefully 
designed to minimize dysfunction.  Yet the most simple solution to dysfunction, making all data totally 
private, is incompatible with the benefits to the organization and individual of  sharing certain kinds of 
data and process improvements.  The LEAP toolkit is designed to find a suitable balance between 
totally public and totally private measurement data.
Portable – Useful reflective software engineering support cannot be locked to a particular organization 
such that the developer must “give up” the data and tools when they leave the organization.  Rather, this 
support should be akin to a developer’s address book; a kind of “personal information assistant” for 
their software engineering skill set, that they can take with them across projects and organizations.
The Leap toolkit provides the developer with a toolkit which helps them to structure and record their insights and 
experiences. The kinds of structured insights and experiences they can record include: the size of the work 
product; the time it took to develop it; the defects that they or others found in it; the patterns that they discovered 
during the development of it; checklists that they used during or designed as a result of this development; 
estimates for time or size that they generated or revised during the development; and the goals, questions, and 
measures they used to motivate the data recording.
We began development of the Leap toolkit in the Fall of 1997.  After a year of intense development we publicly 
released the Leap toolkit and used it in a graduate level advanced software engineering class at the University of 
Hawaii, Manoa.  With the feedback from the student users, outside users and members of our research group we 
have continued to develop the Leap toolkit.  In the Spring of 1999 we again used the Leap toolkit in an 
undergraduate advanced software engineering class.  Both of these classes helped us solidify our ideas about 
reflective software engineering.  In the Fall of 1999 Professor Johnson is going to teach a graduate level class on 
Reflective Engineering at the University of Hawaii, Manoa.  I propose to use this class as an evaluation of the 
Leap toolkit.


1.4 Evaluation of Leap’s ability to support Reflective Engineering by studying 
time estimation methods 
To evaluate Leap’s ability to support Reflective Software Engineering I am going to conduct an experiment and 
a small case study in parallel.  During the study, we will evaluate fourteen different quantitative time estimation 
methods based upon historical project size and time data.  Thirteen of the fourteen methods are all closely related 
and can be accurately described.  The fourteenth methods is the student’s own estimate depends entirely on each 
student.  I will conduct the case study to determine how each student derives their estimate.  Twelve  of the 
thirteen primary estimation methods can be described by three orthogonal  axes, type of the size data (planned vs 
actual) , granularity of the size estimate (lines of code vs methods), and estimation model (average, linear 
regression, exponential regression).  Figure 1 shows the 12 primary estimation methods and the two additional 
estimation methods.  The PSP defines a process for deciding which method to use either A, B, or C depending on 
the number and correlation of the historical size and time data.   All twelve quantitative methods and the PSP 
method, which is based upon three of the twelve methods, require enough historical data to produce an estimate.


























1.4.1 Main twelve methods
Average based:  The time estimate is based upon the average of all the previous time and size data.  The data sets 
are defined by whether the data is planned size or actual size and what the granularity of the size estimate is.  
Table  shows the names of the four average based methods:

Method Name
Method Description
APL
Averages of planned LOC data
AAL
Averages of actual LOC data. 
PSP choice C
APM
Averages of planned method data
AAM
Averages of actual method data

Linear Regression based: The time estimate is based upon a the linear regression line.  The data sets are defined 
by whether the data is planned size or actual size and what the granularity of the size estimate is.  Table shows 
the names and descriptions of the linear regression based methods.

Method Name
Method Description
LPL
Linear regression and 70% confidence 
interval of planned LOC data
PSP choice A
LAL
Linear regression and 70% confidence 
interval of actual LOC data
PSP choice B
LPM
Linear regression and 70% confidence 
interval of planned method data
LAM
Linear regression and 70% confidence 
interval of actual method data

Exponential Regression based:  The time estimate for the exponential regression based methods uses an 
exponential regression for calculating the time estimate.

Method Name
Method Description
EPL
Exponential regression and 70% 
confidence interval of planned LOC 
data
EAL
Exponential regression and 70% 
confidence interval of actual LOC data
EPM
Exponential regression and 70% 
confidence interval of planned method 
data
EAM
Exponential regression and 70% 
confidence interval of actual method 
data

1.4.2 PSP
The PSP method describes a time estimation technique that is based upon the developer’s historical data and how 
closely correlated the size and time data are.  The following a brief description of the PSP time estimation 
method.

If planned size data in lines of code (LOC) and actual time data’s correlation is greater than 0.5 use 
linear regression on the planned size and actual time.  This is defined as choice A, using my 
terminology it is LPL.  
If the planned size and actual time are not strongly correlated then check the correlation between actual 
size and actual time.  If the actual size data in LOC and the actual time data’s correlation is greater than 
0.5 then use linear regression on the actual size data and the actual time. This is defined as choice B, 
using my terminology it is LAL.
If the actual size and actual time data are not strongly correlated then use the average of actual size and 
actual time. This is defined as choice C, using my terminology it is AAL.  
The PSP text notes that using linear regression with a large negative beta0 can lead to incorrect estimates.  Also 
using linear regression when the estimated size is close to the extremes of the historical size range also may lead 
to incorrect estimates.  Beyond stating these warnings the text does not give any guidelines how to handle these 
cases.  Should you fall back upon choice C or AAL when the use of linear regression is questionable?  For this 
experiment I will use choice C when the linear regression value is obviously incorrect. E.g. when the estimate is 
negative.

1.4.3 Student’s Estimate
Leap will present each student with the 13 different estimates.  The student will then be able to provide their own 
estimate.  They can choose one of the 13 estimates, or choose their own.   We will ask the students to record the 
method that they use to come up with their estimate.
At points during the course I will interview the students to find out how they are picking their estimate and how 
they feel about their estimates vs the quantitative estimates.

We are going to evaluate these 14 different estimation methods to determine if there is a more accurate method 
for effort estimation from size.  Since we can do this Leap is a success.  Even if we find that there is no 
significant difference between the estimation methods we still learn that Leap is a valuable tool for Reflective 
Software Engineering.  It is also a good tool for investigating individual development.
2 The Experiment
2.1 Introduction 
This experiment will evaluate the 14 different quantitative estimation processes and determine if there is any 
significant difference between the estimation methods.  I will compare the estimation methods in general and for 
each individual student.
2.2 Experimental environment
The experiment will be performed in a student environment in the Introduction to Reflective Software 
Engineering course at the University of Hawaii Manoa.  The students in the class will be developing 11 
(software) projects and recording their software processes in the Leap toolkit.  By reflecting on their experiences 
and the data they collect about their software development processes they should learn how to improve their 
development processes.  During the development process they will be asked to estimate the size and amount of 
effort each of each software project.  The Leap toolkit provides an automated tool for looking at historical 
development data and deriving effort estimations based upon historical size and effort data.  The Leap Time 
estimation tool provides students with 10 different effort estimates based upon the students historical data.  The 
student will the make their own estimate of how long the project will be.

2.3 Experimental variables
2.3.1 Independent and Dependent variables
Since the objective is to evaluate the different quantitative time estimation methods, the independent variable 
will be the estimation technique.  This means that there is one independent variable that can take on 14 different 
values: the 12 method values, the PSP value and the student’s estimate.
The general accuracy of the estimation method applies to the estimation of the mean value and of the standard 
deviation, and the actual estimate itself, there are three dependent variables in this experiment.  The first two 
dependent variables are for the general case.  The third dependent variable is for each individual student’s 
estimate.  The relative prediction error will be calculated for both the mean and the standard deviation for the 
entire class.   The following two dependent variables will be calculated for each of the 14 different estimation 
methods for the entire class:
? Mean prediction error = | estimation mean – actual mean | / actual mean
? Standard deviation prediction error = | estimation std – actual std | / actual std
For each individual student the following dependent variable will be calculated for each of the 14 different 
estimation methods.
? Relative Predicted Error = | estimated time – actual time | / actual time
These measures cannot be measured until after the task has been completed.  The different estimates must be 
calculated and chosen for each project before the project is started.  The Leap toolkit will provide 13 of the 14 
estimates automatically. The students will record their estimate and the method they use to obtain their estimate.
2.3.2 Blocking variable
Since the purpose of the experiment is to determine the effect of the estimation method on the prediction error 
and different projects may have an effect on the prediction error I will use a blocking variable to account for this 
effect.  The reason that the different projects may have an effect on the prediction error is that it may be easier to 
estimate the size of some project than others.  To distinguish between the effects of the different projects and the 
effects of the estimation methods I have added a blocking variable associated with the project number.
2.4 The Design
The design for this experiment has two parts, the class as a whole and each individual student.  
At the beginning of every project the student will develop a planned size and planned effort for the project.  
After the third project,  Leap will estimate the effort according to the different alternatives (treatment, alt 1 – alt 
13). The student will produce the 14th estimate.  During the project the students will record the amount of effort 
in Leap.  After the project is finished, the student will measure the actual size of the project and  total the actual 
amount of effort in Leap.  After all the projects are finished, I will calculate the relative prediction error for the 
mean and standard deviation for each of the fourteen estimation methods.  I will calculate these values for the 
entire class and for each individual student.
We cannot use the data from the first three projects since the quantitative estimation methods require at least 
three data points.  With eleven projects we will still have enough data points to evaluate the different estimation 
methods.  Since the estimates are independently generated except for the students’ estimate there is no problem 
with the order of the alternatives.  
2.5 Analysis
I am using the following relationship to model the experiment yij = µ + ti + ßj + ei where:
yi = the relative prediction error for alternative i
µ = the overall mean
ti = the effect of  the ith treatment (estimation method)
ßj = the effect of the jth block (project)
eij = residual for the ith and jth treatment,  
This model can be analyzed with standard analysis of variance (ANOVA) procedures with the null hypothesis: 
	H0 = µ1 = µ2 = µ3 = ... = µ14          Where µi = µ + ti.; i = {1, 2, 3, ... , 14}.
The null hypothesis states that there is no effect of estimation methods on the prediction error.
For the general comparison of the 14 methods the entire class‘ data will be compared.  The mean prediction error 
and standard deviation prediction error will be calculated for each method using the entire class’ data. 
Mean prediction errori = | Class’ estimation meani  - Class’ actual meani | / Class’ actual meani
Standard deviation prediction errori  = 
| Class’ estimation std devi - Class’ actual std devi | / - Class’ actual std devi 
Where i = {1, 2, 3, ... , 14} and represents the estimation method.
The null hypothesis can be tested for the entire class.  Rejecting the null hypothesis only means that there is a 
difference between the accuracy of the estimation methods it does not indicate which estimation technique is 
more accurate.   If the null hypothesis is rejected I will use the Least Significant Method to distinguish between 
the different alternative estimation methods. 
For each student I will consider the relative predicted error only.  The equation for relative predicted error is
	Relative Predicted error = | estimated time – actual time | / actual time
I will perform similar calculations and ANOVA to determine if there is an individual difference in estimation 
methods. 
2.6 Example Data
The following table shows some example data from my own Java development experience.  This data is from a 
pilot study that I conducted this spring and summer.  I have recorded the planned size and effort for over 20 
projects beginning in December 1997.  This data reflects the ten most recent projects that I have data for.  I 
treated these ten projects just like the projects the students in the class will.  Leap generated the 13 quantitative 
estimates and I provided the student’s estimate.  Since it is a single subject’s data I can only calculate the relative 
predicted error for this data.
Table 4.6.1: Example Project Data

Project
Project
Project
Project
Project
Project
Project
Mean
Std Dev
Method
4
5
6
7
8
9
10


APL
263
1150
386
63
48
60
151
303.00
393.84
AAL
191
763
217
38
30
37
160
205.14
258.35
LPL
315
717
509
0*
0*
0*
109
235.71
287.33
LAL
186
268
183
0*
0*
9
72
102.57
109.18
EPL
394
6634
234
118
116
99
102
1099.57
2242.82
EAL
176
321
157
139
136
112
99
162.86
74.35
APM
319
1346
405
65
101
70
174
354.29
456.16
AAM
329
1336
343
55
72
50
126
330.14
460.87
LPM
0*
179
484
0*
0*
0*
145
115.43
179.84
LAM
136
387
318
18
42
14
111
146.57
149.23
EPM
42
237
227
123
132
103
109
139.00
69.83
EAM
124
747
186
144
143
114
105
223.29
232.45
PSP
191
763
509
38
30
37**
109
239.57
286.23
Student
240
837
265
93
56
37
141
238.42
277.94
Actual
198
3047
380
139
42
26
248


* Actual prediction was negative so rounded the value to 0 since negative time makes no sense.
** PSP algorithm says use choice A(LPL), but that value is negative so I changed the choice to  
C(AAL).
Based upon the above data I calculated the relative predicted error for all 14 estimation methods and all 7 
projects.  Table 4.6.2 shows the values for the relative predicted error.
Table 4.6.2: Relative predictive error
Method
Project 4
Project 
5
Project 
6
Project 
7
Project 
8
Project 
9
Project 
10
APL (1)
0.3283
0.6226
0.0158
0.5468
0.1429
1.3077
0.3911
AAL (2)
0.0354
0.7496
0.4289
0.7266
0.2857
0.4231
0.3548
LPL (3)
0.5909
0.7647
0.3395
1.0000
1.0000
1.0000
0.5605
LAL (4)
0.0606
0.9120
0.5184
1.0000
1.0000
0.6538
0.7097
EPL (5)
0.9899
1.1772
0.3842
0.1511
1.7619
2.8077
0.5887
EAL (6)
0.1111
0.8947
0.5868
0.0000
2.2381
3.3077
0.6008
APM (7)
0.6111
0.5583
0.0658
0.5324
1.4048
1.6923
0.2984
AAM (8)
0.6616
0.5615
0.0974
0.6043
0.7143
0.9231
0.4919
LPM (9)
1.0000
0.9413
0.2737
1.0000
1.0000
1.0000
0.4153
LAM (10)
0.3131
0.8730
0.1632
0.8705
0.0000
0.4615
0.5524
EPM (11)
0.7879
0.9222
0.4026
0.1151
2.1429
2.9615
0.5605
EAM (12)
0.3737
0.7548
0.5105
0.0360
2.4048
3.3846
0.5766
PSP (13)
0.0354
0.7496
0.3395
0.7266
0.2857
0.4231
0.5605
Student (14)
0.2121
0.7253
0.3026
0.3309
0.3333
0.4231
0.4315

The analysis of variance for the relative predictive error data is shown in Table 4.6.3. 
Table 4.6.3: ANOVA for predicted error
Source of 
variation
SS
Df
MS
F0
p-value
Treatment 
(estimation 
method)
7.54237769
13
0.58018289
2.03137219
0.07729167
Block (project)
14.2348694
6
2.37247824
8.30666732
0.00634850
Error
22.2776831
78
0.28561132


Total
44.05493033918
97




Philip, Is the significance level of 0.1 ok or should I use 0.05 and not reject the null hypothesis?
Table 4.6.3 shows that the estimation methods does (not) play a significant role in the relative predicted error.  I 
can(not) reject the null hypothesis that there is no difference between estimation methods for myself.  The results 
do indicate that our blocking variable the project number has a significant effect on the relative predicted error.  
This implies that making good size estimates is much more important than the time estimation method.
Comparison of the different estimation methods.  Figures 4.6.1 shows the mean treatment effects µi for the 
different estimation methods.
Figure 4.6.1: The mean treatment effects for relative prediction error.
Figure 4.6.1 shows that the estimation methods APL, AAL, LAM, PSP and the student’s were the five most 
accurate estimation methods.  My own estimates had the lowest mean prediction error with simple average of 
actual LOC to effort being second best.  While EPL, EAL, EPM, and EAM were the worst four estimation 
methods.  This implies that my development does not follow an exponential curve.

2.7 Threats to Validity
2.7.1 Threats to Internal Validity
One common internal threat is due to maturation, which is concerned with the effect that the participants‘ 
attitude to the experiment changes during the experiment.  Since this experiment takes a whole semester this is 
an important threat.  One of the goals of Reflective Software Engineering is to change the attitude of the 
participants.  The direct result of this is that the students‘ own estimate may be drastically affected.  This will 
have no effect on the other quantitative estimation methods that depend upon historical data not the participants 
attitude.  The students will also be graded in the class on their data so they will be motivated to collect accurate 
data for the entire semester.

2.7.2 Threats to External Validity
There are two primary threats to external validity, the development projects the developers are doing and the 
developers themselves.  Fist the development projects tend to be small and only take about a week to complete.  
This is not representative of industrial projects, however many industrial project may be reduced to smaller 
subprojects with a similar time frame.  The language and domain of the problems are also unique to the 
experiment and may need to be addressed by replication of  this experiment in different domains and languages.  
The language used in this experiment is Java.  Java is becoming a very popular language for programming.  
However, Java is not widely used in industrial programming.

Second, the developers will all be graduate students with little or no industrial experience.  They will soon be 
graduating and then will be junior developer in industry.  These results might be applicable to junior 
programmers.  Since some of the results are focussed on the individual the same experiment may have to be 
replicated to determine if the results apply to other individuals.
2.8 Possible Results 
2.8.1 Null hypothesis supported for the class and for each student
This result implies that there is no preferred method for estimating time from size.  This would be a very 
interesting result because it means that developers do not need to choose between methods for estimation.  They 
can choose the easiest method.  This result would tend to invalidate the PSP’s time estimation method since it is 
complex and would be no better than any other method.
2.8.2 Null hypothesis supported for the class but rejected for some or all students
This result implies that there is not a generic best method for estimating effort from size for all developers, but 
individual developers may find benefits from some methods more than others.   This would tend to invalidate the 
PSP’s time estimation method since it is generic for all developers.  It would also support the concepts of 
Reflective Software Engineering, individual developers can gain benefit from learning how they develop 
software engineering.  They could find out which estimation technique works best for them and thus produce 
more accurate effort estimations for future projects.
2.8.3 Null hypothesis rejected for the class but supported for each student
This result is very odd.  It implies that there is a generic method for estimating the effort for the combination of 
developers but for each individual developer they cannot determine which estimation technique is best for them.
2.8.4 Null hypothesis rejected for the class and rejected for some or all students 
This result implies that there is a better method for all developers and that there is a best method for each 
individual developer.  This is another very interesting possibility.  If we can find out which estimation technique 
is best for this class then we might find a good general estimation technique that all software developers should 
be using.  It could turn out that the best method is the PSP’s time estimation method. 
3 Plan for the Research
Task
Milestone
Finish Proposal
6 September 1999
Add estimation methods to Leap
10 September 1999
Form committee 
10 September 1999
Defend Proposal
24 September 1999
Data Collection
1 September 1999 – 15 December 1999
Data Analysis
15 December 1999 – 14 January 2000
Write Dissertation
1 October 1999 – 31 March 2000
Defend Dissertation 
April 2000
Graduate
May 2000

