%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 4-case-study.tex -- 
%% Author          : Philip Johnson
%% Created On      : Wed Apr  8 14:24:28 1998
%% Last Modified By: Philip Johnson
%% Last Modified On: Wed Aug 11 11:18:14 1999
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1998 Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 3 pages


\section{The Case Study}
\label{sec:case-study}

To gain insight into the occurrence and significance of collection and
analysis data quality problems, we conducted a case study. The case
study was designed to investigate the following hypothesis:

\begin{quotation}

{\em Data quality problems during collection and analysis can distort the 
PSP data's representation of the programmer's actual behavior, leading to 
invalid process improvement changes.}

\end{quotation}


\subsection{Case Study Design}
\label{sec:design}

The case study began by teaching a one semester course on the PSP, modified
in certain ways in an attempt to improve the quality of PSP data.  The 10
students in the course submitted all of their paper forms to the instructor
after each assignment. Some errors required students to correct and
resubmit prior forms.  The set of paper PSP forms collected over the course
of the semester comprises the {\em original} PSP dataset, and can be
thought of as one experimental treatment.

Next, the second author entered each data value from the original PSP dataset into a
database system that she developed. This database system implements
automated calculation of the derived measures, and detects a subset of the
possible errors that can exist in a PSP dataset.

She also developed a second system to support correction of some of the errors
found through the first database system.  This system corrects a subset of
erroneous data values according to a set of correction rules (specified in
Section \ref{sec:rules}).  Application of these rules produced the second,
{\em corrected} PSP dataset, which can be thought of as the second
experimental treatment. Note that we do not claim that this second dataset
is completely correct, merely that it corrects a set of clearly inaccurate
calculations from the first dataset.

Given this approach, the case study design is similar to a 
within-subjects comparison of a ``control'' treatment (the original PSP
dataset) to the ``experimental'' treatment (the corrected PSP dataset).
Our data analysis is designed to determine whether significant differences exist
between these two treatments. 

In addition to the test of our primary hypothesis, we used the database
systems to perform several additional analyses on the observed  errors to
understand their cause and potential significance to PSP data values and
the method itself.

\subsection{The Modified PSP Curriculum}
  
  The projects used for this study were obtained from a software
  engineering class taught by Philip Johnson, in which the PSP was taught
  over the course of a semester using nine project assignments. There were
  ten students in the class, and 89 completed projects.
   
  Because of the concern with data quality from prior experience teaching
  PSP, the instructor made four principal modifications to the standard PSP
  curriculum: increased process repetition,  increased process description,
  technical reviews, and tool support.  For replication purposes, a more
  detailed description of the curriculum used in this couse is available
  at the website: http://www.ics.hawaii.edu/$\sim$johnson/613s98/.
  
  {\bf Increased process repetition.} In the standard PSP curriculum,
  students are assigned 10 programs during the semester (in addition to
  several midterm and final reports). Over the course of these ten
  programs, students practice seven different PSP processes, which means
  that the development process used by the students changes for seven out
  of ten programs.  From our initial experience with the PSP, we found
  that the overhead of this almost constant ``process acquisition'' led to
  data errors and could overwhelm the effort related to actual development.
  To ameliorate this situation, the modified curriculum included only five
  PSP processes, enabling students to practice most
  processes at least twice before moving on to a new one. The modified
  curriculum also included only nine programs instead of ten, providing
  additional time in each program for data collection and analysis.
  
  {\bf Increased process description.} In our initial experiences teaching
  the PSP, the instructor found that students had a great deal of trouble
  learning to do size and time estimation correctly.  For example, PSP time
  estimation requires choosing between three alternative methods for
  estimation depending upon the types of correlations that exist in the
  historical process data from prior programs.  To help resolve this and
  other problems, the instructor added four additional worksheets: (1) a
  Time Estimating Worksheet to provide a guide through the various methods
  of time estimation; (2) a Conceptual Design Worksheet to help in
  developing class names, method names, method parameters, and method
  return values; (3) an Object Size Category Worksheet to help in size
  estimation; and (4) a Size Estimating Template Appendix to provide a
  place to record planned and actual size for prior projects.
  
  {\bf Technical reviews.} At the completion of each project, students
  divided into pairs and carried out a technical review of each other's
  work.  A two-page checklist facilitated this process.  It included such
  questions as ``Did the author follow the PSP Development Phases
  correctly?'' and ``Is the Projected LOC calculated correctly?''  A second
  ``Technical Review Defect Recording Log'' form included columns for
  number, document, severity, location, and description. Students were
  given approximately 60 minutes to do the review.  The technical review
  forms were submitted with the completed projects.  The instructor
  reviewed the projects a second time for grading purposes, using the
  Technical Review Defect Recording Log to record any additional mistakes.
  
  {\bf Tool support.} Finally, the instructor provided four spreadsheets to
  support records of planned and actual data values. In addition, students
  were provided with well-tested tools to count non-comment source lines of
  code for Java programs, to compare two versions of a Java program and
  report non-comment lines of code added and deleted, and to perform
  certain statistical analyses.  (In the textbook PSP curriculum, students
  ``bootstrap'' their environment by implementing these tools themselves.
  While elegant pedagogically, this approach unfortunately introduces a
  potentially significant source of data quality problems, since these
  freshly developed tools with no usage history are used to generate many
  of the measures used in later data analysis.)

  In addition to these curriculum modifications, the instructor
  emphasized data quality throughout the course, as recommended in the
  textbook.  For example, he augmented the lecture notes in the
  Instructor's Guide with fully worked out examples of the PSP process data
  for a fictitious student to show how data is collected and analyzed for
  each assignment and accumulated over the course of the semester.  He
  dedicated lectures to collection and analysis of data periodically
  throughout the semester. He regularly showed the class aggregate
  statistics on class performance.  He met with students individually and
  in groups throughout the semester to go over their assignments and PSP
  data while they were in the midst of planning, design, code, compile,
  test, and/or postmortem; but prior to project turn-in.  He uncovered and
  removed many, many PSP data errors through these meetings which are not
  counted in our results.  He did technical reviews of every assignment's
  PSP data, and circulated problem reports throughout the semester
  summarizing issues discovered from student data.

\subsection{Case Study Instrumentation}
  
We developed a database application to support analysis of PSP data from
PSP0 to PSP2, using the Progress 4GL/RDBMS.  In order to reduce
opportunities for making mistakes, this tool was designed to require a
minimum amount of user input and to provide the user with default values
whenever possible.  Apart from task and scheduling template values, the
application automated all analysis stage calculations, from determining
delta times for Time Recording Log entries to performing linear regression
for size estimation. In addition, the application guides the user through
the appropriate forms and fields in the order most appropriate for the
current process and phase.
 
\subsection{Data Collection}
  
  Once the database application was ready, we entered data from the student
  project PSP forms and compared each student value with the
  value computed by the application.  Although every discrepancy
  between the manually generated data and the application-generated data
  could be considered an error, we only counted an error at its insertion
  point.  For example, in a Time Recording Log entry for the Design phase,
  if {\it Stop} is incorrectly subtracted from {\it Start}, {\it Delta
    Time} will be incorrect.  Even if all other calculations are done
  correctly for the rest of the project, {\it Time in Phase, Design,
    Actual}; {\it Time in Phase, Total, Actual}; {\it Time in Phase,
    Design, To Date}; {\it Time in Phase, Total, To Date}; {\it Time in
    Phase, To Date \%}; and {\it Time in Phase, To Date} values for an
  indefinite number of future projects will all be inaccurate to some
  degree.  And this is just for the most simple process, PSP0!  In more
  advanced processes, {\it LOC/Hour}, time estimation, {\it Cost-Performance
    Index}, and {\it Defect Removal Efficiency} values could all be
  affected for both the current project and future projects.  To eliminate
  this combinatorial explosion in the number of errors, we counted this as a
  single error in {\it Delta Time}. 
  
  Although we analyzed the project data quite carefully, we do not feel
  confident that we have uncovered all or even most of the errors in this
  case study.  While our database application does enable us to determine
  the correctness or incorrectness of values generated during the analysis
  stage of our data quality model, it provides only limited insight into
  collection stage errors.  For example, in the Time Recording Log, it was
  possible to check the {\it Delta Time} computation, but not the accuracy
  of {\it Date}, {\it Start}, {\it Stop}, or {\it Interruption Time}.  Of
  course, the tool could not, in general, detect the absence of entries for
  work that was done but not recorded.  Two other areas that created
  similar problems were the Defect Recording Log and the measured and
  counted {\it Program Size} fields for the Project Plan Summary.

\subsection{Data Analysis}
  
  In order to analyze the 1539 errors uncovered by the PSP data entry tool,
  we developed a second database application, the PSP Error Data Analysis
  Tool.  For each error discovered, we tracked the person who made the
  error, the method by which the error was found (technical review,
  instructor review, or comparison with the PSP tool results), the
  assignment in which the error occurred, the PSP process used for that
  assignment, the PSP phase in which the student was working when the error
  occurred, the general error type, the specific error type, the severity
  of the error, the age of the error (number of assignments since the
  introduction of the PSP operation in which the error occurred), the
  incorrect and correct values (where applicable), and an optional comment
  for noting issues of interest in that error.

  \subsubsection{Error Correction}
  \label{sec:rules}

  Although our initial analysis of our case study data revealed many
  errors, the sheer presence of errors might only lead to imprecision,
  rather than inaccuracy. In other words, it was possible that these errors
  were only ``noise'', similar in magnitude to naturally occurring random
  fluctuations in behavior, but not sufficient to actually change the
  trends or interpretations of PSP data.
 
  To test this hypothesis, we attempted, where possible, to fix errors so
  that original and corrected versions of the data could be compared.  It
  soon became clear that errors fell into three classes.  First, there were
  errors where the correct value could be determined.  This class included
  such values as {\it LOC/Hour} that were wrong simply because of an
  incorrect calculation. These errors were easily fixed by correctly
  performing the calculation in question.  Second, there were errors where
  the correct value could not be determined, such as a blank {\it Phase
    Injected} for a Defect Recording Log entry.  Fortunately, most errors
  in this class occurred in fields that didn't affect other fields, such as
  missing header data or missing dates in the Defect Recording Log.  Third,
  there were errors where the correct value could be guessed.  In a Time
  Recording Log entry with {\it Start} 10:00, {\it Stop} 10:30, {\it
    Interruption Time} 0, and {\it Delta Time} 40; it is clear that there
  is a problem, but not clear which field is incorrect and should be
  corrected.  However we can guess that there was a problem calculating
  {\it Delta Time} and assume that the other values are valid.  To correct
  this third class of errors in an explicit and consistent fashion, we
  developed a set of rules.  Underlying each of our rules is the assumption
  that primary data is more likely to be accurate than calculations
  performed upon it. The following lists each of the rules along with
  the number of times it was used in the case study.
      
  Rule 1 (used 53 times): Defects in Time Recording Log entries should be
  handled by assuming that the start/stop/interruption times are correct
  and that the delta time is wrong, unless two Time Recording Log entries
  overlap.  In that case, the preceding and following entries and the delta
  time for the current entry should be used to formulate plausible
  start/stop times.  Generally this will mean starting the second entry
  where the first one stops.

      
  Rule 2 (used 5 times): If a Time Recording Log is missing an entry for
  an entire phase, but the Project Plan Summary form contains a value for
  the phase under {\it Time in Phase (min.), Actual}, an appropriate Time
  Recording Log entry should be formulated with fabricated date and time
  values.
        
  Rule 3 (used 28 times): For conflicts between a Defect Recording Log and a Project Plan Summary
  it should be assumed that the number of defects and the phases recorded
  in the Defect Recording Log are correct and that the discrepancy occurred
  as a result of incorrectly adding up the numbers of defects
  injected/fixed per phase and/or incorrectly transferring these totals to
  the Project Plan Summary form.
      
  Rule 4 (used 10 times): If, for the Defect Recording Log, the total of
  all fix times for defects removed in a certain phase is more than the
  time recorded for that phase in the Time Recording Log, a Time Recording
  Log entry should be inserted with start and stop times that, combined
  with the existing Time Recording Log entries for the phase, will produce
  a delta time of the total fix times plus one minute for each defect.
  This will represent the minimum amount of time required to find and
  remove the recorded defects.
      
  Rule 5 (used 1 time): To provide a value for a blank {\it Time in Phase
    (min.), Plan} field on the Project Plan Summary form, the value for
  {\it Time in Phase (min.), Actual} for the same phase should be
  used. Note that this rule, if used widely, would itself introduce
  error into the correction process. However, we used it only once 
  on one project and it has negligible impact upon our results.
    
  Rule 6 (used 62 times): Conflicts in {\it Program Size (LOC)} fields on
  the Project Plan Summary form should be handled by assuming that {\it
    Base, Deleted, Modified, Added, and Reused} are correct and that errors
  are the result of incorrect calculations for {\it Total New and Changed}
  and {\it Total LOC}.  Actually, this is not a truly satisfactory
  assumption because {\it Total LOC, Actual} should be a measurement rather
  than a calculation and should therefore be relied upon.  However, given
  correct values for {\it Base, Deleted, Modified, Added,} and {\it
    Reused}, it is possible to calculate {\it Total LOC}, whereas it is
  impossible to even guess at the correct values for the other fields.
  Unfortunately, defects in the {\it Program Size (LOC)} fields were some
  of the most common defects.  


\subsubsection{Data Comparison}

After we partially corrected the project data according 
to the rule set, we investigated which values to compare to best reveal the
effects of errors.  Projects 8 and 9 had the most fields to
compare since they were completed using PSP2, and provided the best
opportunities for observing the cumulative effect of errors made in earlier
projects. Project 9 was the best project for comparison because students
had had the most practice in PSP by the time this project was completed and
because it provided more time for cumulative effects to exhibit their true
characteristics. Unfortunately one student did not complete this project,
resulting in fewer data points for the final project.
   
One of the more interesting areas for comparison would have been size and
time estimation.  This was not possible due to the difficulties in
adequately correcting the {\it Program Size (LOC)} fields. Instead, we
selected a few fields from each of the other major sections of the Project
Plan Summary, including some fields that resulted from fairly simple
calculations but represented to date values from all nine projects, and
other fields that were more local to the current project but were the
result of more difficult operations.

\subsection{Threats}

We tested the hypothesis investigated in this study by comparing two
PSP datasets: an uncorrected PSP dataset obtained from our students, and a
(partially) corrected PSP dataset produced through automated analysis
and implementation of correction rules.  In this section, we discuss
threats to the internal validity (whether the approach used is actually
valid for testing the hypothesis) and to external validity (whether
the results obtained in this study are applicable to external 
industry and academic practice of the PSP). 

\subsubsection{Internal Validity}

One threat to internal validity is an instrumentation effect.  This could
have occurred in two ways.  First, there could exist defects in the design
and/or implementation of the database system used to create the partially
corrected PSP dataset.  To minimize this threat, great care was taken in
the development of this database system to ensure the accuracy of its
computations, and all data entered was re-checked at least once to ensure
that there were no transcription errors.  It is also relevant to note that
Anne Disney, who designed and implemented the database, has worked
professionally for many years doing database development using the same
DBMS employed in this study.  

A second threat to internal validity occurs from our use of correction
rules. It is conceivable that a correction rule, if improperly designed,
could introduce a systematic bias into corrected dataset that produces an
artificial difference between the two datasets not related to underlying
programmer behavior.  To minimize this threat, we evaluated each of our
rules for the potential presence of such systematic bias.  One rule, in
fact, does have the potential to produce this problem, but we used this
rule in only one case in the entire dataset and our results are not sensitive
to the specific value chosen.

\subsubsection{External Validity}

One threat to external validity is the sample size and nature of our
subjects. Our sample size of 10 students leaves open the possibility
that the results could be an artifact of the individuals involved
in the study.  A related threat involves the use of students for the
study. Perhaps professional software engineers would approach the 
learning of the PSP in a different manner than students, given that
the rewards and motivation structure in industry are quite different 
from academia.

Another threat to external validity is the instructor.  Clearly, the level
of PSP data quality during both collection and analysis is influenced by
the quality of instruction.  It may be possible to obtain different
outcomes merely through alternative approaches to instruction. Furthermore,
the instructor in this study has not attended the official SEI-sponsored PSP
instructor training course.  However, as discussed in Section
\ref{sec:education}, the PSP datasets submitted by students in the case
study show precisely the same sorts of trends reported by other
instructors, and as discussed in Section \ref{sec:instruction}, the number
of PSP dataset errors detected in our study is actually quite low, when
viewed as a percentage of the total number of possible errors.  In
addition, the case study semester was the second time the instructor taught
the PSP curriculum, and student evaluations were overwhelming positive. The
case study outcomes do not appear to be the result of lack of instructor
familiarity with the material or the result of simple student apathy
regarding the course.

While we attempted to minimize these threats to both internal and external
validity of this study, they are still real.  The most effective way to
evaluate these threats is through replication of this study in other
environments using different data verification mechanisms, different
subjects, and different instructors.  We hope that this study will
demonstrate the need and importance of such replication efforts in the PSP
community.




