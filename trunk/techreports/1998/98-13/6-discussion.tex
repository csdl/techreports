%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 6-discussion.tex -- 
%% Author          : Philip Johnson
%% Created On      : Wed Apr  8 14:25:13 1998
%% Last Modified By: Philip Johnson
%% Last Modified On: Wed Aug 11 10:47:53 1999
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1998 Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 

\section{Discussion}
\label{sec:discussion}

This paper reports on the results of analysis of the data from a single PSP 
class with only 10 students.  As with any case study, care must be taken
in interpreting these results.  We do not know whether this data is 
representative of PSP courses in general, and if the way we teach the
PSP is representative of the way the PSP is taught by others. Data quality
problems might be less prevalent in other PSP courses; on the other hand,
they might just as easily be more prevalent.

While we do not claim that these results are representative of all PSP
courses, neither do we believe that they are an artifact of some peculiarity
and/or failing of our environment.  First, this case study was performed
after the instructor had taught the PSP for one semester in a graduate
level course, and instituted it within his research group, and adopted it
himself for his own software development activities.  By the time of this
study, we were quite experienced as both teachers and users of the PSP.
Second, as already noted, we were concerned with data quality problems from
the beginning, and instituted curriculum modifications specifically
intended to raise data quality. The overall error rate of less than 5\%,
while quite small, was still not sufficiently small to prevent significant
differences between original and corrected data sets. Third, our results cannot
be due to our lack of enthusiasm for the PSP: both of us consider it to be
one of the most powerful software engineering practices we have adopted in
our careers.  The second author, for example, has used her automated PSP
tool to gather data on over 120 of her industrial projects over the past
two years.  Fourth, our results cannot be due to lack of enthusiasm for the
PSP by our students, as the post-course comments reveal, most of the
students indicated that they found the class to be very useful and
interesting.

\subsection{Recommendations for research and practice}

Based upon the results of this study, we have the following recommendations 
for future research and practice of the PSP:


\subsubsection{Replication} 

We believe this study provides strong evidence for the need for more
research on collection and analysis data quality in the PSP.  Current
studies of the PSP appear to take the accuracy of PSP data for granted, or 
else simply assume that tool support can eliminate all sources of 
data quality problems. 
This study is the first to methodically examine the assumptions underlying
data quality in the PSP and subject them to empirical investigation. Our
results indicate that the PSP community may be overly optimistic about the
quality of PSP data, particularly when produced using the traditional, manual
approaches that lack integrated, PSP-specific tool support. Even when such
support is provided, the possibility of measurement dysfunction introduces
substantial threats to the accuracy of the data in the collection phase
as discussed below in Section \ref{measurement-dysfunction}.  Better
understanding of the true extent of PSP data quality problems requires
replication of this study, or at least further PSP research that includes
PSP data quality verification as an explicit design component.  To 
support this endeavor, researchers are invited to peruse a website
containing curriculum materials from this course at
http://www.ics.hawaii.edu/$\sim$johnson/613s98/.  



\subsubsection{Software engineering education}

We continue to believe that the PSP has substantial educational value in
software engineering, despite the issues we have raised with data quality.
Students learn valuable, concrete skills concerning defect management and
planning in the PSP curriculum.  Additionally, the PSP provides students
with a framework for empirically evaluating the usefulness of any other
process improvement techniques or programming methods they come across in
the future.  


\subsubsection{PSP tool support}

We believe that integrated tool support for the PSP is required, not
merely helpful, to obtain high analysis-stage PSP data quality.  We also
believe that integrated tool support will make adoption of the PSP
substantially easier, since the most common complaint made by students
using the manual PSP in our classes is the time and effort required to fill
out the forms.  Currently, we have designed and implemented a Java-based
toolset for integrated empirical software process improvement that
automates many of the analysis stage computations in the PSP, and which
extends the PSP paradigm with support for group review and patterns
\cite{Moore98}. We are currently using this toolset, called Leap, in a
software engineering course and will deploy it in an extensively
redesigned PSP-like curriculum in Fall 1999.  

\subsubsection{PSP research design}

We believe that the results of this case study have a number of
implications for current and future research on the PSP.

First, until questions raised by this study with respect to PSP data
quality are resolved, PSP data should not be used to evaluate the PSP
method itself. In other words, we believe that it is not yet appropriate to
assume that changes in PSP measures during (or after) a training course
accurately reflect changes in the underlying developer behavior.  A
statement such as ``The improvement in average defect levels for engineers
who complete the course is 58.0\%'', if based upon PSP data alone, might
only reflect a decreasing trend in defect recording, not a decreased trend
in the defects present in the work product.

Second, our research on the PSP has demonstrated that high quality pedagogical
design is not equivalent to high quality experimental design.  In other
words, some of the features of the PSP with respect to pedagogy are bugs
with respect to experimental design. 

One problem in the PSP with respect to experimental design concerns
uncontrolled instrumentation. The PSP programming exercises incrementally
build a set of tools for use in gathering and managing PSP data.  This is
elegant pedagogically, since it enables an instructor to use the PSP and
have the students build partial tool support for it as they go along.
Unfortunately, this is disastrous from an experimental design viewpoint,
since it means that crucial primary data measures (size) and derived
measures (size and time estimates) are calculated from a set of student
programs with no experimental control over their quality and accuracy.  We
know from bitter experience that writing a high quality size counting and
differencing tool for Java that handles all aspects of the language and
produces both a meaningful measure of size and differences in size between
two versions of a program is a nontrivial programming project. It
requires extensive design, implementation, and field use far beyond the 10
days available for this program in the PSP curriculum.  For the PSP
curriculum to be useful experimentally, there must be control over and
verification of the instrumentation.

Another problem in the PSP with respect to experimental design concerns the
lack of control over curriculum modifications.  For example, the SEI study
notes that ``there are many cases where instructors tailored the training
course (including selection of assignments, data collection requirements,
and sequence of introduction for process changes.)''  Our course also
deviates from the standard curriculum. 
  
Yet another problem in the manual PSP with respect to experimental design
concerns systematic bias in the data. For example, the PSP curriculum
requires, in an academic setting, a full semester course. In academic
settings, the workload on students tends to be light during the beginning
of the semester, become heavier after midterms, and reach a peak near the
end of the semester.  For PSP measures to be accurate, students must
maintain a consistent level of process data collection throughout the
course of the semester.  From our personal experience, we have observed
that a portion of the students in our PSP classes appear to begin to ``cut
corners'' in their recording of defects and time near the end of the semester,
presumably due to external pressures on their time and energies.  This
``end of semester crunch'' can introduce a systematic bias into PSP data,
leading to, for example, artificial decrease in defect density values
near the end of the course.

Another example of systematic bias can occur from what we term the ``process
overhead ceiling effect''.  Many students complain that the amount of
effort collecting and analyzing PSP process data can equal, exceed, or
interfere with the time and focus required to actually develop the
programs.  Early in the course, process overhead consists almost purely
of time and defect data collection, so students devote a great deal
of time and energy to that task.  By the end of the semester, the
total process overhead of the PSP has risen dramatically, since estimation,
time and schedule planning, and so forth have all been added. If 
at least some portion of the students decide to limit the amount of 
time spent on process collection and analysis, the most likely place
to cut corners is, once again, in defect recording, which would 
once again produce an artificial decrease in defect density values near the 
end of the course.

A final example of systematic bias occurs from the format of the manual PSP
forms themselves. As we note in our results, the case study students
frequently transferred a ``Total'' LOC value from one form to another
instead of the ``New and Changed'' LOC value.  Since the Total value is
always greater than ``New and Changed'', a systematic bias toward inflated
system sizes is present. We found other situations in which the design
of the forms lead to consistent user errors. 

From an experimental design standpoint, uncontrolled instrumentation and
systematic bias are threats to the internal and external validity of any
study which both uses the manual PSP and which draws conclusions about
underlying programmer behavior based purely upon the PSP data.  One example
of research suffering from these threats is the Software Engineering
Institute technical report by Hayes and Over \cite{Hayes97}.  The report
refers to collection of ``paper forms'', indicating the manual PSP.  There
is no mention of any control over the quality and accuracy of PSP
instrumentation, such as the size counter.  There is no mention of any
rigorous validation of the PSP data. Instead, the researchers simply claim
that ``the quality and accuracy of the data used in any given class tend to
be exceptional.''  Unfortunately, our case study shows that even an
accuracy of over 95\% in the PSP dataset is insufficient to obtain data
accurately reflecting underlying programmer behavior. Furthermore, our
original dataset is quite consistent in its outcome with the 
aggregate outcome reported by the SEI.  The research design presented
in their report cannot detect the data quality problems in our original
dataset, and so presumably cannot detect data quality problems present in
any of the datasets actually used in the study. Finally, although the
researchers subjected the PSP data to extensive statistical analysis, these
analyses all assume the absence of systematic bias in the dataset, an
assumption which we believe to be incorrect in the manual PSP.

We are happy to note that not all PSP evaluations are based upon PSP data
alone. For example, in one industrial case study, evidence for the utility
of the PSP method was based upon reductions in acceptance test defect
density for products subsequent to the introduction of PSP practices
\cite{Ferguson97}.  Although alternative explanations for this trend can be
hypothesized (such as the PSP-based projects were more simple than those
before and thus acceptance test defect density would have decreased
anyway), at least the evaluation measure is independent of the PSP data and
not subject to PSP data quality problems.

\subsubsection{Collection data quality and measurement dysfunction}
\label{measurement-dysfunction}

Unfortunately, integrated tool support is not a ``magic bullet'' that will
solve all PSP data quality problems.  As our simple model of PSP data
quality shows, no matter how perfectly we are able to automate the analysis
stage, overall PSP data quality will still depend largely upon the data
quality from the collection stage.

Our case study was able to detect substantial numbers of analysis errors
which could be eliminated through appropriate automation.  Our case study
was also able to detect the potential presence of substantial collection
errors, but the solution to this issue is much more complex.  It is
currently beyond the state of the art to accurately and completely automate
the collection of all primary process measures (time, size, defects) for a
programmer.  For the foreseeable future, we must rely on users of the PSP
to accurately and consistently record primary data values.  

In our research on the collection data quality problem, we have gained
insight from research on ``measurement dysfunction'' \cite{Austin96}.
According to Austin, whenever you measure an attribute of an organization
with the goal of improving the organization's performance, you run the risk
of worsening the organization's performance as a direct result of the
measurement.  This is because there are at least two uses to which a given
measurement can be applied: for information and for motivation.

Informational measurement ``tells about an organizational process... It is
used to learn from and to plan.''  In the PSP, all measures are intended to
be informational.

Motivational measurement, on the other hand, ``is used to quantify the
value of compensation for compliance with objectively verifiable standards
of work.'' In other words, motivational measurement is used to evaluate the
performance of individuals. In the PSP, no measures are meant to be
motivational.

Although this seems straightforward, a principal claim of Austin's research
is that any individual measure is "value-free" with respect to its
application: it can be used for informational purposes, motivational
purposes, or both.  Importantly, it is impossible for an organization to
guarantee that a measure, once collected, will never be used for
motivational purposes.  Thus, individuals in an organization may tend to
operate under the assumption that any measures of individual performance
can be used for motivational purposes, regardless of the stated intention
of the organization with respect to that measure at the time it is taken.

We find the measurement dysfunction perspective quite revealing with
respect to the PSP, because in any PSP academic or industrial training
situation, the ``organization'' collects the PSP measures from the
individual.  Even though competant PSP instructors always inform the
students that they will not be evaluated on the actual values of their PSP
data they collect, measurement dysfunction theory indicates that
individuals may still act under the assumption that they might at some
point become accountable for the values they submit.  As PSP data provides
very revealing and potentially dangerous information about a programmer's
practice, the appropriate PSP data to provide the organization for
motivational measurement may be quite different from the appropriate data
for personal, informational measurement.

We conjecture that collection stage data quality requires, at a minimum, a
combination of low collection overhead along with environmental features
that minimize the potential for measurement dysfunction. Overhead can be
reduced through tool support that makes manual recording of time, defect,
and size data fast and accurate.  Minimizing measurement dysfunction
requires, in essence, the property of privacy for PSP data---in other
words, that the organization does not and cannot have access to an
individual's PSP data.

Measurement dysfunction, unfortunately, introduces yet another obstacle to
the use of PSP data for experimental purposes.  In order to teach a PSP
course effectively, the instructor must inspect the PSP data submitted by
students.  However, this essential educational feature violates the privacy
of an individual's PSP dataset, an essential feature to minimize
measurement dysfunction.  The problem of measurement dysfunction, on top of
the problems cited earlier, lead us to question if collecting PSP data from
an educational setting is a fundamentally unsound approach to assessing
underlying programmer behavior. If this is true, we must redesign our
current paradigms for research using the PSP.







