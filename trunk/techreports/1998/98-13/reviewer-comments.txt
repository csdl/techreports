Greetings, 

My thanks to the reviewers for their efforts. Here are our replies to their
questions and comments:

* "First, I believe from internal evidence that the authors have not been
  through PSP Instructor training.  This has two implications: tool support 
  and quality control. Official instructors have had access to automated
  tools since at least 1995 (now available from the Addison-Wesley site)
  that address some of the issues the authors note.  Automation should be
  considered critical for addressing data collection problems, as the
  authors note."

The reviewer is correct that we have not been through PSP Instructor
training, and the paper will make that explicit.  However, I just checked
the Addison-Wesley site and the files available there are
the same ones we used and made available to our students. The excel
spreadsheets available there do not satisfy our requirements for
"integrated tool support", since they do not, for example, take care of the
computation of size data or automate selection of the appropriate PROBE
size estimation method.  As the reviewer notes, automation *should* be
considered critical, yet this is not yet clear within the PSP community.
For example, the same Addison-Wesley site containing the spreadsheets also
includes electronic copies of the manual forms, with instructions on how to
print them out and use them in the course.  

* "It is possible that 'quality control' of the instructors could have
  addressed some of the issues identified ...  It is impossible to tell
  whether QC was a significant factor ... Most of the papers cited used
  official PSP instructors."

This is true. However, the mere fact that an instructor has been through
the PSP Instructor training does not guarantee that they will follow the
recommended procedures to the letter once back in their own environment.
For example, the SEI technical report states, "There are many cases where
instructors tailored the training course (including selection of
assignments, data collection requirements, and sequence of introduction for
process changes.)"  Although I am sure I would have benefited to some
extent from "official" training, it is important to recognize that 
official instructors depart from the standard approach as well, and that 
these variations are not accounted for in the other studies. 

Second, our training materials were unofficially reviewed by a member
of the SEI, who evaluated them very positively and in fact noted that
my presentation was actually better than the standard materials in
several areas (size estimation, GQM).

* "Characterizing this as 'control' vs. 'experimental' data is an
  overstatement."...

Accepted.  I have changed this.

* Third, modifying the PSP curriculum from 5-7 processes will have an
  undetermined effect on second order learning... comparability with what
  the "official PSP" research suggests is confounded.

Agreed, but the "official PSP" research is *also* based upon various,
uncontrolled modifications, as the quote above from the SEI tech
report reveals.  The point is, there is no control over PSP curriculum
in any of the research, and it is unclear whom, if anyone, actually
teaches it exactly as documented in the textbook.

* "While the five chosen by the authors may be effective, they are not
  described in the paper, so reproducability is affected."

Good point, thanks. I will add a pointer to a website containing a
syllabus and other materials for those who want to know precise
details about my revisions.

* "Adding technical reviews is another PSP modification that may have
  introduced a behavior change."

True.  Not sure what to do with this one other than to note that it is a
variation from the standard curriculum.  

* "It appears that 159 of the 1539 errors were guesstimated.. It would be
  interesting to note how much the corrected data was massaged..."

As noted in the paper, in 159 cases, we applied a rule (such as to assume
the delta time was incorrect when the start, stop, and interrupt fields
were found to be inconsistent with the delta field).  These rules were only
applied in situations where we believed there was a strong likelihood that
the rule would in fact correct the data, although we could not guarantee
it.  In selected cases, we additionally performed a sensitivity analysis to
ensure that our choice of corrected value would not significantly impact
upon the results obtained.  We feel confident that these 159 corrections
were reasonable, but it is one more reason  why replication of this study
would be useful.

* "The paper implies that times were recorded in hours."

Sorry---times were recorded in minutes and we presented them in hours for
readability. We'll make that explicit.

* "The authors state that the corrected data should have led to the wrong
  conclusion ... yet the authors consider PSP to be one of the most
  powerful software engineering practices they have observed."

I guess this tells you what we think of the *other* software
engineering practices we have observed :-) :-) :-).  More seriously,
we still strongly believe in the "meta-level" practices and insights
of the PSP (such as the utility of recording time, size and defect
data), despite our severe reservations about the PSP "standard
curriculum".  By analogy, Simula was the first language to include
object-oriented constructs.  30 years later, object orientation is a
fundamental element of all modern languages, although none use the
exact language constructs employed by Simula.  We believe that a
similar evolution can and should occur in our community---recognition
of the seminal role of the PSP in bringing to light the potential
power of individual software engineering data collection and analysis,
but also recognition that we can  move forward with improvements to
make the techniques more powerful and more accessable to a wider
audience.

Philip Johnson














