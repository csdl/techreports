\section{Comparative Analysis}
\label{joint}

As discussed in Section~\ref{companal}, one characteristic of empirical
research is that the results of any single study must be viewed with
suspicion. A single study's specific results become accepted over time
through integration with additional experimental findings.

The simplest form of integration occurs when an experiment is replicated
without modification. In this case, the structure of the studies is similar
and the integration is relatively straightforward -- however, mistakes or
biases in the original design will permeate all the studies.  Partial
replication occurs in cases where an experimenter identifies a problem with
the initial experimental design and modifies it before proceeding.

The integration described in this paper is performed on two studies that
were designed and implemented independently.  In this case, special
attention must be paid to understanding the similarities and differences
between the two studies so that their results can be compared. We call this
process reconciliation. We used the following five step process to
reconcile our experiments.

\subsection{The Reconciliation Process}

\begin{enumerate}
  
\item {\em Standardize independent variables.} 
  
  The two experiments were initially designed with different independent
  variables. However, we found that that some of the independent
  variables from the UM study could be made more similar to those in the 
  UH study by holding out some data as well as regrouping some of the original 
  UM independent variables.
  
\item {\em Standardize dependent variables.}

  The two experiments also initially differed with respect to the original 
  dependent variables. Sometimes a dependent variable from one study 
  was missing in the other. In some cases we could generate the missing 
  dependent variable by re-analyzing the raw data. Other times, both 
  experiments referred to the same dependent variable but defined it 
  somewhat differently. In these cases, we attempted to reconcile the 
  two studies by developing a common definition and then re-analyzing 
  both sets of data. 

\item {\em Develop common hypotheses.}
  
  Once we established a set of common independent and dependent variables,
  it was relatively straightforward to define a set of common hypotheses.

\item {\em Analyze data separately.}
  
  Using the same statistical techniques employed in the original
  studies, we then tested our new common hypotheses. Despite the common
  variables, we felt that the presence of significant differences in the
  artifacts inspected, the subject population, and the data collection
  instruments precluded the combination of the raw data into a single data
  set. Instead, we kept the two data sets separate and analyzed them
  independently.

\item {\em Compare results.}
  
  Based on the preceding analysis, we looked for similarities and
  differences in the results of the two experiments. When appropriate, we
  augmented these findings with information taken from the individual studies.
        
\end{enumerate}

The next sections discuss the specific actions we took in each step
of the reconciliation process.


\subsection{Standardize Independent Variables}

Both studies had a primary independent variable: review method.  
In the UH experiment,
this variable had two values corresponding to two, one-phase review methods:
EGSM (real groups) and EIAM (nominal groups).  In the UM experiment, this
variable had three values corresponding to three, two-phase review methods:
detection followed by collection (DC), preparation followed by inspection
(PI), and detection followed by more detection (DD).

We created a standard independent variable for both studies by
re-characterizing the UM study in terms of the UH design's concept of real
and nominal groups.  As a result, the data from the first round of both the
DD and DC methods maps to nominal groups, and the data from the second
phase of the PI method maps to the real group. 


\subsection{ Standardize Dependent Variables}

The original UH study measured dependent variables concerning total
issues, total defects, effort, false positives, duplicates, and synergy.
The original UM study measured total issues and defects in the same way as
the UH study, but measured false positives differently and did not measure
effort, duplicates, or synergy.  In addition, the UM study measured gain
rates (the percentage of new faults found during the second phase), which
cannot occur in the UH experiment due to its one-phase design.

When we attempted to reconcile the false positive variable, we found that
differences in the artifact type (code vs. requirements
specification), made it hard to apply either of the original false 
positive definitions to the other experiment. Therefore, we reconciled this 
by redefining false positives in a way that would apply to both studies. 
This definition is ``all non-true defect issues are false positives.''

The duplicates variable used in the UH study could be generated by
reanalysis of the UM data. Finally, effort and synergy could not be 
reconstructed for the UM study,
and gain rates could not be reconstructed for the UH study, so those
variables are not considered in this comparative analysis.

        
\subsection{Develop Common Hypotheses}

Given the new common independent variable (review method, with two 
values: real groups and nominal groups) and the new common dependent
variables (issues, defects, false positives, and duplicates), we generated
the following common hypotheses. The combined hypotheses conform
to the current ``conventional wisdom'': that review meetings provide some
sort of synergy or catalyst that significantly improves review outcome or its
efficiency.

\begin{itemize}

\item C1: Real groups will find more significantly more defects than nominal groups. 

\item C2: Real groups will find significantly more issues than nominal groups.
  
\item C3: Real groups will produce significantly fewer false positives than
  nominal groups.

\item C4: Real groups will produce significantly fewer duplicates than nominal groups. 

\item C5: Real groups will find certain faults significantly more frequently
      than nominal groups.

\end{itemize}


\subsection{ Analyze Data Separately}


Up to this point, the reconciliation process has followed the
approach of meta-analysis.   However, we felt that
differences in the experimental design and
instrumentation of the two studies prohibited the next
step in meta-analysis, which would have been to combine the
reconciled data values. Instead, we chose to test the
hypotheses on each data set separately, and then present 
a  ``side-by-side'' comparison of the results.


\paragraph{Hypothesis C1: Real groups will find more significantly more defects than nominal groups.}

Figure~\ref{combCorr} shows the observed defect density of real and nominal
groups in both studies.  In both studies the observed defect density
for nominal groups was not statistically different from that of real
groups. Therefore, we cannot reject the null hypothesis and cannot conclude
that inspections with meetings find more defects than those without. As 
mentioned earlier, this does not necessarily mean that there isn't any
difference, but neither of these two studies can detect one.


\begin{figure*}[t]
\hrule
\vskip 0.5em
\centerline{\psfig{figure=combCorr.prn,height=2.5in,width=6in}}
\vskip 0.5em
\caption{{\bf Fault Detection Ratios by Independent Variable.} This figure shows the 
performances of real and nominal groups in both studies. We found no statistically
significant difference in either study (UH p=.34, UM p=.74).}
\vskip 0.5em
\hrule
\label{combCorr}
\end{figure*}


\paragraph{Hypothesis C2: Real groups will find significantly more issues than nominal groups.} 

The total number of issues raised by an inspection team is one measure of
the number of questions or concerns noted by the reviewers. Since no method 
found all defects, high numbers of issues might 
correlate to improved inspection effectiveness.  Therefore, an important
question is whether inspections with meetings raise more issues than
inspections without them.  Figure~\ref{combIss} shows the total number of
issues raised by real and nominal groups in both studies.  In both
studies, nominal groups generated significantly more issues than real groups did.
Therefore, we can reject hypothesis C2 and conclude instead that inspections
without meetings appear to raise more issues than those with meetings.

\begin{figure*}[t]
\hrule
\vskip 0.5em
\centerline{\psfig{figure=combIss.prn,height=2.5in,width=6in}}
\vskip 0.5em
\caption{{\bf Total Issues by Independent Variable.} This figure shows the 
number of issues raised by real and nominal groups in both studies. We 
found that nominal groups generated significantly more issues in both studies
(UH p $< .001$, UM p=.02).}
\vskip 0.5em
\hrule
\label{combIss}
\end{figure*}


\paragraph{Hypothesis C3: Real groups will produce significantly less false positives than nominal groups.} 

Even if inspections with meetings do not find more defects or raise more
issues than those without meetings, they may have other benefits.
Hypothesis C3 tests whether meetings help to filter out incorrect issues
(false positives). Since the work product's author would otherwise have to 
resolve them, this filtering may be beneficial in reducing rework effort.

Figure~\ref{combFp} shows the proportion of false positive issues to total
issues raised by nominal and real groups in both studies. We found that
real groups had a significantly smaller percentage of false positives in
both studies.  Therefore, we confirm hypothesis C3 and conclude that
inspections with meetings appear to do a better job at filtering out false
positives.


\begin{figure*}[t]
\hrule
\vskip 0.5em
\centerline{\psfig{figure=combFp.prn,height=2.5in,width=6in}}
\vskip 0.5em
\caption{{\bf False Positive Density by Independent Variable.} This figure shows the 
proportion of false positives to total issues for real and nominal groups in both 
studies. We found nominal groups generated significantly more false positives than 
real groups in both studies (UH p $< .001$, UM p=.02).}
\vskip 0.5em
\hrule
\label{combFp}
\end{figure*}

\paragraph{Hypothesis C4: Real groups will produce significantly less
duplicates than nominal groups. }

This hypothesis might seem tautological at first glance: since real groups
cannot produce duplicates by definition, they must produce less duplicates 
than the nominal groups!  However, this hypothesis is not
guaranteed to be true: reviewers working independently in nominal groups
might rarely overlap in the defects they discover.  In this case, although
real groups produce no duplicates by definition, the number of duplicates
produced by nominal groups would be so few as to not differ significantly
from real groups. Such a result would argue strongly in favor of nominal
groups, since it would imply that little additional effort would be
required in a nominal group-based process to weed out duplicates after
combining the results. 

However, the actual results show that nominal groups do produce significant
numbers of duplicates: an average of 37\% for the UH study, and 20\% for
the UM study.  Thus, the hypothesis that real groups will produce
significantly less duplicates than nominal groups (or, put another way,
that nominal groups will produce significantly more than 0 duplicates) is
confirmed.


\begin{figure*}[t]
\hrule
\vskip 0.5em
\centerline{\psfig{figure=combDup.prn,height=2.5in,width=6in}}
\vskip 0.5em
\caption{{\bf Duplicate Density by Independent Variable.} This figure shows the 
proportion of defects found by two or more members of a nominal group to the 
total defects found by the group (real groups have no duplicates). We found that nominal 
groups in UH had significantly more duplicates than those in the UM study (p $< .001$).}
\vskip 0.5em
\hrule
\label{combDup}
\end{figure*}

\paragraph{Hypothesis C5: Real groups will find certain faults significantly more frequently than nominal groups.}

As discussed in Section~\ref{um-experiment}, even if inspections with
meetings do not find more total defects than those without meetings, the synergy
of a meeting might potentially enable reviewers to find certain types of
defects more easily than they could working alone.  If these
``meeting-sensitive'' defects are quite expensive to detect otherwise, then
this capability alone might justify the use of review meetings.

To examine this hypothesis, we compared the detection probability for each
defect during inspections with and without meetings. Figure~\ref{combSpec} 
shows the results for the UM and UH studies.  In the UM study, 23 of 68
defects (33\%) were found more frequently using real groups than nominal
groups, and 9 of these (13\%) were found significantly more often using
meetings.  In the UH study, 35 of 87 defects (40\%) were found more
frequently with real groups, and 12 of these (13\%) were found
significantly more often using meetings.

These findings confirm the hypothesis: in both studies, real groups did
appear able to find certain faults more frequently than the nominal
groups.  However, these results should be viewed with some caution. 
The  significance test depends on the assumption that identifying each 
defect can be modeled as a Bernoulli trial and that our observations give
us a reasonable estimate of the detection probability for each fault.  Since
both these assumptions are questionable, we use these results only as an 
indicator that some defects are found more or less often than others.
It remains an open question as to whether classes of defects exist whose 
detection costs and importance alone justify the use of meetings. 

\begin{figure*}[t]
\hrule
\vskip 0.5em
\centerline{\psfig{figure=combSpFault.prn,height=3.5in,width=6in}}
\vskip 0.5em
\caption{{\bf Individual Defect Detection Probabilities for Inspections with and
without Meetings.} This figure shows the differences in defect detection probability
for all 87 defects in the UH study and for all 68 defects in the UM study. The 
interpretation of this diagram follows the explanation provided for 
Figure \protect\ref{specFaults}.}
\vskip 0.5em
\hrule
\label{combSpec}
\end{figure*}


\subsection{Comparison of Results}

The two studies, once reconciled and compared, reveal strikingly similar
findings for all of the major hypotheses, as well as some interesting
differences.

\subsubsection{Defect detection rates}

First, neither study showed that real groups find significantly more
defects than nominal groups (and neither study showed the converse: that
nominal groups find significantly more defects than real groups).  Instead,
Figure \ref{combCorr} shows that in each study, the two methods had very
similar defect detection rates.  This does not imply that the two methods
are {\em exactly} equal with respect to their defect detection
capabilities: it is quite possible that a well designed experiment with a
sufficiently large number of trials could detect a statistically
significant difference between the two methods. However, the results of
these two studies  cast strong doubt upon the prospect that a {\em substantial}
difference exists between inspections with and without meetings due to this factor
alone.  In other words, it appears extremely unlikely
that meeting-based review methods actually detect, for example, twice as many 
defects as nominal group methods (or vice-versa).  Instead, it seems likely that 
the presence or absence of meetings has little effect on review effectiveness,
and that other factors are more important. 

Figure \ref{combCorr} also reveals an interesting difference between the
two studies: the average defect detection rate for the UH study was
approximately 45\%, while the average defect detection rate for the UM
study was around 30\%.  We conjecture that this difference in magnitude
results from two differences in the design of the studies.  In the
UH study, students reviewed source code for the implementation of a program
they had just completed, and so they were intimately familiar with both
the problem domain (Employee databases or Two pass assemblers) and the
representation (C or C++). In the UM study, students reviewed two formal
requirements specification documents.  Compared to the UH students, the UM
students were neither as familiar with the problem domain (automobile
cruise control and water level management) nor the representation (SCR
tabular requirements notation).  Perhaps the greater defect detection rate
observed in the UH study results from the increased familiarity of the 
UH students with the problem domain and representation. 

Finally, the average defect detection rates for both studies fall
substantially below the rates often provided for review techniques in the
literature (such as 60-80\% defect detection effectiveness).  There are
many possible explanations for this difference, but the existence of this
difference suggests that more research on the actual defect detection
effectiveness of review should be conducted.

\subsubsection{True and False Issue Generation}

Both studies found that nominal groups generated significantly more issues
than real groups.  In fact, Figure \ref{combIss} shows that  nominal groups 
generated about twice as many issues as real groups. 

Since the defect detection effectiveness of real and nominal groups was 
almost equal, it follows that differences in issue generation were
offset by inverse differences in false positive density. Figure 
\ref{combFp} illustrates that, in both the UM and UH studies, nominal groups 
generated substantially more false positives than real groups.  

Once again, however, the two studies differed in the magnitude of these
two variables. For example, the UH nominal groups generated an average of
around 35 defects, while the UM nominal groups generated an average of over
50 defects. The UH real groups generated an average of around 15 defects,
while the UM real groups averaged over 20.  In other words, the UM groups
generally outperformed the UH groups with respect to issue generation.
This trend continues with respect to false positive generation, where the
UM groups again ``outperformed'' the UH groups with higher levels of false
positives.

We conjecture that these differences in magnitude may be attributable
to differences in the artifact type used in the two studies. The UM
students reviewed a requirements document, for which the concept of
a defect is more nebulous than for source code, the material reviewed
by the UH students. Thus, the greater ambiguity in what constitutes
a defect for requirements documents could explain why the UM students
generated both more issues (it is easier to generate plausible defects
when the nature of a defect can be more broadly construed) and more
false positives (it is easier to generate non-defects, for exactly the
same reason) than the UH students. 

\subsubsection{Duplicate Density}

Both studies again agreed that nominal groups generate significant amounts 
of duplicate defects, as shown in Figure \ref{combDup}.  

As might be expected by now, the two studies showed significant differences
with respect to the magnitude of duplication.  Interestingly, this
is consistent with capture-recapture techniques ~\cite{eick.et.al.1992} for 
estimating the number of defects remaining in a previously inspected artifact. 
This research says that, in general, the amount of overlap in N independent 
reviews will
increase as the observed defect density increases.  The teams in the UH
study found a greater percentage of defects than did the teams in the UM
study and, as predicted by the capture-recapture research, the amount of
duplication increased as well.

\subsubsection{Meeting-sensitive defects}

Finally, both studies found instances of defects that were found
significantly more frequently by real groups than nominal groups.  
Although this confirms the hypothesis, both studies only detected 
a small number of these defects and, therefore, the implications of
these findings are unclear. Our initial examination of the 
meeting-sensitive defects did not immediately suggest their defining 
characteristics. We believe that this is an important direction for 
future research.

