\section{RELATED WORK} 


Reviewing software is as old as programming itself. However, 
the first structured, measurement-based review process was 
Michael Fagan's five-step Inspection method \cite{Fagan76}. 

\begin{itemize}
\item {\em Overview:} the author presents an overview of the scope and
purpose of the work product.

\item {\em Preparation:} reviewers analyze the work product with the goal
of understanding it thoroughly. 

\item {\em Inspection meeting:} the inspection team assembles and the 
reader paraphrases the work product. Reviewers raise issues that are 
subsequently recorded by the scribe. 

\item {\em Rework:} the author revises the work product. 

\item {\em Followup:} The moderator verifies the quality of rework and 
decides if reinspection is required. 

\end{itemize}

\subsection{Review Meetings}

Two aspects of Fagan Inspection are especially relevant to determining 
meeting effectiveness in formal technical review. First, the goal
of the preparation phase is to thoroughly understand the work product's 
``intent and logic'', {\em not} to identify defects. Only during the 
Inspection meeting does defect identification become an
explicit goal.  Fagan notes that ``sometimes flagrant errors are found
during [preparation], but in general, the number of errors found is not
nearly as high as in the [inspection meeting]'' \cite{Fagan76}.  Second,
the Inspection meeting involves a specific technique, paraphrasing, which
generates an in-depth analysis of the entire document in real-time by the
review team during the meeting.

These two factors, the preparation goal and the meeting technique, have
been manipulated extensively in the design of new FTR methods by other
researchers and practitioners.  One common modification is to introduce
defect detection as an explicit goal in preparation. In these cases, the
reviewers note defects on a preparation form or on the work product itself
prior to the inspection meeting. In this case, reviewers have two preparation 
goals: comprehending the work product and detecting defects.

A second modification is to change the meeting technique from paraphrasing
to defect collection \cite{Gilb93,Humphrey90}.  This shifts the focus of the
meeting away from the work product and onto the issues raised during
preparation. 

The Active Design Review technique \cite{parnas.1985} invented by David Parnas
and David Weiss makes even more radical modifications to the preparation goals 
and meeting technique. Active Design Reviews were designed to address three perceived
weaknesses in existing methods:

\begin{itemize}
\item If reviewers do not adequately comprehend the document, then they are
      unlikely to discover the important defects. 
\item Each reviewer should have a specialized area of concern to minimize 
      overlap and maximize coverage of the work product.
\item A meeting of the whole group is unnecessary for defect collection. 
\end{itemize}

Active Design Reviews address these issues by requiring reviewers to fill
out individually customized questionnaires during preparation that assess
their comprehension of the work product and point them toward areas prone
to defects.  The group meeting is eliminated. Instead, the author meets
with each reviewer individually to go over their questionnaires and gather
feedback on the work product. Parnas and Weiss deployed this method for the 
design of a military flight navigation system with favorable results, although he
did not report any quantitative measures of effectiveness.

Larry Votta built upon Parnas' research in a study of Lucent (formerly 
AT\&T) developers \cite{votta.1993}.  He collected data on the 
perceived utility of meetings by developers as well as 
several statistics on their outcome. His
data showed that within the development environment studied, scheduling
conflicts appeared to lengthen the total time of an inspection by
approximately 30\%.  Furthermore, he was unable to demonstrate the presence
of ``synergy'' within the inspection meetings. A related study by Votta and 
others found that 90\% of
the defects were found during the preparation phase, leaving only 10\%
discovered during the meeting \cite{eick.et.al.1992}.  These results appear to
support Parnas and Weiss' claim that whole group meetings are unnecessary for defect
collection.

Parnas and Weiss' claim and Votta's results stand in direct contradiction 
to those of Fagan.  While Fagan observed that many more errors are found at the
inspection meeting, Votta and his colleagues observed the opposite. 
This conflict might be caused by differences in the goals
and techniques for preparation and meeting between the two methods.  In
Fagan Inspection, the objective of preparation is comprehension, and defect
discovery does not become an explicit goal until the Inspection meeting. In
both Active Design Reviews and the Inspection method as practiced by development
groups at Lucent, the objective of preparation is both comprehension and
defect discovery. Defect collection, not discovery, is the primary goal
of the Inspection meeting.  Given these different goals, it is not
surprising that Fagan found the Inspection meeting so productive for defect
discovery, while Votta et al.~did not.

Thus, while Votta's work does provide evidence that whole group meetings
may not be necessary for an Inspection method whose meeting goal involves
defect collection, it does not provide evidence that meetings are not
necessary for an Inspection method whose meeting goal is defect discovery
through paraphrasing. Furthermore, Fagan asserts that ``a team is most
effective if it operates with only one objective at a time.''  If Fagan is
right, then perhaps mixing comprehension and defect discovery during
preparation by Lucent developers decreases review effectiveness.

To provide insight into the issues surrounding review preparation and
meetings, two research groups, one at the 
University of Hawaii and one at the University of Maryland, independently
designed and conducted controlled experimental studies. Their goal was 
to assess the effect of team meetings on defect detection effectiveness 
and to determine whether superior alternatives exist.  This article 
resulted from our recognition that the two studies were similar enough 
in motivation to attempt comparative analysis.

\subsection{Comparative Analysis}
\label{companal}

We believe that no single study gives unequivocal results.
Therefore, it is imperative that the research community try 
to integrate and compare
studies that address a common hypothesis. This is the only way to gain 
confidence that empirical results are real and not just due to random 
variation. However, integrating multiple studies in a credible way isn't
simple. In this case, the two studies address the same issue, but they were 
conceived and executed independently. Thus, direct comparison of the results
is impossible because the studies differ considerably in their designs, 
instrumentation, subject population, and analysis methods.
 
A classic approach to understanding what several studies say about 
some phenomenon is to conduct a literature review, qualitatively 
summarize existing results, and manually synthesize them. The drawback 
of this approach is that it lacks precise methods for combining 
different results. 

A statistical approach for integrating multiple studies is called 
meta-analysis~\cite{glass}. This approach has two steps. First, the 
experimenters attempt to reconcile the primary experiments -- i.e define 
a common framework with which to compare different studies.  This involves
defining common terms, hypotheses, and metrics, and characterizing key 
differences.  Next, the data from the primary experiments are transformed 
or recalculated according to agreed upon definitions. In the second step
the transformed primary data is combined and reanalyzed.
Unfortunately, it is not always clear when meta-analysis is appropriate,  
what statistical models should be used, or when it is acceptable to 
combine data from disparate sources. 

Our approach for comparing these two experiments falls between the classic 
approach and meta-analysis.  As with meta-analysis, we reconciled the two 
experiments, but we did {\bf not} combine any of their data.  We found 
that the reconciliation process highlighted many of the similarities 
and differences between the two experiments, allowing us to better 
understand what data were comparable and which were not. We discuss the 
specific steps used in this reconciliation process in Section~\ref{joint}.

The following two sections describe both experiments, 
their designs, analysis, and results. Subsequent sections
describe the reconciliation of the experiments and the  
comparative analysis of their results.
