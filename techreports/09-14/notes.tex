\chapter{notes}

According to Chester Barnard \cite{citeulike:1414171} intuition is the most appropriate 
cognitive tool when used in working with short time horizons and with data which is either 
poor quality or very limited. He is advocating, that intuition is not only an efficient 
tool but it provides sufficient support in the situations where the logical reasoning 
cannot be applied - ``... to material that is so insecure that it cannot bear the weight 
of ponderous logic... '' 
While he is being overoptimistic in the saying that ``Our logical methods and our endless 
analysis of things has often blinded us to an appreciation of structure and organization...''
And finally he says: ``To understand the society you live in, you must feel organization - 
which is exactly what you do with your non-logical minds...'' (\cite{citeulike:1414171}, p. 317)

Barnard's work was succeeded by Herbert Simon, who being a scientists took a somewhat 
different approach - he worked on understanding of the intuition phenomena. At the beginning 
he followed Barnard's mysterious intuition concept, but after all the advances in artificial
intelligence and cognitive sciences, he conducted that intuition is a subconscious pattern 
recognition. Thus, it is a rational but not concious analytical method appropriate for decision 
making \cite{citeulike:6708618}. 

Maybe in the software development process (or software process enactment) it is impossible 
to apply sort of \textsl{satisficing} \cite{citeulike:10055914} - ... maybe?



citeulike:10055684

The Institute of Electrical and Electronics Engineers defines software engineering as 
“the application of a systematic, disciplined, quantifiable approach to development, 
operation, and maintenance of software; that is, the application of engineering software”
IEEE Standard Computer Dictionary, Institute
of Electrical and Electronics Engineers, New
York, 1990.

"There are two ways of constructing a software design. One way is to make it so simple 
that there are obviously no deficiencies. And the other way is to make it so complicated that
there are no obvious deficiencies."
- C.A.R. Hoare

``Based on our PSP and TSP experience, software engineers can be taught new methods and
convinced to use them. The courses are challenging and require a lot of work but the 
resulting benefits are substantial. The graduate or senior-level undergraduate PSP 
course must be approached more as an experience than an intellectual exercise. 
Merely explaining the methods will produce little or no meaningful benefits. 
The students must work through the course exercises, measure their work, and 
analyze their data to see improved performance. When they do this, the course works. 
If they just read the text or listen to lectures, they are generally wasting their 
own and the professor's time.''
Why Don't They Practice What We Preach?
Watts S. Humphrey

This approach to explaining things around us dates back at least to Epicurus
(342?-270?BC) (Li 1993, p. 274). Let’s consider theory formulation in science as the
process of obtaining a compact description of past observations together with future ones.
Let us suggest that the preliminary data of an investigator, the hypothesis proposed, the
experimental design and setups, the trials performed, the outcomes obtained, the new
hypothesis formulated, etc., can be encoded as an initial segment of an infinite binary
sequence. The investigator obtains increasingly longer initial segments of an infinite
binary sequence by performing more and more experiments. To describe the underlying
regularity in the sequence, the investigator tries to formulate a theory that governs the
sequence based on the outcome of past experiments. Candidate theories or hypotheses
are identified from the sequences starting with the observation of the initial segment.
There are many different possible infinite sequences or histories on which the
investigator can embark. The phenomenon the investigator is trying to understand or the
strategy used can be stochastic. In this type of view, a phenomenon can be identified
with a measure, i.e. probability distribution, on a continuous sample space.
This research attempts to express the task of learning a certain concept in terms of
sequences over a basic alphabet. We express what we know as a finite sequence over the
alphabet. An experiment to acquire more knowledge is encoded as a sequence over the
alphabet, the outcome is encoded over the alphabet, new experiments are encoded over
the alphabet, and so on. This way we can view a concept as a probability distribution
(measure) over a sample space of all one way infinite binary sequences. Each sequence
corresponds to one never ending sequential history of conjectures, refutations, and
confirmations. The distribution can be said to be the concept of phenomenon involved.
We can predict what is likely to turn up next with an initial segment. Using Bayesian
analysis (Bayes 1763) to compute the conditional probability, we can predict and
extrapolate future outcomes. This is the general thrust of this research.

There are many studies about the proper formulation for learning curves for
different problem sets. The majority of the learning curve models indicate that the time
to perform a task decreases with the number of times a task has been performed. This is
covered extensively in the literature. A review of the relevant historical studies is shown
in Chapter II (in 10. Learning Curves, p90). Chapter III develops the learning curve
formulations used in this research (Appendix G Learning Curve, p443).