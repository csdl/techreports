% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{fixltx2e} % Fixing numbering problem when using figure/table* 
\usepackage{subfig}
\usepackage{tabularx,ragged2e,booktabs}
\usepackage{todonotes}
%
\newcolumntype{C}[1]{>{\Centering}m{#1}}
\renewcommand\tabularxcolumn[1]{C{#1}}
\setlength{\parindent}{10pt}
\renewcommand{\tablename}{\bf Table}

%
\usepackage{makeidx}  % allows for indexgeneration
%
\begin{document}
%
\mainmatter              % start of the contributions
%
\title{Time Series Classification Using SAX Representation and Vector Space Model.}
%
\titlerunning{Timeseries features discovery with SAX and TF-IDF weighting}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Pavel Senin\inst{1}
\and Sergey Malinchik\inst{2}
}
%
\authorrunning{Pavel Senin} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
%\tocauthor{Ivar Ekeland, Roger Temam, Jeffrey Dean, David Grove,
%Craig Chambers, Kim B. Bruce, and Elisa Bertino}
%
\institute{Collaborative Software Development Laboratory,\\
Information and Computer Sciences Department,\\
University of Hawaii at Manoa,\\ Honolulu HI 96822, USA,\\
\email{senin@hawaii.edu}
\and
Lockheed Martin Advanced Technology Laboratories,\\
Executive Campus, Suite 600, Cherry Hill, NJ 08002, USA,\\
\email{sergey.b.malinchik@lmco.com}}


\maketitle              % typeset the title of the contribution

\begin{abstract}
Ability to discover characteristic patterns in time series paves the road for many
downstream analyses while enabling interpretability of results. 
In this paper we propose a novel method for time series features discovery based on two existing
techniques - Symbolic Aggregate approXimation and Vector space model - SAX-VSM. 
This method is capable to automatically discover and rank time series features by their
“importance” to the class, which not only creates well-performing classifiers, but, in turn,
provides interpretable class generalization and facilitates clustering. The accuracy of this
technique, as shown  through our experimental evaluation, is at the level of the current state of
the art.  
While being relatively computationally expensive within a learning phase, our method provides fast,
precise, and interpretable classification.
\keywords{Knowledge discovery, Algorithms, Experimentation}
\end{abstract}
%
\section{Introduction}
%
Time series classification is an increasingly popular area of the research. Within last decades,
many time series representations and similarity measures were proposed. These, usually, can be
divided in two major categories. The first category of classification techniques is based on
the shape-based similarity metrics - where distance is measured directly between time series
points. Classic examples of methods from this category are nearest neighbor (k-NN) classifiers
built upon Euclidean distance or DTW \cite{1NN}. The second category consist of classification
techniques based on the structural similarity metrics involving some high-level representations
of time series. Examples from this category include Discrete Fourier Transform based classifier
\cite{DFT}, or SpADe \cite{spade}. 

The existence of these two categories can be explained by differences in the performance of these
techniques. While shape-based similarity methods virtually unbeatable on short, often pre-processed,
time series data \cite{benchmark}, they usually fail on long and noisy data sets \cite{indexing}
where feature-based techniques demonstrate superior performance. Also, feature-based methods
require less storage and usually have faster classification time, thus, they often implemented in
industrial settings. 

As one of the possible alternatives, recently, the time series shapelets were introduced 
\cite{shapelet} and gained popularity. A shapelet is a short time series ``snippet'' that is a 
representative of class membership. Thus, potentially, this approach combines the best of two
categories - superior performance of shape-based similarity methods, and the high-throughput 
capacity of feature-based methods\cite{logical}. While demonstrating a superior interpretability, 
robustness, and similar to kNN algorithms performance, shapelets-based approaches are quite 
time-consuming - which makes their adoption to many-classes problems difficult. 

As per current state of the art, it worth noting, that despite to recent development, to date, the
best overall algorithm in the field is the simple nearest neighbor classifier, which is accurate,
and robust. Moreover it depends on a very few parameters \cite{comparison}
\cite{classifiers} \cite{benchmark}. However, while being virtually unbeatable in performance,
k-NN techniques have a number of disadvantages, where the major is that they do not offer any 
insight into the data.

In this work, we propose yet another alternative to 1NN algorithm. Similarly to shapelets, our
technique rests on finding time series subsequences that are representatives of classes. However,
instead of recursive search for shapelets one-by-one, our algorithm finds and weights by
``importance'' all potential candidate subsequences at once. This, in turn, enables a discovery of
characteristic patterns.

\section{Background}
Our methodology is based on two well-known techniques. The first technique is Symbolic Aggregate
approXimation \cite{sax}, which is a high-level symbolic representation of time series
data. The second technique is a well known in Information Retrieval workflows Vector Space 
model \cite{salton}. By utilizing SAX, our algorithm, at first, transforms time series into collections of 
SAX words. At the following step, it utilizes \textit{tf$\ast$idf} terms weighting for classifier 
construction. Finally, it uses Cosine similarity metric for classification.

SAX algorithm, however, requires three parameters to be provided as input, and there is no efficient 
solution for parameters selection exists as per today. To solve this problem, we employ a global 
optimization scheme that converge relatively quickly and yield a deterministic optimized solution. 
The solution is based on the divided rectangles (DIRECT) algorithm \cite{direct} which is a 
derivative-free global optimization scheme that reaches a deterministic solution and does not require 
selecting values for any parameters. In addition, DIRECT has the added benefit of possessing both 
local and global-optimization properties. 

\subsection{Symbolic Aggregate approXimation (SAX)}
Symbolic representation of time series, once introduced \cite{sax}, has attracted much attention by
enabling application of numerous string-processing algorithms, bioinformatics and text data mining 
tools to temporal data. This method provides significant reduction of the dimensionality and
a low-bounding to Euclidean distance metrics, which guarantees no false-dismissal  \cite{hot_sax}.

Algorithm consists of following steps: at the first step, the time series (or a sub-series) are normalized
by energy \cite{goldin_kanellakis}. Next, the dimensionality of the time series is reduced by using 
of PAA \cite{paa}. In PAA, times series is divided into equally sized segments, and the mean values 
of the points within segments, once computed, constitute a lower dimensional vector - which 
represents the original time series. Finally, in the third step of SAX algorithm, the PAA representation 
of the time series is discretized into the string by use of alphabet lookup tables. The construction of 
such tables rests upon the assumption that normalized time series tend to have have Gaussian 
distribution \cite{larsen_marx}, thus, by determining the breakpoints under Gaussian curve one 
can obtain equally sized areas under the Gaussian curve used for chosen SAX alphabet size.

Note, that in further SAX development \cite{hot_sax} for application to problems of discords
and motifs discovery, streams analyses and clustering \cite{streaming_sax}, authors proposed a 
sampling strategy and a distance function specifically designed in order to avoid trivial and 
degenerative solutions. In our work we use this combination in \textit{CLASSIC} sampling strategy.

\subsection{Bag of words, corpus}
``Bag of words'' is a widely used term in Information Retrieval (IR) for abbreviation of an
unordered collection of words (or terms). Although, similar definitions, such as \textit{``bag of
features''} \cite{bag_features} or \textit{``bag of patterns''} \cite{bag_patterns} were previously
proposed for techniques built upon SAX, we will use ``bag of words'' since it clearly reflects our
workflow. 

``Corpus'' (text corpus) is another IR term which is usually used for structured set of texts or
bags of words. Here, we will use this term in a very similar way - for abbreviating an organized
collection of bags of words. 

\subsection{Vector Space Model (VSM)}
We use Vector space model exactly as it is known in information retrieval - for disambiguating class
entities \cite{salton}. Bags of words, representing each of the training classes, are converted into
the \textit{vectors}, whose dimensions correspond to separate “terms” - observed in a corpus SAX
words. 

To construct these vectors, we employ the \textit{tf$\ast$idf} weighting scheme for terms which is
defined as a product of two factors: term frequency (\textit{tf}) and inverse document frequency
(\textit{idf}). For the first factor we use logarithmically scaled term frequency:
\begin{equation}
 \mbox{tf}_{t, d} =  \begin{cases} \log(1 + \mbox{f}_{t,d}), &\mbox{if f}_{t,d}>0  \\
0, & \mbox{otherwise} \end{cases}
\end{equation} 
where $t$ is the term and $d$ is a bag of words (document).

The inverse document frequency we compute as usual:
\begin{equation}
 \mbox{idf}_{t, D} =  \log_{10}\frac{|D|}{|d \in D : t \in d|} = \log_{10}\frac{N}{\mbox{df}_{t}}
\end{equation} 
where $N$ is the cardinality of corpus $D$ and the denominator $df_{t}$ is a number of documents
where the term $t$ appears.

Then, $\textit{tf$\ast$idf}$ value for a term $t$ in the document $d$ of a corpus $D$ is defined as 
\begin{equation}
 \mbox{tf * idf}(t, d, D) =  \mbox{tf}_{t, d} \times \mbox{idf}_{t, D} = \log(1 + \mbox{f}_{t,d})
\times \log_{10}\frac{N}{\mbox{df}_{t}}
\end{equation} 
for the all cases where $\mbox{f}_{t,d}>0$ and $\mbox{df}_{t}>0$ or zero otherwise.

\subsection{Cosine similarity}
Cosine similarity is a similarity measure between two vectors based on the inner product space
and is the cosine value of an angle between them. For two vectors $a$ and $b$ that is:
\begin{equation}
 \mbox{similarity}(a,b) = cos(\theta) = \frac{ \sum\limits^{n}_{i=1} a_{i} \times b_{i} }{
\sqrt{\sum\limits^{n}_{i=1} a_{i}} \times \sqrt{\sum\limits^{n}_{i=1} b_{i}} }
\end{equation} 


\section{Our classification algorithm: SAX-VSM}
Our classification algorithm is similar to a common, two-phase IR text by query ranking workflow
based on the text corpus. 

The training of SAX-VSM classifier is relatively computationally expensive 
(Figure \ref{fig:precision-runtime}), because it involves a construction of text corpus by extraction 
of SAX words from all labeled series, and its post-processing with VSM. However, there is no need 
to maintain an index of training series or to keep all of them in memory: the algorithm simply iterates 
over all labeled series, incrementally building bags of SAX words for each of training classes.
Once built, the corpus is processed with \textit{tf$\ast$idf} and also discarded. Only the set of $N$ 
real-valued vectors (where $N$ is the number of classes) is retained after training.

The classification procedure itself is fast and consists of converting of an unknown time series
into a vector of SAX words frequencies, and its classification by computation of $N$ cosine values.
The unknown time-series is assigned to the class with which cosine value is maximal 
(i.e. the angle is minimal).

\begin{figure}[H]
   \centering
   \includegraphics[width=120mm]{figures/overview.eps}
   \caption{An overview of algorithm workflow: at first, known labeled series are converted into
bags of words using SAX; secondly, \textit{tf$\ast$idf} statistics is computed. Unknown series are
classified by the label of the \textit{tf$\ast$idf} weights vector having the minimal angle (maximal
cosine value) with the words frequency vector build by SAX application to the series.}
   \label{fig:overview}
\end{figure}

\subsection{Training phase}
In order to employ Information Retrieval techniques, we transform all training instances from a
single TRAIN class into the single bag of words. For this, our algorithm converts a real valued time
series into SAX string representation configured by five parameters: the sliding window length (w),
the number of PAA frames per window (p), the SAX alphabet size (a), and by the complexity reduction
strategy (s). Note, that each subseries, extracted with sliding window is normalized by energy
before being processed with PAA \cite{goldin_kanellakis}. If, however, the standard deviation is
below a fixed epsilon, similarly to SAX approach, we do not apply normalization
\cite{sax}. Further we shall discuss the parameters selection procedure in the following section.

By applying this procedure to all $N$ training classes, we build a corpus of $N$ bags (or documents
in
IR terms), which in turn we process with \textit{tf$\ast$idf}. This provides $N$ vectors, each of
which is a characteristic representative of a class. \textit{tf$\ast$idf} statistics naturally
``highlights’’ class terms bearing unique to the class features (in our context - curves) by
assigning higher weight. Contrary, it ``cancels’’ those terms, which present in the majority of
classes by assigning low weights. Note however, that this weights are not binary (need to refer to
shapelets?), the range of their values is continuous, facilitating smooth boundaries between
classes, which allows to define outliers (i.e. series which are not representatives of any of known
classes).

Intuitively, \textit{tf$\ast$idf} statistic ``refines’’ frequency-based vector space model by
elevating weight coefficients for terms which are true representative of the class, while lowering
weights for terms tend to ``confuse’’ a classier.

Once N vectors are weighted with \textit{tf$\ast$idf}, they are normalized and used in the
classification procedure.
\begin{figure}[tbp]
   \centering
   \includegraphics[width=125mm]{figures/precision-runtime.ps}
   \caption{Comparison of precision and runtime of SAX-VSM and 1NN Euclidean classifier on
CBF data: SAX-VSM performs much better with limited amount of training samples at relatively high
cost while with the growth of labeled data, both classifiers demonstrate similar performance.
SAX-VSM is much faster in classification and comparable with 1NN classifier cost when training time
is accounted.}
   \label{fig:precision-runtime}
\end{figure}

\subsection{Classification phase}
Similarly to training phase, in order to classify an unknown time-series, we transform it into the
terms vector using the same SAX parameters which we have used in training phase. Then, we compute
cosine values between this vector and those built in the training phase - which represent $N$ known
classes. The series is then assigned to the class whose vector yields the maximal cosine similarity
value.

\section{Results}
We have proposed a novel algorithm for time series classification based on the SAX
representation of time series and Vector space model called SAX-VSM. Here we present a range of
experiments assessing its performance for classification and clustering.

\section{Experimental evaluation}
To evaluate our approach, we selected XX data sets from the UCR time series repository.
We selected these particular data sets with intention to evaluate the performance of our approach
not only against 1NN Euclidean and DTW classifiers, but recently proposed shapelet decision trees
and a shapelet transform \cite{bagnal}.
We used train/test split and all reported results are testing accuracy. All SAX parsing parameters
and the bag construction strategy were found exclusively on a training set in leave one out 
cross-validation fashion. The test set was used only once with the final set of parameters. 
Our algorithm implementation is publicly available at \cite{jmotif}.

{\scriptsize
\begin{table}%
\caption{\bf Classifiers error rates comparison.
 \label{tab:perf_table1}}
\centering
\begin{tabularx}{\linewidth}{@{} l *5X @{}}\toprule[1.5pt]
\bf Dataset &\bf 1NN-Euclidean &\bf 1NN-DTW &\bf Shapelet Tree &\bf  Shapelet SVM &\bf 
SAX-VSM\\\midrule
%\bf Variable Name & \bf Regression 1 & \bf Mean & \bf Std. Dev & \bf Min & \bf Max\\\midrule
%text        &  text     & text      &  text     &  text     &text\\
%\bottomrule[1.25pt]
%\end {tabularx}
%\begin{tabular}[h]{  l | c | c | c | c |  c  }
%\hline
%Dataset           & 1NN-Euclidean  & 1NN-DTW       & Shapelet Tree & Shapelet SVM & SAX-VSM \\
%\hline
Adiac             & 0.389   & 0.396           & 0.700        & 0.762         & \textbf{0.381}\\
Beef              & 0.467   & 0.467           & 0.500        & 0.133         & \textbf{0.033}\\
ChlorineConcentration  & 0.350 & 0.350        & 0.412        & 0.439         & \textbf{0.332} \\
Coffee            & 0.250   & 0.180           & 0.036     & \textbf{0.0}     & \textbf{0.0} \\
ElectricDevices   & 0.913   & 0.913           & 0.451     & 0.756            & \textbf{0.329} \\
FaceFour          & 0.216   & 0.170           & 0.159     & 0.023            & \textbf{0.0} \\
Gun Point         & 0.087   & 0.093           & 0.107     & \textbf{0.0}     & 0.007 \\
Lightning7        & 0.425   & \textbf{0.274}  & 0.507     & 0.314            & 0.301 \\
SonyAIBO          & 0.306   & 0.274           & 0.155     & \textbf{0.133}   & 0.176 \\
SyntheticControl  & 0.120   & \textbf{0.007}  & 0.057     & 0.127            & 0.013 \\
Trace             & 0.240   & \textbf{0.0}    & 0.020     & 0.020            & \textbf{0.0} \\
ECG               & 0.120   & 0.230           & 0.149     & \textbf{0.007}   & 0.09 \\
\bottomrule[1.25pt]
\end{tabularx}
\end{table}
%\hline
%\end{tabular}
}

\subsection{Exploratory analysis}

\subsection{Gun Point data}
We use a well-studied \textit{Gun Point} dataset to show the interpretability of real-life
classification results. The \textit{Gun Point} data contains time series of an actor performing the
motion of drawing a gun, and the classification problem is to determine whether or not she were
holding a gun or imitated the action (the \textit{Gun/NoGun problem}). In \cite{shapelet}, they
identified that the best shapelet for classification was when the actor lowered her arm without a
gun - the ``overshoot'' phenomena causing the dip in the time series. In \cite{bagnal}, when
reporting 10 best shapelets for a similar problem, they confirmed the phenomena with 7 shapelets,
adding 3 shapelets reflecting arm rising motion with a gun. Both shapelets clusters are confirmed by
our approach as shown at the Figure \ref{fig:shapelet-like-patterns}, moreover, in the \textit{Gun}
set, our approach also highlighted the upward arm motion.

\begin{figure}[tbp]
   \centering
   \includegraphics[width=150mm]{figures/shapelet-patterns.ps}
   \caption{Examples of two best weighted sub-series from each class of Gun Point dataset. 
   Note, the upward arm motion is more ``important'' for a Gun class, whether downward arm motion
for Point class. These result align with previous work \cite{shapelet} and \cite{bagnal} in which 
similar locations were reported. Second to best patterns outline the the differences in aiming
(Gun) and smooth ``propless'' hand movement in Point class.
   }
   \label{fig:shapelet-like-patterns}
\end{figure}


\subsection{OSU Leaf data}
According to the original data source, Ashid Grandhi, with the current growth of digitized data,
there is a huge demand for automatic management and retrieval of various images. The \textit{OSU
Leaf} dataset consist of curves obtained by color image segmentation and boundary extraction (in the
anti-clockwise direction) from digitized leaf images of six classes: \textit{Acer Circinatum, Acer
Glabrum, Acer Macrophyllum, Acer Negundo, Quercus Garryana and Quercus Kelloggii}.

While the authors of the original work were able to solve the problem of leaf boundary curves
classification by use of DTW achieving 60\% accuracy, as we pointed above, DTW provides a very
little information about why it succeeds of fails. In contrast, by using SAX-VSM we were able to
discover a number of class-specific characteristic patterns for each of six leaf classes. Moreover, 



We use a well-studied \textit{Gun Point} dataset to show the interpretability of real-life
classification results. The \textit{Gun Point} data contains time series of an actor performing the
motion of drawing a gun, and the classification problem is to determine whether or not she were
holding a gun or imitated the action (the \textit{Gun/NoGun problem}). In \cite{shapelet}, they
identified that the best shapelet for classification was when the actor lowered her arm without a
gun - the ``overshoot'' phenomena causing the dip in the time series. In \cite{bagnal}, when
reporting 10 best shapelets for a similar problem, they confirmed the phenomena with 7 shapelets,
adding 3 shapelets reflecting arm rising motion with a gun. Both shapelets clusters are confirmed by
our approach as shown at the Figure \ref{fig:shapelet-like-patterns}, moreover, in the \textit{Gun}
set, our approach also highlighted the upward arm motion.
\begin{figure}[tbp]
   \centering
   \includegraphics[width=130mm]{figures/AcerCircunatum.eps}
   \caption{Illustration of best discriminating patterns found by our technique for OSULeaf
dataset. These patterns align with well known in botany discrimination techniques by lobe
shapes, serrations and the leaf base.}
   \label{fig:shapelet-acer-patterns}
\end{figure}

\section{Clustering}
Clustering is a common tool used for data visualization, exploration, and partitioning. 
In addition, clustering serves as a subroutine in many others data mining algorithms.

\section{Hierarchical clustering}
We applied hierarchical clustering procedure to Synthetic Control dataset clustering eighteen
time series in total, three from each of the classes \textit{increasing trend, decreasing trend,
upward shift, downward shift, periodic, and normal}. Figure \ref{fig:hc} shows
clustering. Obviously, it shows superiority of SAX-SVM distance performance over Euclidean in
hierarchical clustering.

\begin{figure}[tbp]
   \centering
   \includegraphics[width=115mm]{figures/clustering.eps}
   \caption{A comparison of hierarchical clustering application to Synthetic Control dataset.
   Complete linkage was used for these plots.
   }
   \label{fig:hc}
\end{figure}

\section{kMeans clustering}
Again, reflecting concepts of IR field, kMeans clustering based on SAX-VSM approach effectively
is the spherical kMeans approach, which has a number of known advantages. First of all, it can be
parallelized for efficient computation \cite{modha}, and usually converges quickly. Furthermore,
computed centroids generalize cluster members and can be used as a ``model'' to classify future 
time series.

\section{Conclusion}
In our opinion, the superior classification performance of our approach is based on 
a number of factors. 
First of all, our approach is very different in nature from those 
based on convenient distance measures such as Euclidean distance or DTW - to some 
extent we do not pay attention to ordering of time points outside of sliding window. 
Surely, overlapping windows do carry information about initial ordering, but this 
is fading away along the steps of our algorithm.
Secondly, we are able to efficiently tolerate noise by leveraging agglomeration 
and mediation with PAA. 
Thirdly, the SAX alphabet cuts provide flexible boundaries for capturing similar 
sub-series. 
And finally, Vector space model and textit{tf$\ast$idf} statistics provide us 
with efficient discrimination and measurement toolkit: the tf part of the weighting scheme
captures the frequency of observed feqtures within the class, while the idf part 
captures the feature informativeness (features that appear in many classes 
are less informative than those that appear rarely)

\begin{figure}[tbp]
   \centering
   \includegraphics[width=120mm]{figures/corrupted.eps}
   \caption{Illustration of corrupted CBF classes (a) and performance comparison of Euclidean
1-NN and SAX-VSM classifiers (b). Illustration of terms growth for corpus and individual
classes (c).}
   \label{fig:corrupted}
\end{figure}

\section{Future directions}
Note, that just as in Linguistics, such a corpus could be built from bags of SAX words obtained with
different parameters, or, moreover, by parsing every single series by more than once. This,
potentially, not only improves performance of our technique, but also facilitates multivariate
series classification.

\begin{algorithm}
\caption{Class Bag of Words construction}
\label{alg1}
\begin{algorithmic}[1]
\REQUIRE A dataset $D$ containing at least one time series
\ENSURE Return a class WordBag
\STATE $W \leftarrow$ sliding window size
\STATE $P \leftarrow$ PAA size
\STATE $A \leftarrow$ SAX alphabet size
\STATE $B \leftarrow$ \{the resulting bag\}
\FOR{ $t$ in $D$}
 \FOR{ $i\leftarrow 1$ to $|t|-W$}
 \STATE $w \leftarrow SAX(subseries(t,i,W), P, A))$ \{convert subseries into word\}
 \STATE $B \leftarrow w$ \{put the word into the bag, updating frequency\}
 \ENDFOR
\ENDFOR
\RETURN $B$
\end{algorithmic}
\end{algorithm}

%
% ---- Bibliography ----
%
\begin{thebibliography}{5}
\bibliographystyle{splncs}
%

\bibitem {1NN}
Xi, X., Keogh, E., Shelton, C., Wei, L., Ratanamahatana, C.A. 2006. 
Fast time series classification using numerosity reduction. 
In Proc of the 23rd international conference on Machine learning (ICML '06). 1033--1040.

\bibitem {shapelet}
Ye, L., Keogh, E.:
Time series shapelets: a new primitive for data mining. 2009.
In Proc of the 15th ACM SIGKDD international conference on Knowledge discovery and data
mining. 947--956.

\bibitem {logical}
Mueen, A., Keogh, E., Young, N. 2011.
Logical-shapelets: an expressive primitive for time series classification.
In Proc of the 17th ACM SIGKDD international conference on Knowledge discovery and data
mining. 1154--1162.

\bibitem {sax}
Lin, J., Keogh, E., Wei, L., Lonardi, S. 2007.
Experiencing SAX: a novel symbolic representation of time series.
Data Mining and Knowledge Discovery, 107--144.

\bibitem {salton}
Salton, G., Wong, A., Yang., C.S. 1975.
A vector space model for automatic indexing. 
Commun. ACM 18, 11, 613--620.

\bibitem {larsen_marx}
Larsen, Richard J. and Marx, Morris L. 2000. 
An Introduction to Mathematical Statistics and Its Applications (3rd Edition).
Prentice Hall.

\bibitem {paa}
Keogh, E., Pazzani, M.J. 2000.
A Simple Dimensionality Reduction Technique for Fast Similarity Search in Large Time Series
Databases. 
In Proc PADKK '00. 122-133.

\bibitem {hot_sax}
Keogh, E., Lin, J., Fu, A. 2005.
HOT SAX: Efficiently Finding the Most Unusual Time Series Subsequence. 
In Proc ICDM '05. 226-233.

\bibitem {streaming_sax}
Lin, J., Keogh, E., Lonardi, S., Chiu, B. 2003.
A symbolic representation of time series, with implications for streaming algorithms. 
In Proc 8th ACM SIGMOD workshop DMKD '03. 2-11.

\bibitem {bag_features}
Lin, J., Khade, R., Li, Y. 2009.
Finding Structural Similarity in Time Series Data Using Bag-of-Patterns Representation. 
In Proc SSDBM '09. 461-477.

\bibitem {bag_patterns}
Lin, J., Khade, R., Li, Y. 2012.
Rotation-invariant similarity in time series using bag-of-patterns representation. 
J. Intell. Inf. Syst. 39, 2 , 287-315.

\bibitem {goldin_kanellakis}
D.Q., Kanellakis, P.C., 1995.
On Similarity Queries for Time-Series Data: Constraint Specification and Implementation. 
In Proc CP '95. 137-153.

\bibitem {bagnal}
Lines, J., Davis, L.M., Hills, J., Bagnall, A. 2012.
A shapelet transform for time series classification. 
In Proc 18th ACM SIGKDD KDD '12. 289-297.

\bibitem {jmotif}
paper Authors: 
Accompanying information for this paper. 
https://code.google.com/p/jmotif/

\bibitem {modha}
Dhillon, S.I., Modha, D.S. 1996.
A Data-Clustering Algorithm on Distributed Memory Multiprocessors. 
In Proc Workshop on Large-Scale Parallel KDD Systems, SIGKDD, 245-260.

\bibitem {DFT}
Agrawal, R., Faloutsos, C., Swami, A. 1993.
Efficient Similarity Search In Sequence Databases.
In Proc FODO '93. 69-84.

\bibitem {classifiers}
Salzberg, S.L. 1997.
On comparing classifiers: Pitfalls to avoid and a recommended approach. 
Data Mining and Knowledge Discovery, 1, 317–328.

\bibitem {comparison}
Ding, H., Trajcevski, G., Scheuermann, P., Wang, X., and Keogh, E. 2008.
Querying and Mining of Time Series Data: Experimental Comparison of Representations and Distance
Measures. 
In Proc of the 34th VLDB. 1542–1552, 2008

\bibitem {words}
Rath, T.M, Manmatha, R. 2003.
Word image matching using dynamic time wrapping. 
CVPR, II:521–527, 2003.

\bibitem {indexing}
Keogh, E. 2002. 
Exact indexing of dynamic time warping. 
In Proc of the 28th international conference on Very Large Data Bases (VLDB '02). 406-417.

\bibitem {benchmark}
Keogh, E., Kasetty S. 
On the need for Time Series Data Mining Benchmarks: a survey and empirical demonstration.
Proc. ACM KDD 2002, pp. 102-111.

\bibitem {direct}
Björkman, M., Holmström, K.
Global Optimization Using the DIRECT Algorithm in Matlab.
Advanced Modeling and Optimization, 1999, 1(2),17-37

\bibitem {spade}
Chen, Y., Nascimento, M. A., Ooi, B. C., Tung, A. K. H.:
Spade: On shape-based pattern detection in streaming time series. 
In Proc. of the IEEE 23rd International Conference on Data Engineering, 2007, pp. 786–795.


\end{thebibliography}

%\clearpage
%\addtocmark[2]{Author Index} % additional numbered TOC entry
%\renewcommand{\indexname}{Author Index}
%\printindex
%\clearpage
%\addtocmark[2]{Subject Index} % additional numbered TOC entry
%\markboth{Subject Index}{Subject Index}
%\renewcommand{\indexname}{Subject Index}
%\input{subjidx.ind}
\end{document}
