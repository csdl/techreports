%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% jmis-experiences.tex -- 
%% RCS:            : $Id: jmis-experiences.tex,v 1.2 94/02/18 11:13:41 rbrewer Exp $
%% Author          : Robert Brewer
%% Created On      : Thu Feb  3 10:23:40 1994
%% Last Modified By: Philip Johnson
%% Last Modified On: Tue Nov  1 08:26:51 1994
%% Status          : Unknown
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1994 University of Hawaii
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% History
%% 3-Feb-1994		Robert Brewer	
%%    Created

\section{Experiences with URN}

%%%   (This section presents the experimental design and results from the URN
%%%   usage.)

% [Paragraph talking about motivation for the experiment, and the hypothesis to
% be tested.]

We have discussed the techniques we believe are necessary to improve the
Usenet reading experience in the previous section. The success of this
method depends upon on both URN's ability to create meaningful weighting
functions based on users' votes, and users' ability to create a meaningful
representation through addition and subtraction of keywords. Our experiment
was designed to test the hypothesis that using only these votes and keyword
manipulations, URN can generate weighting functions that accurately
represent the relevance of an article to a particular user. First we
explain the method we used to perform the experiment. Next we present the
quantitative results from the experiment.  Finally we discuss what
conclusions we can draw from these results.

\subsection{Method}

The experimental evaluation of URN consisted of a two week usage of the
system with articles input from a single newsgroup. Six members of the
CSDL group were asked to use URN on a regular basis to read the newsgroup
``comp.software-eng".  If they were previously reading this newsgroup with
other newsreading software, they were asked to unsubscribe from the
newsgroup for the duration of the usage. Figure \ref{tab:general-stat}
shows the experimental statistics.

\begin{figure}[htb]
  \begin{center}
  \small
  \begin{tabular} {|l|c|} \hline
    Number of users & 6 \\ \hline
    Duration of experiment & 10 days \\ \hline
    Number of articles input & 175 \\ \hline
    Total time spent by all users & ~ 13.5 hrs \\ \hline
    Size of body text & 343 Kbytes \\ \hline
  \end{tabular}
  \caption{\ls{1}
  {\em Statistics from URN experiment.}}
  \label{tab:general-stat}
  \normalsize
  \end{center}
\end{figure}

Users were instructed to read articles using URN, and to vote on each article
based on how interesting it was to them personally. They were also asked to add
or delete keywords from articles when they felt it was appropriate. In order to
gain better insight into URN's weighting function generation capabilities,
users were asked to read all articles, even if they had very large negative
weights. This is quite different from `normal' usage where users
would probably mark all articles below a certain threshold weight as read
automatically.

During the experimental period, extensive data was collected on the
subjects' interaction with the system. The data was collected using EGRET's
customizable metrics-gathering facilities. Whenever a user performed a
semantically interesting action, URN recorded what action occurred, who
performed it, what entity it was performed on, and the time that it
occurred. This data stream is then saved to the database at the end of each
session. Our analysis is based primarily on these metrics.

\subsection{Results}

We will discuss two of the different kinds of events we recorded: voting
events, and keyword manipulation events.

\subsubsection{Votes and Weights}

Each time a user voted on an article, URN recorded several pieces of
information: the user's vote, the article voted on, the type of vote
(interesting, ambivalent, uninteresting), the weight of article at that time,
and the rank of the article in the Selector at that time.

This information can be used to analyze URN's ability to adaptively generate
weighting functions. We calculated the average weight of an article voted as
interesting for each two day period, combining the votes from all six users
(see Figure \ref{fig:weight-goodvote}). Note: articles that were voted on but
had a weight of zero have been excluded from this average because they
represent articles for which URN had no interest-level prediction.

\begin{figure*}[t]
  {\centerline{\psfig{figure=figures/all-averages.ps}}}
  \caption{\ls{1}
  {\em Weight of an article voted as interesting averaged over all
  users vs. time.  This graph combines together approximately 330 votes.}}
  \label{fig:weight-goodvote}
\end{figure*}

Due to the relatively small number of data points from this experiment,
there is considerable variability in the results. In particular only a
handful of ``interesting'' votes occurred on days 9 and 10, resulting in
the downward trend in the graph. Apart from this anomaly, one can clearly
see a trend of articles voted as interesting rising in weight, and
articles voted as uninteresting falling in weight.

Figure \ref{fig:johnson} shows a more complete view of user \#1's votes. The
graph shows the weights of articles voted interesting and uninteresting by one
user on a per-session basis. Once again, articles voted on which had zero
weight are not displayed. We can see the weights diverge from zero over time
causing interesting articles to get higher weights and uninteresting articles
to get lower weights. Each data point in this scatter plot may represent
multiple articles, if they had the same weight and were voted upon in the same
day.

Other users had similar patterns of usage. Figure \ref{fig:cmoore} shows user
\#2's votes. In this figure we also see the positive trend for interesting
articles and the negative trend for uninteresting articles. However, in each of
the last two sessions we see that some articles with high negative weight were
voted interesting, and vice versa. There are two explanations for these
anomalous votes. One reason for these cross-overs is the small number of data
points available. In some sessions, only one article was voted as interesting
or uninteresting which can skew the results for that session. The other
problem, which we discuss in more detail later on, is the inability for users
to indicate whether they are voting on whether the subject area of an article
is interesting to them, or whether the quality of the article is worthy of
their interest. In a discussion with user \#2, we found that the latter reason
caused these cross-over votes.  The user had been following a particular thread
for a few days, but the quality level of articles in the thread had fallen, so
he started voting articles in that thread as uninteresting.

Figure \ref{fig:dat} shows a user with a very different pattern. Apparently
this user found most articles in the group uninteresting. In fact, the only
articles that were voted as interesting had zero or negative weight. User \#3's
pattern of usage resembles that of a kill file because there were no articles
voted on with positive weight. It might be the case that this user found the
newsgroup chosen for the experiment rather uninteresting, or it might be that
this kill-file-like usage of weighting functions is just this user's style. Due
to URN's adaptive nature, this different way of using weighting functions is
automatically supported.

%%% \begin{figure*}[t]
%%%   {\centerline{\psfig{figure=figures/philip-scatter.ps}}}
%%%   \caption{A scatter plot of one user's voting behavior over the
%%%   experimental period. Over time, interesting articles were more positively
%%%   weighted, while uninteresting articles were more negatively weighted.}
%%%   \label{fig:johnson-scatter}
%%% \end{figure*}

\begin{figure*}[t]
  {\centerline{\psfig{figure=figures/johnson.ps}}}
  \caption{\ls{1}
  {\em A scatter plot of user \#1's voting behavior over the experimental
  period. Over time, interesting articles were more positively weighted,
  while uninteresting articles were more negatively weighted. Note that
  interesting articles are offset from uninteresting articles to make the
  plot more legible.} }
  \label{fig:johnson}
\end{figure*}

\begin{figure*}[t]
  {\centerline{\psfig{figure=figures/cmoore.ps}}}
  \caption{\ls{1}
  {\em A scatter plot of user \#2's voting behavior over the experimental
  period. Over time, interesting articles were more positively weighted,
  while uninteresting articles were more negatively weighted.  However, in
  the last two sessions, some articles with high negative weight were
  voted interesting, and vice versa.}}
  \label{fig:cmoore}
\end{figure*}

\begin{figure*}[t]
  {\centerline{\psfig{figure=figures/dat.ps}}}
  \caption{\ls{1}
  {\em A scatter plot of user \#3's voting behavior over the
  experimental period. This user's weighting functions caused almost all
  articles to be weighted negatively, resulting in this plot with all
  negative values.}}
  \label{fig:dat}
\end{figure*}

\subsubsection{Keyword Manipulation}

Keyword manipulation metrics were also recorded. Whenever a user chose to
add or delete a keyword, the article and keyword in question were recorded.
The keyword statistics can be seen in Figure \ref{tab:keyword-stat}. As
this table illustrates, about 10\% of the keywords in the system were
changed by the user. One concern raised about our shared keyword
manipulation scheme was whether or not users would accidentally or
purposefully delete keywords added by other users, or re-add keywords
deleted by other users. During the experiment, only one user deleted a
keyword created by another. In this case, the first user had added a
keyword containing an extra space, and the second user merely fixed the
error by deleting the incorrect keyword and re-adding it without the extra
space.

%Do analysis on keyword manipulation with goal of showing that substantial user
% manipulation of structure occurs

\begin{figure}[htb]
  \begin{center}
    \small
  \begin{tabular} {|l|c|} \hline   
    Number of total distinct keywords & 782 \\ \hline
    Number of keywords added by users & 33 \\ \hline
    Number of keywords deleted by users & 53 \\ \hline
  \end{tabular}
  \caption{\ls{1}
  {\em URN Keyword Statistics}}
  \label{tab:keyword-stat}
  \normalsize
  \end{center}
\end{figure}


%%% [Patterns to look for: density of keyword additions over time, does the
%%% first person to use URN each day do most of the keyword additions for that
%%% day?, do people delete each other's keywords?, how often are the keywords
%%% for an article changed after some people have voted on it?]

\subsection{Discussion}

This experimental evaluation of URN raised several interesting research
questions.  The most obvious and pressing is the distinction between the
category an article belongs to and the overall quality of an article. These
are two orthogonal characteristics of an article, but in URN their
representation is merged. While URN keywords primarily specify the content
of the article, they also contain the author's name by default.  Therefore
the keywords for an article in URN represent to some degree both the
category and the quality of an article.

This merged representation causes problems when users vote on articles.
They must condense their interest in both the category and the quality of
an article into a single value. For example, during the experiment one user
read an article which was an appropriate, high-quality response to a
posting that was unrelated to the newsgroup. The user was faced with a
conundrum: should the user vote on the article as uninteresting and thereby
assign an unfavorable weight to the author of a high-quality article, or
should the user vote on the article as interesting and risk seeing more
articles on this uninteresting topic? The opposite situation also occurs
when low-quality posts are made on high-interest subjects. This was seen
clearly in the weight/vote plots for users: sometimes articles with large
negative weight were voted interesting, and vice versa. In addition, users
were not explicitly told whether their votes were supposed to reflect the
category or the quality of the article, which led to different users
interpreting the intent of voting in different ways!  

An important lesson learned from our experiment is that a collaborative
system for Usenet must make a distinction between these two
characteristics. One possible way is to record two votes for each article:
one for the relevance of the category to the user, and one for the user's
assessment of the quality of the article.

The current version of URN makes no effort to preserve the thread structure
of articles at the selection level; all articles are presented in weight
rank order regardless of their thread-level position. During the
experiment, one user found that the article with the highest weight was
actually a reply to the article with the second-highest weight. This raises
interesting issues regarding how the weights of articles in a thread should
be combined into a weight for a whole thread. Should the articles with the
highest weight in a thread be presented first, or should the thread be
presented in chronological order? There is a presentational tension between
seeing the highest rated articles and following the logical flow of the
conversation.

URN's keyword manipulation system is based on the assumption that that the
collaborative group using it have fairly similar interests and views, so
that they can come to a consensus as to the set of keywords that describe
an article. However, even in small like-minded groups, users may have
widely varying representations about the structure of a textual artifact
\cite{csdl-93-14}. In this usage we did not encounter any serious
problems, but it must be considered in any longer term experiment.

Users expressed a desire to be able to give URN a wider range of feedback
on an article. Often when using the system, users wished that they could
indicate a stronger preference than the binary interesting or uninteresting
provided. Research in psychology suggests that subjects can give meaningful
feedback with up to a 7 point rating scale \cite{magic7}, which could
easily be implemented in a future version of URN.
