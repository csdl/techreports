%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ijhcs.tex -- 
%% RCS:            : $Id: ijhcs.tex,v 1.4 1994/09/02 19:12:21 dxw Exp $
%% Author          : Dadong Wan
%% Created On      : Wed Dec  1 16:25:27 1993
%% Last Modified By: Dadong Wan
%% Last Modified On: Fri Sep  2 08:57:59 1994
%% Status          : Unknown
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1993 University of Hawaii
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% History
%% 1-Dec-1993		Dadong Wan	
\documentstyle [12pt,nftimes,
                apalike,
                /group/csdl/tex/definemargins,
                /group/csdl/tex/lmacros]{article}
\input{/group/csdl/tex/psfig}

\begin{document}
\title {Experiences with {CLARE}: a Computer-Supported\\ Collaborative
Learning Environment\silentfootnote{This paper is a revised and expanded
version of one appearing in the Proceedings of the 1994 ACM Conference on
Computer Supported Cooperative Work, October 22-26, 1994, Chapel Hill,
North Carolina, U.S.A.}}

\author{Dadong Wan\\
Center for Information Technology \& Management\\
Walter A. Haas School of Business\\
University of California\\
Berkeley, CA 94720-1900, USA \\ \\
Philip M. Johnson\\
Department of Information and Computer Sciences\\
University of Hawaii\\
Honolulu, HI 96822, USA}

\maketitle
\begin{abstract}
  Current collaborative learning systems focus on maximizing shared
  information. However, ``meaningful learning'' is not simply information
  sharing but also knowledge construction.  CLARE is a computer-supported
  learning environment that facilitates meaningful learning through
  collaborative knowledge construction. It provides a semi-formal
  representation language called RESRA and an explicit process model
  called SECAI. Experimental evaluation through 300 hours of classroom
  usage indicates that CLARE does support meaningful learning. It also
  shows that a major bottleneck to computer-mediated knowledge
  construction is summarization.  Lessons learned through the design and
  evaluation of CLARE provide new insights into both collaborative
  learning systems and collaborative learning theories.
\end{abstract}

\thispagestyle{empty}

\newpage
\thispagestyle{empty}
\tableofcontents
\newpage
\ls{1.5}

\pagenumbering{arabic}

\section{Introduction} 
\label{sec:introduction}

Collaborative learning is ubiquitous. As we engage, for example, in
creating solutions to research problems, designing software systems, or
analyzing market trends, we are essentially engaged in learning. More and
more often, we learn and work in collaboration with others. Collaborative
learning takes many forms, such as face-to-face discussions, peer tutoring,
group brainstorming, or joint projects. Thus, it is not surprising that
many types of collaborative learning tools have been developed over the
years. One
example is virtual classroom systems, which range from plain e-mail,
computer conferencing, and bulletin-board systems, to more specialized
systems such as EIES \cite{Hiltz88} and CoSy \cite{Mason89}. Virtual
classrooms allow learners to interact with their peers and instructors, as
well as access online information in a manner independent of time and
geographic locations. They augment traditional classroom learning by
removing the requirement for physical co-presence and by improving access
to information and people.

A second example is hypermedia systems, such as Intermedia
\cite{Yankelovich88} and Mosaic \cite{Andreessen93}. These systems
typically provide distributed mechanisms for structuring large information
spaces. They also provide mechanisms for presentation and integration of
various media formats, including text, graphics, voice, and video.
Hypermedia systems improve access by removing the constraint of text-based
interaction, broadening the scope of sharable information, and reducing the
effort required to make information sharable.

While virtual classrooms and hypermedia systems are successful in improving
information access, they do not typically offer explicit mechanisms to help
learners better assimilate the information, the context surrounding its
creation and use, and the perspectives of the author and other
learners. Simply improving information access without also
increasing the learner's capability to interpret such information leads
directly to the problems of ``information overload'' and
``lost-in-hyperspace.'' While it is essential to explicitly support both
learning and access, major research and development efforts, such as the
National Information Infrastructure Initiative in the United States, still
focus on the latter. New software systems are, therefore, needed to provide
learners with structural and process-level support on how to comprehend new
information, how to relate new information to what they already know, and
how to identify, compare, and integrate different interpretations of the
same information. In other words, how to meaningfully learn in an
environment of vastly improved information access.

This paper presents our approach to supporting meaningful learning through
a process of computer-mediated collaborative knowledge construction. The
next section describes the motivation behind the current work. Section
\ref{sec:clare-approach} presents three essential components of our
approach: a representation language for meaningful learning called RESRA, a
process model to guide the use of RESRA called SECAI, and a computational
environment designed to support this language and process called CLARE.
The following section describes our evaluation of this approach through
approximately 300 hours of classroom usage. Analysis of the results
indicates that the approach is effective in facilitating meaningful
learning. It also reveals the strategies of knowledge construction used by
learners, and several significant obstacles to meaningful learning within
this paradigm.  The following section connects this research to related
work in learning theory, knowledge representation, and computer-supported
cooperative work. The final section revisits the essential contributions of
this research and outlines promising future research directions.


\section{Motivation}
\label{sec:motivation}

CLARE is motivated by two major developments: one is technological and the
other is pedagogical. The former concerns the predominant emphasis on {\it
access\/} by existing collaborative learning systems. The latter is related
to the increasing recognition of the importance of {\it meta-knowledge\/}
in human learning. While the technological development propels the need for
computational support beyond information sharing, the theoretical
development forms a conceptual underpinning for the current approach.


\subsection{Technological biases in collaborative learning systems}

Virtual classrooms and hypermedia systems are the two most widely used
collaborative learning environments. Despite their seemingly differences,
the two share a similar focus, namely, support for information
sharing. While virtual classrooms aim at removing the geographical and
temporal constraints of face-to-face interactions by providing the learner
access to the right information/people at the right time, hypermedia
systems help overcome the text-only, linear structure of traditional
printed media by allowing non-linear integration of video, audio, graphics,
and text. Computer-mediated communication systems such as EIES
\cite{Hiltz88}, for instance, have been successfully used to overcome the
same-time, same-place constraints of traditional classrooms, and to
increase student participation outside physical classrooms. Hypermedia
systems, such as Intermedia \cite{Yankelovich88} and NoteCards
\cite{Halasz87Notecards}, are used to browse and navigate large shared
information spaces. However, virtual classrooms and hypermedia systems
suffer from their own problems: ``information overload'' and
``lost-in-hyperspace,'' respectively. At a deeper level, these problems are
rooted in the same cause: the lack of explicit, fine-grained
characterization and representation of the thematic structures of the
artifacts under concern. In virtual classrooms, for example, online
discussions typically take place within various ``interest groups.'' Such
divisions are generally coarse-grained. The relationships between various
interest groups are often implied rather than explicitly specified. In
hypermedia systems, the power of linking and the emphasis on non-linearity
often lead to the excessive use of such features. The result is a 
spaghetti-like network
of nodes whose semantics are difficult, if
not impossible, to understand. As a result, the potential of both virtual
classroom and hypermedia systems for supporting deep-level collaboration
among a group of learners is severely limited.

At a more fundamental level, the above problems with virtual classroom and
hypermedia systems are the manifestation of a techno-centric approach to
learning support underlying these systems. They indicate that insufficient
attention is paid to the underlying theories of human learning --- one of
which is described below.


\subsection{Meta-learning and concept mapping}

While technologists continue to improve the functionality and the interface
of software tools, theorists are breaking new grounds in understanding
human learning. One important development in educational psychology is the
{\it theory of meaningful learning,\/} also known as the {\it assimilation
theory of cognitive learning\/}. The main premise of this theory is that
(1) the most important factor influencing human learning is the learner's
prior knowledge; (2) human learning is evidenced by a change in the meaning
of experience (as opposed to a change in behavior); and (3) the key role of
the educator is to help students reflect on their experience and to
continuously construct new meanings \cite{Ausubel63,Ausubel78}. To
facilitate this process, Novak and Gowin \cite{Novak84} --- two of the main
proponents of this theory --- have developed two meta-cognitive strategies: {\it
concept maps\/} and {\it Vee diagrams\/}. Both are tools for representing
changes in the knowledge structure of students over time and for helping
them {\it learn how to learn\/}.

Concept mapping represents the first true attempt to provide explicit
support for meta-learning \cite{Novak84}. It has enjoyed a wide acceptance
in the learning research community \cite{Cliburn90,Novak90,Roth92}. As a
collaborative learning tool, however, it suffers from two major problems:

\begin{itemize}
\item {\it Lack of ontological structures:\/} Concept maps, like the
  designer's sketch pad, give the learner full freedom in deciding what to
  draw and how to draw it. The representation does not dictate nor provide
  any structural heuristics on how it should be used. While this
  flexibility makes concept maps extremely expressive, it also adds little
  structure on which useful manipulations can be applied, and which human
  learners may use to help them decipher the map. The latter is especially
  significant in collaborative settings, for this arbitrariness implies
  that it is difficult to compare, contrast, and integrate concept maps
  generated by different learners.
  
\item {\it Lack of process-level support:\/} Concept mapping does not
  prescribe, nor does it provide any means for capturing and abstracting
  the strategies learners use to arrive at a conceptual understanding and
  representation of the subject content of learning artifacts.
\end{itemize}

The above problems with concept mapping prompt for better approaches and
computer-based tools to support learning as collaborative knowledge
construction. CLARE represents our attempt to meet the challenge.


\section{CLARE approach}
\label{sec:clare-approach}

CLARE, which stands for {\it Collaborative Learning And Research
Environment,} is a computer-based learning environment that supports
collaborative construction of knowledge from research papers.  It
consists of three essential components:

\begin{itemize}
\item A knowledge representation language called RESRA that serves as
  a meta-cognitive framework for understanding scientific research
  literature and learners' perspectives;

\item A process model called SECAI that prescribes a systematic procedure
  to guide learners in interpreting, discussing, and relating
  knowledge elements derived from the selected research artifact; and

\item A computer-based environment that integrates RESRA and SECAI into a
  consistent, hypertext-based interface. 
\end{itemize}

The above components of CLARE are elaborated in turn in the following
sections.


\subsection{RESRA}
\label{sec:resra}

RESRA, which stands for {\it REpresentational Schema of Research
Artifacts,} is a semi-structured knowledge representation language designed
specifically to facilitate collaborative learning from scientific text,
such as research papers. It has the following design goals:

\begin{itemize}
 \item A mapping tool that highlights essential thematic features and
  relationships within and across scientific text, and that helps
  expose gaps and ambiguities in existing knowledge;

\item An organizational tool that allows incremental, fine-grained
  representation and integration of scientific artifacts;
  
\item A communication tool and a shared frame of reference that
  highlights similarities and differences between learners'
  points of view; and
  
\item A tool for learning about the norms and conventions governing formal
  communication of scientific knowledge.
\end{itemize}

RESRA is based upon three premises: (1) human knowledge can be represented
in term of a small number of node and link primitives; (2) the use of these
primitives to characterize scientific artifacts and subsequent group
activities (i.e., comparison, argumentation, and integration) are a meaningful
learning process; and (3) different learners are likely to generate
different representations of the same artifact; by comparing these
representations, one can discern fine-grained similarities and differences
in points of view held by individual learners. 

To achieve the above design goals, RESRA introduces three conceptual
constructs: {\it node primitives\/}, {\it link primitives}, and {\it
canonical forms\/}. Node primitives represent discrete thematic features
of the artifact, such as {\it claim\/}, {\it concept\/}, and {\it
theory\/}. They also explicitly represent the learner's points of view in
terms of {\it critique\/}, {\it question\/} and {\it suggestion\/}.

Link primitives describe relationships between thematic features
represented by node primitives. For example, in a typical research paper, a
{\it claim\/} is made with respect to a particular {\it problem\/} and must
be supported by some {\it evidence\/}. In RESRA, these relationships are
expressed as ``{\it claim\/} \( \stackrel{responds-to}{\longrightarrow} \)
{\it problem''\/} and ``{\it evidence\/}
\(\stackrel{supports}{\longrightarrow}\) {\it claim\/},'' where {\it
responds-to\/} and {\it supports\/} are link primitives, and the two
expressions are called RESRA {\it tuples\/}.
   
The canonical form characterizes typical artifact-level thematic
structures, such as {\it concept paper\/}, {\it empirical study\/}, and
{\it survey paper\/}, as a directed graph of RESRA node and link primitives
or a collection of RESRA tuples. For example, in software engineering, one
important type of research artifact is an ``experience paper.'' Such
artifacts report the experience of an organization with a software package
or strategy, including the problem it attempted to solve or alleviate, the
initial justification for adopting that software or strategy, and how the
actual outcome compared to the expected result. In RESRA, a canonical form
for the experience report can be described as in Figure
\ref{fig:experience-crf}.

\begin{quotation}
  \fbox{Figure \ref{fig:experience-crf} goes here.}
\end{quotation}

Below is an example instantiation of the template depicted in Figure
\ref{fig:experience-crf}:

\begin{itemize}
\item {{\it Problem\/}: ``Our organization has failed to produce
  high-quality software despite the increased staff and the
  state-of-the-art environment.''
  
\item {\it Claim\(_{1}\)\/}: ``Cleanroom engineering is the development
  methodology we should adopt to improve the software quality in our
  organization.''
  
\item \it Claim\(_{2} \)\/}: ``Cleanroom engineering helps produce zero
  defect software using statistical quality control.''
  
\item {\it Evidence\/}: ``The average number of errors per KLOC has
  decreased substantially but it took us twice as long to complete a
  system of the same-size.''
\end{itemize}

Learners in software engineering may use Figure \ref{fig:experience-crf} as
a template to guide in their interpretation and evaluation of other
experience papers in that domain, and their efforts in constructing their
own experience papers so that they may also conform the same structure. In
a group setting, such structural knowledge can be used as a shared
framework for learners to engage in discussions about the content of
related artifacts.


\subsubsection{RESRA primitives}

Figure \ref{tab:resra} summarizes the eleven RESRA node primitives. One
major feature of RESRA is its modular design: RESRA node and link
primitives are divided into four logical groups, which correspond to one of
the four main components of the SECAI process model: {\it summarization\/},
{\it evaluation\/}, {\it argumentation\/}, and {\it integration\/} (see
Section \ref{sec:secai}). Figures \ref{fig:sum-resra},
\ref{fig:eval-resra}, \ref{fig:arg-resra} provide a graphical
representation of RESRA's summarative, evaluative, and argumentative
primitives, respectively. RESRA's integrative primitives are unique in that
they focus exclusively on relationships between existing representations
created by different learners. They are comprised of only four link
primitives:

\begin{itemize}
\item {\it is-similar-to;\/}
  
\item {\it share-same-perspective-as;\/}

\item {\it contains;\/} and
  
\item {\it is-related-to.\/}
\end{itemize}

\begin{quotation}
 \fbox{Figure \ref{tab:resra} goes here.}
\end{quotation}

\begin{quotation}
 \fbox{Figure \ref{fig:sum-resra} goes here.}
\end{quotation}

\begin{quotation}
 \fbox{Figure \ref{fig:eval-resra} goes here.}
\end{quotation}

\begin{quotation}
 \fbox{Figure \ref{fig:arg-resra} goes here.}
\end{quotation}


\subsubsection{An example use of RESRA}

Figure \ref{fig:fagan} shows an example use of RESRA that summarizes a
seminal paper on software code inspection \cite{Fagan76}.  The {\em source}
node in the upper left corner provides a reference to the artifact under
study. The paper addresses one {\em problem}, against which three {\em
claim}s are made. To support these {\em claim}s, the author introduces
three {\em concept}s and two {\em method}s. The latter are used to generate
{\em evidence}, which in turn supports two of the three {\em claim}s made
earlier.

\begin{quotation}
 \fbox{Figure \ref{fig:fagan} goes here.}
\end{quotation}

Several interesting observations can be made about Figure \ref{fig:fagan}.
First, the representation captures what is important in the research paper:
it is not simply an outline of the paper but rather a map of its deep
knowledge structure that reflects the learner's mental model about the
author's intent. Second, as discussed later in this paper, different
learners will derive quite different representations of this same
artifact. Figure \ref{fig:fagan} is merely one of many possible
representations. By comparing and contrasting these distinct
representations, one can gain a better understanding of not only what this
artifact is really about but perhaps more interestingly, how different
learners interpret the same content. Third, also as described later,
constructing these representations is non-trivial and requires significant
learner effort, for RESRA node primitives, such as the {\em problem} node
in Figure \ref{fig:fagan}, may be implied rather than explicitly
stated by the author. Thus, the learner must infer these objects from the
context of the paper, and relate them to other RESRA objects. Deriving such
a representation is a meaningful learning experience because learners must
ask themselves many deep-level questions: what is the claim(s) being made?
With respect to what problem? Is a given theme a {\it claim\/} or {\it
theory\/}? How are those themes related? Are there any ``orphan'' or
unconnected themes?  How does the knowledge structure differ from the
canonical form? Answers to these and other related questions reveal what
the artifact really means to individual learners.

\subsection{SECAI}
\label{sec:secai}

SECAI, which stands for {\it Summarization, Evaluation, Comparison,
Argumentation, and Integration,\/} defines an explicit process model for
collaborative learning from scientific text. Figure \ref{fig:secai} shows
how these activities are related together to support collaborative
knowledge construction. The world outside the concentric circles consists
of various types of scientific artifacts, which constitute raw materials of
learning.  At the center lies the visible outcome of the group process
--- the shared knowledge-based being generated. Metaphorically,
collaborative learning with SECAI pulls learners from an external,
isolated, and individual position inward toward an internal, integrated,
and collaborative perspective on the artifact.

\begin{quotation}
 \fbox{Figure \ref{fig:secai} goes here.}
\end{quotation}

The first phase of SECAI is called {\it exploration}, which consists of two
activities: {\it summarization\/} and {\it evaluation\/}. During this
phase, learners derive a personal representation of the artifact and an
evaluation of its content, both expressed in terms of RESRA. This phase is
performed {\it privately}, that is, learners are not allowed to see what
other learners are doing or have done.  This policy prevents learners from
being distracted by each other's work or from free-riding off the work of
others.  The result of exploration is a set of representations similar to
that shown in Figure \ref{fig:fagan}\foot{Figure \ref{fig:fagan} shows
summarization but not evaluation results. In a typical CLARE exploration
session, the users both {\it summarize\/} and {\it evaluate\/} the content
of the selected artifact.}.

The second phase of SECAI is called {\it consolidation}, which consists of
three activities: {\it comparison\/}, {\it argumentation\/}, and {\it
integration\/}.  During comparison, learners can discern the similarities
and differences between their representation and those of other learners.
Comparison is made at three levels: {\it artifact\/}, {\it tuple\/}, and
{\it node primitive\/}.  At the artifact level, learners compare their
classifications of the artifact type, such as whether a research paper
appears to be a concept paper or an experience report. In addition, they
also compare their representations with respect to the canonical form
selected for the artifact to see how their representations deviate from the
archetype. At the tuple level, learners compare their derived
relationships, such as each learner's interpretation of the supporting
evidence for a claim. Finally, at the node primitive level, learners
compare the content of the individual nodes they created and the source
text referenced by them.

Comparison activities provide a basis for {\it argumentation\/}. For
example, suppose that John compares his representation to Jane's, and
determines that his representation contains a {\em problem} node that is
apparently missing from Jane's representation.  He might then generate a
{\em critique} node concerning Jane's representation, noting that it is
missing an important problem raised in the artifact.  Jane might respond by
agreeing that her representation omitted an important problem.
Alternatively, Jane might respond that one of her {\em problem} nodes in
fact subsumes the problem noted by John.  Another potential response might
be to disagree with John's interpretation of the artifact content as a
problem: that it was actually a method or claim of the research, as
described in Jane's representation. This process of comparison and
subsequent argumentation leads to an improved understanding of the meaning
of the artifact. Perhaps as importantly, it reveals other learners'
perspectives on the same artifact.

The final step in the consolidation phase is {\it integration\/}, where
learners create explicit links between their individual representations to
improve their collective coherence and consistency. Going back to our
hypothetical learners, if Jane realized that John had correctly identified
a problem missing from her representation, she could integrate her
representation by linking John's {\em problem} node into her representation
in the appropriate places.  Alternatively, if Jane believed that one of her
{\em problem} nodes subsumed the problem identified by John, she could
integrate his representation by creating an {\em contains} link between
his and her nodes.

The large shaded arrows in Figure \ref{fig:secai} indicate the direction in
which SECAI {\it pulls} the group process: as learners proceed through
the various activities, the level of collaboration among learners increases
and, concurrently, a group knowledge base emerges.  This dynamic knowledge
base articulates both areas of consensus and areas of disagreements among a
group of learners as they summarize, evaluate, and integrate their
individual perspectives on the learning artifact.


\subsection{CLARE system}
\label{sec:clare}

\subsubsection{Overview}

At an implementation level, CLARE is a client/server-based distributed
learning system running in a Unix/X-Window environment. It was built on top
of the EGRET framework for exploratory collaboration
\cite{csdl-92-01,csdl-93-09}. CLARE has two main features. First, it
provides a seamless integration of RESRA and SECAI through a consistent,
hypertext-based interface. The transparency is achieved through
context-sensitivity -- functions available to the learner at a given point
are constrained by the type of the current node and the phase.  Second,
CLARE includes an instrumentation mechanism that unobtrusively gathers
fine-grained process data about the learner's usage behavior. Each time the
user performs a semantically interesting action, such as creating a new
node or link, a timestamp representing this event and the time at which it
occurred is recorded. The instrumentation also detects periods of idle-time
for correcting elapsed-time calculations.  This instrumentation provides
essential data for answering such process-level questions as: what is the
sequence in which learners visit nodes, and does this navigation strategy
differ among learners? How much time do learners spend on each portion of
the document?  How much time do learners spend on each activity of SECAI?
How are node creation and link creation related procedurally?
Answering these and other questions was the goal of a set of experiments we
performed using CLARE, discussed in Section \ref{sec:findings}.


\subsubsection{A session scenario}

A typical CLARE session begins by converting the scientific text to be
studied into CLARE's internal hypertext format. The full text of the
document is split up into smaller chunks called {\it source nodes\/}, each
one corresponding to a physical section or subsection of the document. A
top-level source node, also known as the ``table of contents'' node, is
created containing links to all detailed nodes. Since the current
version of CLARE does not provide support for graphical images, tables and
figures are shown in the hypertext network as logical references and
supplied to the learners as hardcopy documents.

Figure \ref{fig:explore} shows a snapshot of the CLARE user interface
during the exploration phase. The left window shows a source node
corresponding to one section of the artifact under study.  The node is
connected to other source nodes via the {\it Up\/}, {\it Next\/}, and {\it
Prev\/} links displayed on the first line of that node.  Learners navigate
the scientific artifact under study by following these links.

\begin{quotation}
 \fbox{Figure \ref{fig:explore} goes here.}
\end{quotation}

To summarize a paragraph or any arbitrary block of text, the learner first
highlights the text by dragging the mouse over it, and then selects the
corresponding node type from the {\sf Summarize\/} menu. CLARE creates a
new node of the chosen type (e.g., {\em problem}, {\em evidence}) with the
default field template, and displays it in the lower right window. An
explicit link is also automatically added between the selected text in the
left window and the newly created node. The learner then enters annotative
comments about the summarization in the {\bf Description} field.

The above process is repeated until the learners believe they have fully
summarized the document.  Evaluation nodes are created analogously,
although both source nodes and summary nodes may serve as the targets of
evaluation.  The learner adds RESRA link primitives between two eligible
nodes by choosing {\sf Link Mode\/} from the {\sf Summarize\/} menu. The
upper right window in Figure \ref{fig:explore} shows what the current
learner has created so far for the current artifact.

Figure \ref{fig:consolidate} shows a snapshot of the CLARE user interface
during the consolidation phase. The upper left window displays a
comparative view of the {\em problem\/} instances created by three learners
(Peter, Cam, and Rose) during exploration. It highlights the differences
and similarities between these learners with respect to their views on the
problem the original authors attempt to address. Similar comparisons can
also be made for the other RESRA primitives. By clicking on the bold
italicized text, learners can see the actual node instance corresponding to
a given entry. If learners want to know the precise place in the artifact
from which the summarization node was derived, they can follow the link in
the {\bf Summarization} field.

\begin{quotation}
 \fbox{Figure \ref{fig:consolidate} goes here.}
\end{quotation}

To raise a question or make a critique on the {\em problem} node, one
selects the corresponding RESRA node type from the {\sf Argument\/} menu in
the lower right window. This creates a new node instance of the selected type and
links it to the {\em problem} node.  When the creator of the {\em problem}
node sees this new link, he can explain or defend his position using a
similar procedure. He might also declare his {\em problem} as {\sl
similar-to\/} another node using the {\sf Integrate\/} menu.

The upper-right window in Figure \ref{fig:consolidate} summarizes the state
of all learner's work. In this example, Peter has created ten {\em claim}
nodes, while Cam has seven, and Rose has six. Clicking on an entry
leads to a detailed listing for the corresponding user, from which
individual node instances for any learner can be retrieved.

\section{Experiences with CLARE}
\label{sec:findings}

\subsection{Method}

\paragraph{Hypotheses.}

The experiment was intended to be an exploratory study of the viability of
CLARE as a new collaborative learning system. Specifically, it tested three
general hypotheses:

\begin{itemize}
\item RESRA is effective for characterizing the content of research
  literature and highlighting different points of view of individual learners; 
  
\item SECAI is a useful process model for guiding collaborative
  construction of knowledge; and
  
\item CLARE is a viable environment for facilitating collaborative
  learning.
\end{itemize}

The above hypotheses were broken down into ten detailed hypotheses,
described fully elsewhere \cite{csdl-93-14}.

\paragraph{Task.}

The experimental task involved collaborative learning through systematic
analysis and discussion of selected research papers. It consisted of two
components: {\it individual preparation\/} and {\it group
deliberation\/}. Traditionally, such activities take place in a seminar
setting, where students are assigned to read a common set of research
papers, write reviews, and discuss their findings in class. Through these
activities, students are expected to gain deeper understanding of the
domain addressed by the paper (e.g., concepts, problems, methods,
theories). At the same time, they are also expected to improve their
critical skills in evaluating other people's work, identifying problems,
developing alternative solutions, and working with other learners.

The experimental task differed from a traditional seminar mainly in that it
was carried out in a computer-mediated environment. All learning activities
were governed by the process and data models prescribed by CLARE. For
example, there was no writing of paper reviews in the traditional sense,
nor face-to-face discussions. Instead, students studied papers by creating
nodes and links of selected types in CLARE. They interacted with other
students in a similar fashion, i.e., via reading and reacting to the online
artifacts they created in the CLARE database.


\paragraph{Measures.}

Three types of data were collected during the CLARE experiment:

\begin{itemize}
\item {\it Assessment:\/} Learners' subjective assessment of their learning
  experience and CLARE features. Assessment data was gathered through two
  questionnaires administered after each experiment session;
  
\item {\it Outcome:\/} What each user accomplished during each CLARE
  session. Outcome data is the actual content of the online database
  created during each CLARE session; and
  
\item {\it Process: \/} Metrics data about the steps the learners took to
  accomplish required tasks and the time they spent on each step. Such 
  data was gathered automatically through built-in instrumentation
  mechanisms.
\end{itemize}

The combination of the above data provides important evidence about the
viability of RESRA and CLARE, insights about collaborative learning using
CLARE, and limitations of the system.


\paragraph{Subjects.}

The subjects were 16 upper-level undergraduates (i.e., juniors and
seniors), who were enrolled in ICS414 (Software engineering II), and 8
graduate students who were enrolled in ICS613 (advanced software
engineering). Both classes were offered at the Department of Information
and Computer Sciences at the University of Hawaii during Fall, 1993. The
subjects were divided into groups of four: four groups in the first class
and two groups in the second class. The groups remained the same throughout
the experiment.

\paragraph{Procedures.}

Each experiment group consisted of four students, who were randomly
assigned. First-time CLARE users were given a 30-minute overview of the
objective, the basic approach and the overall architecture of CLARE,
followed by a 30-minute demo of the basic functionality and interface
features of the system. A detailed user's guide \cite{csdl-93-15} was
provided to each user at this time.

Prior to the experiment, the selected research papers were input into the
CLARE database by the experiment coordinator in collaboration with the
instructor. All papers were broken down into a set of semantic chunks or
nodes, typically corresponding to sections in the paper, and connected
together via hypertextual links. Since graphics and figures were not
handled by CLARE, hard copies of them were provided to all subjects.

Following the SECAI process model (see Section \ref{sec:secai}), the actual
session was divided into two phases: {\it exploration\/} and {\it
consolidation.\/} Exploration consists of two activities: {\it
summarization\/} and {\it evaluation\/}, both of which were carried out
privately by individual learners. The learners were asked to use
predefined RESRA primitives to describe key features and relationships in
the paper.

Consolidation consists of three main activities: {\it comparison\/}, {\it
deliberation\/}, and {\it integration\/}. The comparison mode helps the
learner to see what the other group members did during the previous phase
and, perhaps more importantly, discern ambiguities, inconsistencies,
differences, and similarities in their interpretations. The deliberation
process involves challenging other students' positions through request for
clarifications, critiquing, identifying deficiencies, and suggesting
alternative views. In response to challenges from others, the learners were
expected to defend and elaborate their positions. At the end, students were
asked to integrate nodes and tuples they created in the previous steps by
adding explicit links between them.

After exploration and consolidation, subjects were asked to fill out two
questionnaires: {\it assessment\/} and {\it feedback\/}, which were
followed by informal discussions about the learners' experience and problems
encountered during the session just completed.

All CLARE experimental activities were carried out in an asynchronous
fashion. Subjects were told that they were not supposed to discuss the
paper content outside CLARE. The average length of each experiment was one
week, which was about equally distributed between exploration and
consolidation. Pilot tests were conducted prior to the actual experiments
to validate the questionnaires.

The CLARE experiments were conducted between September and October, 1993.
During this period, the undergraduate class studied three papers, and the
graduate class studied two papers.


\subsection{Results}
\label{sec:results}

Figure \ref{fig:summary-stat} provides a short summary of the experimental
data. The subjects collectively accumulated about 300 hours of usage time,
and created about 1,800 nodes with a total text size of nearly 400
kilobytes. A total of over 80,000 timestamps were gathered during these
sessions.

\begin{quotation}
 \fbox{Figure \ref{fig:summary-stat} goes here.}
\end{quotation}


The following sections highlight some important results from CLARE
experiments.


\subsubsection{Assessment results}

\paragraph{RESRA.}

The post-session survey showed that approximately 84\% of the learners
found that RESRA provided a useful means for characterizing the important
content of research papers, and that 90\% of the learners agreed that RESRA
helped expose different points of view on the same artifact. Figure
\ref{fig:usefulness} also shows that RESRA node primitive was ranked as one
of the top features of CLARE: 82\% of the users viewed it either very
useful or extremely useful.


\paragraph{SECAI.}

The survey showed that 86\% of the learners agreed or strongly agreed that
the SECAI process model encourages each learner to do his or her part; 92\%
of them indicated that the model in fact facilitated the formation of
individual views on the paper. The above result was supported by Figure
\ref{fig:usefulness}, which indicates that the SECAI model is another top
CLARE feature, viewed by 79\% of the users as either very useful or
extremely useful.


\paragraph{CLARE.}

The post-session survey responses showed that CLARE is a novel and useful
collaborative learning tool: approximately 70\% of learners indicated that
CLARE helped them understand the content of research papers in a way not
possible before, and that nearly 80\% of learners indicated that CLARE helped
them understand their peers' perspectives in a way not possible
before. Responses from the survey also provided feedback about the
usefulness of individual components of our approach. Figures
\ref{fig:usefulness} and \ref{fig:barriers} show relative ranking about the
usefulness and the user-friendliness of CLARE features. The RESRA node primitives
and the SECAI learning model were ranked the highest, assessed by 82\% and
79\% of the users, respectively, as very or extremely useful. The least
useful features were the online examples, assessed by 25\% of the users as
not useful. RESRA canonical forms, the comparison mode, and link primitives
were received mixed reactions from the user.

\begin{quotation}
 \fbox{Figure \ref{fig:usefulness} goes here.}
\end{quotation}

\begin{quotation}
 \fbox{Figure \ref{fig:barriers} goes here.}
\end{quotation}

In addition to the above assessment data, qualitative comments about the
approach were revealing. The following response from a subject shows that,
in at least one instance, CLARE succeeded in fostering meaningful learning:

\begin{quotation}
\small
\noindent \begin{verbatim}
... I don't quite know how to use it [CLARE] very
well yet, but it really helped me get more out of
the artifact we read.  Without CLARE I would have
just read the artifact and not really studied it or
learned about the subject. CLARE made me look at the
artifact from another point of view.  That point of
view was what is the author trying to tell me and
how is the author trying to tell me that information
...  Before I used CLARE I just read the artifacts.
Now using CLARE I look for the meaning of the
artifact and learn more about the subject...
\end{verbatim}
\normalsize
\end{quotation}


\subsubsection{Issues in collaborative learning}
\label{sec:issues}

While it is important to assess the viability of CLARE as an environment
for collaborative knowledge construction, it is of more long-term
significance to assess what actually happened during the usage of the
system, and what types of problems, if any, learners encountered.  Detailed
analysis of the outcome and process data revealed a number of interesting
issues regarding collaborative learning using CLARE.  These issues are
discussed fully elsewhere \cite{csdl-93-14}. In the following sections, we
present four of the most significant: failures in summarization,
mis-interpretations of RESRA, summarization strategies, and collaboration
using CLARE.


\paragraph{Failures in summarization.}

RESRA was intended to represent {\it essential\/} themes of research
literature, and for evaluating, deliberating, and integrating these
themes. In other words, its focus is on the {\it major} rather than {\it
minor\/} themes of the artifact. Figure \ref{fig:fagan} provides an
expert's summarization of \cite{Fagan76}, which consists of 11 nodes that
describe the major themes of that paper.

Table \ref{tab:fagan} shows a comparison of the summarization result of the
same paper by 16 learners and the expert's representation shown in Figure
\ref{fig:fagan}. The 16 subjects analyzing this artifact generated an
average of 19 nodes, which is 8 more than that of the expert's. Given this
large number, one would expect that all major themes would have been
captured, as well as a few minor ones. However, analysis of the content of
those nodes reveals that:

\begin{itemize}
\item None of the 16 learners correctly identified the problem;
  
\item Seven learners correctly identified one or two of the three
  major claims;
  
\item Ten learners had the evidence right; and
  
\item Only six learners had one of the two methods right.
\end{itemize}

In place of missing major themes, learners identified many minor themes of
the paper in their representations. One possible explanation for such
distortions in summarization might be that the current experiment did not
explicitly differentiate major themes from minor ones. On the other hand,
such differentiation would be of no help if users cannot conceptually
distinguish major themes from minor ones. The latter is exactly a
deficiency that a collaborative knowledge construction approach has the
potential to overcome.

\begin{quotation}
 \fbox{Figure \ref{tab:fagan} goes here.}
\end{quotation}


\paragraph{Variations in RESRA interpretation and usage.}

RESRA was interpreted in a wide variety of ways among the learners.
Despite the presence of hands-on training, detailed user documentation, and
online examples, many subjects still failed to grasp the semantics
of RESRA primitives, as evidenced by a substantial number of times in which
RESRA nodes and links were used incorrectly.  For example, though {\em
theory\/} is defined as ``a systematic formulation about a particular
problem domain...'', the following use of the primitive clearly does not
satisfy this definition:

\ls{1.0}
\small
\begin{verbatim}
   TYPE: theory
   SUBJ: No single development improves the situation
   DESC: No single development aids in improving the software problem,
         at least not with respect to productivity, reliability or
         simplicity.
\end{verbatim}
\normalsize
\ls{1.5}

Another common RESRA usage error is the confusion between learners' views
and those of the author's. The purpose of summarization is to capture the
conceptual themes of a paper as intended by the author; learner's opinions
are represented using RESRA evaluative primitives, i.e., {\it critique},
{\it question}, and {\it suggestion}. The following example node, however,
contains a learner's {\it claim}, not the author's:

\ls{1.0}
\small
\begin{verbatim}
   TYPE: claim
   SUBJ: Careless Programming
   DESC: If return codes are not checked by the programmer,
         it is most likely a sign of careless programming.  Time
         and effort was not taken by the programmer to insure that 
         values returned by a module is indeed valid and correct 
         and therefore causes errors.
\end{verbatim}
\normalsize
\ls{1.5}

Other typical types of errors in using RESRA include evidence nodes
containing no evidence, suggestions containing no proposals from the
learner, claims that are ``neutral,'' evidence stated as claims,
explanations or predictions identified as theories and problems treated as
learner's disagreements with the author's claims instead of what the author
attempts to address.


\paragraph{Summarization strategies.}

The process data also reveals a set of stereotypical strategies used by
learners in summarizing the content of an artifact. The strategies are
characterized by the sequencing of summarative node and link instance
creation: 
   
\begin{enumerate}
\item {\it Nodes only:} Create summarative nodes only. No attempt is made
  to connect them together using RESRA link primitives.
  
\item {\it Nodes for an entire document, then links:} First create
  summarative nodes for the entire artifact, and then link them together.
  
\item {\it Nodes, then links, but one section at a time:} Create
  summarative nodes for a single source node and/or its adjacent source
  nodes, and then create links between them; repeat the same process until
  all source node are summarized.
  
\item {\sl (2) first, then (3).\/} A combination of the first and second
  strategy. First create summarative nodes for the entire artifact,
  followed by a wave of link creation. Next, selectively create additional
  summarative nodes, immediately followed by the link creation.
\end{enumerate}
  
A majority of summarization was done through a single-round navigation of
the source nodes, including 36\% of learners who created no summarative
links at all. The use of such summarization strategies might have in part
accounted for the fact that, despite the large number of summarative nodes
created, major themes of a paper were still missing. This is because
discerning the major themes of a paper and the relationships between them
often requires relating different parts of the paper, which is quite
difficult to achieve without going through multiple passes of the artifact.


\paragraph{Collaboration in CLARE.}

In the SECAI model, explicit collaboration among learners takes place in
the form of comparing and questioning their representations, deliberating
reasonings behind them, and ultimately, integrating them into a coherent
whole. Figure \ref{fig:arg-example} shows an example collaborative
representation network generated by four first-time CLARE users. Of a total
of 92 nodes in the network, 34 were created during the argumentation phase.
Of the 34 nodes, 32 are {\it evaluative \/} in nature. These 32 evaluative
nodes in turn can be categorized into two groups: pointing out the correct
use of RESRA primitives and identifying ambiguities/inaccuracies in other
learners' representations. In ``critique642,'' for instance, Mary points
out that, in ``claim528,'' Scott totally mis-interpreted the original
authors' meaning. To assess the accuracy of Scott's representation, the
process data showed that Mary in fact verified the node content with the
source text from which Scott derived his node.

Figure \ref{fig:arg-example} also shows the presence of {\it
constructive\/} (as opposed to {\it evaluative\/}) argumentation, in which
learners do not merely critique or question each other's positions but
engage in active knowledge-building by formulating new problems, proposing
alternative claims, supplying additional evidence, and so on. In
``evidence662,'' for example, Chris counters Mary's claim (``claim522'')
with new evidence.

Another noticeable feature about Figure \ref{fig:arg-example} is the
absence of integration activities, which turns out to be quite typical
across CLARE sessions: very few learners elected to add explicit
integrative links between their representations or vote for best
representations. As a result, the group knowledge base consists of
substantial amount of redundancy and inconsistency.

\begin{quotation}
 \fbox{Figure \ref{fig:arg-example} goes here.}
\end{quotation}


\subsection{Discussion}
\label{sec:discussion}

\subsubsection{RESRA paradox}

The evaluation results from the previous section provide a strong
indication that RESRA, SECAI, and CLARE together provide a useful means for
meaningful learning through collaborative knowledge construction.  Users
indicated that the RESRA representation and the SECAI process model
provided effective and novel support for representing the meaning of an
artifact and comparing their interpretation to others. The results also
indicated that users were not quite satisfied with the user interface of
the CLARE prototype. Analysis of the experimental data provided new insight
into the strategies used by subjects as they constructed their
representations. The results also yielded a surprising paradox: though
users rated RESRA quite highly, they also made a substantial number of
errors in its application. This raises the following question: is RESRA
still useful for its intended purpose, despite individual variations in its
interpretation and deviations from its intended usage?

The answer to this question appears to be a qualified yes, at least in the
current context of use. In other words, the perceived utility of RESRA is
not necessarily positively correlated with the {\it correctness\/} of its
usage. It is possible, particularly at the node primitive level, for a
learner to use a primitive incorrectly but usefully, simply because node
and link primitives force the learner to analyze the artifact in terms of
high level conceptual abstractions.  Analyzing the artifact in this way
provides a useful means to discover structural and content issues that
might not be perceived otherwise. In addition, incorrect use of primitives
also creates collaborative opportunities, since the mistake might be
identified by another learner.  In many cases, the creation,
identification, discussion, and correction of representation errors can
facilitate meaningful learning both at the content level (by constructing
an awareness of the true nature of the artifact under study) and at a
meta-level (by constructing an awareness of the true nature of the
representational language.)

However, the incorrect use of RESRA node primitives also raises a major
barrier to effective use of the representation, since it prevents the
effective use of RESRA link primitives. For example, an {\em evidence\/}
either {\it supports\/} or {\it counters} a {\em claim}.  At the node
primitive level, it may not be of great consequence to misrepresent a {\em
claim\/} as a {\it theory\/}.  However, this mistake has significant
consequences at the link primitive level, since the link primitives for
claims will not be available.  Thus, incorrect node primitive choices can
inhibit the creation of links between nodes, a characteristic found in
several subjects' representations.

Furthermore, RESRA was intended as a shared meta-cognitive framework for
collaborative learning. The failure to agree on the meanings of individual
meta-level constructs (i.e., RESRA primitives) among learners will
inevitably lead to wide variations in the representations of specific
artifacts, as already noted in Section \ref{sec:issues}. Such variations
make it difficult to make meaningful comparison between representations
among learners and to uncover differences between them. As a result, they
limit the potential of RESRA as a collaborative tool.


\subsubsection{Summarization bottleneck}

During the design of RESRA, SECAI, and CLARE, we expected that the
summarization activity would be rather straightforward and that most
collaborative interaction would occur during argumentation and integration.
The results surprised us: most of the subjects' effort and collaborative
activity centered on summarization, and the summarization activity was not
at all straightforward: subjects made substantial errors both in choice of
RESRA primitives and in choice of the artifact content to summarize.  We
now realize that this seemingly straightforward process of reconstructing a
scientific paper within a simple representation language poses an awesome
challenge for many learners. Since summarization is essentially knowledge
representation, this bottleneck can be traced down to ontological and
epistemological factors. The ultimate success of a collaborative learning
environment has everything to do with how well it meets this initial
challenge. We expect this problem will arise no matter what the nature of
the representation language. Rather than hope for a ``silver bullet''
knowledge representation language, collaborative learning system designers
should instead ensure that process-level mechanisms exist to overcome these
breakdowns when they inevitably occur.


\section{Related Work}
\label{sec:related work}

\subsection{Theoretical backgrounds}

CLARE is a theory-driven collaborative learning environment. Specifically,
it is grounded in two theoretical tenets: social constructionism
\cite{Berger66,Knorr-Cetina81} and the assimilation theory of cognitive
learning \cite{Ausubel63,Novak84}. The former affirms the social nature of
learning and the imperative of engaging learners in collaborative knowledge
construction, as opposed to merely information sharing. It provides a
philosophical foundation for the learning activities that CLARE
supports. The latter is centered on the concept of {\it meaningful
learning\/}, which defines learning as an ongoing process of relating new
knowledge to what the learner already knows. Meaningful learning emphasizes
explicit use of meta-knowledge to enhance human learning. To this end, it
introduces two important concepts: {\it progressive differentiation\/} and
{\it integrative consolidation\/}. The former states that, since meaningful
learning is a continuous process wherein new concepts gain greater meaning
as new relationships are acquired, concepts are never finally
learned. Rather, their meanings are constantly revised, and made more
explicit as they become progressively more differentiated.  The latter
refers to the fact that meaningful learning is enhanced when the learner
recognizes new relationships between related concepts and propositions.

The above view that human learning is {\it meaning-making} places knowledge
representation at the focal point of the human learning process, since
knowledge representation defines not only the form in which a certain type
of knowledge is highlighted to the learner, but also the process by which
such a form is derived. Moreover, knowledge representation languages, which
are called {\it meta-cognitive tools\/} by educational researchers, are the
standard language for characterizing both knowledge structures and
corresponding cognitive structures. They help the learner differentiate and
organize newly acquired meanings.


\subsection{RESRA and knowledge representation}

RESRA is related to {\it schema theory\/} in cognitive psychology, which
contends that human minds store and retrieve knowledge about the external
world in terms of abstract chunks called schemas \cite{Stillings87} and
that the schema plays in an essential role in the selection, abstraction,
interpretation, and integration of information \cite{Alba83}. RESRA is also
closely related to some knowledge representation research in AI,
particularly RA \cite{Swaminathan90}, which proposes an episodic
representation for research literature.

RESRA has benefited from a number of semi-structured representation
schemes, for example, IBIS \cite{Kunz70,Conklin88}, DRL\cite{Lee91What},
Toulmin's rhetorical model \cite{Toulmin58,Cavalli-Sforza92}. However, it
differs from those schemes in that, among other things, it is fully
integrated with an explicit process model (i.e., SECAI) that defines how
the scheme be used.

{\it Concept maps\/} and {\it Vee diagrams\/} are two meta-cognitive tools
proposed by Novak and Gowin's to support meaningful learning
\cite{Gowin81,Novak84}. Functionally, they are very similar to knowledge
representation schemes in AI. In fact, the term ``concept map'' is
sometimes used interchangeably with ``semantic-net.'' While the
effectiveness of concept mapping is well supported empirically
\cite{Cliburn90,Novak90,Roth92}, its inadequacy as a collaborative learning
tool and its lack of computerized support have directly prompted the
current research (see Section \ref{sec:motivation}.)

Figure \ref{fig:vee} depicts a simplified version of the Vee diagram for
understanding the nature of knowledge and knowledge construction. The left
side of the Vee is the {\it thinking-side\/}, while the right side is
called {\it doing-side\/}. The two sides are linked together by the
event/object that is under study. To answer the focus question in the
middle requires an integration of issues raised from both sides. At a
conceptually level, the Vee diagram offers an elegant and powerful means of
exposing deep-level structure of knowledge: by constructing a Vee diagram
for each problem situation, a learner can see inter-relationships among
different knowledge components and gaps between them, if any.

\begin{quotation}
 \fbox{Figure \ref{fig:vee} goes here.}
\end{quotation}

Vee was originally designed to help students and teachers better understand
the nature of science laboratory work. It adopts a {\it bottom-up\/}
approach to knowledge, starting from the actual event/object under
study. This approach, however, was not considered adequate for CLARE, since
CLARE treats scientific text as the primary source of knowledge. Because
scientific artifacts are {\it episodic\/} in nature \cite{Swaminathan90},
artifact {\bf A} may cover only the left side of the Vee, or even only the
upper left portion of the Vee, while artifact {\bf B} may cover the right
side of the Vee. As a result, it is not always possible, nor necessary, to
construct an entire Vee, in order to understand the content of the selected
artifact.  RESRA integrates many of knowledge primitives found in the Vee
diagram. Through its canonical forms, RESRA also extends the heuristic
value of the Vee by providing structural guidance on learning from
scientific artifacts.


\subsection{CSILE}

CLARE falls into a special genre of computer-based learning environments
called {\it collaborative knowledge construction tools\/}, as contrasted
with information sharing tools, such as EIES \cite{Hiltz88}, Intermedia
\cite{Yankelovich88}, and Mosaic \cite{Andreessen93}. CLARE is similar to
CSILE (Computer-Supported Intentional Learning Environment)
\cite{Scardamadia93,Scardamadia92} in that both systems reify a social
constructionist paradigm and provide an environment conducive to
collaborative knowledge construction. However, CLARE's representation
scheme provides a meta-cognitive framework for collaborative learning,
while CSILE's four thinking types (``I know,'' ``high-level questions,''
``plan,'' and ``problem'') only allow learners to categorize their
intentions. In addition, CLARE provides an explicit process model to guide
the application of the representation and the process of collaboration,
which is absent in CSILE. Both CLARE and CSILE have built-in
instrumentation mechanisms which allow unobtrusive gathering of data about
the usage behavior of learners.


\section{Future Directions}
\label{sec:conclusions}

\subsection{Summary}

This paper presents an overview of and major findings on a
computer-based approach for supporting learning as a process of
collaborative knowledge-building.  CLARE differs from other learning
systems in three important ways. First, it provides a semi-structured
knowledge representation language that serves as a shared meta-cognitive
framework to facilitate communication and collaboration among
learners. Second, it defines an explicit process model of collaborative
learning. Third, it implements fine-grained instrumentation mechanisms to
gather detailed process data concerning the behavior of its users. Analysis
of experimental data confirms that CLARE is a novel environment that
fosters meaningful learning. It shows that RESRA and SECAI provide useful
structural and process-level guidance on how to collaboratively construct
knowledge. In addition, analysis also reveals a number of issues for
further research.


\subsection{Toward a theory of collaborative learning}

Computer-supported collaborative learning is still a quite recent
phenomenon for which no coherent theoretical frameworks yet exist. Many
current learning theories, such as the ones on which CLARE is based, do not
explicitly address such essential issues as how people develop shared
mental models of the same artifact or problem situation, how to deal with
differing terminologies for the same construct, and so on. As evidenced
from the CLARE experimental result, these problems are intrinsic and also
essential to collaborative knowledge-building, and cannot be answered by
simply extrapolating proposals from individual-based learning
theories. New and better theoretical explanations are needed to guide the
future system development and experimentation. 


\subsection{Supporting learning in collaborative work}

Collaborative learning is not confined to classroom settings or scientific
artifacts. Rather, it is part of every work situation in which
artifact-based collaboration is required, such as software development,
business proposal development, and so forth. One long-term goal is to
develop computer-supported environments that foster collaborative learning
across task domains through tailorable structural and process-level
support. RESRA and SECAI represent the first step in this direction.


\subsection{Integrating CLARE with information sharing tools}

Collaborative learning involves both information sharing and collaborative
knowledge construction. However, most systems choose to emphasize one at
the price of the other. For example, virtual classroom and hypermedia
systems, as mentioned earlier, are essentially information sharing
tools. CLARE, in contrast, was designed to facilitate knowledge
construction. However, this separation of the two facets reflects the
system designer's decision rather than the learner's requirement. To most
learners, knowledge construction is a direct outgrowth or even an integral
part of meaningful information sharing. Hence, it is natural that essential
capabilities of CLARE be embedded in information sharing tools such as
Mosaic, or vice versa.


\subsection{CLARE as an assessment tool}

We intend to apply the CLARE's approach to collaborative learning as a
means to assess the quality of the research/learning artifact and to help
authors improve the quality of the artifacts they create. We hypothesize
that a good scientific artifact contains a clearly-articulated knowledge
structure and thus is easier to summarize using CLARE than a bad artifact.
As a simple test of this hypothesis, we invite you---our readers---to
contemplate a RESRA representation of this paper, and send us your
assessments and discoveries.


\paragraph{Acknowledgment:}
\small
\noindent The authors gratefully acknowledge the assistance of the other
members of the Collaborative Software Development Laboratory in all phases
of this research: Danu Tjahjono, Robert Brewer, Cam Moore, and Rosemary
Andrada.  Support for this research was partially provided by the National
Science Foundation Research Initiation Award CCR-9110861.  \normalsize

\newpage
\ls{1.0}
\bibliography{/group/csdl/techreports/93-14/bib/clare,/group/csdl/techreports/93-14/bib/csdl-trs,/group/csdl/techreports/93-14/bib/cscl-systems}
\bibliographystyle{apalike}


%%%------------------- figures and tables --------------------------------
\pagestyle{empty}
%\newpage
%\listoffigures

\newpage
\begin{figure}[h]
 \centerline{\psfig{figure=Figures/experience-crf.eps,width=3.0in}}
  \caption{A RESRA canonical form for ``experience paper''} 
  \label{fig:experience-crf}
\end{figure}

\newpage
\ls{1.0}
\small
\begin{figure}[h]
  \begin{center}
    \begin{tabular} {|l|p{2.5in}|p{2.5in}|} \hline   
      {\bf Node Type} & {\bf Description} & {\bf Example} \\ \hline \hline
      
      Problem & A phenomenon, event, or process whose understanding
      requires further inquiry. & Meta-learning is not adequately
      supported by existing tools. \\ \hline
      
      Claim & A position or proposition about a given problem
      situation.  & Cleanroom engineering provides a viable solution
      in producing zero defect software. \\ \hline
      
      Evidence & Data gathered for the purpose of supporting or
      refuting a given claim. & The use of cleanroom techniques led
      to a 5-fold reduction of defects in project Alpha. \\
      \hline 

      Theory & A systemic formulation about a particular problem
      domain, derivable through deductive or inductive procedures. &
      Ausubel's theory of meaningful learning. \\ \hline
      
      Method & Procedures or techniques used to generate evidence for
      a particular claim. & Delphi study; nominal grouping technique;
      waterfall software development model. \\ \hline
      
      Concept & A primitive construct used in formulating theory,
      claim, or method. & Meta-learning; Knowledge representation.
      \\ \hline
      
      Thing & A natural or man-made object that is under study.  &
      Rock; Intermedia.  \\ \hline
      
      Source & An identifiable written artifact, either artifact
      itself or a reference to it. & An
      article by Ashton; the notes from Kyle's talk. \\ \hline 
      
      Critique & Critical remarks or comments about a particular
      claim, evidence, method, source, et al., or relationships
      between them. & Applications of cleanroom
      engineering appear limited to domains with well-defined requirements.
      \\ \hline
      
      Question & Aspects of a claim, theory, concept, etc., about
      which the learner is still in doubt. & How does box-structured
      design differ from object-oriented design? \\ \hline
      
      Suggestion & Ideas, recommendations, or feedback on how to
      improve an existing problem statement, claim, method, et al.  &
      I would like to see cleanroom engineering used in some
      non-conventional domains, such as groupware. \\ \hline
    \end{tabular}
    \caption{A synopsis of RESRA node primitives.}
    \label{tab:resra}
  \end{center}
\end{figure}
\normalsize
\ls{1.5}

\newpage
\begin{figure}[h]
 \centerline{\psfig{figure=Figures/sum-resra.eps,height=4.0in}}
  \caption{\ls{1.0} RESRA summarative node and link primitives.}
  \label{fig:sum-resra}
\end{figure}

\newpage
\begin{figure}[h]
 \centerline{\psfig{figure=Figures/eval-resra.eps,height=4.0in}}
 \caption{\ls{1.0} RESRA evaluative node and link primitives.}
  \label{fig:eval-resra}
\end{figure}

\newpage
\begin{figure}[h]
 \centerline{\psfig{figure=Figures/arg-resra.eps,height=4.0in}}
 \caption{\ls{1.0} RESRA argumentative node and link primitives.}
  \label{fig:arg-resra}
\end{figure}

\newpage
\begin{figure}[h]
 \centerline{\psfig{figure=Figures/fagan.eps,width=5.0in}}
 \caption{\ls{1.0} An expert's RESRA representation of Fagan's paper on code inspection.}
  \label{fig:fagan}
\end{figure}

\newpage
\begin{figure}[h]
  \centerline{\psfig{figure=Figures/secai.eps,width=5.0in}}
  \caption{\ls{1.0} The SECAI process model for collaborative learning.}
  \label{fig:secai}
\end{figure}

\newpage
\begin{figure}[h]
  \centerline{\psfig{figure=Figures/explore.eps,width=6.0in}}
  \caption{\ls{1.0} A user view of CLARE during the exploration phase.  The left
  hand window contains a portion of the artifact under study.  The lower
  right window contains an evidence node created by the learner.  The upper
  right window summarizes what the learner has created during exploration
  thus far.}
  \label{fig:explore}
\end{figure}

\newpage
\begin{figure}[h]
  \centerline{\psfig{figure=Figures/consolidate.eps,width=6.0in}}
  \caption{\ls{1.0} A user view of CLARE during the consolidation phase.  The upper
  left window contains a comparative summary of the problems identified
  by each learner in the scientific artifact.  One of the actual problem
  node instances is displayed in the lower right hand window.  The lower
  left window displays the portion of the scientific text from which this
  problem was derived. The upper right window contains a summary of the
  activities of each learner.}
  \label{fig:consolidate}
\end{figure}

\newpage
\begin{figure}[h]
  \begin{center}
  \begin{tabular} {|c|r|r|r|r|} \hline   
    Exp. & Logins & Time (hrs)& Nodes & Size (Kb) \\
    \hline \hline {\bf 1a.}  & 120 & 82.85 & 472 & 90.02 \\ \hline {\bf
    1b.}  & 115 & 67.90 & 513 & 107.97 \\ \hline {\bf 1c.}  & 84 &
    53.68 & 440 & 105.16 \\ \hline {\bf 2a.}  & 85 & 54.42 & 162 &
    39.42 \\ \hline {\bf 2b.}  & 53 & 37.55 & 207 & 49.67 \\ \hline
    \hline {\bf Total} & 457 & 296.40 & 1794 & 392.24 \\ \hline
   \end{tabular}
    \caption{Selected CLARE statistics.}
     \label{fig:summary-stat}
  \end{center}
\end{figure}

\newpage
\begin{figure}[hbtp]
 \centerline{\psfig{figure=Figures/features.ps,width=5.0in}}
 \caption{Learners' ranking on the usefulness of CLARE features}
  \label{fig:usefulness}
\end{figure}

\newpage
\begin{figure}[hbtp]
  \centerline{\psfig{figure=Figures/barriers.ps,width=5.0in}}
 \caption{Learner's ranking on the ease of use of CLARE features}
  \label{fig:barriers}
\end{figure}

\newpage
\ls{1.0}
\small
\begin{figure}[h]
  \begin{center}
  \begin{tabular} {||l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c||c||} \hline
  Type &u\(_1\) & u\(_2\) & u\(_3\) & u\(_4\) & u\(_5\) &  u\(_6\) &  u\(_7\) &  u\(_8\) &  u\(_9\) &  u\(_{10}\) &  u\(_{11}\) &  u\(_{12}\) &  u\(_{13}\) &  u\(_14\) &  u\(_{15}\) &  u\(_{16}\) & u\(_x\) \\ \hline \hline
Problem&  2&  1& 2& 1&  1& 0& 2& 1&  0&  0& 0& 2&  1&  0& 2&  1& 1 \\ \hline
Claim  &  10& 2& 6& 8&  4& 4& 7& 8&  11& 8& 6& 27& 11& 8& 13& 4& 3 \\ \hline
Concept&  1&  0& 3& 3&  0& 0& 0& 0&  0&  3& 1& 1&  0&  0& 2&  1& 3 \\ \hline
Method &  3&  1& 4& 5&  3& 4& 2& 8&  0&  5& 1& 2&  3&  2& 1&  1& 2 \\ \hline
Evidence& 5&  3& 0& 10& 1& 8& 3& 11& 1&  1& 3& 6&  6&  6& 3&  2& 1 \\ \hline
Theory  & 1&  0& 1& 0&  1& 0& 1& 0&  1&  2& 1& 3&  1&  0& 0&  0& 0 \\ \hline
Thing   & 4&  0& 0& 9&  0& 0& 0& 0&  0&  0& 0& 0&  0&  0& 0&  0& 0 \\ \hline
Other   & 0&  0& 0& 0&  0& 0& 1& 0&  0&  0& 0& 0&  0&  0& 0&  0& 0 \\ \hline \hline 
Total   & 26& 7& 16&36&10&17&15&28&  13& 19&12&41& 22& 16&21& 9& 10 \\ \hline \hline 
\end{tabular}
\end{center}
  \caption{\ls{1.0} Comparison of 16 learner's summarizations of [Fagan76] with an
  expert's representation. Columns show instance counts of various RESRA
  node primitive types.}
  \label{tab:fagan}
\end{figure}
 \normalsize
\ls{1.5}

\newpage
\begin{figure}[h]
 \centerline{\psfig{figure=Figures/rep-all.eps,height=3.9in}}
 \caption{An example collaborative representation network by four CLARE users} 
  \label{fig:arg-example}
\end{figure}

\newpage
\begin{figure}[h]
 \centerline{\psfig{figure=Figures/vee.eps,width=3.5in}}
 \caption{Novak and Gowin's Vee Diagram (Novak \& Gowin, 1984)}
  \label{fig:vee}
\end{figure}


\end{document}







