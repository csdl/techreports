%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% project.tex -- 
%% Author          : Philip Johnson
%% Created On      : Tue Mar 31 11:44:58 2009
%% Last Modified By: Philip Johnson
%% Last Modified On: Fri Apr 10 11:48:14 2009
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 2009 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
\pagenumbering{arabic}
\renewcommand{\thepage} {C--\arabic{page}}

\renewcommand{\thesection} {C.\arabic{section}}
\setcounter{section}{0}

\section{Project Description}

\subsection{Project Vision, Goals, Objectives, and Outcomes}


{\em Describe the CT-centric vision, goals, objectives, and anticipated
outcomes of the proposed project. Clearly indicate how they will contribute
to realization of the three CPATH program goals: (1) contribute to the
development of a globally competitive U.S. workforce with CT competencies
essential to U.S. leadership in the global innovation enterprise; (2)
increase the number of students developing CT competencies by infusing CT
learning opportunities into undergraduate education in the core computing -
computer and information science and engineering - disciplines, and in
other fields of study; and, (3) demonstrate transformative CT-focused
undergraduate education models that are replicable across a variety of
institutions.  }

\bigskip

Jeannette Wing has written, ``Computational thinking involves solving
problems, designing systems, and understanding human behavior, by drawing
on the concepts fundamental to computer science'' \citep{Wing06}.  In her
presentation ``Computational Thinking and Thinking About Computation'',
Wing refines her view of these fundamental computer science concepts in terms of 
the ``Two As'': Abstraction and Automation.  Activities
related to the first ``A'' include: choosing the right abstractions, operating
at multiple levels of abstraction, and defining relationships between
abstractions.  Activities related to the second ``A'' involve mechanizing the
first A via precise notations and models.  In essence, automation amplifies
the power of abstraction.  Computational thinking, from this perspective,
involves the correct choice of abstraction combined with the correct choice
of automation.

The vision of this proposal is to develop and institutionalize a new
approach to computational thinking where abstraction and automation combine
to transform the use of {\em empirical thinking} in software development.

What is empirical thinking?  The term ``empirical'' is variously defined as
``derived from experiment and observation rather than theory''; ``evidence
or consequences observable by the senses''; and ``capable of being verified
or disproved by observation or experiment.''

Given these definitions, it is clear that some degree of empirical thinking
is already commonplace in software development.  For example, beginning
programmers use empirical thinking when they ``observe'' the output of the
compiler to learn how to write syntactically correct programs.  Beginners
also tend to make extensive use of ``experimentation'': they execute their
program with example data, compare the actual behavior to what they expect,
then make modifications until the observed behavior matches their
expectations.

These examples of empirical thinking, while satisfactory for beginning
programmers, do not scale well because they lack both abstraction and
automation. Thus, they fail to constitute the kind of computational
thinking to be supported by the CPATH program.

One would expect that as students progress into more advanced software
development courses, the curriculum would scale in at least two
ways. First, the complexity, size, and number of people involved in a
software development project would scale upwards.  Second, the level of
abstraction and automation in their empirical thinking would scale
commensurately. Unfortunately, while advanced software development courses
certainly require students to develop significantly more sophisticated
systems than their introductory counterparts, the use of empirical thinking
remains mostly non-abstract and non-automated.  The principle computational
support for advanced programming classes is an integrated development
environment such as Eclipse or Visual Studio. While this is a significant
advance over vanilla text editors, such IDEs provide relatively little in
the way of abstraction or automation for empirical thinking about the
products and processes of software development.

Supporting abstraction in empirical thinking for software development
generally means creating quantitative models for important development
concepts.  For example, test quality is an important concept that is
commonly emphasized in advanced software development courses.  One
quantitative model for test quality is line-level test coverage, which is
generally expressed as the percentage of source lines of code in the
software exercised by the test cases.  Another important concept is
complexity, and quantitative models such as afferent and efferent coupling
or cyclomatic complexity provide abstract, empirical representations
for this concept.  Even ``agile'' concepts such as ``commit early, commit
often'' or ``collective code ownership'' can support abstract, empirical
models. For example, ``commit early'' can be modeled as the percentage of
files in the system that are committed within a certain number of days of
their creation.  ``Collective code ownership'' can be modeled by the
percentage of files in the system that have been edited by every member of
the development team.

Supporting automation for these abstractions of empirical thinking for
software development means tool support for collecting, analyzing,
disseminating, and interpreting these abstractions.  For example, an
automated process can run once a day and calculate the current coverage and
complexity values for the system.  These values can be made available to
the user by a web application. Alternatively, email ``alerts'' can be sent
to the developers when coverage crosses a threshold and becomes too low, or
coupling crosses a threshold and becomes too high.  Plugins to development
tools like IDEs can collect information on what files are edited when in
order to determine the age of a file when it is first committed, or the
degree of collective editing on the file.

The goal of this research is to explore, evaluate, and institutionalize
advanced techniques and technologies for abstraction and automation of
empirical thinking in software development.  For example, we recently performed
an initial evaluation of a system and associated curriculum called the
``Software Intensive Care Unit'' \citep{csdl2-09-02}.  In this approach,
sensors attached to development tools automatically collect raw software
development process and product data and abstract it into a set of ten
``vital signs'' that provide an empirical way for students to gauge the
``health'' of their ongoing projects.  The Software ICU supports both
automated and abstract empirical thinking by students about the current
state and past history of both their projects and their group processes.

To achieve our goal, we will pursue three concrete objectives, organized
according to the three years of this grant.  

First, we will perform a study during the 2009-2010 academic year at the University of Hawaii in which we will validate and extend the findings from our initial case study of the Software ICU during 2008.  We will also begin evaluation of another pedagogical approach to advanced empirical thinking called Devcathlon, which is currently under development and scheduled for initial release by the time this project begins.

Second, we will partner with other academic institutions and departments
during the 2010-2011 academic year to gain insight into the issues that
occur when integrating empirical thinking into other advanced software
development courses.  In addition, we will begin exploring the issues
involved in adapting these initial materials both downward (into more
elementary curriculum); upward (into post-scholastic, professsional
settings); and outward (into related disciplines such as engineering or
information technology).

Third, we will use the data gathered and the collaborations formed during
the first two years to form an open source consortium to further spread the
use of abstract, automated empirical thinking.  This consortium will itself
be transformative in that it will combine open source technologies (such as
the Software ICU and Devcathlon) with open source curriculum materials
(which facilitate the introduction of the materials) with open source data
(results from the application of these technologies and pedagogies, which
can be used to guide future evolution of these approach).  The goal is to
go far beyond web site publication and create a social network of students
and educators interested in advancing the use of empirical thinking in
software development

Our vision, goal, and objectives are designed to produce outcomes that
directly support the goals of the CPATH program.  First, establishing
automated, abstract empirical thinking as an integral component of software
development courses can significantly improve the competitiveness of our
software engineering workforce by giving them facility with a powerful tool
for computational thinking.  Second, our approach begins by infusing a new,
empirical form of computational thinking into the advanced software
development curriculum and then propogates it downward, upward, and
outward.  Third, we will collect data and experiences on the effectiveness
of this approach across a variety of institutions and pedagogical models.

\subsection{Intellectual Basis/Related Work}

{\em Describe the intellectual basis for the project and discuss related
prior work.  Include a review of the research literature relevant to the
project and provide corresponding references. }

\bigskip

\subsubsection{Research from the empirical software engineering community}

As noted above, empirical thinking occurs naturally in software development
activities.  There is a well-established research community focusing on the
promulgation and advancement of empirical techniques in software
development.  Organizations such as the International Software Engineering
Research Network (ISERN), journals such as Empirical Software Engineering,
and conferences such as the International Symposium on Empirical Software
Engineering and Measurement all provide active forums for the role of
empirical thinking in software development.

Unfortunately, review of these forums indicates that the role of empirical
thinking in the software development curriculum receives relatively little
attention.  For example, in the 328 articles published in Empirical
Software Engineering from 2002 to 2008, we found only one article that
focused explicitly on the use of empirical techniques as part of software
development pedagogy \citep{Pfahl03}.  Through review of the other articles,
we found that while students are frequently employed in empirical research,
the goal of their participation is to support the testing of a research
hypothesis unrelated to classroom pedagogy.  For example, \cite{Babar08}
used students to support a controlled study of the differences between
distributed and face-to-face meetings for software architecture evaluation.
\cite{Carver06} used students as subjects to test an approach for helping
novice programmers learn software inspection techniques more quickly.
\cite{Host00} performed a study in which students were used to determine if
the data collected from students differs from data collected from
professionals.  Of course, our purpose is not to criticize the use of
students as subjects for empirical research, but rather to point out that,
at least in the case of the Journal of Empirical Software Engineering,
there is little research on teaching students empirical techniques.

Of course, it may be that this journal is not the appropriate venue for
such efforts.  Unfortunately, review of other forums yields similar
results.  The Empirical Studies of Programmers workshop series (now
discontinued) focused on the use of empirical technique to understand
programmer behavior, rather than teaching students how to use empirical
thinking to affect their own behavior.  The Empirical Software Engineering
and Measurement conference series has provided a forum for a variety of
applications of empirical techniques, including those for testing and
analysis, coordination and communication, estimation, modeling and
architecture, inspections, defect classification, and fault-prone module
prediction.  The only paper we found from the ESEM conference series
that focused explicitly on the introduction of empirical techniques into
the classroom setting was  \citep{csdl2-03-12}.

\subsubsection{Research in the software engineering education community}

The previous section demonstrates that the empirical software engineering
community does not (yet) focus on the pedagogy of empirical thinking. Fortunately, 
there is more evidence of interest in this approach in the software education 
research community.

In 1995, Watts Humphrey authored {\em A Discipline for Software
Engineering}, a ground-breaking text that adapted organizational-level
software measurement and analysis techniques to the individual developer
along with a one semester curriculum. These techniques are called the
Personal Software Process (PSP), and form the basis for the Team Software
Process (TSP), which extends the method to groups of developers. 

The PSP is the best known and most widespread approach to empirical thinking in the advanced software development curriculum.  The approach requires students to develop a series of software projects, typically six to eight during a single semester.  Both process and product measures are gathered about each project, and the measurements become increasingly detailed as the semester proceeds. After the first three projects are completed, the students can use the completed projects as historical data to support quality improvement (by identifying repeated types of defects) and estimation (through simple linear regression).  The PSP and TSP enjoy strong support from the Software Engineering Institute, which has published a number of case studies indicating success in a classroom setting and which sponsors a yearly symposium to publicize academic and industry experiences.  The PSP/TSP enable support for very basic levels of abstraction and automation of empirical thinking. For example, the PSP Dashboard is a tool that allows students to enter the data they collect and which will automate the calculation of regression lines. 

Conn developed a metrics-based software engineering course called the 
IS Integrated Capstone Project \cite{Conn04}.  The metrics were closely aligned
with the PSP/TSP format, though some of the process constraints were relaxed. 

Robillard designed a project-based course in which students were required
to fill out logs that specified the time spent on various activities
\cite{Robillard98}.  However, minimal abstraction and no automation was supported.

One recent research effort shows how abstract and automated empirical thinking
can be used in introductory programming courses. The Retina system automatically collects editing and compilation data on beginning
programmers,  which it then abstracts using a recommendation and suggestion
subsystem \cite{Murphy09}.  Retina can notice, for example, when a student
is getting many more errors per compilation than other students in the
class, and recommend that the student might want to break the work down
into smaller pieces.  Retina is designed around the needs of introductory
programming classes, where students typically work alone, do not use a wide
range of development tools, and a significant amount of energy is devoted
to obtaining a syntactically correct program.  Nevertheless, it demonstrates that there is significant potential for the use of abstract, automated, empirical thinking throughout the software development curriculum.

\subsubsection{From empirical to scientific and evidence-based thinking}

Integrating computational thinking concepts (automation and abstraction) with empirical thinking has the potential to transform and improve the software 
development curriculum in profound ways. One of the most important benefits of introducing empirical thinking is that it is a necessary precursor for scientific and evidence-based thinking in software development. 

One of the most eloquent descriptions of the difference between empirical and scientific thinking is provided by John Dewey \citep{Dewey10}.  In his chapter ``Empirical and Scientific Thinking'', Dewey begins by noting that empirical thinking, which is based purely on observation, has been used by humans throughout history as a way of understanding through association.  For example, upon repeatedly noticing that when the sky is lowering upon sunset, rain follows the next day, one might form the ``conclusion'' that if the sky is lowering at sunset, rain will follow.  He follows this with a discussion of the danger of confusing correlation with causality, and introduces the scientific method as a way of addressing this problem.  From Dewey's point of view, the scientific method involves active experimentation under controlled or semi-controlled conditions (as opposed to passive observation) and the formation of testable theories that introduce causal mechanisms (for which evidence can be gathered to support, refute, or refine).  A major focus of the empirical software engineering community is to develop experimental techniques applicable to software development practice that replace simple observation with more controlled, hypothesis-driven analysis.

A related effort is the application of evidence-based medical research techniques to software development \citep{Kitchenham04,Kitchenham04a}, which involves a five step method: (1) Convert the need for information [about a software engineering practice] into an answerable question; (2) Track down the best evidence available for answering the question; (3) Critically appraise that evidence using systematic review for its validity (closeness to the truth), impact (size of the effect), and applicability (usefulness in software development practice); (4) Integrate the critical appraisal with current software engineering knowledge and stakeholder values [to support decision-making]; (5) Evaluate the effectiveness and efficiency in applying Steps 1-4 and seek ways to improve them for next time.  While promising, application of systematic reviews and the integration of empirical software engineering data from multiple sources has been found to be challenging \cite{Jedlitschka04}.

\subsubsection{Conclusions}

What did we figure out?  Basically, the current situation is that the empirical software engineering community has a strong emphasis on showing industrial applicability, and that issues related to pedagogy are largely ignored.  In the educational community, the focus has been dominated by the PSP/TSP, which is one possible approach to empirical thinking but has a very limited focus in terms of applicability.  It also has high overhead.  Smaller efforts are focussed on introductory classroom settings and simulations.  

We need to comment here on how this is a good time to start a push on abstract, automated empirical thinking.   That the barrier beforehand had a lot to do with inadequate technology. The problem is to avoid having to devote too much of your focus in the classroom to infrastructure issues.   That was also a barrier.  For example, most modern development tools have plugin support; there are a large variety of open source tools available that make empirical analyses available in a classroom setting. 


\subsection{Current State}

{\em Provide a current assessment of undergraduate education in the relevant participating organizations.  Describe prior pilot programs or planning activities conducted to date, if any, and their outcomes.  Where appropriate, provide institutional data to document the current environment by uploading data into the Supplementary Docs section in FastLane.}

PSP/TSP approaches require a significant amount of manual data
collection and analysis due to the nature of the analyses of interest. In
prior research \cite{csdl2-00-03}, we implemented extensive tool support
for PSP/TSP style of data collection and analysis, but still found the
overhead to be substantial \cite{csdl2-01-12}. In contrast, the Software ICU provides
significantly more automation of data collection and analysis, but focuses
on different kinds of data collection and analyses than the TSP/PSP.


Things we can put into this section:   for over 10 years, my software engineering curriculum at the University of Hawaii has involved empirical thinking.  During the late 1990's, I approached it from the standpoint of the PSP.  Frustrated by the low level of automated support, we built the LEAP framework to raise both the abstraction and automation levels.  We then decided that there was essentially a glass ceiling to the level of abstraction and automation possible with the PSP/TSP approaches due to the nature of the questions they wanted to answer. So, we started the Hackystat Framework in order to explore whether we could raise the level of abstraction and automation further by redirecting the kinds of questions to address.

We have done three case studies about empirical thinking so far using the Hackystat Framework.  The most recent one was completed in Fall of 2008 and involves the Software ICU.  

We also have a current development effort involving Devcathlon, which supports another kind of empirical thinking embedded in a game-like environment. 

SoDeT will be implemented using the Hackystat Framework, which
provides an infrastructure for automated software development process and
product metric collection.  Hackystat includes a variety of features useful
for SoDeT, including unobtrusive, sensor-based data collection, 
platform and tool independence, open source licensing, integration with
visualization packages such as JFreeChart and statistical analysis packages
such as R, support for both individual and group-based software development, 
and experiment management services. 

Hackystat is a relatively mature open source project, with approximately 50
stable releases spanning five years of development, and over 300,000 lines
of source code. Data collection sensors are available for over 30 tools
including Eclipse, Emacs, JBuilder, Jupiter, Jira, Visual Studio, Ant,
JUnit, JBlanket, CCCC, DependencyFinder, Harvest, LOCC, Office, CVS, and
SVN.  

Hackystat is being used in a variety of academic and industrial contexts.
At the University of Hawaii, Hackystat is integrated into the undergraduate
and graduate software engineering curriculum, and is used by approximately
50 students per year to provide experience in empirically-based software
project management \cite{csdl2-03-12}. It has been used at NASA's Jet
Propulsion Lab to analyze the daily build process for the Mission Data
System \cite{csdl2-03-07}.  Hackystat is used as a testbed for collecting
and analyzing developer productivity data by researchers at the University
of Maryland and SUN Microsystems as part of the DARPA High Productivity
Computing Systems program \cite{Hochstein05,csdl2-04-03,csdl2-04-22}.
Finally, Hackystat is actively used in industrial settings, including
software development groups at Expedia, Teradata, Motorola, Applied Systems
Intelligence, InfoSys, Wolters Kluwer, and other companies.

\subsection{Implementation Plan}

{\em Describe in detail the CT-centric activities to be undertaken to realize the project vision, goals, objectives and anticipated outcomes. 

Define, or describe how the proposing team will attempt to define, the core computing concepts, methods, technologies and tools to be integrated into promising new undergraduate education models.  Describe your plans to identify and implement effective strategies to develop and assess CT competencies in the relevant learning communities.  Identify the stakeholder cohort, e.g. K-20 administrators, faculty, teachers, students, etc., that will participate in and/or benefit from the activities. If relevant, describe how change will be effected and sustained in the participating organizations.  

Describe project milestones in the context of a project timeline and identify responsible parties and expected outcomes for each milestone.  Summarize this information in a figure that you upload into the Supplementary Docs section in FastLane. 

Describe how project outputs and outcomes will be disseminated to the relevant stakeholder groups and to the national community and if relevant, how project resources will be made available to others to adopt or adapt. Identify proactive measures to find and support adopters of promising models and/or practices. Describe plans for outreach to other groups or interested institutions that will take place during the project.  
}

Idea is to unify various communities.

Idea is to create minimal new infrastructure; instead, utilize existing.  Create tracks at existing conferences and meetings, don't establish new stuff.  Use Open Seminar, etc. 


\subsection{Collaboration and Management Plan}

{\em Provide a collaboration and management plan that will guide project implementation.  Describe how the project leadership team will form, orient, manage, and reinforce relationships in the project.  Provide evidence of the commitment of the participating organizations to effect and sustain the anticipated project outcomes; letters of collaborative support should be uploaded into the Supplementary Docs section in FastLane. }

Unify the PSP and Hackystat approaches. also PROM etc. Jazz. 

Create an internationally-based consortiium

\subsection{Evaluation Plan}

{\em Provide an evaluation plan that will inform the project progress and measure its impact.  Include a description of the instruments/metrics used to measure, document, and report on the project's progress.  Identify the evaluator who will be responsible for the evaluation component and discuss their expertise related to the evaluation as well as any other linkages to the project or organizations involved.  }



%% Letters of support
%%  Referentia
%%  Blanca Polo (community college level)
%%  Gail Kaiser 
%%  Vic Basili/UMD.  Special issue? 
%%  Lionel Briand? 
%%  Pekka
%%  Hakan
%%  Laurie Williams, Open seminar, Eclipse/Jazz
%%  Barry Boehm
%%  Dan Port (ITM) 
%%  Brian Pentland 
%%  The friend of Brian who visited apache foundation, etc. 
%%  Claes
%%  Morisio
%% Japan? 

%%  Contents of letter: importance of empirical thinking in software engineering; soundness of this research approach; potential interest in collaboration; 


