%%% \documentstyle [12pt,/group/csdl/tex/definemargins,
%%% /group/csdl/tex/lmacros]{report}
%%% \input{/home/3/dxw/c/tex/psfig}
%%% \special{header=/group/csdl/tex/psfig/lprep71.pro}
%%% \renewcommand{\horizontalline} {\rule{6.0in}{.010in}}
%%% \begin{document}
%%% \ls{1.2}
%%% 
%%% \tableofcontents
%%% \newpage
%%% \pagenumbering{arabic}

\setcounter{chapter}{5}
\chapter{Evaluation Results}
\label{sec:results}

This chapter presents the results from the use of CLARE. Section
\ref{sec:c6-overview} summarizes the experiments that have been conducted.
Section \ref{sec:synopsis} provides an overview of the results. The
subsequent five sections discuss the actual results. Section
\ref{sec:c6-hypothesis} describes the findings with respect to each of the
ten hypotheses identified in the previous chapter.  Section
\ref{sec:rep-issues} discusses main issues that arose from the use of the
RESRA language, including a list of common representation errors derived
from the usage data. Section \ref{sec:strategies} identifies several usage
strategies employed by learners during summarization and evaluation.
Section \ref{sec:case} presents a detailed analysis of one CLARE session.
The purpose of this section is to bring together all previous discussions
through a single, actual example, and to compare this example with the
hypothetical usage scenario described in Section \ref{sec:example}. This
chapter closes with a summary of the major conclusions from this
experimental evaluation of CLARE.


\section{Summary of the experiments}
\label{sec:c6-overview}

Five sets of CLARE experiments were conducted between September 2, 1993 and
October 12, 1993. The first three sets involve 16 students from a
senior-level, second-semester software engineering class. This class was
divided into 4 groups, which correspond to the 4 pre-existing project
groups. The experiment was repeated three times on different research
papers, which resulted in a total of 12 experiments.  The subject studied
by these groups was ``software quality assurance.'' The following three
papers were used in these experiments:

\ls{1.0}
\begin{itemize}
\item ``No silver bullet: essence and accidents in software engineering''
  by Frederick Brooks;

\item ``Design and code inspections to reduce errors in program
  development'' by Michael Fagan; and
  
\item ``An empirical study of the reliability of UNIX utilities'' by
  Barton Miller, Lars Fredriksen, and Bryan So.
\end{itemize}
\ls{1.6}

The remaining sets of experiments involved 8 graduate students from a class
in advanced software engineering. The class was randomly assigned to two
groups, each consisting of four students. Each group participated in two
CLARE sessions, which resulted in 4 experiments. The subject of study was
``requirements engineering.'' The following two papers from the
pre-assigned reading list were used in the experiments:

\begin{itemize}
\item ``The Automated Requirements Traceability System (ARTS): an
  experience of eight years'' by R.F. Flynn and M. Dorfman; and
  
\item ``Supporting systems development by capturing deliberations during
  requirements engineering'' by B. Ramesh and Vasant Dhar.
\end{itemize}

The duration of most experiment sessions was one week.  Some sessions
lasted two to three days longer because of interruptions from other class
activities.

Table \ref{tab:summary-stat} provides the summary statistics of the 16
CLARE sessions. These experiments have amounted to a total of nearly 300
hours of usage time\footnote{The usage time reported here is actually {\it busy
minutes\/} the learners spent in CLARE. The actual connection time was
significantly larger. See Section \ref{sec:data-collection} for the
definition of {\it busy minutes\/}.}, and generated about 1,800 nodes with
a total text size of nearly 400 kilobytes. A total of over 80,000
timestamps about the usage process were also gathered during these
sections.  The relatively large standard deviation in the number of
connections per session, the amount of effective usage time, the number of
nodes created, and the total size of these nodes indicates that the level
of participation varies greatly among individual learners. A noticeable
trend can be observed from the data shown in Table \ref{tab:summary-stat}:
as the learners progressed from the first into the second and/or third
CLARE sessions, there was a substantial decline in the usage time while at
the same time an increase in the level of learning activities, as reflected
in the number of nodes created and the total size of these nodes, for
instance, in the experiments D and E.  This figure seems to suggest that,
at the outset, learners spent quite a portion of their time merely in
getting accustomed to the system. As time went on, they were able to become
more focused on the task on hand.

\ls{1.0}
\small
\begin{table}[hbtp]
    \caption{Summary statistics on CLARE experiments}
    \begin{center}
    \begin{tabular} {||c|p{0.8in}|p{0.8in}|p{0.8in}|p{0.8in}||} \hline   
    {\bf Experiments} &   {\bf No. Connections} & {\bf Usage Time (hrs)}&  {\bf
    Node Counts} & {\bf Text Size (Kb)} \\ \hline \hline 
    {\bf A.  (4x4)}    &   120 &   82.85   &   472   & 90.02 \\ \hline
    {\bf B.  (4x4)}    &   115 &    67.90 &     513 &  107.97 \\ \hline
    {\bf C.  (4x4)}    &   84 &    53.68  &    440  & 105.16 \\ \hline \hline
    {\bf D.  (2x4)}    &   85 &    54.42  &    162  &  39.42 \\ \hline
    {\bf E.  (2x4)}    &   53  &   37.55   &   207   & 49.67 \\ \hline  \hline
    {\bf Total}        &   457  &  296.40   &  1794   & 392.24 \\ \hline  \hline
    {\bf Indiv. Avg.}  &   7   &  4.63  &     28   &   6.13 \\ \hline  \hline
    {\bf Indiv. Std.}  &   6   &  3.19  &     16   &   4.58 \\ \hline
    \end{tabular}
    \end{center}    
    \label{tab:summary-stat}
\end{table}
\normalsize
\ls{1.6}

Figure \ref{fig:time-distribution} shows a more detailed view of the
distribution of the usage time per session spent by individual learners and
groups. Despite the relatively small sample size (group n = 6), several
patterns of participation already become visible. For example, group 3 is
the most balanced but with relatively moderate traffic; group 1 and 6 are
both active groups except that the latter does not have an inactive member
like learner 4 in group 1; and Group 5 and Group 1 share a similar pattern
but differ in the level of participation. The average time devoted to the
two phases of CLARE seems quite constant across all groups: learners spent
on an average about 70\% of their time in Phase I (exploration) and 30\% in
Phase II (consolidation). This time pattern provides a basis for explaining
certain usage results to be discussed later in the chapter.

\begin{figure}[htbp]
 \fbox{\centerline{\psfig{figure=Figures/time-distribution.eps,width=5.0in}}}
 \caption{Distribution of average CLARE usage time during
 exploration and consolidation}
  \label{fig:time-distribution}
\end{figure}


\section{Overview of the results}
\label{sec:synopsis}

The CLARE experiments generated three types of data: assessment, outcome,
and process. The combination of this data provides these important
findings:

\begin{itemize}
\item Evidence about the viability of RESRA and CLARE;
  
\item Insights about collaborative learning from scientific text using
  CLARE; and

\item Evidence about the limitations of RESRA and CLARE.
\end{itemize}

The main findings in these three areas are summarized in Table
\ref{tab:result-summary}.

\ls{1.0}
\small
\begin{table}[hbtp]
  \caption{Summary of major CLARE experimental findings}
  \begin{center}
    \begin{tabular} {||c|p{4.0in}|p{0.5in}||} \hline   
    {\bf No.} &   {\bf Description}  & {\bf Sections} \\ \hline \hline

    {\sf RESRA\(_1\).} & RESRA is effective in exposing different points of
    view on scientific text. & \ref{sec:c6-resra-hypothesis} \\ \hline  
    
    {\sf RESRA\(_2\).} & RESRA is a useful means for characterizing the
    content of scientific text. & \ref{sec:c6-resra-hypothesis} \\ \hline
 
    {\sf RESRA\(_3\).} & RESRA can be interpreted in many different
    ways.  & \ref{sec:rep-issues}, \ref{sec:case} \\ \hline
    
    {\sf RESRA\(_4\).} & Effective use of RESRA requires support for
    representation-level deliberation. & \ref{sec:rep-issues},
    \ref{sec:case}, \ref{sec:c6-discussions} \\ \hline
    
    {\sf RESRA\(_4\).} & Most learners showed difficulties in using
    RESRA to map the content of research papers. &
    \ref{sec:c6-resra-hypothesis}, \ref{sec:rep-issues},
    \ref{sec:case} \\ \hline \hline
    
    {\sf CSCL\(_1\).} & Summarization is the most important, difficult,
    and labor-intensive step in the SECAI model. &
    \ref{sec:case} \\ \hline
     
    {\sf CSCL\(_2\).} & The major themes of a research paper were
    not always recovered, even by a group of learners & \ref{sec:case} \\ \hline
    
    {\sf CSCL\(_3\).} & Summarization alone might be sufficient as a
    basis for argumentation. & \ref{sec:case}  \\ \hline
   
   {\sf CSCL\(_4\).} & Learners adopted four general strategies in their
   summarization of research artifacts. & 
   \ref{sec:sum-strategies}  \\ \hline
   
   {\sf CSCL\(_5\).} & A {\it linear, one-pass\/} summarization strategy
   does not seem sufficient for reconstructing the major thematic
   map of a research paper. & \ref{sec:case} \\ \hline \hline
   
   {\sf CLARE\(_1\).} & CLARE is a novel and useful environment for
   supporting {\it meaning-oriented \/}collaborative learning. &
   \ref{sec:c6-clare-hypothesis} \\ \hline
   
   {\sf CLARE\(_2\)} & Two most useful features of CLARE is the node
   primitives and the SECAI process. & \ref{sec:c6-clare-hypothesis}
   \\ \hline
    
   {\sf CLARE\(_3\).} & The greatest barrier to using CLARE is the user
   interface. & \ref{sec:c6-clare-hypothesis} \\ \hline
  \end{tabular}
  \end{center}
   \label{tab:result-summary}
\end{table}
\normalsize
\ls{1.6}

The assessment survey and qualitative feedback from the CLARE users show
that CLARE is a novel and useful learning tool: about 70\% of learners
indicated CLARE helped them understand the content of research papers in a
way not possible before, and almost 80\% of learners indicated that CLARE
helped them understand their peers' perspectives in a way not possible
before. The essence of CLARE, as discovered by one learner, was that
``...Before I used CLARE I just read the artifacts.  Now using CLARE I look
for the meaning of the artifact...''  Similar results were also reported on
RESRA: 84\% of the learners found that RESRA provides a useful means for
characterizing the important content of research papers, and 90\% of the
learners agreed that RESRA helped expose different points of view on an
artifact.

The experimental data also provides important insights on computer
supported collaborative learning from scientific text. First, using RESRA
to summarize the content of a research paper is the most time-consuming and
difficult step in the SECAI process: most of the exploration time, which
amounts to about 66\% of the total usage time, was devoted to
summarization. Yet, the analysis of the CLARE database reveals a great deal
of mis-representations, missed major themes, and/or links between links
these themes, and so forth. Second, data from the consolidation phase shows
that most argumentation was directed at summarative rather than evaluative
ones. This finding implies that summarization itself might suffice as a
basis for argumentation. Third, the analysis of the database content also
reveals that learners often failed to identify and represent major themes
of selected artifact at both the individual and group levels. Instead, they
singled out many relatively minor features. And finally, the process data
reveals that most learners adopted a {\it linear, one-pass\/} strategy
during summarization.  This finding, together with the large number of
missed major themes and the links between these themes, seems to suggest
that this summarization strategy is not sufficient for discerning the major
themes of an artifact and the relationships between them, much less for
representing them in RESRA.

The CLARE evaluation also reveals a number of problems about RESRA and
CLARE. The analysis of the CLARE database shows that most learners
encountered difficulties in using RESRA primitives to map the content of
the research artifacts, as manifested by the large number of
mis-representations. The wide variations of representation among learners
and the different types of errors they committed also indicate that RESRA
is subject to many interpretations. At the CLARE level, several problems
also became evident during the evaluation. On the top of this list is the
less-than-intuitive interface. Other factors, such as lack of online
tutorial, difficulties in link creation, lack of sufficient training, and
system crashes, were also mentioned.

The remainder of this chapter provides more detailed discussion on these
results.

\begin{figure}[htbp]
 \fbox{\centerline{\psfig{figure=Figures/resra.ps,width=5.0in}}}
 \caption{How many times did you find important contents and
 relationships cannot be expressed in RESRA node and link primitives,
 respectively?}
  \label{fig:c6-resra}
\end{figure}


\section{Hypothesis evaluation}
\label{sec:c6-hypothesis}

\subsection{RESRA}
\label{sec:c6-resra-hypothesis}

\subsubsection{H\(_1\): Effectiveness of RESRA in representing important
contents of scientific text}

The post-session survey showed that 84\% of users\footnote{The assessment
survey was conducted after each CLARE session. Since all learners
participated in multiple sessions, they were counted as separate entries.}
found that RESRA provides a useful means for characterizing what is
important in a research paper. Figure \ref{fig:c6-resra} also indicates
that nearly every 3 out of 4 learners encountered problems with RESRA node
and link primitives while using CLARE. The latter raises some deeper issues
related to the interpretation and use of RESRA.  A close review of the
content of the CLARE database reveals a wide variety in the level of
understanding of RESRA primitives and how they should be used. For example,
some learners showed a good grasp of RESRA by their correct use of the node
and link primitives. On the other hand, cases were found in which about
half of the nodes created by the learner were used incorrectly.  Such a
high error-rate indicates that, in those cases, the learner lacks a basic
understanding of RESRA, the content of a paper, or both. Most people showed
at least one representation error during the experiment. The error rate
within each individual also tends to vary greatly from paper to paper.  No
apparent correlation, however, was found between the type of papers studied
and the rate of representation errors.

The outcome data indicates that important thematic components of a paper
were not always represented, sometimes even at the group level. Instead,
many minor themes were represented. In addition, relationships between
these thematic components were often omitted: 23\% of users did not link
the summarative nodes they created. Common representational differences and
RESRA usage errors are discussed in Section \ref{sec:rep-issues} and
\ref{sec:case}.

To conclude, RESRA was viewed as useful despite that it was poorly
understood and often, incorrectly used.


\subsubsection{H\(_2\): Effectiveness of RESRA in facilitating the integration
of different points of view}

RESRA was found useful in exposing different points of view on the artifact
among different learners: 90\% of the users indicated that they either
agree or strongly agree with the above statement. The claim is further
substantiated by the variety of representations of the artifact found in
the database. Although 64\% of the users affirmed that CLARE supports the
integration of different points of view, the actual outcome and process
data indicates that integration was often not done. One of the reasons
might be the current CLARE interface, which does not allow simultaneous
display of many nodes on the screen, and thus does not support smooth
integration.

To conclude, RESRA is effective in exposing different points of view but
its facilitative role in the integration of these views is not yet
evidenced.


\subsubsection{H\(_3\): Viability of RESRA as a framework for collaborative
construction of knowledge}

The confirmation of the previous two hypotheses indicates that RESRA
represents a viable framework for facilitating collaborative learning.
However, as shown in Section \ref{sec:rep-issues}, the usefulness of RESRA
as a collaborative tool was limited by the general lack of a shared
understanding of the semantics of the representation language. Because
different learners had different interpretations of, for example, what
constitutes an \fbox{{\sf evidence}} or \fbox{{\sf method}}, the
resulting representation became difficult to compare and integrate.

The analysis of the CLARE database indicates that most of argumentation in
the consolidation phase was triggered by summarative instead of evaluative
nodes. Part of the reason might be the relatively small number of
evaluative nodes created during the exploration phase: half of the users
did not create any evaluative node in this phase. On the other hand, it
also indicates that summarization by itself might be sufficient as a basis
for collaborative deliberation.  Furthermore, evidence on RESRA-triggered
discussions was also found in the database. The following argumentative
nodes are two examples:

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}

    \item {\sf Type}: \fbox{{\sf critique}}
 
    \item {\sf Subj}: Nice structure.
 
    \item {\sf Desc.}: I like the way you structured your analysis.
      Proposing a theory and backing up different parts of the theory
      with different parts of the paper.  I think you've effectively
      captured the essence of the paper.  You've reached CLARE nirvana.
  \end{itemizenoindent}
  
  \begin{itemizenoindent}
    \item {\sf Type}: \fbox{{\sf critique}}
 
    \item {\sf Desc.}: Again I'm not try to "flame" you, but you've
      only found two evidences to support it and I found three and
      called it pure conjecture.  I felt this way because the author
      failed to present any counter evidence (perhaps because he knew
      doing so would prove him wrong?).
  \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\subsubsection{H\(_4\): Effectiveness of CRF in characterizing the artifact-level
thematic structure}

The survey shows that 67\% of the users indicate that the predefined CRFs
captured the main thematic structure of selected research papers.  However,
the process data also reveals that only 28\% of the users actually invoked
any CRF related functions, and only 18\% of them invoked CRF functions
multiple times. This data indicates that many learners were unaware the
availability of CLARE's CRF capabilities, much less able to exploit them
during summarization.


\subsubsection{H\(_5\): Effectiveness of CRF in helping expose
ambiguities and gaps in the content of an artifact}

Figure \ref{fig:ambiguity} shows the distribution of the frequency at which
CRFs had sensitized the learners about ambiguities and gaps in the research
paper: 31\% of the users indicated that CRFs had never helped them, while
63\% of them indicated that CRF had sensitized them at least once or twice
about weaknesses of the paper they had just read.

\begin{figure}[htbp]
 \fbox{\centerline{\psfig{figure=Figures/ambiguity.ps,width=4.5in}}}
  \caption{How many times did CRFs sensitize you about ambiguities and gaps
  in the paper?} 
  \label{fig:ambiguity}
\end{figure}


\subsubsection{H\(_6\): CRF helps lead to a consensual view on a learning
artifact}

Evidence from the database does not seem to confirm this hypothesis. The
main reason seems because CRFs are only useful for modeling the structure
of major themes of an artifact, while many learners were found representing
both major themes and minor themes. In the latter case, the major themes
often became {\it lost\/} in the mist of the minor ones.


\subsection{SECAI}

\subsubsection{H\(_7\): The dichotomy of exploratory and consolidation
phases facilitates collaboration}

The survey showed that 86\% of the learners agreed or strongly agreed that
the CLARE's two-step process model encourages each learner to do his or her
part; 92\% of them indicated that the model in fact facilitates the
formation of individual views on the paper. However, current experiments do
not provide empirical data to either support or counter the claim that the
SECAI process model facilitates explicit collaboration among users.



\subsubsection{H\(_8\): Over time learners exhibit a migration from
summarative to evaluative activities}

Although 61\% of the users agreed that the ability to see their peers'
points of view helps improve their skills in evaluating research papers,
the data from the experiment showed no such migration took place at either
individual or group levels. One explanation might be that the testing of
the above hypothesis requires a longer duration experiment than that
performed here.


\subsection{CLARE}
\label{sec:c6-clare-hypothesis}

\subsubsection{H\(_9\): Fine-grained comparison leads to effective
deliberation and integration}

Nearly 80\% of the users found that CLARE's comparison mode is useful in
highlighting different points of view. Since the current version of CLARE
does not provide precise metrics on how this mode was used, it is difficult
to assess whether the above subjective view is consistent with the usage
pattern, for example, whether an argumentative node was created due to the
use of the comparison mode. It is evident, however, that the usefulness of
the current comparison mode diminishes rapidly as the number of nodes of a
given type (e.g., \fbox{{\sf claim}}) increases. Since the average number of
summarative nodes in the current experiment was relatively large, it might
have limited the potential usefulness of this function.


\subsubsection{H\(_{10}\): CLARE is a viable platform to support
collaborative learning}

\begin{figure}[hbtp]
 \fbox{\centerline{\psfig{figure=Figures/freq-use.ps,width=5.0in}}}
  \caption{How often should CLARE be used in classroom settings?}
  \label{fig:freq-use}
\end{figure}

Assessment data from CLARE users shows that the system represents a viable
means for supporting collaborative learning from scientific text. About
70\% of the users indicate that CLARE helped them understand the content of
research papers in a way that was not possible before. Almost 80\% of the
users indicate that CLARE helped them understand their peer's perspectives
in a way that was not possible before. Figures \ref{fig:freq-use} shows
further evidence on the viability of CLARE: about 70\% of the users
indicate that CLARE should be used in classroom settings at least once a
semester.  This figure is essentially consistent with Figure
\ref{fig:future-use}, which indicates that, if CLARE were currently
available for public usage, 65\% of the users would recommend using it to
study research papers.

\begin{figure}[hbtp]
 \fbox{\centerline{\psfig{figure=Figures/future-use.ps,width=5.0in}}}
  \caption{Would you recommend using CLARE for studying research papers?}
  \label{fig:future-use}
\end{figure}

In addition to the above quantitative indicators, users' comments, such as
the ones quoted below, also highlight the usefulness of CLARE:

\ls{1.0}
\small
\begin{quotation}
  ``... I would just like to say that I really like CLARE.  I don't quite
  know how to use it very well yet, but it really helped me get more out
  of the artifact we read.  Without CLARE I would have just read the
  artifact and not really studied it or learned about the subject. CLARE
  made me look at the artifact from another point of view.  That point of
  view was what is the author trying to tell me and how is the author
  trying to tell me that information.  This point of view is new to me.
  Before I used CLARE I just read the artifacts.  Now using CLARE I look
  for the meaning of the artifact and learn more about the subject. I
  would like to continue to use CLARE to read more papers.'' \\ \par
  
  ``I think CLARE can be used for all types of research papers, not just
  SE [software engineering]-related ones. There might be a few
  adjustments/amendments needed but CLARE already is designed to
  accommodate any type of change (as I see it).''
\end{quotation}
\normalsize
\ls{1.6}


Figure \ref{fig:clare-features} shows the relative ranking by the learners
on the usefulness of CLARE user-level functions. The RESRA {\it node
primitives\/} and the {\it SECAI learning model\/} were ranked the highest,
viewed by 42\% and 40\% of the users, respectively, as {\it extremely
useful\/}. At the bottom of the list was the {\it online examples\/}: 25\%
of the users considered them as {\it not useful at all\/}. {\it RESRA
templates,\/} {\it comparison mode,\/} and {\it link primitives\/} received
mixed reactions from the user. See the hypotheses {\bf H\(_1\)}, {\bf
H\(_5\)}, {\bf H\(_9\)} in this section for possible explanations about the
last three rankings.


\begin{figure}[hbtp]
 \fbox{\centerline{\psfig{figure=Figures/features.ps,width=5.0in}}}
 \caption{Learners' ranking of the usefulness of CLARE functions}
  \label{fig:clare-features}
\end{figure}


Figure \ref{fig:clare-barriers} shows the ranking of barriers that might
have prevented users from effectively using CLARE. At the top of this list
is the {\it user interface\/}: 70\% of users considered it at least {\it
somewhat an obstacle\/}, and 25\% of users considered it as {\it a great
obstacle\/}.  At the bottom of this list is the {\it node primitives\/}:
47\% of users did not consider it as an obstacle at all. {\it Link
primitives\/} and {\it link mode\/} (the mode for creating links between
nodes) were considered as moderate barriers.

\begin{figure}[hbtp]
  \fbox{\centerline{\psfig{figure=Figures/barriers.ps,width=5.0in}}}
 \caption{Barriers to effective use of CLARE}
  \label{fig:clare-barriers}
\end{figure}

In addition to the system-level barriers mentioned above, the initial usage
of CLARE was also negatively affected by the following factors:

\begin{itemizenoindent}
\item {\it Time-consuming.\/} This factor seems already evident from
  Table \ref{tab:summary-stat}, which shows that each learner spent on an
  average of almost 5 hours on each CLARE session. The same concern was
  also raised a number of times during feedback. The following user
  comments echo this sentiment:

  \ls{1.0}
  \small
  \begin{quote}
     ``CLARE is very time consuming. The time it takes to evaluate a
     paper with CLARE is lengthier than the time it takes to write a
     critical evaluation the old fashion way.'' \\ \par

    ``I think that it [CLARE] is very interesting. Unfortunately I
    haven't been able to give my complete effort on this project
    because I am already spending 20+ hours a week on other things from
    ICS414.'' \\ \par
    
    ``I think CLARE is very useful for papers that are worth spending a
    lot of time on. We can't afford, though, to spend so much time on a
    paper.''
  \end{quote}
\normalsize
\ls{1.6}

\item {\it Lack of adequate training.\/} As part of the experimental
  procedure, an hour-long overview and demo of CLARE were given to all new
  users. However, such a level of training does not seem sufficient.  In
  fact, one user mentioned during a post-session discussion that at least
  one or two class-sessions should be devoted exclusively to teach students
  how to use RESRA. The situation was further complicated by the lack of
  online help and tutorial in the current version of CLARE, and a relatively
  tight schedule within which the experiments were conducted. The following
  user remark says it all:

  \ls{1.0}
  \small
  \begin{quote}
    ``As with all powerful system[s], it takes a while to learn. I believe
    the students frustration will be less as they get more familiar
    with the package. A tutorial (online) might help.''
  \end{quote}
    \normalsize
    \ls{1.6}
    
  \item {\it Improper selection of the research paper.\/} All papers
    used in the CLARE experiment are research-oriented. To undergraduate
    learners such as those from ICS414, who were not exposed to the
    research literature before, the content, style, and even the length
    of such text might be a source of problems, as shown in the following
    comment:

    \ls{1.0}
    \small
  \begin{quote}
   ``We should have worked with a small article so that we had time to
    actually test the function in CLARE ...''
  \end{quote}
  \normalsize
  \ls{1.6}
\end{itemizenoindent}

Despite the above system-level and environment barriers, CLARE
was found to be a novel and useful tool among its users. One key source of
its novelty and usefulness comes from the representation-based paradigm to
collaborative learning. The following section discusses some common
representation problems learners encountered during their use of the
system.


\section{Issues on the RESRA representation}
\label{sec:rep-issues}

The analysis of the CLARE database has revealed (1) a wide variation among
different learners in what contents they chose to represent, and how they
represented them; and (2) a wide variety of incorrect usage of the RESRA
representation.  This section uses selected examples from the experiments
to illustrate some main differences in the usage of the representation, and
to describe a list of common errors in using the RESRA primitives.

For a in-depth view of these variations, see Section \ref{sec:case}, which
provides a detailed case analysis of a CLARE session.


\subsection{Categorizing representational differences}
\label{key-rep-issues}

The outcome data suggests that the following factors might have contributed
to the wide variations of the representations among users: the granularity
of representation, the distinctions between major themes and minor themes,
between learners' views and authors' views, and between connotative and
denotative interpretations of RESRA primitives.


\paragraph{``Major themes'' versus ``minor themes.''}

RESRA was designed to represent essential themes of scientific text, and
for evaluating, deliberating, and integrating these themes.  In other
words, its focus is on the {\it major themes\/} rather than the {\it minor
themes\/} of the artifact. Figure \ref{fig:fagan} provides an example RESRA
representation of \cite{Fagan76}, which consists of 11 nodes that describe
the major themes of that paper.

\begin{figure}[hbtp]
 \fbox{\centerline{\psfig{figure=Figures/fagan.eps,width=5.0in}}}
 \caption{A RESRA representation of major themes of [Fagan76]}
  \label{fig:fagan}
\end{figure}

\ls{1.0}
\small
\begin{table}[hbtp]
  \caption{Comparison of 16 learner's summarizations of [Fagan76] with the
  {\it base representation\/}}
  \begin{center}
  \begin{tabular} {||l|c|c|c|c|c|c|c|c|c||} \hline
  {\bf User} & {\sf problem} & {\sf claim} & {\sf concept} & {\sf method} &
  {\sf evidence} & {\sf theory} & {\sf thing} & {\sf other} & {\bf Total}
  \\ \hline \hline

1 & 2 &    10 &   1 &   3 &   5 &   1  &  4 &   0 &   26 \\ \hline
2 & 1 &    2 &    0 &   1 &   3 &   0  &  0 &   0 &   7  \\ \hline
3 & 2 &    6 &    3 &   4 &   0 &   1  &  0 &   0 &   16  \\ \hline
4 & 1 &    8 &    3 &   5 &   10 &  0  &  9 &   0 &   36  \\ \hline
5 & 1 &    4 &    0 &   3 &   1 &   1  &  0 &   0 &   10  \\ \hline
6 & 0 &    4 &    0 &   4 &   8 &   0  &  0 &   1 &   17 \\ \hline
7 & 2 &    7 &    0 &   2 &   3 &   1  &  0 &   0 &   15 \\ \hline
8 & 1 &    8 &    0 &   8 &   11 &  0  &  0 &   0 &   28 \\ \hline
9 & 0 &    11 &   0 &   0 &   1 &   1  &  0 &   0 &   13 \\ \hline
10 & 0 &   8 &    3 &   5 &   1 &   2  &  0 &   0 &   19 \\ \hline
11 & 0 &   6 &    1 &   1 &   3 &   1  &  0 &   0 &   12 \\ \hline
12 & 2 &   27 &   1 &   2 &   6 &   3  &  0 &   0 &   41 \\ \hline
13 & 1 &   11 &   0 &   3 &   6 &   1  &  0 &   0 &   22 \\ \hline
14 & 0 &   8 &    0 &   2 &   6 &   0  &  0 &   0 &   16 \\ \hline
15 & 2 &   13 &   2 &   1 &   3 &   0  &  0 &   0 &   21 \\  \hline
16 & 1 &   4 &    1 &   1 &   2 &   0  &  0 &   0 &   9  \\ \hline
{\bf Base} & 1 & 3 &   3 &   2 &   1  &  0 &   0 &  0 & 10 \\
 \hline \hline
\end{tabular}
\end{center}
\label{tab:fagan}
\end{table}
\normalsize
\ls{1.6}

Table \ref{tab:fagan} shows a comparison of the summarization result of
\cite{Fagan76} by 16 learners and the {\it base representation\/} shown in
Figure \ref{fig:fagan}. Given the large average number of nodes created
(average = 19) per learner, one would expect that virtually all the major
themes of the paper be captured. The analysis of the content of those nodes
shows a different picture: none of the 16 learners had the right problem;
only 7 learners were able to correctly identify one or two of the three
major claims; 10 learners had the evidence right; and 6 learners had one of
the two methods right. These figures indicate that most attention of these
learners was devoted to minor themes instead of major ones.

The current version of RESRA/CLARE does not differentiate major themes from
minor ones, which perhaps was partially responsible for the problem. On the
other hand, this differentiation might have been of no help if the problem
learners had was not that they did not know how to use CLARE to represent
major themes but that they could not have identified those major themes
even if they had tried. The latter is exactly a learning deficiency that
CLARE attempts to help the learner overcome.

\paragraph{Granularity.}

Another source of representational variation is the {\it grain size\/} used
by learners in the node instances they created: some learners tend to
create large nodes, which bring together, for instance, all related
evidence with the corresponding claim, or vice versa. Others tend to
generate relatively small nodes, which sometimes split a single theme into
multiple nodes. For example, the following \fbox{{\sf claim}} describes 6
reasons that justify the authors' approach:

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}

  \item {\sf Type}: \fbox{{\sf claim}}
 
\item {\sf Subj}: 6 reasons
 
\item {\sf Desc.}: There are six reasons stated by the author why study of
  testing is important: 1. To contribute testing community a list of real
  bugs, which help the researchers to evaluate testing cases and verification
  strategies.  2. We have found the bugs that provides the security holes,
  therefore, additional bugs might be able to predict future security holes.
  3. Some errors are caused by careless inputs.  Some unexpected errors might
  be able to recover through this method.  4. We would like to have some
  meaningful and predictable response.  5. Noisy phone line shouldn't crashed
  the system.  6. Lastly, we would like to compare between this new method
  with the traditional testing strategies.

 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}

For the same content, however, another learner created 6 separate
\fbox{{\sf claim}} nodes, one of which is shown below:

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}

    \item {\sf Type}: \fbox{{\sf claim}}
 
    \item {\sf Subj}: Importance of our procedure (1)
 
    \item {\sf Desc.}: Provides a large list of real bugs that can provide as
      test cases to be used for more sophisticated testing and verification
      strategies.

 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}

Examples variations in granularity such as the one above are quite common
in the CLARE database. This finding calls into question the usefulness of
any findings based solely on the node count.


\paragraph{``Learners' views" versus ``authors' views.''}

In CLARE, the purpose of summarization is to capture the conceptual themes
of a paper as intended by the author; learner's opinions at this stage are
represented using RESRA evaluative primitives, i.e., \fbox{{\sf critique}},
\fbox{{\sf question}}, and \fbox{{\sf suggestion}}. The analysis of the
CLARE database uncovered RESRA node instances violating this rule: for
example, learners sometimes state their own \fbox{{\sf problem\/}} or
\fbox{{\sf claim\/}} as if it were the author's.  The following node was
created during summarization:

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}

  \item {\sf Type}: \fbox{{\sf claim }}
 
 \item {\sf Subj}: Careless Programming
 
 \item {\sf Desc.}: If return codes are not checked by the
   programmer, it is most likely a sign of careless programming.  Time
   and effort was not taken by the programmer to insure that values
   returned by a module is indeed valid and correct and therefore
   causes errors.

 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}

As pointed out by another learner in the following node created
during the consolidation phase, the view expressed in the above node was
not the author's, but of the learner who created the node:

\ls{1.0}
 \small
\begin{quote}
  \begin{itemizenoindent}

\item {\sf Type}: \fbox{{\sf question}}
 
\item {\sf Subj}: Your opinion?
 
\item {\sf Desc.}: You said that because return codes are not check
  that it is grounds for careless programming.  Is this your opinion?
  Or does the author really "claim" this?  I traced your node back to
  the source node and did not get the same opinion after reading it.
 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}

The above confusion of the learner's view and the author's view
seems partially attributable to the fact that the current CLARE does not
allow the learner to use other RESRA primitives except the evaluative ones
to express his or her views during summarization. For example, learners
cannot make a counter or alternative claim with the author, although they
can do so with each other during the consolidation phase.


\paragraph{``Connotative'' versus ``denotative'' interpretations of RESRA.}

RESRA's vocabulary consists of a set of familiar, everyday terminologies,
such as \fbox{{\sf problem\/}}, \fbox{{\sf method\/}}. It redefines those
terms with new and more specific meanings. However, as the CLARE usage data
from the current experiments suggests, many learners still rely on the
connotative meanings instead of the denotative ones when using RESRA. A
large number of incorrect usages described in the next section are related
to connotative interpretations of RESRA primitives.


\subsection{Common errors in using RESRA}
\label{common-errors}

This section documents a number of common incorrect usages of RESRA node
primitives found in the CLARE database. The purpose is twofold: to show the
extent and the variety of mis-interpretations of the representations by the
learners, and to provide a set of examples so that future users of CLARE
might avoid similar pitfalls.


\paragraph{Treating learner's ``problem'' as the author's ``problem.''}

The node below is an example of treating the learner's disagreement with
the author's position as a \fbox{{\sf problem}}. The node should be a
\fbox{{\sf critique}} instead:

\ls{1.0}
\small
\begin{quote}
 \begin{itemizenoindent}
   
 \item {\sf Type}: \fbox{{\bf problem}}
 
\item {\sf Subj}: Flawed Werewolf Allegory
 
\item {\sf Desc.}: The werewolf allegory used to suggest the need for a
  silver bullet is flawed.  The author argues that werewolves are
  terrifying because ``they transform unexpectedly from the familiar into
  horrors.''  This is false.  Werewolves are indeed predictable and will
  only transform during the times of the month when there is a full moon.
  
  The same predictability can be applied to the familiar software
  project.  If the design phase is carried in a precise and detailed
  manner, problems associated with the project can be predicted and
  ultimately avoided.
 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\paragraph{Treating \fbox{{\sf evidence}} as \fbox{{\sf claim}}.}

The following node is derived from a discussion on software changeability;
it is one piece of \fbox{{\sf evidence}} used by the author to support the
claim that changeability is an essential attribute of software systems, not
a \fbox{{\sf claim}} which he intends to elaborate:

\ls{1.0}
\small
\begin{quote}
 \begin{itemizenoindent}

\item {\sf Type}: \fbox{{\sf claim}}
 
\item {\sf Subj}: changes due to machine vehicles
 
\item {\sf Desc.}: New computers, disks, displays are always appearing.
  Software must be changed to conform to these new machine vehicles.

\end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\paragraph{Treating \fbox{{\sf claim}} as \fbox{{\sf theory}}.}

The following representation is an example of incorrect use of \fbox{{\sf
theory}} where \fbox{{\sf claim}} is more appropriate:

\ls{1.0}
\small
\begin{quote}
 \begin{itemizenoindent}

   \item {\sf Type}: \fbox{{\sf theory}}
 
   \item {\sf Subj}: No single development improves the situation
 
   \item {\sf Desc.}: No single development aids in improving the software
     problem, at least not with respect to productivity, reliability or
     simplicity.

 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\paragraph{\fbox{{\sf claim}} may not be ``neutral.''}

A \fbox{{\sf claim}} represents a {\it position\/} which can be {\it
supported\/} or {\it countered\/}. The following node is {\it
definitional\/} and neutral, and thus should be a \fbox{{\sf concept}}
instead:

\ls{1.0}
\small
\begin{quote}
 \begin{itemizenoindent}
 \item {\sf Type}: \fbox{{\sf claim}}
 
\item {\sf Subj}: Essence of software entities.
 
\item {\sf Desc.}: The software entity is made up of data sets,
  relationships among data items, algorithms, and invocations of functions.
  Software entities are abstract as well as precise and detailed.
 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\paragraph{\fbox{{\sf concept}} that makes a ``claim.''}

The following node makes a claim about a property of software systems,
i.e., that software is invisible. It does not define the concept of
``software system'':

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}

    \item {\sf Type}: \fbox{{\sf concept}}
 
\item {\sf Subj}: Software is invisible
 
\item {\sf Desc.}: Software is invisible and unvisualizabe.  It is not
  easily captured by a geometric abstraction like other concepts.
 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\paragraph{A ``concept'' cannot be a \fbox{{\sf theory}}.}

A concept might be a building block of a theory but itself cannot be a
theory. The following node should be a \fbox{{\sf concept}} instead:

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}
    
  \item {\sf Type}: \fbox{{\sf theory}}
 
  \item {\sf Subj}: Hawthorne Effect
 
\item {\sf Desc.}: The human bias of Hawthorne Effect supposedly plays a
  role in any production experiment.  If not handled adequately, it isn't
  clear whether the effects observed in a production experiment is due to the
  Hawthorne Effect or to newly implemented changes.

 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\paragraph{Treating \fbox{{\sf concept}} as \fbox{{\sf thing}}.}

A \fbox{{\sf thing}} is ``an object, event, or process that is the target
of an inquiry.'' ``Silver bullet'' is used metaphorically in the current
paper, and should be identified as a \fbox{{\sf concept}} instead:

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}
    
  \item {\sf Type}: \fbox{{\sf thing}}
 
  \item {\sf Subj}: silver bullet
 
  \item {\sf Desc.}: something that will kill the "monster of missed
    schedules, blown budgets, and flawed products."

 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\paragraph{Too ``mundane'' as a \fbox{{\sf concept}}.}

In a group that is devoted to requirement engineering and a paper that
exclusively describes such a system, the following \fbox{{\sf concept}}
node seems redundant:

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}

\item {\sf Type}: Concept
 
\item {\sf Subj}: Requirement
 
\item {\sf Desc.}: An authorized and documented need of a customer.

 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\paragraph{``Explanation'' \(\neq\) \fbox{{\sf theory}}.}

One primary purpose of a theory is to help explain a phenomena.  However,
the explanation itself does not constitute a theory:

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}

  \item {\sf Type}: \fbox{{\sf theory}}
 
  \item {\sf Subj}: complexity
 
  \item {\sf Desc.}: Complexity is at once both the main
    reason behind the power of software systems, and the
    main source of problems in developing it.
 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}

\paragraph{Learner's ``claim'' does not belong to \fbox{{\sf critique}}.}

The following node contains the learner's claim on high-level languages; it
does not identify weaknesses in what the author did or said, as a
\fbox{{\sf critique}} should:

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}

\item {\sf Type}: \fbox{{\sf critique}}
 
\item {\sf Desc.}: High-level languages have been extremely helpful in
  improving the reliability of software by standardizing certain reusable
  functions that the programmer need not create from scratch.
 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\paragraph{\fbox{{\sf evidence}} does not contain evidence.}

An \fbox{{\sf evidence}} contains qualitative or quantitative fact intended
to support or counter a given \fbox{{\sf claim}}.  The \fbox{{\sf
evidence}} node below does not provide the actual evidence; rather, it
points out the corresponding claim:

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}
    
  \item {\sf Type}: \fbox{{\sf evidence }}
    
  \item {\sf Subj}: REMAP provides design decision support
 
  \item {\sf Desc.}: This is evidence to show that REMAP provides
    mechanisms for maintaining and reasoning with dependencies among its
    primitives to arrive at design decisions.

 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\paragraph{``Method \(\neq\) \fbox{{\sf method}}.}

In RESRA, a \fbox{{\sf method}} is defined as a ``procedure or technique
used for generating evidence for a particular claim.'' The process for
``creation of primitives'' described in the node below is a \fbox{{\sf
method\/}} in the everyday sense but does not meet the definition of the
RESRA \fbox{{\sf method}}:

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}

    \item {\sf Type}: Method
 
    \item {\sf Subj}: Creation of Primitives
 
    \item {\sf Desc.}: The method adopted by the authors in arriving
      at a set of primitives for modeling design history is to start
      with a set of previously known standard primitives and then
      augment them on the basis of experiments made with expert
      analysts.

    \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\paragraph{Treating \fbox{{\sf critique}} as \fbox{{\sf claim}}.}

The following node does not contain the author's claim. Rather, the learner
attempted to point out what the author did:

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}

    \item {\sf Type}: \fbox{{\sf claim}}
 
    \item {\sf Subj}: ARTS - a viable tool
 
    \item {\sf Desc.}: Several claims have been made on the
      robustness of ARTS. While some of them are facts, others do not
      have a well supported evidence.

 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\paragraph{\fbox{{\sf suggestion}} contains no new proposal(s).}

A \fbox{{\sf suggestion}} should include ``ideas, recommendations, or
feedbacks on how to remedy or improve an existing \fbox{{\sf problem\/}},
\fbox{{\sf claim\/}}, \fbox{{\sf method\/}}, et al.'' The following node
does not satisfy this criterion, and should be a \fbox{{\sf critique}}
instead:

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}
  \item {\sf Type:\/} \fbox{{\sf suggestion}}
    
  \item {\sf Desc.}: Sure the author offers some examples of why he
    considers software the most complex thing that man has made.  But I think
    that his automobile analogy is pretty flimsy (automobiles are very
    complex).  I just don't think that he supported the claim enough.  And
    of course he did try to counter it in any way.
  \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}

%\paragraph{\fbox{{\sf critique}} does not ``critique.''}

\paragraph{"Prediction" \(\neq\) \fbox{{\sf theory}}.}

Although one important property of a \fbox{{\sf theory\/}} is its ability
to predict the outcome of a particular phenomena, the {\it prediction\/}
itself does not constitute a theory. This node should be a \fbox{{\sf
claim}} whose truth value is yet to tested.

\ls{1.0}
\small
\begin{quote}
  \begin{itemizenoindent}
  \item {\sf Type:\/} \fbox{{\sf theory}}
    
  \item {\sf Subj: Effectiveness of Ada\/}

  \item {\sf Desc.}: Prediction that Ada will have provided training
    for programmers in modern software-design techniques.

  \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\paragraph{Learner's questions but not author's \fbox{{\sf problem}}.}

The purpose of summarization is to capture the {\it author's\/} view, not
the {\it learner's\/}. When a learner encounters problems with the author's
view, he or she creates a \fbox{{\sf question}} or \fbox{{\sf critique}}
node. Below is a wrong way of using the \fbox{{\sf problem}} primitive:

\ls{1.0}
\small
\begin{quote}
 \begin{itemizenoindent}
   
 \item {\sf Type}: \fbox{{\sf problem}}
 
\item {\sf Subj}: Some Questions
 
\item {\sf Desc.}: 1. What is the model of REMAP?  2. What is the
  relationship between the REMAP and IBIS?  3. What is the prototype that has
  been completed ?
 \end{itemizenoindent}
\end{quote}
\normalsize
\ls{1.6}


\subsection{Implications}
\label{sec:implications}

The examples used above are merely a small sample of incorrect ways of
using RESRA. Nevertheless, they have important implications in the future
development of CLARE: while diversity in the interpretation and use of
RESRA is encouraged during the exploration phase, to fully exploit the
collaborative potential of the system requires two additional conditions to
be met:

\begin{itemize}
\item A basic level of shared understanding of the semantics of the RESRA
  language, including the differentiation of denotative and connotative
  interpretations of the primitives; and
  
\item A set of guidelines on the grain size of the representation, and
  the representation of major versus minor themes, and the learner's views
  versus the author's views.
\end{itemize}

Section \ref{sec:future-directions} explores both issues in more
detail. The next section shifts away from the current focus on the database
to the findings based on the process data.


\section{CLARE usage strategies}
\label{sec:strategies}

Over the course of the experiment, CLARE automatically accumulated over
80,000 timestamps that detail the process steps used by learners in their
interactions with the system. Such process data provides an important
source of information for understanding the usage behavior of the learner.
This section presents findings from this data; it discusses various
strategies used by the learners when they navigate the source artifact,
summarize and evaluate its content, and deliberate and integrate different
points of view and interpretations.

\subsection{Navigation}

Despite the hypertextual structure of the CLARE database, the process data
reveals that the majority of learners still followed the traditional,
linear way of navigating and understanding the text, that is, they
progressed sequentially from one source node to the next.  A complete
linear traversal of all source nodes is called a {\it round\/}.  Table
\ref{tab:round} shows the distribution of the number of rounds among
learners in various experimental sessions. Note that only 1-2 true {\it
random walkers\/} were found. The data also shows that nearly three-fourths
of learners completed their summarization and evaluation in less than two
rounds. This figure implies that few of them have in fact devoted a
separate round to browsing, summarization, or evaluation. Instead, they
seemed to concurrently engage in all three activities. This result is
somewhat surprising given that an implicit, {\it three-step\/} strategy,
({\sf browse\/} \({\Rightarrow}\) {\sf summarize \/} \({\Rightarrow}\) {\sf
evaluate\/}) is defined in the user documentation, and explicitly discussed
during the CLARE training session.

\ls{1.0}
\small
\begin{table}[hbtp]
  \caption{Distributions of the number of {\it rounds\/} on source nodes}
  \begin{center}
    \begin{tabular} {||c|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}||} \hline   
      {\bf No. of rounds} & {\bf Exp. A (4x4)} & {\bf Exp. B (4x4)} &
      {\bf Exp. C (4x4)} & {\bf Exp. D (2x4)} & {\bf Exp. E (2x4)} & {\bf
      Total}\\ \hline \hline
      
      1 and below & 3 & 2 & 5 & 2 &  3 & 15 \\ \hline

      1 - 2 & 7 & 8 & 7 & 5 & 3  & 30 \\ \hline

      2 and above & 4 & 4 & 2 & 0 & 0  & 10 \\ \hline

      Random & 1 & 1 & 1 & 1 & 2  & 6 \\ \hline
     Missing & 1 & 1 & 1 & 0 & 0  & 3 \\ \hline
    \end{tabular}
    \end{center}    
    \label{tab:round}
\end{table}
\normalsize
\ls{1.6}

There are three possible explanations for the above dominance of the linear
navigation behavior. First, since the research papers used in the
experiment were designed for a linear, non-hypertext environment, it is
perhaps more comprehensible to approach them linearly, despite their
hypertextual presentation in CLARE.  Second, some learners were not
accustomed to the hypertext interface. They felt more comfortable with the
traditional approach, as indicated in the following remark:

\ls{1.0}
\small
\begin{quote}
  More and more, I think that my problems with CLARE stem simply from a
  dislike of hypertext. I am frustrated that I can't easily skim old
  material.  Tracing links and following nodes with the mouse-middle
  button is (IMNSHO)\footnote{Stands for ``In My Not So Humble Opinion.''} a
  poor substitute for having the pieces of paper in front of you. When I
  wish to respond to something it is exasperating to have the node
  disappear as a new window opens up over it. Can these problems be
  addressed by current technology? I don't know. Are they critical? I
  guess that depends on one's viewpoint. To me, yes.
\end{quote}
\normalsize
\ls{1.6}

And finally, as shown in Figure \ref{fig:source-structure}, CLARE currently
does not allow easy {\it random\/} access. For example, if a user wants to
jump from the source node 1 to the source node 3, he or she first needs to
go through the root node or source node 2\footnote{CLARE does have an {\sf
sbuff} facility which allows a random selection of any source node to
visit.  Because of this function is available only from the pulldown menu,
many users may not be aware of it.}.

\begin{figure}[hbtp]
 \fbox{\centerline{\psfig{figure=Figures/source-structure.eps,width=4.5in}}}
 \caption{The source node structure of the CLARE database}
  \label{fig:source-structure}
\end{figure}


\subsection{Summarization}
\label{sec:sum-strategies}

The analysis of the process data reveals that, based on the order in which
nodes and links are created, four summarization strategies were present:

\begin{itemize}
  \item {\sf Sum:} Create summarative nodes only. No attempt is made to
  connect them together using RESRA link primitives;
  
\item {\sf Sum \({\Rightarrow}\) Link:} First create summarative nodes
  for the entire artifact. Then link them together, if possible;
  
\item {\sf Sum \( \Leftrightarrow \) Link:} Create summarative nodes for
  a single source node, or between that node and the adjacent source
  nodes. Then create links between them. Repeat the same process until
  all source node are summarized;
  
\item {\sf Sum \({\Rightarrow}\) Link \({\Rightarrow}\) (Sum \(
  \Leftrightarrow \) Link):} A combination of Strategy 2 and 3. First
  create summarative nodes for the entire artifact, followed by adding
  links between these nodes. Next, selectively create additional
  summarative nodes and, as necessary, add links between them.

\end{itemize}

Table \ref{tab:summarization strategies} shows the distribution of the
above four strategies in various CLARE sessions. Note that 36\% of learner
sessions adopted Strategy 1, which creates no summarative links at all.
There are three potential explanations for this high percentage of absence
in link creation:

\begin{itemize}
\item {\bf Time pressure:} Since learners had to create summarative nodes
  before they could link them together, it is possible that, by the time
  they finish the former, they were tired, or ran out of the allocated
  time, or both.
  
\item {\bf Difficulty with RESRA link primitives:} As mentioned earlier,
  learners tend to create many summarative nodes on minor themes of a
  paper. Since RESRA link primitives are designed for representing the
  relationships between major themes, learner might have found it difficult
  applying these primitives to detailed nodes.
  
\item {\bf Difficulty in using CLARE's link mode:} The text-based
  interface of CLARE's link mode might have prevented some learners from
  using it.
\end{itemize}

\ls{1.0}
\small
\begin{table}[hbtp]
    \caption{Distribution of summarization strategies}
    \begin{center}
    \begin{tabular} {||c|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}||} \hline   
      {\bf Strategies} & {\bf Exp. A} & {\bf Exp. B} & {\bf Exp. C}
      & {\bf Exp. D} & {\bf Exp. E} & {\bf Total}\\ \hline \hline
      
      {\sf Sum } & 5 & 5 & 3 & 4 &  6 & 23 \\ \hline

      {\sf Sum } \({\Rightarrow}\) {\sf Link} &  6 & 1  & 1  & 3  & 0  & 11 \\ \hline

      {\sf Sum } \( \Leftrightarrow \) {\sf Link\/} & 2 & 7 & 7 & 0 & 2  & 18 \\ \hline
            
      {\sf Sum } \({\Rightarrow}\) {\sf Link\/} \({\Rightarrow}\)
      ({\sf Sum} \( \Leftrightarrow \) {\sf Link\/}) & 1 &3 &1 &1 &
      0& 6\\ \hline
      
      other & 2 & 0 & 4 & 0 & 0 & 6 \\ \hline
    \end{tabular}
    \end{center}
    \label{tab:summarization strategies}
\end{table}
\normalsize
\ls{1.6}

The majority of summarization was done through a single-round navigation of
the source nodes, which might have in part accounted for the fact that,
despite the large average number of summarative nodes created, major themes
of a paper were still often missed.  Discerning the major themes of a paper
and the relationships between them often requires relating different parts
of the paper, which is very difficult to achieve via a single pass of the
artifact.


\subsection{Evaluation}

Table \ref{tab:evaluation strategies} shows four major strategies used by
the learners when evaluating the content of research papers and the
distribution of these strategies in various experiments. The data reveals
that half of the learners did not do evaluation at all during exploration
phase. Among the learners who did evaluation, the majority of them (84\%)
did so by mixing summarization and evaluation into a single stream of
activity, i.e., {\sf Sum} \(\Leftrightarrow\) {\sf evaluation} or vice
versa. There was no single instance in which a separate round was devoted
solely to evaluation.

\ls{1.0}
\small
\begin{table}[hbtp]
    \caption{Distribution of evaluation strategies}
    \begin{center}
    \begin{tabular} {||c|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}||} \hline   
      {\bf Strategies} & {\bf Exp. A} & {\bf Exp. B} & {\bf Exp. C} &
      {\bf Exp. D} & {\bf Exp. E} & {\bf Total}\\ \hline \hline
      
      {\bf No evaluation } & 8 & 7 & 7 & 6 &  4 & 32 \\ \hline

      {\sf Sum } \({\Leftrightarrow}\) {\sf Evaluation} &  5 & 8  & 3  & 0  & 1  & 17 \\ \hline

      {\sf Sum } \( \Rightarrow \) {\sf Evaluation\/} & 1 & 0 & 0 & 1 & 2  & 4 \\ \hline
            
      {\sf Evaluation} \({\Leftrightarrow}\) {\sf Sum\/} & 0 & 1 & 1 & 1 & 0& 4\\ \hline

      Missing & 2 & 0 & 4 & 0 & 1  & 6 \\ \hline
    \end{tabular}      
    \end{center}
    \label{tab:evaluation strategies}
\end{table}
\normalsize
\ls{1.6}

%%% \subsection{Argumentation/integration}

\section{Collaborative learning using CLARE: a case analysis}
\label{sec:case}

This section presents a detailed analysis of a CLARE session involving a
group of four learners\footnote{The current data set consists of a total of
16 group sessions. The current group session was selected based on two
primary criteria: a relatively even participation among the group members
and a manageable number of nodes created during both exploration and
consolidation phases. The true identities of the learners who participated
in the current CLARE session are disguised to protect their privacy. For
comparison purpose, the four pseudo-names from Chapter 1 are used
instead. The complete CLARE session summary for the current case can be
found in Appendix \ref{sec:case-session-summary}.}. The primary purpose is
to illustrate the previous findings on the hypotheses, common
representation issues, and usage strategies through a single CLARE
session. The section begins with an overview of the research paper used in
the current example, and a summary of the session to be analyzed.  A
prototypical RESRA representation of the research paper is provided to
serve as a basis of comparison. The following two subsections present a
detailed analysis of the results from the exploration and consolidation
phases, respectively. The section concludes by comparing the result from
the current CLARE session with the hypothetical scenario described in
Chapter 1.


\subsection{Overview of the CLARE session}
\label{sec:case-overview}

The artifact studied in the current example --- ``The Automated Requirement
Traceability Systems (ARTS): an experience of eight years'' by R.F. Flynn
and M. Dorfman \cite{Flynn90} --- was from the designated readings for
ICS613 (``advanced software engineering''). It represents one important
type of research literature found in software engineering, i.e., {\it case
studies\/}, which typically reports the experience related to technology or
methodology adoption in a given organizational context. This specific paper
describes an eight-year experience of developing and using a requirement
management system called ARTS. The purpose is to describe ARTS as a
state-of-the-art system of its type at that time. More importantly, it is
to show that, to be successful in {\it inserting\/} a new system, such as
ARTS, into a software development project, two factors must be taken into
consideration: the flexibility of the tool itself and a good understanding
of the application environment. A conceptual framework called ``the
tool-to-task paradigm'' is proposed to help guide future efforts in this
area. The main thematic components of this paper are captured through the
following RESRA node instances:

\ls{1.0}
\small
\begin{itemize}
\item {\sf Poor practice in requirement management (\fbox{{\sf
  problem}})\/}: Despite its importance in developing large software
  systems, requirement management (RM), especially, traceability
  considerations, is still misunderstood and only rarely performed
  correctly.
  
\item {\sf Critical success factors in RM systems (\fbox{{\sf claim}})}:
  To be successful in developing and using a RM system, two conditions must
  be met: (1) the RM tool must be flexible; and (2) the entire application
  environment must be taken into account.
  
\item {\sf Reasons for the success of ARTS (\fbox{{\sf evidence}})}: Nine
  main factors accounting for the successful adoption of ARTS at Lockheed.
  
\item {\sf Difficulties/problems with ARTS (\fbox{{\sf evidence}})}:
  Detailed list of complications experienced while developing and using
  ARTS.

  \item {\sf Tool-to-task-paradigm (\fbox{{\sf concept}})}: A way of
  understanding of the application environment of a software system that
  consists of four components: {\it situation\/}, {\it computer\/}, {\it
  people\/}, and {\it data\/}.
  
\item {\sf Usefulness of the tool-to-task paradigm (\fbox{{\sf claim}})}:
  The tool-to-task paradigm provides a useful framework to guide the
  development of new RM tools and the application of existing RM tools to
  new contexts.
  
\item {\sf The ARTS system (\fbox{{\sf thing}})}: The requirement
  management system successfully developed and used at Lockheed during the
  past eight years. 
\end{itemize}
\normalsize
\ls{1.6}

\begin{figure}[hbtp]
 \fbox{\centerline{\psfig{figure=Figures/arts-resra.eps,width=4.5in}}}
 \caption{A RESRA representation of [FD91]}
  \label{fig:arts-resra}
\end{figure}

The relationships between the above  RESRA nodes are depicted in
Figure \ref{fig:arts-resra}. As mentioned earlier, this paper is a {\it
case study\/} whose stereotypical RESRA structure (i.e., CRF) is not
defined in the current set of CRFs (see Section \ref{sec:crf}). Hence, no
comparison can be made between the current representation and the standard
CRF. 

\begin{figure}[hbtp]
 \fbox{\centerline{\psfig{figure=Figures/case-time.ps,width=4.5in}}}
 \caption{Distribution of the CLARE usage time}
 \label{fig:case-usage-time}
\end{figure}

The example CLARE session was generated by a group of 4 first-time users
from a graduate course in requirement engineering. The group created a
total of 92 nodes over a period of 10 days: 58 of these nodes were from the
exploration phase and the remaining 38 were from the consolidation phase.
The distribution of the CLARE usage time by the group members is shown in
Figure \ref{fig:case-usage-time}. The group is quite balanced in
participation: each members devoted about 9 hours, two-thirds of which were
spent in the exploration phase, and the remaining spent in the
consolidation phase. The average usage time of this session (9hr) is almost
twice as much as the overall average of the CLARE experiment (see Table
\ref{tab:summary-stat}) This discrepancy is perhaps caused by the fact that
the current group were the first-time CLARE users, who devoted part of
their usage time to learning about the system. The following two sections
present a detailed analysis of this session.


\subsection{Analysis of the exploration result}

Table \ref{tab:case-summarization} lists the distribution of the nodes
created by the 4 group members during the exploration phase. Note that none
of the group members created any evaluative node. Figure
\ref{fig:rep-phase1} shows a graphical representation of the 4 learner's
nodes and links. Note that Todd did not have any links. Figure
\ref{fig:case-source-dist} shows the distribution of summarative nodes
based on the source from which they are derived. Notice that Source Node 3
is the origin of 19 summarative nodes. Hence, it might be viewed as the
{\tt hot} section of the selected paper. In general, the four learners seem
to have a great deal in common. As evident from the following sections,
however, they also differ greatly in many other ways.

\begin{figure}[hbtp]
 \fbox{\centerline{\psfig{figure=Figures/rep-phase1.eps,width=5.5in}}}
 \caption{A comparative view of four RESRA representations of [Flynn90]}
  \label{fig:rep-phase1}
\end{figure}

\ls{1.0}
\small
\begin{table}[hbtp]
\caption{Distribution of summarative nodes by Scott, Chris, Mary, and Todd}
\begin{center}
\begin{tabular} {||l|c|c|c|c|c|c|c|c|c||c||} \hline
{\em User} & {\em PR} & {\em CL} & {\em CO} & {\em ME} & {\em EV} & {\em
TH} & {\em SO} & {\em TI} & {\em OT} & {\em Total} \\ \hline \hline
Scott & 1 & 6 & 0 & 1 & 1 & 1 & 0 & 2 & 1 & 13 \\ \hline
Chris & 4 & 3 & 6 & 1 & 1 & 2 & 0 & 0 & 0 & 17 \\ \hline
Mary  & 4 & 3 & 2 & 1 & 0 & 1 & 0 & 0 & 1 & 12 \\ \hline
Todd  & 3 & 4 & 0 & 5 & 0 & 0 & 0 & 3 & 1 & 16 \\ \hline
 \hline
\end{tabular}  
\end{center}
\label{tab:case-summarization}
\end{table}
\normalsize
\ls{1.6}

\begin{figure}[hbtp]
 \fbox{\centerline{\psfig{figure=Figures/case-source-dist.ps,width=5.0in}}}
  \caption{Distribution of summarative nodes over the source nodes}
  \label{fig:case-source-dist}
\end{figure}


\subsubsection{Scott}

As shown in Table \ref{tab:case-summarization}, Scott created 13
summarative nodes: 6 \fbox{{\sf claim}}, 2 \fbox{{\sf thing}}, 1 \fbox{{\sf
problem}}, 1 \fbox{{\sf theory}}, 1 \fbox{{\sf evidence}}, 1 \fbox{{\sf
method}}, and 1 \fbox{{\sf other}}, respectively. The node ``Traceability
issues'' (problem434), however, does not describe the main problem of the
paper: the authors' concern is not merely traceability issues but
requirement management as a whole. Scott's 6 \fbox{{\sf claim}} nodes are
``basic analysis phase issues'' (claim442), ``on early traceability
implementation'' (claim452), ``ARTS is flexible'' (claim528), ``current
status of ARTS'' (claim538), ``some practical limits'' (claim560) and
``author's conclusions'' (claim574). The current status of ARTS (claim538),
as shown below, is not a \fbox{{\sf claim}}. Furthermore, the lack of
Traceability support in early RM systems (claim452) is in fact part of the
\fbox{{\sf problem}} the authors attempt to address, not a \fbox{{\sf
claim}} for which they provide supporting or countering evidence. The main
thesis of the paper (see Section \ref{sec:case-overview}) is partially
covered by ``claim574,'' i.e., ``Author's conclusions.'' The claim that
``ARTS is flexible'' (claim528) presents a good example that shows the
context-sensitive nature of RESRA primitives: at one level, it is a
\fbox{{\sf claim}} for which the authors provide supporting evidence, for
example, ARTS allows its users to define their own record structures. At
another level, ``claim528,'' along with its supporting evidence, is used as
\fbox{{\sf evidence}} to support a higher-level claim, i.e., the successful
insertion of a RM tool into an existing project requires two conditions,
one of which is that the RM tool must be flexible.  Although the current
paper fits to the latter, Scott's representation took the former approach.

The node ``Theory behind ARTS'' (theory442), contains no ``theory'' about
the ARTS system. Instead, it describes what the system was supposed to
provide, e.g., requirement traceability. The other six nodes about
different aspects of ARTS are: ``overview'' (thing458), ``internal
structure'' (thing570), ``features'' (method460), ``current status''
(claim538), ``enhancements'' (other524), and ``shortcomings''
(evidence554). First, like ``theory442,'' many of these nodes were
incorrectly represented. For instance, while the authors {\it describe\/}
the features and status of ARTS, the former was treated as \fbox{{\sf
method}} but latter, a \fbox{{\sf claim}}. Second, quite an amount of
overlapping is found between such nodes as ``overview,'' ``features,'' and
``internal structure.'' It is unclear why these nodes should be separate
nodes instead of just one, as described in Section \ref{sec:case-overview}.
One possible explanation might be that all these nodes were derived from
different source nodes.  Since Scott adopted an one-pass, linear approach
to summarization, node creation is done on-demand without either forward or
backward reference, which leads to overlapping and fragmentation.

Scott generated 9 RESRA tuples, which are depicted in the upper-left region
of Figure \ref{fig:rep-phase1}. A number of these tuples are rendered
invalid because the incorrect use of node primitives. For example, since
the node ``theory behind ARTS'' (theory456) does not exist, the two {\it
strengthens\/} links leading to this node are no longer valid.  Similarly,
the tuple ``Features of ARTS (method460)''
\(\stackrel{generates}{\rightarrow}\) ``shortcomings of ARTS''
(evidence554) is also spurious since ``Features of ARTS'' is not a
\fbox{{\sf method}}, as pointed out earlier. It is also unclear that
``shortcomings of ARTS'' is an \fbox{{\sf evidence}} in the current
context, since it is not connected to any \fbox{{\sf claim}} node. The two
links related to the node ``On early traceability implementation
(claim452), i.e., {\it is-alternative-to\/} and {\it suggests\/}, are also
invalid because this node is not a \fbox{{\sf claim}} but part of the
\fbox{{\sf problem}}. The remaining three tuples do somewhat capture the
actual relationships that exist between these nodes:

\begin{itemize}
\item {\sf Author's conclusion (claim574)\/}
\(\stackrel{generates}{\rightarrow}\) {\sf traceability issues (problem434)\/}

\item {\sf Some practical limit (claim560)\/}
  \(\stackrel{presupposes}{\rightarrow}\) {\sf Author's conclusion(claim574)\/}

\item {\sf Author's conclusion (claim574)\/}
  \(\stackrel{presupposes}{\rightarrow}\) {\sf ARTS is flexible (claim528)\/}
\end{itemize}



The above analysis of the summarization result by Scott shows a large
number of incorrect use of RESRA node and link primitives. This result
demonstrates that (1) Scott did not have a good understanding of the
semantics of RESRA, and/or (2) Scott did not have a good grasp of the main
ideas of the paper. The latter is precisely one of the problems CLARE is
intended to help Scott overcome.  However, the achievement of the latter is
contingent on a good grasp of the RESRA language. The other factor that
might have accounted for the above problem is Scott's summarization
strategy: the one-pass, linear approach does not seem sufficient to uncover
the thematic structure of a paper; instead, it encourages Scott to create a
representation that parallels the presentational structure of the paper.


\subsubsection{Chris}

Chris' summarization of \cite{Flynn90} consists of 17 nodes: 4 \fbox{{\sf
problem}}, 3 \fbox{{\sf claim}}, 6 \fbox{{\sf concept}}, 2 \fbox{{\sf
theory}}, 1 \fbox{{\sf method}}, and 1 \fbox{{\sf evidence}}, respectively.
Overall, these nodes together captured most of main thematic features of
the paper. Compared to the prototypical and Scott's representations, both
of which contain one general \fbox{{\sf problem}}, Chris' four \fbox{{\sf
problem}} nodes (``RM and traceability'' (problem424), ``inadequacy of
traceability methods'' (problem436), ``RA tools'' (problem438) and ``poor
RA and project failure'' (problem446)) are a finer differentiation of the
problem that the original paper attempts to address. This detailed
breakdown of the problem, however, requires \fbox{{\sf claim}} to be
handled in a similar fashion. Chris identified 3 \fbox{{\sf claim}}: ``ARTS
supports traceability'' (claim544), ``Improvement of RM with a DB tool''
(claim454), and ``tools have to be flexible'' (claim468). The first two are
directly targeted at the first two problems identified earlier.  Chris' two
\fbox{{\sf theory}} nodes, i.e., ``steps in RE process'' (theory444) and
``life cycle model'' (theory502), are not well-grounded, however. The first
node, for example, describes the process steps of the requirement
management at Lockheed. It should be a \fbox{{\sf method}} instead of a
\fbox{{\sf theory}}, for the implementation of that process would generate
evidence to support or counter the assumptions underlying it. A similar
comment can be made to the node ``life cycle model'' (theory502).

Chris also identified six \fbox{{\sf concept}}: ``requirement,''
``requirement analysis,'' ``traceability,'' ``identification,''
``allocation,'' and ``flowdown.'' First, some of these concepts,
noticeably, ``requirement'' and ``requirement analysis,'' are so commonly
used in the current paper and group setting (i.e., graduate students in
requirement engineering) that it is unclear why they should be singled out
as something important. Second, concepts such as ``identification'' and
``allocation'' are an integral part of their ``parent,'' high-level
conceptual construct (i.e., ``Steps in the RE process'' (theory44)). When
treated separately, they begin to lose important part of their
``identity,'' and thus become less meaningful. Because of such dependency,
it is preferred that these concepts be not treated as separate entities.

Chris generated only one \fbox{{\sf evidence}} node: ``lack of flexibility
in tool (ARTS) leads to a number of difficulties in their use''
(evidence470). The node is significant because, in the previous section,
Scott identified a \fbox{{\sf claim}} named ``ARTS is flexible.'' It does
not seem very likely that the paper makes a claim that ARTS is flexible
while at the same time, shows the difficulties in using the system because
of its inflexibility.  The contradiction must come from the interpretation.
In fact, the picture about what the authors really attempt to convey
becomes clear when looking closely at the tuple relationship of the current
evidence: ``ARTS' lack of flexibility and resulted difficulties''
(evidence470) \(\stackrel{supports}{\longrightarrow}\) ``Tools have to be
flexible'' (claim468). In other words, the author uses the flexibility and
inflexibility of ARTS and resulted benefits/difficulties as \fbox{{\sf
evidence}} to {\it support\/} the claim that requirement management tools
have to be flexible. It is apparent that Scott mis-construed the intended
thesis of the paper.


\subsubsection{Mary}

Mary's summarization of \cite{Flynn90} consists of 12 nodes: 4 \fbox{{\sf
problem}}, 3 \fbox{{\sf claim}}, 1 \fbox{{\sf concept}}, 1 \fbox{{\sf
method}}, 1 \fbox{{\sf theory}}, and 1 \fbox{{\sf other}}, respectively. Of
the 4 \fbox{{\sf problem}}, the node ``Need of traceability'' (problem412)
is similar to Scott's ``traceability issues'' (problem434) and Chris' ``RM
and traceability'' (problem424) and ``Inadequacy of traceability methods''
(problem436). The node ``requirement inadequacies'' (problem410) describes
the bottleneck of requirement specification and management in light of the
changing needs of the user. It is similar to Chris' ``poor requirement
analysis and project failures'' (problem446), and parts of Scott's ``basic
analysis phase issues'' (claim442). The other two problems are ``problem
with ARTS'' (problem542) and ``difficulty with tools'' (problem562), both
of which are in fact described by the authors. However, it is unclear that
they are intended ``focal points'' of the paper.

Like Scott, Mary had several nodes that describe different aspects of ARTS:
``ARTS and traceability'' (claim418), ``evolution of ARTS'' (other440),
``success of ARTS'' (claim518), ``reasons behind success'' (claim522),
``problem with ARTS'' (problem542), ``how it works, how to use''
(method492), and ``description of ARTS'' (concept550). Most of these nodes
used incorrect RESRA node types. For instance, the node ``ARTS and
traceability'' (claim418) describes how traceability is supported in ARTS.
It is similar to ``how it works, how to use'' (method492) and ``description
of ARTS'' (concept550). They can be merged into a single node. The resulted
node, based on the RESRA definitions, will not be a \fbox{{\sf claim}}, nor
a \fbox{{\sf method}}, but a \fbox{{\sf thing}}, for ARTS is a physical
object under study.  The nodes ``success of ARTS'' (claim518), ``reasons
behind success'' (claim522), and ``problem with ARTS'' (problem542) are
evidence for supporting the claim that the successful development and use
of requirement management tools require that (1) the tool is flexible, and
(2) the entire application environment is taken into account. The node
``future directions'' (concept576), in fact, contains a number of less
substantiated claims that are intended for future researchers on the
subject. An example of such claims is that hypertext is a promising
technology for enhancing requirement traceability.


\subsubsection{Todd}

Todd's representation of \cite{Flynn90} consists of 16 nodes: 3 \fbox{{\sf
problem}}, 4 \fbox{{\sf claim}}, 5 \fbox{{\sf method}}, 3 \fbox{{\sf
thing}}, and 1 \fbox{{\sf other}}. The three \fbox{{\sf problem}} he
identified are: ``requirement traceability'' (problem466), ``major problems
with RM'' (problem532), and ``difficulties and limitations'' (problem594).
The first two nodes are roughly equivalent to Chris' ``RM and
traceability'' (problem424) and the combination of ``inadequacy of
traceability methods'' (problem436) and ``requirement analysis tools''
(problem438), respectively. As described earlier, the theme ``difficulties
and limitations'' (problem594) is used by the authors as counter-evidence
to support their primary claim. Hence, in the current context it should be
an \fbox{{\sf evidence}} instead of a \fbox{{\sf problem}}. Two of the four
\fbox{{\sf claim}} nodes created by Todd are considered incorrect. The node
``large system development process'' (claim530) presumably refers to the
life cycle model which, as pointed out earlier, is a \fbox{{\sf method}}.
The ``need for developing ARTS'' (claim548) is only briefly mentioned as
part of the introduction to the system, and is therefore too peripheral to
be considered as a \fbox{{\sf claim}}. Most of the five \fbox{{\sf method}}
nodes are also questionable in conforming to the semantics of the
primitive. For example, the node ``output report generation'' (method586)
discusses the flexibility of report generation in ARTS. Hence, it should be
represented as \fbox{{\sf evidence}} to support the main claim. Similarly,
the node ``improper use of ARTs'' (method584) indicates that a tool is
often used in unintended ways.  As a result, the design of the tool should
take into account the entire application environment. The latter problem is
precisely one of the two components of the main thesis. Hence, the current
node may also be viewed as \fbox{{\sf evidence}} instead.

The three \fbox{{\sf thing}} nodes: ``brief description of ARTS''
(thing552), ``general description of ARTS'' (thing558), and ``ARTS system
and interface'' (thing528), describe different aspects of the same physical
object, i.e., ARTS. They may be merged into a single \fbox{{\sf thing}}
node called ``the ARTS system,'' just as in the prototypical
representation. Since Todd created no tuples in his summarization, his
perspective on how the above nodes are related to one another cannot be
assessed.

\subsubsection{Summary}

The above analysis of the summarization result of \cite{Flynn90} by Scott,
Chris, Mary, and Todd demonstrates a wide variety in the learners'
understanding of both the content of the paper and the representation
language. It also shows major differences and similarities between the four
representations and points of view. By comparing these representations with
the example described at the beginning of this section, the following
observations can be made:

\begin{itemize}
\item All four learners were all very close in identifying the central
  problem of that paper. However, none of them was able to clearly
  articulate the authors' primary claim.
  
\item The representations from the group as a whole are finer-grained
  than the prototypical one, although individual representations differ
  greatly in their focus (e.g., Chris' \fbox{{\sf problem}}, Todd's
  \fbox{{\sf evidence}}).

\item The representations from the group as a whole show little synthesis of
  thematic elements from various parts of the paper.  Instead, the
  representations bear a close relationship with the linear, presentational
  structure of the paper.
  
\item The learners do not seem to have a good grasp of the semantics of
  RESRA, as shown by the large number of incorrect use of both node and
  link primitives.
\end{itemize}

The following section presents the analysis result from the consolidation
phase of the current example.


\subsection{Analysis of the consolidation result}

\ls{1.0}
\small
\begin{table}[hbtp]
\caption{Distribution of argumentative nodes by Scott, Chris, Mary, and Todd}
\begin{center}
\begin{tabular} {||l|c|c|c|c|c||c||} \hline
{\sf User} & {\bf Claim} & {\bf Evidence} & {\bf Critique} & {\bf Question}
& {\bf Suggestion} & {\bf Total} \\ \hline \hline
{\bf Scott}  & 0 & 0 & 0  & 3 & 3 & 6  \\ \hline
{\bf Chris\/}& 0 & 1 & 3  & 1 & 3 & 8  \\ \hline
{\bf Mary\/} & 1 & 0 & 10 & 2 & 4 & 17 \\ \hline
{\bf Todd\/} & 0 & 0 & 1  & 1 & 1 & 3  \\ 
 \hline
\end{tabular}
\end{center}
\label{tab:resra-dist-phase2}
\end{table}
\normalsize
\ls{1.6}

\begin{figure}[hbtp]
 \fbox{\centerline{\psfig{figure=Figures/rep-all.eps,width=5.5in}}}
 \caption{Overview of summarization and argumentation by Scott, Chris,
 Mary, and Todd} 
  \label{fig:case-phase-2}
\end{figure}

Table \ref{tab:resra-dist-phase2} shows the top-level summary of the
argumentative nodes created by Scott, Chris, Mary, and Todd. Of a total of
34 nodes created during this phase, 32 of them are {\it evaluative\/}
nodes, which indicates that argumentation is dominated by collaborative
evaluation of the representations generated from the previous phase. Figure
\ref{fig:case-phase-2} shows a detailed picture of all summarative nodes
and links from Phase I and the argumentative nodes directed to them.

Mary is the most active member in the group; she created 17 nodes during
the current phase. Todd is least active; he created only 3 nodes. In terms
of the attention received, Scott received the most attention: 11
argumentative nodes (31\%) were targetted at his representation. On the
other hand, Mary received the least attention: only 6 argumentative nodes
(17\%) came to her way. It is interesting to note that Scott's summarative
representation, as described in the previous section, is among the poorest
in the group in terms of distortions, incorrect use of the primitives, and
spurious links but he received the most attention in Phase II. The
seemingly existence of an inverse relationship between the quality of
summarization and the level of contribution to {\it triggering\/} group
discussion in Phase II has several interesting implications. However,
from this single instance it is difficult to conclude that such a
relationship indeed exists.

\subsubsection{Analysis of argumentative nodes}

\paragraph{Scott.}

Scott created 6 nodes during argumentation: 3 \fbox{{\sf questions}} and 3
\fbox{{\sf suggestion}}, respectively. Of the 3 \fbox{{\sf questions}}
nodes, 2 of which are in fact \fbox{{\sf question}} nodes: ``Re: life cycle
model'' (question710) and ``Re: success of ARTS'' (question714). In the
former Scott suggested to replace the ``Waterfall'' model by some newer
models (e.g., ``incremental model'') to ease certain requirement management
problems. In the node ``Re: success of ARTS,'' Scott proposed to use
standard benchmarks to quantitatively evaluate various RM tools.


\paragraph{Chris.}

Chris' contribution at Phase II consists of 8 nodes: 3 \fbox{{\sf
critique}}, 3 \fbox{{\sf suggestion}}, 1 \fbox{{\sf question}}, and 1
\fbox{{\sf evidence}}. The last node deserves special attention since it
belongs to a separate category of argumentation called {\it constructive\/}
argumentation, as opposed to {\it evaluative\/} argumentation to which
most of the current phase belongs. In the constructive argumentation, the
learner does not merely critique or question another learner's position, he
in fact engages in active knowledge-building by formulating new problems,
proposing alternative claims, supplying new evidence, and so on. In the
current example, Chris created an \fbox{{\sf evidence}} node named ``ARTS
is not at all that flexible'' (evidence662), which contains the reference
to another \fbox{{\sf evidence}} node he created in Phase I (i.e.,
``difficulties related to lack of flexibility'' (evidence470)), to counter
Mary's \fbox{{\sf claim}} that ARTS is successful because it is flexible
(``Reasons for success (claim522)).

In other argumentative nodes, Chris proposed various ways to improve ARTS,
including the use of 4GLs and distributed database managers. In ``Can ARTS
be transferred to other platforms?'' (question618), he pointed out the
ambiguity of a sentence used by Scott in his node ``current status of
ARTS'' (claim538). Of a total of Chris' 8 argumentative nodes, 4 of which
are targeted at Scott, 1 at Todd, and the remaining 3 at Mary.


\paragraph{Mary.}

Mary's contribution to the group during the consolidation phase consists of
a total of 16 nodes: 10 \fbox{{\sf critique}}, 2 \fbox{{\sf question}}, 4
\fbox{{\sf suggestion}}, and 1 \fbox{{\sf claim}}. Her critiques can be
grouped into two main categories: pointing out the incorrect use of RESRA
primitives and identifying ambiguities/inaccuracies in other learners'
representations. The former includes ``requirement problem'' (critique636),
and ``tree format'' (critique644), ``methodology'' (critique654),
``requirement?'' (critique558), and ``evolution'' (critique758). In the
last node, for example, Mary pointed out that Todd's treatment of
``evaluation of ARTS'' as a \fbox{{\sf method}} is incorrect by quoting the
RESRA's definition of \fbox{{\sf method}}.  Similarly, in ``methodology''
(critique654), she pointed out that Todd's ``large software development
process'' (claim530) should be a \fbox{{\sf method}} instead of a
\fbox{{\sf claim}}. In ``requirement problem'' (critique636), Mary argued
that Scott was wrong: the inadequacy of the analysis phase should be a
\fbox{{\sf problem}} instead of a \fbox{{\sf claim}}.

The second group of \fbox{{\sf critique}} nodes include ``conventional
traceability method'' (critique742), ``ARTS' adaptability'' (critique642),
``incomplete description'' (critique614), and ``over-statement''
(critique616). In the last node, for example, Mary pointed out that the
author did not say that the success of traceability systems is
``proportional'' to their adaptability, as stated by Scott in his
``traceability issues'' (problem434). In the node ``ARTS' adaptability''
(critique642), Mary also indicated that Scott was wrong in stating that
ARTS is adaptable to various application environment. She also pointed out
that the fact is just opposite --- ARTS mainly runs on DEC VAX and is not
flexible. The process data shows that, to assess the accuracy of Scott's
representation and create \fbox{{\sf critique}} nodes such as the ones
above, Mary in fact verified the representation content against the source
from which Scott's problematic nodes were derived.


\paragraph{Todd.}

Todd's contribution in Phase II consists of 3 nodes: ``requirement
management and tree structure'' (critique728), ``hierarchical allocation''
(question730), and ``development platform'' (suggestion730). The first two
nodes were directed to Chris. The third was directed to Scott.


%%% \subsubsection{Analysis of the integration result}
%%% 

\subsection{Discussion}

The above analysis of an actual CLARE session forms a contrast with the
hypothetical scenario described in Chapter 1. Although the two share the
same objectives (i.e., collaborative learning from research papers) and the
same environment (i.e., CLARE), the outcome was quite different, as
shown in Figures \ref{fig:case-phase-2} and \ref{fig:learning-community}.
At a top-level, some noticeable differences between the real and
hypothetical CLARE sessions are:

\begin{itemize}
\item The outer layer of Figure \ref{fig:case-phase-2} contains a much
  larger number of summarative nodes;
  
\item The outer layer of Figure \ref{fig:case-phase-2} contains no
  evaluative nodes;
  
\item Figure \ref{fig:case-phase-2} does not show the {\it comparison\/}
  layer; and
  
\item The {\it integration\/} layer in Figure \ref{fig:case-phase-2} does
  not contain any nodes.
\end{itemize}

At a deeper level, the following differences were found between the two
CLARE sessions:

\begin{itemize}
\item The two Scotts used a different summarization strategy. In Chapter
  1, the hypothetical Scott adopted a ``two-round, {\sf Sum\/}
  \(\Leftrightarrow\) {\sf Link\/}'' strategy. The real Scott used a
  ``single-round, {\sf Sum\/} \(\Rightarrow\) {\sf Link\/}'' strategy.
  Moreover, the hypothetical Scott invoked the {\sf Template Guide\/}
  function. He also used CLARE's advice to guide his creation of RESRA
  tuples during the late stage of summarization. The the real Scott did
  not use either functions.
  
\item In the hypothetical session from Chapter 1, all group members
  seemed to have little problem with the semantics of RESRA. As shown
  above, in the real session each student had different interpretations of
  RESRA. 
\end{itemize}

The above differences between the hypothetical scenario and the
actual CLARE session illustrate some main usage gaps between them:

\begin{itemize}
\item In a real CLARE setting, most important themes of a paper were not
  always represented. Quite often, many minor themes were included;
  
\item CLARE's instrumentation does not allow one to determine which two
  nodes a user was comparing;

\item Evaluation is not always performed during the exploration phase;

\item Integration is not always performed in the consolidation phase;
  
\item CRF function were under-utilized by many users; and
  
\item Learners had different connotative interpretations of the same RESRA
  primitives.
\end{itemize}

In addition to these discrepancies, several other observations can also be
made. First, most of the learner's attention was still devoted to the
bootstrapping stage of the collaborative learning process, i.e.,
summarization and evaluation; 66\% of the group member's time was spent in
the exploration phase, which does not even include evaluation. Second, the
example also indicated that, even as a group, the main thematic structure
of a paper is always reconstructed. And finally, the collaborative
potential of the RESRA language was limited by the lack of shared
understanding of its semantics and the resulted inconsistent usages.


\section{Summary and conclusions}
\label{sec:c6-discussions}

\subsubsection{Summary}

This chapter discusses main findings from a set of experiments on CLARE.
Results from these experiments have in general confirmed (1) the viability
of CLARE as a novel environment to support collaborative learning from
scientific text, and (2) the usefulness of RESRA as a representational
basis for such an approach. They have also revealed a number of problems
about the system:

\begin{itemize}
\item At the representational level, RESRA is interpreted in many
  different ways. It is also often used incorrectly. Nevertheless, the
  majority of learners still view RESRA as a useful and novel tool.  This
  apparent inconsistency between the two (called ``the RESRA paradox'') is
  discussed below.
  
\item At the implementational level, the user interface, in particular,
  the link mode, is the greatest barrier to effective use of CLARE; and
  
\item At the experimental level, inadequate training on RESRA and CLARE,
  along with other factors such as tight schedules and improper choice of
  research papers, seems responsible for most of user dissatisfaction.
\end{itemize}

The remainder of this section discusses several specific issues
that arose from the experiments. 


\subsubsection{The ``RESRA paradox''}

As reported in Section \ref{sec:c6-hypothesis}, 84\% of learners indicated
that RESRA provides a useful means of characterizing the content of
scientific text, and 90\% of learners agreed that RESRA helps expose
different points of view on an artifact. On the other hand, an analysis of
the CLARE database reveals a great deal of incorrect use of RESRA (see
Sections \ref{sec:rep-issues} and \ref{sec:case}). The latter might be
attributable to a number of factors, including insufficient training and
online examples, and a lack of detailed usage guidelines.

The apparent gap between positive user attitudes about RESRA and incorrect
use of the representation has several important implications. First, the
utility of RESRA is not necessarily the same as the {\it correctness\/} of
its usage. In other words, a learner may use a primitive incorrectly but
still find it useful, since it helps identify what would have been missed
otherwise.  Such things can happen because of the heuristic value of RESRA,
which exists independent from the correctness in the interpretation of the
primitive. Second, because of the abstract or meta nature of RESRA, and the
differences in backgrounds, perspectives, and interests among different
learners, mis-interpretations and incorrect usages might be inevitable,
regardless the amount of training received. In fact, representation-level
errors might also help expose deep-level gaps in the learner's knowledge
structure, and thus create opportunity for potential collaboration among
learners. However, a large number of representation-level errors also limit
the potential of RESRA as a collaborative learning framework.  Section
\ref{sec:future-directions} identifies several extensions to CLARE and
RESRA that will help reduce the number of RESRA usage errors.


\subsubsection{Mapping from text to representation}

The experimental results also revealed that the mapping from scientific
text to a representation such as RESRA is not always straight-forward. As
illustrated in Section \ref{sec:case}, different learners have quite
different and, in many cases, mutually incompatible representations of the
same artifact. A number of factors might have accounted for these
variations:

\begin{itemize}
\item {\it Lack of consistent interpretation of RESRA (see above):\/}
  This language-level inconsistency leads to inconsistent use of RESRA,
  which in turn leads to inconsistent representations.
  
\item {\it Absence of RESRA usage guidelines:} The lack of explicit,
  detailed guidelines on issues such as the granularity of
  representation, {\it major themes\/} versus {\it minor themes,\/} and
  {\it learner's views\/} versus {\it author's views\/} might be
  responsible for a sizable portion of the representational variations.
  
\item {\it Different interpretations of the artifact itself:\/} Because
  of the difference in their backgrounds, perspectives, interests, and so
  forth, different learners have different interpretations of the same
  artifact, regardless the type of representation scheme being used.
\end{itemize}

Because of the dynamic interplay among the above factors, it does
not seem surprising that wide variations are found between different
learners' representations. However, the effectiveness of CLARE as a
collaborative learning environment is contingent upon a basic,
shared understanding of RESRA among learners, and a certain level of
comparability between different learner's representations of the same
artifact. Such a level of consensus has not yet achieved in the current
evaluation experiment. Section \ref{sec:future-directions} describes
specific proposals for alleviating this problem.


\subsubsection{Relationships with other theories and systems}

The CLARE findings described earlier (see Section \ref{sec:c6-hypothesis})
confirm that meta-cognitive tools such as RESRA facilitate {\it meaningful
learning\/} (see the user comments in Section ). They also show that the
constructionist view of learning as collaborative knowledge-building is a
sound theoretical framework for building collaborative learning
systems. The next chapter reviews these two conceptual formulations:
constructionism and cognitive learning theory. In addition, it also
compares CLARE with other collaborative learning systems, such as virtual
classrooms and hypermedia systems. Chapter \ref{sec:conclusions} elaborates
a number of directions in which CLARE can be enhanced.

%%% \newpage
%%% \singlespace
%%% \bibliography{../bib/clare}
%%% \bibliographystyle{alpha}
%%% 
%%% \end{document}





