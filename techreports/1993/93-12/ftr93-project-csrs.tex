%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ftr93-project-csrs.tex -- 
%% RCS:            : $Id: nsf93-project-csrs.tex,v 1.9 93/10/08 11:35:39 johnson Exp $
%% Author          : Philip Johnson
%% Created On      : Mon Aug 16 14:32:13 1993
%% Last Modified By: Philip Johnson
%% Last Modified On: Mon Oct 11 15:22:57 1993
%% Status          : Unknown
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1993 University of Hawaii
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% History
%% 16-Aug-1993		Philip Johnson	
%%    


\subsection{An overview of CSRS}
\label{sec:csrs}

\subsubsection{The formal technical review method}

This section provides an introduction to FTR using CSRS with illustrations
from a recent review.  A comprehensive specification of FTR using CSRS from
a user perspective is provided in \cite{csdl-93-11}; a detailed, current
design-level specification of CSRS is provided in \cite{csdl-92-11}.  CSRS
is a hypertext-based environment; all review artifacts are represented as a
richly interconnected database of typed nodes and links. Import and
export mechanisms allow integration of CSRS with the external software
development process and environment.  

\begin{figure}
 {\centerline{\psfig{figure=process.ps}}}
\caption{The CSRS process model.}
\label{fig:process-model}
\end{figure}

Formal technical review using CSRS involves seven well-defined phases, as
illustrated by the diagram in Figure \ref{fig:process-model}.  Each of these
phases are described next in the context of code review.  CSRS is capable of
supporting other review artifacts (such as design documents), although our
experiments thus far have focussed on code review.

\paragraph{Setup.} In this phase, the moderator and/or the producer decides upon
the composition of the review team and the artifacts to be reviewed.  In
the case of code review, the moderator copies each program entity, such as
a function definition, class declaration, or variable definition into an
individual hypertext node and annotates it when necessary with supplemental
information.  Various links are created between these nodes to represent
explicit and implicit relationships. CSRS already provides tools to
automate much of the Setup phase for source code review, and tool support
for other artifact types is forthcoming.

\paragraph{Orientation.} This phase prepares the participants for private
review by introducing them to the artifacts under review.  The exact nature
of this phase depends upon the kind of review and the review group.  It may
range from a formal overview meeting with a presentation by the producer
about the review artifact structure and behavior, to an informal
notification through e-mail noting the presence of new artifacts to review.
The Orientation phase assumes that the participants are familiar with CSRS;
if not, a separate CSRS training session must be provided.

\paragraph {Private review.} In this phase, reviewers inspect 
artifacts privately and create issue, action and/or comment nodes.  Issue
and action nodes are not publicly available to other reviewers at this
time, but comment nodes are publicly available.  Comment nodes allow
reviewers to request clarification about the logic/algorithm of source
nodes, or about the review process, and may also contain answers to these
questions by other participants.  CSRS supports three common techniques for
analysis of review artifacts: free review, checklist-based review, and
verification-based review.

\begin{figure}                                        
 {\centerline{\psfig{figure=private-summary.ps}}}
\caption{A summary window.}
\label{fig:summary}
\end{figure}

Figure \ref{fig:summary} illustrates a summary
screen that reviewers use to keep track of their progress during private
review.  At this point, the reviewer has partially completed private
review, as indicated by the fact that some of the review artifacts are
reviewed, some have been read but have not been completely reviewed, and
some have not yet even been seen.


\begin{figure}                                        
 {\centerline{\psfig{figure=source.ps}}}
\caption{A source window.}
\label{fig:source}
\end{figure}

By mouse-clicking on a line or through menu operations, the reviewer can
traverse the hypertext network from this screen to a node containing a
source object for review, such as the one illustrated in Figure
\ref{fig:source}.  In this case, the object
under review is the operation {\tt gi*nbuff*make}.  Both pull-down and
pop-up menus facilitate execution of the most common operations during this
phase, such as creating an issue concerning the current source node under
review (as illustrated in Figure \ref{fig:issue}), or proposing a specific action to address an issue.

\begin{figure}                                        
 {\centerline{\psfig{figure=issue.ps}}}
\caption{An issue window illustrating the state of review for one reviewer.}  
\label{fig:issue}
\end{figure}

Reviewers explicitly mark each source node as reviewed when finished. While
reviewers do not have access to each other's state during private review,
the moderator does.  This allows the moderator to monitor the progress of
private review.  Private review normally terminates when all reviewers have
marked all source nodes as reviewed. In the unlikely event that no issues
have been raised, the review would terminate at this point.  If any issues
have been raised, public review begins.

\paragraph{Public review.} In this phase, all nodes are made public, and all review
participants (including the producer) react to the issues and actions by
voting and creating new nodes.  Participants can create new issue, action
or comment nodes based upon existing nodes.  Voting is used to determine
the degree of agreement within the group about the validity and
implications of issues and actions.  This phase normally concludes when all
nodes have been read by all reviewers, and when voting has stabilized on
all issues.  During both public and private review, CSRS automatically
sends a daily e-mail message to any participants who have nodes for review
or voting.


\paragraph{Consolidation.} In this phase, the moderator analyzes the results of
public and private review, and produces a condensed written report of the
review thus far.  In our experience, these consolidated reports are more
comprehensive, detailed, and accurate than typical review reports from
traditional review methods, such as those presented in (Pressman, 1992;
Freedman and Weinberg, 1990; Humphrey, 1989).  Rather than simply a
checklist of characteristics with brief comments about the general quality
of the source, consolidation reports contain a re-organized and condensed
presentation of the analyses provided by reviewers in issue, action, and
comment nodes, thus providing contrasting opinions, the degree of
consensus, and proposals for changes.

CSRS provides the moderator with various tools to support the generation of
a nicely formatted LaTeX document containing
the consolidated report.  If the group reached consensus about all of the
issues and actions during public review, then this report presents the final
outcome of review with respect to artifact assessment.  A second review
outcome is the measurements of review process and products, as discussed
in Section \ref{sec:instrumentation} below.

\paragraph {Group review meeting.} If the consolidated report identifies issues or
actions that were not successfully resolved via public and private review,
a group meeting is the next step.  In the meeting, the moderator presents
the unresolved issues or actions and summarizes the differences of opinion.
After discussion, the group may vote to decide them, or the moderator may
unilaterally make the decision. The moderator then updates the CSRS
database, noting the decisions reached during the group meeting and then
generating a final hardcopy document representing the product of review.
In our review experiments thus far, we have not used CSRS for the group
review meeting, though CSRS support for this phase is underway.

\paragraph{External development.}  During this phase the software is
enhanced or corrected in response to the issues raised during review or due
to other development processes.

\subsubsection{Measuring the process and products of FTR}
\label{sec:instrumentation}

Since the initial design of CSRS in 1991, effective instrumentation for
software review has been a primary research focus.  Instrumentation is a
major feature distinguishing CSRS from other automated review environments
such as ICICLE \cite{Brothers90,Sembugamoorthy90}, Scrutiny
\cite{Gintell93}, INSPEQ \cite{Knight91}, or CSI \cite{Mashayekhi93}.  A
detailed description of the evolution of our instrumentation support
appears in \cite{csdl-93-07}.  CSRS supports both {\em outcome-oriented}
and {\em process-oriented} instrumentation.

\paragraph{Outcome-oriented instrumentation.} Such instrumentation allows review analysts
to query the hypertext database during or after review for information such
as the number of nodes generated of a given type, the set of nodes
containing a particular value in a particular field, or the set of nodes
partaking in a specific relationship to other nodes.

Outcome-oriented measures are very useful: at a minimum, the number and
severity of identified defects provides a first-order estimate of the
quality of the software under review.  Outcome-oriented measures can also
reveal important characteristics of the review team and review process.
For example, marginally productive members of the review team can be
identified (after a sufficient number of review instances) as those who
contribute little, or who contribute non-productively (by simply affirming
comments made by others), or who use review for political purposes.  Such a
method is less expensive and more objective than the strategy put forth in
\cite{Freedman90}, where behavioral data (such as ``Shows solidarity'',
``Shows Tension'', and so forth) is collected by a passive observer of the
review in order to provide feedback on the quality of reviewer
participation.

Outcome-oriented measures can also contribute to empirically-guided process
improvement.  For example, an organization may be able to fine-tune certain
characteristics of review, such as the number of participants or the number
of lines of code under review, by collecting data across a range of review
instances and correlating these factors to, for example, the number of
productive issues raised.

\paragraph{Process-oriented instrumentation.} This instrumentation allows review
analysts insight into the {\em sequence}\/ and {\em duration}\/ of
activities occurring during review.  The analysis and application of this
process instrumentation forms an exciting and intensive focus of our
current research.

Process instrumentation is implemented by a general purpose ``timestamp''
mechanism in Egret (the collaborative environment underlying CSRS, which is
discussed in Section \ref{sec:infrastructure}).  This facility allows
Egret-based applications such as CSRS to insert timestamp calls at
strategic points within their code to record the occurrence of
application-specific events of interest. For example, CSRS timestamps when
the user reads a node, writes a node, closes a node, traverses a link,
marks a node as reviewed, and so forth.  Egret provides mechanisms to
fast-cache timestamps at the local client during a review session, and then
write the cache out to the server database at disconnect time.

One use of timestamp data is to calculate elapsed-time information.  For
example, subtracting the timestamp recorded when a user reads a particular
source node from the timestamp recorded when the user closes that node
yields the elapsed time during which the source node was displayed to the
user.

However, this elapsed display time is not a useful measure of the {\em
effort} spent reviewing a node, as we found out after experimenting with
our initial implementation.  Under a multi-window, multi-tasking
workstation environment, usage of CSRS is frequently interrupted by such
events as: phone calls; impromptu meetings with colleagues; reading and
answering e-mail; and so forth.

In other words, the elapsed display time has two components: the elapsed
time the user is actively working in CSRS while the node is displayed, and
the elapsed time that the node is displayed but the user's focus of
attention is elsewhere.

Our process instrumentation currently detects this latter {\em idle time}\/
via a bottom-up, timer-initiated timestamp that runs in parallel with the
top-down, user-initiated timestamp.  Once per minute, a timer wakes up,
determines whether or not there has been any recent low-level mouse or
keystroke activity within the CSRS application, and if so generates a
``busy'' timestamp. In our experiences thus far, this combination of
top-down and bottom-up timestamping provides high quality measures of
effort with a precision of plus or minus one minute.

\begin{figure}                                        
{\centerline{\psfig{figure=nbuff-data.ps}}}
\caption{A spreadsheet illustrating data-oriented measures including elapsed time.}
\label{fig:elapsed-time}
\end{figure}

Process instrumentation can provide substantial new insight into review.
First, the elapsed-time effort measures can augment the outcome-oriented
measures described above to measure the effort expended on review of
individual function objects and the effort expended upon review as a whole.
Figure \ref{fig:elapsed-time} illustrates some outcome-oriented measures
augmented with elapsed-time effort measures.

Precise and accurate measurement of the amount of effort expended upon
review (including the preparation phase) is notoriously difficult to obtain
in typical industrial settings \cite{Freedman90}.  Additionally, no other
manual or automated review method provides effort data at the grain size of
individual functions.  The availability of this fine-grained data makes
possible interesting new forms of experimentation, as discussed below.

\subsection*{Infrastructure}
\label{sec:infrastructure}

Egret, an environment for exploratory collaborative group work, forms the
core infrastructure of CSRS. Egret implements a multi-user, client-server,
hypertext environment for Unix and X windows.  The server side is a 15 KLOC
C++ system called HBS, which is a completely re-written version of the
Hyperbase database server \cite{Wiil90}. The client side is a 15 KLOC
object-oriented Lisp program using Lucid Emacs.

Egret has several noteworthy architectural features, and those, along with
other research experiences are reported in
\cite{csdl-91-03,csdl-92-01,csdl-92-08,csdl-93-09}.  A detailed, current
design-level description of Egret is reported in \cite{csdl-91-02}.  Egret
provides infrastructure for three systems developed in our laboratory: a
collaborative learning environment called CLARE, reported in
\cite{csdl-92-03,csdl-93-14}, a collaborative interface to Usenet called
URN, reported in \cite{csdl-93-06}, and CSRS itself, reported in
\cite{csdl-92-04,csdl-92-07,csdl-93-07,csdl-93-17}.

The Egret system was designed to determine the requirements for evolution
in the representational structure as well as contents of collaborative
systems for software development.  

\subsection{Results}
\label{sec:results}

Our current results are both {\em design}\/ and {\em empirically}\/
oriented.  Our design results, the product of two years of evolutionary
development, involve a system that addresses many significant issues
previously noted in research on software review and collaborative work
environments.  Our empirical results illustrate the power of CSRS to
collect high-quality, useful data about the process and products of review.

\subsubsection{Design results} 

One realization from our research with CSRS is that introducing substantial
computer support into the fundamentally manual process of traditional
review creates a fundamentally different process.  As a result, we take
issue with research claiming to ``automate'' Fagan code inspection, as in
\cite{Brothers90}.  While ICICLE might provide a very effective form of
computer-supported review, the introduction of substantial computer support
creates a method fundamentally {\em different}\/ from Fagan's code
inspection\foot{In private conversation with developers of computer
supported review systems, we have learned that, in some cases,
industry-sponsored developers feel compelled to claim that their system
implements Fagan's code inspection.  This is because their corporate
software development standards require Fagan code inspection, and their
development would not be funded nor would the resulting system be used if
management believed otherwise.}.

The design of CSRS, therefore, is not motivated by the goal of translating
a ``best practice'' from manual methods into a computer-supported
environment. Instead, our design exploits findings from computer-supported
cooperative work technology, such as those presented in \cite{Nunamaker91},
in conjunction with findings on current review practice, such as those
presented in \cite{Deimel91,Freedman90}.  To illustrate, the next
paragraphs discuss how the design addresses each of the pitfalls in manual
review presented earlier in Section \ref{sec:issues-manual-review}.

\begin{itemizenoindent}
  
\item {\em Insufficient preparation.} The CSRS design virtually
  eliminates the problem of detecting insufficient preparation, since the
  preparation phase in traditional review corresponds to private review in
  CSRS, and since the activities of each reviewer during private review is
  precisely instrumented and known to the moderator.  Of course, CSRS
  cannot force an intransigent participant to bring up the system and look
  at the materials, but it does eliminate the important problem of
  reviewers ``exaggerating'' their degree of preparedness to the review
  leader.
  
\item {\em Moderator domination.} The CSRS design prevents the
  moderator from most forms of domination possible in manual review.
  During private review, of course, all moderator influence is eliminated.
  During public review, the moderator can potentially influence outcome by
  ``flaming'', but cannot physically prevent contributions of other members
  by monopolizing air time as is possible in synchronous review.  Even the
  group meeting phase of CSRS is less vulnerable to moderator domination,
  since the set of issues has already been established and documented.
  
\item {\em Incorrect review rate.} Since most review in CSRS occurs
  asynchronously, the cost of review is much less sensitive to its rate.
  The desire of one participant to review slowly makes no impact on the
  rate (or cost) of review for any other participant.  An asynchronous
  style of interaction also eliminates ``air-time fragmentation''---the
  costly idle time spent by participants in manual face-to-face reviews
  while waiting for a turn to speak.
  
\item {\em Ego-involvement and personality conflict.} On-line and
  asynchronous review allows reviewers time to consider their choice of
  words carefully.  During private review, in fact, it is possible to log
  into CSRS the ``day after'' and modify comments that appear
  inappropriate, after a night's reflection, before any other reviewers
  see them.  In general, diplomacy is much easier in a non-real time
  environment.
  
\item {\em Issue resolution and meeting digression.} As noted previously,
  the constraints of a synchronous meeting context require participants to
  raise issues, but not to resolve them.  In CSRS, the problems of cost and
  digression arising from issue resolution are vastly minimized or
  eliminated.  
  
  More importantly, analysis of node content data from our review
  experiences reveals that issues are frequently difficult to articulate
  {\em except in the context of proposing an action to resolve it}, and
  that a natural tendency of most reviewers, immediately upon the
  identification of an issue, is to suggest a solution.  As a result, the
  current process model for CSRS explicitly {\em encourages} the
  interleaving of action proposal and discussion with issue proposal and
  discussion during review.  Our experiences suggest that capturing action
  proposals when the reviewer first thinks of them, rather than
  artificially delaying their discussion until some future meeting time, is
  certainly a more natural and possibly a more efficient method of review.
  
\item {\em Recording difficulties and clerical overhead.} The CSRS design
  eliminates the role of scribe altogether, except for the group meeting
  phase where the role is considerably simplified.  CSRS also trivially
  resolves the problems of capturing review commentary completely and
  accurately.  CSRS provides the moderator with tools to support
  restructuring and reformatting of the data from its on-line, hypertext
  format into a form suitable for a linear, hardcopy presentation.  Such
  features substantially reduce the clerical overhead normally associated
  with review.
 
  Most important from a research perspective, CSRS collects data and
  process measures automatically and unobtrusively.  This means that
  research on review can be performed in any size organization with any
  amount of resources allocated to review.  The CSRS Consortium discussed
  below is one research activity made feasible by automated and
  unobtrusive metric collection.

\end{itemizenoindent}

Taken as a whole, the CSRS method implements an incrementally increasing
level of both collaborative involvement and review cost.  During private
review, collaboration is actively prevented, and the cost of review to each
participant is restricted to the expense of their personal review efforts.
During public review, partial collaboration is supported by allowing each
reviewer on-line access to the comments made by others, which incurs some
additional cost.  Finally, the review group meeting provides the highest
cost collaboration, but is restricted in scope to only those issues that
cannot be resolved by the previous, lower-cost forms of collaboration.

\subsubsection{Empirical results}

A second class of results from our research involves the empirical data we
have collected and analyzed during the use of CSRS in our organization.

\begin{figure}                                        
 {\centerline{\psfig{figure=summary-data.ps}}}
\caption{A general overview of our review experiences with CSRS to date.}
\label{fig:review-summary-data}
\end{figure}

Figure \ref{fig:review-summary-data}  provides a general overview of our
review experiences performed with CSRS over the past year. Several things
should be noted about these review experiences. First, each of the early
reviews were followed by extensive redesign and enhancement of CSRS and its
instrumentation facilities, though the basic architecture and method has
now stabilized.  Second, the primary goals of these initial experiences
were to validate the essential characteristics of the system and to support
our own quality assurance activities.  The data was not collected, nor is
it presented here in an effort to make any empirically-founded claims about
the general nature of FTR.  However, the results presented here {\em are}\/
intended to make a claim about the power of CSRS for research into the
nature of formal technical review using this system. 

\begin{itemizenoindent}
  
\item {\em Predicting review effort from artifact complexity.} First, we
  are currently investigating whether or not various measures of code
  complexity can serve as a predictive measure of review effort.
  Intuitively, it seems reasonable that the more complicated a program
  entity, the more difficult it is to understand and review, and the
  greater the number of issues and errors it will contain.  We hypothesize
  that these factors will lead to a positive correlation between the
  complexity of a program entity and the review effort required.  A
  similar question is posed as an important area for future research in
  \cite{Fagan86}.
  
  A high correlation will allow prediction of the resources required for
  review based upon the complexity computed for the code. Conversely, it
  will also allow moderators to fit the set of artifacts to the
  resources available for review.  CSRS is uniquely able to support such
  experimentation, since no other review method is able to measure effort
  at the grain size required to assess complexity-related relationships.
  Such a predictive measure will be very significant and useful, since
  FTR activities can account for 15-20\% of all development effort.
  
  We are investigating three popular measures: Halstead's volumetric
  complexity \cite{Halstead77}, McCabe's cyclometric complexity
  \cite{McCabe76}, and simple lines of code. Currently, our analysis has
  been restricted to the private review phase data from the Nbuff-2
  review experience, since later reviews are still in progress, and prior
  reviews did not have idle time correction built into the
  instrumentation.
  
\begin{figure}                                        
 {\centerline{\psfig{figure=complexity.ps}}}
 \caption{Scatter plot of (average effort in seconds, size in lines of
 code) for all reviewed functions during the Nbuff-2 review
 experiment.}
\label{fig:review-complexity-correlation}
\end{figure}

  Figure \ref{fig:review-complexity-correlation} presents a scatter plot that
  shows the relationship between the lines of code for each of the 36
  functions reviewed in Nbuff-2 and the average private review effort
  required (in seconds). There is {\em no} significant correlation ($r^{2}
  \approx 0.27$) between average effort and complexity (in terms of LOC) for
  this data set, although with some imagination, a general trend toward a
  positive correlation might be seen.
  
  Of course, we did not expect to see ``instant significance'' from
  statistical analysis of a single review experience.  What our initial
  investigation did yield are the following refined hypotheses to be
  tested through further data collection and analysis.

\begin{itemize}
\item {\em Additional data collection and analysis will reveal a
  significant correlation between review effort and artifact complexity.}
  One hypothesis is that more data is needed to expose the relationship
  between complexity and effort.  If so, then an interesting question to
  pursue is the nature of this relationship: will a simply linear
  regression model suffice?  We hypothesize that this relationship may
  actually be logarithmic in nature, indicating that once the complexity of
  a function is sufficiently high, review effort levels off.
  
\item {\em Review effort is not strongly correlated with artifact
  complexity, but rather with other factors such as reviewer experience.}
  An alternative hypothesis, also supported by the timeplot data, is that
  other factors are more important in predicting review effort than
  complexity.  For example, the code of inexperienced developers under
  review using CSRS leads to a wide spectrum of issue types, ranging from
  comments about the appropriate use of the programming language, to the
  application platform, to design aesthetics, and so forth.  The code of
  experienced developers has a much narrower range of comments.  We have
  observed that very simple functions written by inexperienced developers
  may engender a great deal of review activity involving these types of
  issues.  If so, then we expect to see that complexity will continue to be
  uncorrelated with review effort, at least when inexperienced developer's
  code is under review, since functions of arbitrary complexity can give
  rise to issues.
\end{itemize}

\item {\em Visualization tools for reviewer behavior.} A second result of our
empirical research is Timeplot, a tool for visualizing the process of FTR by
creating more abstract representations of the timestamp data.  As Figure
\ref{fig:review-summary-data} 
reveals, thousands of timestamps are generated during review, far too many
for non-automated analysis. To reconstruct the sequence of high-level
activities that occur during review, Timeplot produces a simple graphical
representation of each review session.  Figure \ref{fig:timeplot}  shows a Timeplot graph for one of the review
sessions during the Nbuff-2 review.

\begin{figure}                                        
\small
\begin{verbatim}
Time Plot for reviewer: 56
Review: Nbuff-2
Phase: Private
Session Start: Jun 22, 1993 12:11:53

Real   Elapsed  Screen: Review2          Screen: Source           Other Events
Time   Time

12:11  00:00                                                      
12:12  00:01                             ^ gi*nbuff*read          
12:13  00:02                             |                        
12:14  00:03                             v gi*nbuff*read          
  :      :      :                        :                        (Idle 3 minutes)
12:18  00:07    |                        |                        
12:19  00:08    |                        |                        
12:20  00:09    |                        |                        
12:21  00:10    |                        |                        
12:22  00:11    |                        |                        
12:23  00:12    |                        |                        
12:24  00:13    v Issue#354              v gi*nbuff*make          
12:25  00:14    ^ Issue#356              |                        
12:26  00:15    |                        |                        
12:27  00:16    v Issue#356              |                        
12:28  00:17                             |                        
12:29  00:18                             |                        
12:30  00:19                             |                        
12:31  00:20                             v gi*nbuff*read          
12:32  00:21                             v gi*nbuff*read          
12:33  00:22                             v gi*nbuff*get-nbuff     
12:34  00:23    ^ Feedback#358           v gi*nbuff*nbuff-p       
12:35  00:24    v Feedback#358           v gi*nbuff!node-ID       
  :      :      :                        :                        (Idle 49 minutes)
13:25  01:14                             v gi*nbuff*node-ID       
13:26  01:15                             |                        
13:27  01:16                             |                        
13:28  01:17                             v unpack-buffer          
13:29  01:18                             ^ unpack-buffer          
  :      :      :                        :                        (Idle 3 minutes)
13:33  01:22                             v unpack-buffer          
13:34  01:23                             |                        
13:35  01:24                             |                        
13:36  01:25                             |                        
13:37  01:26                             |                        
  :      :      :                        :                        (Idle 4 minutes)
13:42  01:31                             v init-nbuff-local-variables  
\end{verbatim}
\normalsize
\caption{An example timeplot graph from the Nbuff-2 review.}
\label{fig:timeplot}
\end{figure}

Timeplot is the first step in our efforts to characterize review process
behavior analytically.  We hope to use Timeplot as a basis for discovering
the varieties of review behavior possible using CSRS and their impact upon
review outcome.  This can help us to both improve the design of CSRS, and
to improve our understanding of what constitutes an easily reviewed
artifact.  Future research on timeplot may include incorporation of 
grammar-based abstraction mechanisms \cite{Smith91} or statistical
analysis \cite{Gottman90}.  

\item {\em Characterizing the range of review information captured by
  CSRS.} Third, to provide some perspective of the range of artifacts
  captured by CSRS, we are incrementally re-categorizing the types of
  issues and commentary generated during review. So far, we have found that
  approximately 80\% involve traditional review issues of defect discovery.
  However, the remaining 20\% provide either: (a) significant new design
  rationale information; (b) new clarification of the specifications or
  behavior of the application under development; or (c) new clarification
  of the specifications or behavior of Egret or some other underlying
  infrastructure (such as the source language).  The generation and capture
  of these latter artifacts (which are not typically recorded in a
  conventional review---indeed, they are viewed as a ``digression")
  have helped us to improve the specifications and documentation of EGRET
  and other systems, and provided significant aid to developers.
  
\item {\em User satisfaction.} A final result is the subjective impression
  of our organization about FTR using CSRS.  Put simply, all of the
  developers in our (admittedly small) user community express extremely high
  satisfaction with CSRS, viewing it as a highly effective and important part
  of their current and future development activities.

\end{itemizenoindent}






