%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% icse94-related-work.tex -- 
%% RCS:            : $Id: icse94-related-work.tex,v 1.3 94/02/14 15:57:28 johnson Exp Locker: johnson $
%% Author          : Philip Johnson
%% Created On      : Thu Aug 12 16:29:25 1993
%% Last Modified By: Philip Johnson
%% Last Modified On: Tue Feb 15 08:06:09 1994
%% Status          : Unknown
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1993 University of Hawaii
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% History
%% 12-Aug-1993		Philip Johnson	
%%    

\subsection{Issues in FTR research}
\label{sec:research}

Research on FTR has led to a wide variety of review methods.  However,
the current state of FTR research fails to provide clear guidance to
an organization in choosing a well-suited review method.

One deficiency in the literature is the lack of high quality, empirical
studies comparing different review methods to each other.  Past studies
compare review to testing \cite{Basili85,Myers78} or compare different
factors within a single review method (usually Fagan's inspection), such as
the effect of the number of participants or group composition on review
effectiveness \cite{Bisant89,Martin90}.  In addition, although these latter
comparative studies claim to use the same approach (Fagan code inspection),
ambiguities and inconsistencies in the description of the review method
indicate that this key factor was not controlled sufficiently to allow
cross-study comparison.

Another problem with the current state of research on review is conflicting
and/or anecdotal explanations of the causal factors underlying review
outcomes.  For example, researchers have variously attributed an FTR
method's effectiveness to general group interaction \cite{Dunn90,Peele82},
producer--reviewer interaction \cite{Myers78,Parnas85}, lack of
producer--reviewer interaction \cite{Ackerman89,Russell91},
individual effort \cite{Humphrey90}, paraphrasing \cite{Fagan76}, selective
test cases \cite{Ackerman89}, stepwise abstraction \cite{Selby85}, and
checklists \cite{Knight91}.  While these claims are not all mutually
exclusive, they clearly reveal confusion within the community about review
factors and their effectiveness.

These issues in the state of review research are not raised with the intent
of denigrating the research or the researchers.  Instead, they are raised
to highlight the difficulty and cost of obtaining empirically-founded
understanding of a highly manual, knowledge-intensive, and collaborative
activity.  One contribution of this research is to demonstrate how an
instrumented, computer-mediated environment can resolve some of the
methodological difficulties confronting current review researchers.  The
next section, however, presents some of the practical problems in
obtaining good review outcomes with traditional manual review methods.

\subsection{Issues in FTR practice}
\label{sec:issues-manual-review}

Despite the methodological issues and credit assignment variations noted
above, most research tends to agree that manual review, when properly
carried out, is effective.  Research also tends to agree that manual review
is expensive.  For example, one person-year of technical staff time is
required per 20 KLOC for FTR at Bell-Northern Research, and this cost adds
15-20\% new overhead onto development \cite{Russell91}. Boeing Computer
Services found reviews to be ``extremely expensive'' \cite{Glass82}. Such
admissions are usually followed by analyses demonstrating that this
upstream investment is more than recouped through decreases in downstream
rework costs. 

Although manual FTR, when properly carried out, is typically cost-effective
in the long run, this is a significant qualification, since manual FTR is
very difficult to properly carry out. The primary obstacles
to successful practice have been documented \cite{Deimel91,Freedman90,Gilb93}
and include:

\begin{itemizenoindent}
\item {\em Insufficient preparation.} A ubiquitous cause of low quality
  review is when one or more inadequately prepared reviewers attempt to
  ``bluff'' their way through the review process.  This problem is serious
  enough that fairly devious remedies are presented in the literature. One
  such approach is to deliberately leave out one page of the review
  materials when distributing them to participants: those who prepare will
  notice the absence and contact the review leader.
  
\item {\em Moderator domination.} In a group meeting, it is easy for the
  moderator to inadvertantly or premeditatedly abuse this role by
  inhibiting or intimidating the other participants.  This results in
  reduced participation and reduced review quality.

\item {\em Incorrect review rate.} Each minute of a review meeting is
  intrinsically expensive, since it requires the simultaneous attendance
  and involvement of at least three and frequently six to eight technical
  staff personnel.  Thus, the rate of review is critical to its
  cost-effectiveness: too slow and the time (and salaries) of several
  technical personnel is wasted; too fast and the quality of review
  decreases.
  
\item {\em Ego-involvement and personality conflict.} The fact that one
  of the review member's work artifacts is under review can lead to
  significant interpersonnel problems.  Review always requires substantial
  diplomacy and care on the part of each member.

\item {\em Issue resolution and meeting digression.} The expense of
  review meetings and the complexity of software dictates that review
  sessions not evolve into problem-solving sessions.  All instructional
  materials we have seen cite this issue as crucial to review success,
  stating that reviewers must ``raise issues, but don't resolve them.''  They
  also note that it requires significant reviewer effort and continual
  moderator vigilence to prevent such resolution-oriented discussion.
  
\item {\em Recording difficulties and clerical overhead.} Manual review
  requires a scribe to record the outcome of the process.  Capturing the
  information generated during a review meeting completely and accurately is
  extremely difficult, as noted in the literature, and as anyone who has ever
  attempted the role of scribe will attest. Methods involving audio-visual
  aids and a ``telegram style'' of note-taking have been proposed to support
  this process.
  
  Substantial additional clerical overhead is induced if the data
  collection adequate for the purposes of research on review or review
  process improvement is required.  Published review data has only come
  from very large organizations able to allocate resources to this activity.

\end{itemizenoindent}

These problems are not specific to FTR, but appear in virtually all
types of meeting-based group work.  The same list of group process
problems appears in research to motivate the design of an electronic
meeting room system \cite{Nunamaker91}.  

The previous two sections provide evidence for a central claim in our
research: the current manual nature of FTR makes it difficult to
effectively carry out review, and makes it difficult to measure the
process and products of review in such a manner as to understand
review, compare review experiences across organizations, and improve
the process based upon empirical findings. The remainder of this paper
presents how we are addressing these issues in our research on CSRS.





