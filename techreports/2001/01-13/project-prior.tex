\subsection{Results from prior NSF-sponsored research}


The motivation for and design of our proposed research on
developer-centric, ultra-lightweight empirical software project data
collection and analysis follows directly from the findings of our last two
NSF-sponsored research projects.

\subsubsection{The CSRS project}

%\small
%\begin{tabular}{lp{4.5in}}

%Award number: & CCR-9403475 \\
%Program: & Software Engineering \\
%Amount: & \$161,754 \\
%Period of support: & June 1995 to June 1998 \\
%Title of Project: & Improving Software Quality through Instrumented Formal Technical Review \\
%Principal Investigator: & Philip M. Johnson \\
%Publications: & \cite{csdl-93-22,csdl-95-24,csdl-96-14,csdl-97-02,csdl-93-07,csdl-93-17,csdl-94-03,csdl-96-06}
%\end{tabular} \\[3mm]
%\normalsize

In this research, we designed, implemented, and evaluated a collaborative
software review system called CSRS and performed a series of experiments
with the following contributions.

 
\vspace*{-.15in} \paragraph{Design of instrumented review technology.} The CSRS system
illuminates the design and architectural issues involved with the creation
of a review system with both fine-grained measurement support and a process
modeling language able to express a wide variety of current software
review techniques. For example, in addition to traditional review metrics
such as the number of issues discovered during different review phases,
CSRS allows one to measure the time spent on review of any single function
in the source code, and the sequence of functions visited by a reviewer
over the course of review. The CSRS process modeling language allows
emulation of most review techniques such as Fagan inspection \cite{Fagan76}
and Active Design Reviews \cite{Parnas85}, as well as new review techniques
such as FTArm that require an online environment.

\vspace*{-.15in} \paragraph{Instrumentation support for experimentation.}  The measurement support and process flexibility provided by CSRS also
provides excellent infrastructure for controlled experimentation.  We used
it to investigate the costs and benefits of meetings in software review by
implementing two review techniques, one which involved a synchronous
meeting between review team participants to discuss issues, and another in
which all team participant interactions were asynchronous.  Our study found
that the meeting-based method was significantly more costly in terms of
effort than the non-meeting-based method, though we could not detect
any difference in the numbers or types of defects found.
 
\vspace*{-.15in} \paragraph{Comparison with Maryland experimental data.}  
To gain insight into the generality of our findings on the efficiency and
effectiveness of software review meetings, we teamed with Professor Adam
Porter of the University of Maryland to perform a modified form of
meta-analysis we call "reconciliation" on the data resulting from the
CSRS-based experiment and an independently designed and conceived
experiment performed by Porter at the University of Maryland.  Our
reconciliation process differs from traditional meta-analysis in that we
did not attempt to combine the data sets; instead, we formulated a set of
common hypotheses and tested them against each data set independently.
Despite differences in the types of documents reviewed, the backgrounds of
the participants, and the technological infrastructure support between the
two experiments, the two studies confirmed each other's results for all six
common hypotheses, providing further evidence that meeting-based review may
not be the most efficient and effective choice for many development
contexts.
  
\vspace*{-.15in} \paragraph{Usability issues in review technology and measurement infrastructure.}
 In addition to the above positive findings, our research also
  uncovered usability problems with CSRS that
  impeded its adoption in academia and industry.  The feedback we received
  indicates four principle usability problems.  First, CSRS requires a Unix
  back-end server for its hypertext database engine, and some sites did not
  have a Unix server available.  Second, the client interface to CSRS is
  tightly integrated with Emacs, as illustrated in Figure
  \ref{fig:csrsleap}, and many potential users find Emacs
  difficult to use.  Third, the hypertext back-end and Emacs front-end
  constrained review documents to those available in ASCII format; while
  this constraint was unimportant for source code review, it often proved
  problematic for design or requirements documents. Fourth, our
  fine-grained metrics collection and its attendant storage in a
  centralized repository raised concerns among developers that the data
  would be used inappropriately by management.  After a conversation with
  one manager in which he spoke excitedly about the extensions he
  envisioned for CSRS, we realized that developer fears might well be
  justified.


\begin{figure}[t]
 {\centerline {\psfig{figure=csrsleap2.eps}}}
 \caption{{\em Screen shots illustrating the Emacs-based CSRS interface 
for issue reporting (left) and the Java-based Leap interface for 
project size and time estimation (right).}}
 \label{fig:csrsleap}
\end{figure}

\subsubsection{The Leap project}

%\small
%\begin{tabular}{lp{4.5in}}
%Award number: & CCR-9804010 \\
%Program: & Software Engineering \\
%Amount: & \$260,000 \\
%Period of support: & July 1998 to July 2002 \\
%Title of Project: & Project LEAP: Lightweight, Empirical, Anti-measurement
%Dysfunction, and Portable Software Developer Improvement \\
%Principal Investigator: & Philip M. Johnson \\
%Publications: & \cite{csdl2-00-03,csdl-98-13,csdl-98-11,csdl-99-08,csdl-99-07,csdl-98-04}
%\end{tabular} \\[3mm]
%\normalsize

In this research, we built upon the results of
the CSRS project as well as upon a case study of data quality in the Personal
Software Process (PSP).  In the case study, we subjected the data collected
manually by students during the standard semester-long introduction to the
PSP to a retrospective analysis of its accuracy.  We implemented a database
system to cross-check all calculations and and expose six classes of data
defects.  The analysis discovered over 1500 errors made by the 10 students,
errors that in several cases lead to incorrect inferences concerning
process improvement. 

Based upon the usability problems we found with CSRS and the data quality
problems we found with the PSP, we designed, implemented, and evaluated a
Java-based toolkit called Leap.  Leap satisfies four principal
requirements. First, it is {\em lightweight}: it imposes a minimum of
process constraints upon the user, it should be easy to learn and integrate
with other methods and tools, and require minimal investment and commitment
from management.  Second, it is {\em empirical}, providing high quality
collection and analysis of quantitative data.  Third, it reduces the risk
of {\em measurement dysfunction} \cite{Austin96}, or the situation in which
developers consciously or unconsciously skew data collection and analysis
for personal, political or organizational goals.  Finally, the toolkit and
associated data is {\em portable}, allowing the user to continue to build
their personal repository of software engineering data as they move within
and across development organizations.

The Leap project is nearing completion, with the following contributions:
  
\vspace*{-.15in} \paragraph{Technology support for developer-centric, in-process, disruptive 
  software project data collection and analysis.} The Leap toolkit has been
implemented and evaluated both through classroom and industrial use.  The
toolkit consists of approximately 40,000 lines of Java code and runs on all
major platforms.  The tool supports the collection of time, size, defect,
and design pattern data, as well as the definition of size types, defect
types, and projects and their associated components. The design of the Leap
toolkit eliminates broad classes of data quality problems that users
experience in the manual PSP.

\vspace*{-.15in} \paragraph{Case study results on project estimation.} 
The project estimation component of Leap provides 13 different possible
estimation methods, in contrast to the PSP, which implements a single
estimation method called PROBE.  We performed a case study using the Leap
toolkit in which we were able to retrospectively assess the estimation
accuracy of the 13 methods.  There were two surprising results. First, the
PROBE method was sixth out of the 13 with respect to data accuracy. Second,
the most accurate estimation technique was a form of ``guesstimation'', in
which the developers, aided by the empirical data gathered by the tool and
the results of the various estimation techniques, simply entered their best
estimate.
 
\vspace*{-.15in} \paragraph{Usability improvements over CSRS.}
The Leap toolkit addressed many of the usability problems we discovered in
our research on CSRS.  Leap runs on Unix and Windows platforms and provides
a GUI interface, as illustrated in Figure \ref{fig:csrsleap}. Rather than a
centralized server, Leap stores data files locally so that the user has
total control over their own data.  Leap does not provide a collaborative
review mechanism and does not store software development artifacts
internally, so there is no restriction to ASCII representation.
  
\vspace*{-.15in} \paragraph{Usability problems due to ``disruptive'' in-process data
  collection.}  Despite the advances in automation and usability, Leap has
still found limited adoption.  Few students have continued to use it after
the end of the semester in which use was mandated, despite their
acknowledgment that the data collected and analyzed using Leap did improve
their project estimation and quality assurance capabilities.  We are in
contact with a group of industrial developers voluntarily using Leap, but
they appear to be much more highly motivated than the typical developer to
collect and analysis personal software project data.

\medskip

Our successes and failures with CSRS and Leap deeply influence the design
of the current research project, and give us an understanding of the
variety of requirements that must be simultaneously addressed in order for
a measurement-based system to achieve widespread, long-term adoption by
developers across diverse organizations. First, the system's interface must
be very simple and require little initial training in order to derive
non-trivial benefits. Second, those who contribute the data must be those
who derive benefit from the data, and those benefits must occur in the
near-term, not the long-term.  Third, empirical software project data is
highly susceptible to measurement dysfunction, and mechanisms must be
available to provide developers with the confidence that the data will not
be used against them in the future. Fourth, and perhaps most importantly,
even when developers experience substantial benefits from daily collection
and regular analysis of empirical project data under mandated conditions,
most begin ``forgetting'' to collect and analyze their data as soon as they
are left on their own. In the case of PSP, we conjectured that this was due
to the high overhead of manual daily collection and analysis.  However, our
experiences with Leap tool support indicate to us that the fundamental
adoption problem of developer overhead will not be solved through normal,
evolutionary approaches to measurement automation, such as GUI-based
utilities to simplify daily entry of time, size, and defect data and
associated analyses for project planning or post-mortems.
 


















