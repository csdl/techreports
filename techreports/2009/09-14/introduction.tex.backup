\chapter{Introduction}
Contemporary software projects typically have a considerably long life-cycle - well over decade.
Their development and maintenance usually carried out by geographically distributed teams 
and individuals. The development pace, the experience, and the structure of these development 
teams continuously change as developers are joining and leaving. When combined with
schedule and requirements changes, all these create numerous difficulties 
for stakeholders, community, and developers, ultimately affecting the project success. 

This software development complexity phenomena was identified in 1968 as ``Software crisis'' \cite{crisis}, 
and was addressed by bringing the research and the practice of software development 
(or as it was called ``programming'') under the umbrella of Engineering  - in an effort to provide a 
control over the process of software development. 
Following the engineering paradigm, numerous methodologies and models of \textit{software processes}
were proposed \cite{citeulike:10002165}.

A \textit{software process} is a set of activities performed in order to design, develop and maintain 
software systems. Examples of such activities include design methods; requirements collection and creation of ²
UML diagrams; requirements testing; and performance analysis. 
The intent behind a software process is to structure and coordinate human activities in order to achieve 
the goal - deliver a software system successfully in time and under the budget constraints.
Since then, much work has been done in software process research resulting in a number of industrial standards for 
process models which are widely accepted (CMM, ISO, PSP etc. \cite{citeulike:5043104}). 

Nevertheless, software development remains error-prone and more than half of all commercial software development
projects ending up failing or being very poorly executed. Some of them are abandoned due to running over budget,
 some are delivered with such low quality or so late that they are useless, and some, when delivered, 
are never used because they do not fulfill requirements. 

Not only industrial software development suffers from process complexity issues. FLOSS (Free/Libre/Open Source Software)
plagued by similar issues originating from loosely-organized developer collaboration structure. 
As was shown, most of FLOSS projects never reach a ``magic'' 1.0 version \cite{citeulike:12480029}. 
Among others, the great "infant mortality rate" of OSS projects is related to a burnout, inability to acquire a critical
mass of users, loss of leading developer(s), and forking \cite{richter2007critique}.

%The recent study by Standish Group (Rubinstein, ``Chaos Reports'', 2006) indicates,
%that while `\textit{`Software development shops are doing a better job creating software than they were 
%12 years ago}'', still, only ``\textit{35\% of software projects in 2006 can be categorized 
%as successful meaning they were completed on time, on budget and met user requirements}”.
%Another, widely acknowledged problem with existing models is their rigidity - usually 
%once a project executed, the cost of incorporating a change in the process or a product 
%not only becomes significant, but also grows proportionally to the project execution time.

The cost of this lost effort is enormous and may in part be due to our incomplete understanding of software process.

\section{Motivation}
There is a long history of software processes research. However, almost all of the approaches can be divided
into two categories.

First category consists of traditional to engineering \textit{top-down} approaches to software process 
design through proposing and evaluating a specific patterns of software development. 
For example, the Waterfall Model process proposes a sequential pattern in which 
developers first create a Requirements document, then create a Design, then create an Implementation, and 
finally develop Tests. Contrary, the Test Driven Development process proposes an iterative pattern in which the
developer must first write a test case, then write the code to implement that test case, then refactor the system for
maximum clarity and minimal code duplication. 

A problem with top-down techniques is that the developer, manager, or a researcher, have to ``invent'' the adequate to
the task process in the first place, which is far from trivial \cite{citeulike:5043104} \cite{citeulike:1986013}. 
Moreover, it was shown, that the process inventors are often limited in their scope and tend to assume an idealized
versions of real processes, thus, often producing ``paper lions''
- process models which are likely to be disruptive and unacceptable for end users, at least in their proposed form
\cite{citeulike:9758924}. In addition, the full evaluation cycle of proposed processes is considerably long and
expensive.

The second category consists of alternative, \textit{bottom-up} techniques for process reconstruction through
noticing of recurrent patterns of behaviors. One of the first work in the field by Cook and Wolf
shows a possibility to automatically extract a process model through mining of event logs
\cite{citeulike:328044} \cite{citeulike:5120757} \cite{citeulike:5128143}. Later, it was shown by Huo et al., that
through event logs analyses it is also possible to improve existing processes by process enactment
\cite{citeulike:7691059} \cite{citeulike:7690766}. 

The bottom-up approaches also affected by a number of issues. A chief among these is the observability
issue - it is usually difficult to conduct a full depth study due to the privacy concerns, moreover, it is expensive
to observe a process performed by a team for a whole life-cycle of a project. 
Another issue is the capacity of the process discovery techniques - these often need to fine-tuned in order to
reconstruct processes from events, especially those, which are distributed and concurrent. 

Recently, the interest for process reconstruction has been revived. It is driven by increase in public data, that were
made available by the proliferation of open source communities. Currently, software artifacts are abundant: source
code, defect records, mailing list communications, etc., - all these are easily accessible online. This effectively
removes the high cost of observation, that previously made large-scale analysis of software projects unfeasible 
for most researchers. Since 2004, the International Conference on Software Engineering (ICSE) has 
held a Working Conference on Mining Software Repositories (MSR). 
Its original call for papers stated MSR's purpose as \textit{``... to use the data stored in these 
software repositories to further understanding of software development practices ... 
[and enable repositories to be] 
used by researchers to gain empirically based understanding of software development, 
and by software practitioners to predict and plan various aspects of their project''}
\cite{msr2004} \cite{citeulike:7853299}. 
Several other venues, International Conference on Predictive Models in Software Engineering \cite{promise12}, 
International Conference on Open Source Systems, 
the Workshop on Public Data about Software Development, 
and the International Workshop on Emerging Trends in FLOSS Research have also played
an important role in shaping and advancing this research domain.

The problem of process discovery from software process artifacts, however, remains. Moreover, the coarse
granularity of publicly availably software process artifacts introduced additional challenges. In particular,
the problem of software development events inference become cornerstone. 
%The software artifacts created merely for supporting of software development activities rather than collected for the
%process discovery. However, the ability of such system to
%infer software processes is questionable.
%Since events characteristic patterns discovery and their interpretation. 
In this work, I propose that the event inference problem can be viewed as a pattern recognition problem.
Then, the software development event inference task is to categorize the observed patterns at hand into known
event categories. Moreover, if we accept that certain patterns correspond to software development actions, then
we can split the process reconstruction into two parts - a pattern recognition and a network analysis camps
corresponding to two
major camps of artificial intelligence (AI), with hand-coded systems corresponding to symbolic AI,
and behavior networks corresponding to connectionist AI. The argument that behavior networks
correspond to connectionist AI is made in (Roitblat 1991) (though that paper predates the term
behavior networks, which originated in (Maes 1991b)).



While the nearest neighbor algorithm has the advantages of simplicity and not
requiring extensive parameter tuning, it does have several
important disadvantages. Chief among these are its space and time
requirements, and the fact that it does not tell us anything about
why a particular object was assigned to a particular class.

In my work, I am following the non-intrusive, approach for the software process analysis focusing on the discovery 
of recurrent behaviors from software process artifacts. I believe, that if there exists a mechanism which facilitates 
the discovery of recurrent behaviors, it might be possible to associate these with building blocks of larger processes. 
In turn, this empirical knowledge would help to refine our understanding of existing process models and to improve 
a process of a design of novel software development methodologies.

The work in this thesis addresses a fundamental aspects in discovery of recurrent behaviors - their significance assessment.
At first, I will show a generic time-series classification algorithm which facilitates discovery of class-characteristic features.
Then, through the exploratory case studies I will evaluate its applicability to publicly available software process
artifacts.

%Finally, there is third, people-oriented approach to software processes pioneered
%by Weinberg as early as 1971
%\cite{citeulike:262020}. This wok shifts focus from technical aspects towards understanding
%and improving how software processes are shaped by humans and social behaviors. recently been revived.  


%All these - the uneven and often unpredictable performance of software processes with experiences 
%ranging from success to complete failures, and the lack of efficient research methodologies, clearly indicate, that our
%understanding of underlying ``software process mechanics'' is incomplete. This fact continues to fuel further research
%in software processes.

%Not surprisingly, as per today, most of the research in the field is focused on engineering-like software processes
%following 
%business interests and leveraging the ability to perform experimentation in a controlled environment - where it is
%possible to 
%evaluate a proposed by solution to (often \textit{cherry-picked}) research problems. 
%One problem with this approach to process development is that it requires the process analyst to invent, or to notice 
%a recurrent pattern of behavior, addressing the problem in the first place \cite{citeulike:5043104}. 
%In addition to difficulties with ``invention'' itself, another issue pinpointed by van der Alast in
%\cite{citeulike:9758924}, 
%is 

%Nevertheless, despite to success of formalized approaches to manufacturing processes in other 
%engineering fields, current state-of-the-art industrial models of software processes perform somewhat 
%poorly and do not deliver consistently. 
%For example, the recent study by Standish Group (Rubinstein, ``Chaos Reports'', 2006) indicates,
%that while `\textit{`Software development shops are doing a better job creating software than they were 
%12 years ago}'', still, only ``\textit{35\% of software projects in 2006 can be categorized 
%as successful meaning they were completed on time, on budget and met user requirements}”.
%Another, widely acknowledged problem with existing models is their rigidity - usually 
%once a project executed, the cost of incorporating a change in the process or a product 
%not only becomes significant, but also grows proportionally to the project execution time.

%Along with engineering-like industrial process models, a number of software processes 
%emerged from hobbyists and were successfully adopted by practitioners seeking for alternative solutions.
%Some of these, proven to be applicable for large scale industrial projects 
%(Android OS: LoC$>$15M, $\sim$2K contributors). 
%Among others, the Free/Libre/Open-Source Software model (FLOSS) and the software craftmanship 
%approaches gained a significant credibility among practitioners.
%While the former \textit{holistic} software process paradigm emphasizes extensive collaboration, 
%frequent releases and removes the boundary between developers and customers, the latter is focusing 
%on the roles of highly motivated, creative and skilled individuals in a process of software creation. 

\section{Research questions, hypothesis, and a scope of the dissertation}
Software is coded by humans. Whether in team or individually, we perform a number of daily activities - 
commuting to an office, answering emails, attending meetings, writing a code, and many others. 
The ordering of these activities and their durations are driven and constrained by internal and external factors, 
such as a role,motivation, working schedule, project phase, physical location of the office, etc. 
When summarized together, the range of these activities and the mannerisms form our behavioral portraits.

A subset of these activities can be attributed as relevant to the project goal - delivering a software. 
Thus, one of the research questions addressed in my work is the \textit{partitioning of the activities}. 

By limiting the scope of my research to a discovery of recurrent behaviors only from software process 
artifacts, I address the above question at large (but not in full) by simply considering only related to software 
process entities.
This, however, leads to other research questions: 
\textit{is it possible to discover recurrent behaviors from a very limited set of software process by-products?} 
and, if it is possible, 
\textit{will the discovered behaviors be meaningful, i.e. interpretable?}

Some of the previous research, especially in MSR field  \cite{citeulike:9114115, citeulike:7853299}, 
indicates, that by application of a variety of a techniques it is not only possible to discover evidence of 
software process \cite{citeulike:9007622}, but, at least partially, to infer the process as a whole \cite{citeulike:5128808}. 
In these works, it was shown, that many of techniques can be applied to software process artifacts, 

These combine into the research hypothesis - \textit{it is possible to discover recurrent behaviors 
from software process artifacts by application of appropriate data-mining techniques} - 
which I investigate in this dissertation. 

In order to approach this hypothesis questions, I choose to reduce the problem of mining of software artifacts to a narrower,
but more generic software treat software artifacts as time-series and design an interpretable time 
series classification technique, which, 
as I will show, temporal 


Exactly as it sounds, my research is interdisciplinary - it combines together knowledge discovery and software process 
analysis, focusing on a very narrow subject - exploring approaches for recurrent behaviors or ``programming habits'' 
discovery from software process artifacts.
While I will show, that recurrent and significant software-development behaviors can be discovered,
their precise categorization, effect, and performance are beyond the scope of this thesis.

\section{Contributions}
I show a novel, generic algorithm for interpretable time series classification: SAX-VSM. 
While the performance of this algorithm is at the level of current state of the art, it offers an outstanding feature -
discovery, generalization and ranking of class-characteristic structural features. This feature, in turn, enables
knowledge discovery by offering much clearer insight into data specificity than any other competing technique.
In addition, SAX-VSM uses only N weight vectors for classification of unlabeled data by computing N cosines, where N is
a number of classes, - therefore it is very fast and has a very small memory footprint.
Overall, I expect this algorithm to play an important role in future because of the growing ubiquity of time series and
growing interest in behaviors.

I provide SAX-VSM implementation to the community. This implementation uses several computational tricks to optimize,
reduce, and reuse computation. Within last years, this implementation was regularly downloaded and used in academia and
industry. 

Powered by SAX-VSM, through the application of Software Trajectory Analysis (STA) to software process artifacts, I show
through the case studies: that it is possible to discover known recurrent behaviors, thus positively confirming the
research hypothesis. In the PostgreSQL study I was able to discover characteristic recurrent behaviors in source code
editing churns corresponding
to Software release and to the Commit Fest processes.
that STA and SAX-VSM can be used as a knowledge discovery tool in the StackOverflow case study. In particular, I show,
that the temporal primitives discovered by the algorithm provide not only quantitative evidence for processes
interpretation, but can be used for a qualitative assessment of discovered recurrent behaviors.
In Android case study...

Finally, I provide an implementation of STA analysis framework to the community. 

\section{Organization of the dissertation}