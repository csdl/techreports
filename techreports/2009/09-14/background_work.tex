\chapter{Prior and related work}\label{chapter_background_work}
Software Trajectory Analysis consists of two components: 
the \textit{software artifacts retrieval and measurement machinery} (i.e. the data assimilation layer), 
and the \textit{software trajectory characteristic patterns discovery module} (i.e. the data analysis layer). 
The high-level overview of the information flow through these is show at the Figure \ref{fig:sta-full-overview}.

The artifacts retrieval and measurement machinery refers to a way that software artifacts are collected, 
measured, and enriched with metadata. 
Currently, STA is capable of retrieving and processing the data from typical to OSS development Software 
Configuration Management system (SCM) components such as version control, 
defect management, and communications management systems. 
In addition, as I shall show, it is capable to work with other data sources such as Q\&A 
websites and Hackystat \cite{csdl2-10-09}.

Note, that STA is not limited only to these data sources.
As public repositories are highly heterogeneous and continuously evolving, STA adopts a common to the 
Software Repository Mining field (MSR) strategy for data assimilation, unification, and off-line enrichment,
where public artifacts are retrieved and stored ``\textit{as is}'' (i.e. mirrored) at first, 
measured at second, and enriched with with metadata at the final step 
\cite{citeulike:12550438} \cite{german04_softchange} \cite{cvsanaly}.
Similarly to other systems for mining software repositories, STA relies on the relational database engine 
for data storage and indexing -- this solution not only enables an interactive workflow and a federated 
access to the data, but allows for effective measurements partitioning and aggregation, which is 
an \textit{essential capability} for efficient software trajectories construction.
Overall, STA data assimilation layer is designed in a way that conforms to the field's best practices
allowing its extension for any data source that is capable of providing data for STA analysis.
%\fxnote{maybe cite the data enrichment with geolocation metadata for stakcoverflow analyses?}

The software trajectory characteristic patterns discovery module refers to an analytical machinery that 
is responsible for discovery of characteristic recurrent patterns in a set of software trajectories provided as 
an input. Conceptually, this module can embed \textit{any data mining algorithm} which is capable of 
discovering recurrent patterns from sequential data, such as one of the numerous algorithms for time series 
motif discovery \cite{citeulike:13197378}.
However, the specificity of software trajectories and the pattern of interest, i.e. recurrent behavior, 
is placing a number of constraints that limit the applicability and the performance of known algorithms.
First of all, the algorithm must be capable \textit{to discover recurrent patterns without any prior knowledge 
about their length, shape, amplitude, and occurrence frequency}, as these are expected to naturally differ 
between projects, problems, or even subsets of trajectories from the same project.
Secondly, it must be capable to \textit{learn from a very small training data set} --
the property that has been shown crucial in predictive modeling and knowledge mining from software 
repositories where data is sparse \cite{citeulike:6055293}.
And finally, the algorithm must provide an automated mechanism for 
\textit{patterns ranking according to their relevance} in order to allow their efficient 
review by human experts, since it is impossible to define a pattern ``interestingness'' or 
``importance'' a priori.

The discussed in this thesis STA characteristic patterns discovery module implementation relies on SAX-VSM,
a novel algorithm for characteristic patterns discovery from time series, that I shall propose in the following Chapter. 
This algorithm has been designed specifically in order to address aforementioned requirements.

\begin{figure}[t]
   \centering
   \includegraphics[width=150mm]{figures/Flow-analysis.eps}
   \caption{The high-level STA overview. Software artifacts are retrieved, enhanced, and measured within the
   data assimilation layer. Next, based on the user input, classes of software trajectories are constructed.   
   In turn, the data analysis layer performs comparative analyses of software trajectories yielding sets
   of ranked class-characteristic behaviors.
   Note, that for clarity only two classes of trajectories shown while STA analysis module is capable of
   discovering class-characteristic patterns from many classes at once.}
   \label{fig:sta-full-overview}
\end{figure}

Further in this chapter, in order to relate Software Trajectory Analysis to other research and to position 
it among other work, I shall discuss previous work from few research areas.
At first, since STA is designed for software measurements analysis, I provide a background on software 
measurements and evidence for their tight coupling with software processes. 
Next, I briefly discuss my earlier exploratory studies conducted with previous STA implementations. 
Finally, I review relevant to mining software repositories research focusing on recurrent behaviors discovery.
The work relevant to the time series characteristic patterns discovery and SAX-VSM will be discussed 
in the next Chapter.

\section{Software measurements}
As in all other Engineering fields, measurements are used in Software Engineering in order to establish a 
systematic approach to software development which provides the control over software processes, facilitates
their improvement, and, most importantly, makes their result predictable. 
In addition, software measurements enable scientific research.

\subsection{Software measurement history}
According to Fenton \cite{citeulike:1525462}, the history of measurements in Software Engineering dates 
back to mid-1960's  ``\textit{...when the Lines of Code metric was used as the basis for measuring the 
productivity and effort...}'' , which in fact predates the establishment of Software Engineering as an 
independent discipline \cite{naur_crisis_68}. 
Much of early research concerned with software measurements has been driven by the need for resource model 
prediction and forecasting \cite{citeulike:1525462}, whereas later research has extended towards the problem 
of software process management \cite{citeulike:13158802}.

Probably the earliest published work outlining close relations of software measurements and software 
processes is the ``Software project forecasting'' by DeMillo and Lipton \cite{demillo1980software} where they 
point out that software measurements create a basis which allows practitioners and researchers to be 
``\textit{rational and objective}'' about software processes. 
Remarkably, the authors refer to even earlier notes by Perils, Sayward, and Shaw, who emphasized the role of software 
measurements in the software process management, saying that `\textit{the purpose of software metrics is 
to provide aids for making optimal choice at several points in the life cycle}''.

With time, the increasing understanding of software measurements objectiveness and their ability to reflect 
the state of software processes led to the development of measurement-based strategies for software 
process management and improvement. 
For example, one of the pioneering strategies for global software process improvement, 
Total Software Quality Management (TSQM), relies on the set of ten explicitly defined software 
process and product metrics ranging from low level product metrics of Lines of Code and Design Complexity to 
high-level project management metrics of Schedule and System Testing Progress \cite{citeulike:13071448}.
Similarly, the local strategy for software process improvement, Personal Software Process (PSP), relies on 
the broad range of software metrics \cite{citeulike:13072239}.

In addition to playing an important role in software process management and forecasting, software 
measurements become ubiquitous in scientific research. 
In the research field of Empirical (or as it also called Experimental) Software Engineering (ESE), 
researchers use measurements and experimentation as the basis for research hypotheses 
generation and their investigation \cite{citeulike:766768}. 

Recently, due to the proliferation of open source software development and advancements in public software
project hosting solutions, a new research area called Mining Software Repositories (MSR) has been established 
within ESE field. MSR is specifically concerned with application of analytical techniques to public software 
repositories \cite{citeulike:12550438} \cite{citeulike:4534888} \cite{citeulike:2710928}, thus, 
the research work from this field is one of the most relevant to my research.

\subsection{Software measurement theory}
In science and in Engineering, measurements allow to formally characterize attributes of an entity by assigning 
them a numerical, a boolean, or a symbolic value. 
The choice of the value type depends on the used measurement criteria, such as a dimension, a level, 
or a degree. Ultimately, the chosen criteria and the scale of used values shall enable an intuitive 
and precise quantitative comparison between attributes regardless of their qualitative similarity or 
difference, as it was pointed by Chapin \cite{citeulike:13158806}. 
In addition, the measurement units and scales are usually standardized in order to enable the global comparability.

An entity in Software Engineering can be a physical object, such as a program or a use case diagram, 
an event, such as a software release, or a software artifact, such as a bug report.
A measurable entity's attribute can be its property or a feature, such as the program's size, the 
amount of defects discovered during testing, and the usability of a software system.

Further, attributes are usually divided into two categories: internal and external. 
While measures for internal attributes are computed based on the entity itself, external attribute 
measures depend on the both the entity and the environment in which it resides -- for example a 
software system testing time varies depending on the performance of a test server.

Finally, as pointed by Fenton \cite{citeulike:1803429}, there are two broad types of measurements: direct
and indirect. While direct measurements of an attribute do not depend on any other attributes, 
indirect measurements involve measurements of one or more other attributes. 
As an example of a direct measurement consider the size of system source code or the time developers spent 
on project. In contrast, a module defect density (ratio of the defects number and the module size), 
or a requirement stability (ratio of initial requirements and total requirements) are indirect measurements.

\subsection{Software measurements in STA}
Software Trajectory Analysis is built for the analysis of software measurements whose purpose is to enable 
recurrent behaviors discovery. In particular, STA exploits the sequential dependency of consecutive 
measurements for discovering recurrent structural patterns in their dynamics, which, as I hypothesize, 
reflect (i.e. directly linked with) recurrent behaviors.

\begin{figure}[t]
   \centering
   \includegraphics[width=115mm]{figures/SEI-measurements.eps}
   \caption{An illustration of relation between software measurements and key responsibilities 
   in project management from SEI Guidebook \cite{citeulike:10567306}.}
   \label{fig:sei-measures}
\end{figure}

This approach builds upon previous work that confirmed the feasibility of software processes inference through 
observations (i.e. measurements) of their effects on software product evolution and indicated the possibility of 
recurrent behaviors discovery. 

As a particular example confirming the observability of software processes through software product measurements 
consider the de-facto industrial standard for software measurements application provided by 
Software Engineering Institute (SEI) in their guidebook \cite{citeulike:10567306}. 
In particular, the authors focus on the software process execution variability issue that significantly 
affects the software project's schedule and the resulting software system quality. 
To address this issue, they propose a methodology based on implementation of a continuous software product 
and process measurement program that would allow for continuous assessment of the software processes 
variability enabling a ``real-time'' software process control. The Figure \ref{fig:sei-measures} shows a schematic 
illustration of such program.

Hackystat, the ``parent'' system of STA, is another study that extends the applicability of continuous 
measurements and confirms the possibility of software process understanding through the analysis of recurrent 
behaviors \cite{citeulike:557296}. 
As pointed by the authors, the visual comprehension of measurements variability and pattern collocations enables 
``\textit{emergent knowledge that one state variable appears to co-vary with another in the current project context}'',
allowing for process improvement activities \cite{citeulike:557296}. 

As an example indicating the possibility of recurrent behaviors discovery through measurements, consider the 
study by Hindle et al. \cite{citeulike:10377345} discussed in the Section \ref{chapter2_section-tsanalysis} of 
this chapter that shows an example of recurrent behaviors detection by the Fourier Transform -based analysis.

STA extends previous approaches built for software measurements analysis by providing an automation for 
characteristic patterns discovery from software process and product measurements, which, as I expect, shall 
aid in understanding of recurrent behaviors and their role and effect in software processes.

\section{Previous work on STA}
Current implementation of Software Trajectory Analysis is generic by the design. In fact, it can be applied 
to almost any kind of sequential software measurements that carry potentially useful to the research 
questions information. 
This generality is granted by the STA core algorithm for characteristic patterns discovery (i.e. SAX-VSM). 
Previous STA implementations were not generic -- they were ``hard-coded'' for specific exploratory studies 
which I discuss in this section. These studies has provided valuable insights into the problem of recurrent 
behaviors discovery and into a number of aspects of the system design and implementation.

\begin{figure}[t]
   \centering
   \includegraphics[width=150mm]{figures/STA12-schema-draft.eps}
   \caption{The Schematic overview of first two STA implementations. 
   Left panel shows a schematic representation of information flow in Hackystat: raw measurements, metadata, 
   and their abstractions seamlessly absorbed by STA, processed, and presented to the user.
   Right panel shows the information flow in a more generic, second STA implementation designed for the 
   analysis of Android OS public repository.}
   \label{fig:STA12-schema}
\end{figure}

\subsection{STA v. 1.0: mining Hackystat software telemetry streams}
The very first Software Trajectory Analysis (i.e. the Pilot) implementation has been designed specifically for the analysis 
of the Hackystat data called ``software telemetry''. Since this data has been collected automatically by so-called 
``sensors'' installed at the developer's system and deployment environments, it is characterized by the high consistency  
that enables unprecedented insight into performed processes, as I have already discussed in \ref{section_software_telemetry}. 
Effectively, by offering efficient data collection, storage, and retrieval mechanisms, and most importantly consistent, 
fine-grained data, Hackystat has provided an ideal testbed for STA feasibility study.

The overview of the pilot Hackystat-based STA implementation targeting the recurrent behaviors discovery is shown 
at the left panel of Figure  \ref{fig:STA12-schema}.
The pilot STA implementation has been based on two analytical techniques: the discretization of time-series with SAX \cite{sax}, 
that effectively translated real-valued telemetry streams into strings, and the occurrence frequency (i.e. support) -based 
discovery of recurrent patterns.

As I have shown in \cite{csdl2-10-09}, this approach demonstrated the feasibility of recurrent behaviors discovery 
through the mining of frequently occurring symbolic patterns, i.e. time series motifs \cite{sax}. 
Consider an example of recurrent behaviors discovery shown at the Figure \ref{fig:STA1-results}, where software 
trajectories built of the development effort measurements shown at the left and their clustering based on Euclidean 
distance between vectors of symbolic patterns occurrence frequencies shown at the right. Clearly, the hierarchical clustering 
process divided the set of trajectories separating two developers (\#2 and \#7) from the rest. 
Further investigation of the data revealed, that these two developers demonstrated the most consistent development 
behavior (when discretized by 4 days window) as they spent considerable amounts of time working on the project almost every 
day whether the rest of the study participants did not. 
Thus, the results of STA analysis were found consistent with the ground truth.

In addition to indicating the feasibility of automated recurrent behaviors discovery through the analysis of measurements, 
the experience with the pilot system highlighted a number of issues.
The chief issue threating the external validity of the study has been the fact that a small scale class-room experimentation 
simply does not provide an adequate coverage of the studied phenomena. 
For example, it is possible that in the discussed above experiment some of the developers which demonstrated inconsistent 
behavior may simply had their ``sensors'' mis-configured or malfunctioning, which is difficult to recognize automatically.
The second significant issue identified by the pilot STA has been the problem of the data mining algorithm parameters 
selection, as these have to be defined as the input but their proper values are difficult to guess.

Note, that the pilot STA also implemented a recurrent behaviors mining workflow based on the application of 
a frequent patterns mining algorithm called Apriori \cite{citeulike:775528} to development event records. 
As I have shown in \cite{citeulike:13159603}, this approach has shown a satisfactory performance. 
However, since development events are impossible (as I shall discuss in section \ref{section_understanding})to recover from 
public software artifacts, this workflow has not been used in the following STA implementations.

\begin{figure}[t]
   \centering
   \includegraphics[width=145mm]{figures/STA1.eps}
   \caption{Results of pilot STA study. 
   The left pane shows eight software trajectories that are Hackystat Development Effort telemetry streams \cite{citeulike:557296} 
   collected in the course of two months.
   The right pane shows a hierarchical clustering of developer behaviors obtained by computing Euclidean distance between vectors
   of recurrent patterns frequency built with SAX discretization \cite{sax}. 
   Note two groups discovered by clustering that correspond to consistent (developers \#2 and \#7) and inconsistent development effort.}
   \label{fig:STA1-results}
\end{figure}


\subsection{STA v. 2.0: experience with Android OS repository}
The second STA implementation has been developed targeting analyses of measurements obtained by measuring artifacts 
from public software repositories.

The decision to use public software repositories in the second exploratory study has been made in order to increase its 
significance by addressing all of the proposed by Gasser et al. \cite{citeulike:13058334} essential characteristics for 
empirical studies based on mining of software artifacts:  
(1) they must reflect a real-life phenomena, 
(2) provide adequate phenomena's coverage, 
(3) examine representative levels of variance, 
(4) demonstrate an adequate level of statistical significance,
(5) provide results that are comparable across projects,
(6) be reproducible. 

Unfortunately, due to the much coarser granularity and inconsistency of measurements collected from public artifacts, 
an issue that I discuss further in this Chapter, the original approach to the data analysis based on the observed patterns 
frequency failed, and the additional exploratory study of time series mining techniques has been conducted using 
2012 MSR challenge data \cite{MSRChallenge2012}.
By experimenting with a number of time series discretization and aggregation techniques and with various distance 
functions and ranking schema, I found, that the common to the Information Retrieval (IR) research field toolkit called 
Vector Space Model (VSM) \cite{citeulike:300428} that is based on \tfidf weighting schema and Cosine similarity, 
demonstrated a satisfactory performance. 
As I have shown in \cite{csdl2-11-10}, STA based on the discretization with SAX \cite{sax} and mining with VSM 
\cite{citeulike:300428}, has been found capable to discover characteristic behaviors in pre- and post- release software 
trajectories constructed of the New Lines of Code change record measurements.

In addition, to combat the lack of Android software repositories internal and external connectivity and the heterogeneity 
of data formats -- also the common issues in the MSR field -- in the second STA implementation I had followed the state of 
the art MSR approaches for data integration \cite{citeulike:13058334} \cite{cvsanaly}. 
In particular, similarly to a previously developed solution called softChange \cite{german04_softchange}, second STA mirrors 
repositories and builds its own data storage facility by using the relational database engine as it is shown at 
the Figure \ref{fig:sta-assimilation}.

While the details of data processing and recurrent behaviors discovery performed within the second exploratory study will be 
discussed later in the Chapter \ref{chapter_results}, consider an example shown at the Figure \ref{fig:STA2-results} 
for two classes of software trajectories that reflect pre- and post- release dynamics in counts of New Lines of Code in 
the Android OS kernel repository. 
The left panel of the figure shows that it is possible to cluster characteristic behaviors corresponding to different time intervals 
where pre- and post- release behaviors are clearly separated. 
The right panel shows that by using pre- and post- release clusters centroids it is also possible to classify other time-intervals, 
which validates the discovered recurrent patterns characteristic capacity and the overall correctness of the approach.

Similarly to the pilot implementation, the experience with second STA implementation highlighted the same problem of 
parameters selection which also become more significant since the proposed methodology was found sensitive to improper parameters 
selection. Note that later, in order to address this issue, I have explored the possibility to employ a parameters optimization 
scheme and implemented a DIRECT-based approach \cite{citeulike:12563460} that aids in SAX-VSM parameters selection as 
I shall discuss in the next Chapter.

\begin{figure}[t]
   \centering
   \includegraphics[width=145mm]{figures/STA2-draft.eps}
   \caption{An example of discovery of recurrent patterns in software trajectories constructed by measuring Android OS 
   repository source code change artifacts.
   The left panel shows the hierarchical clustering of pre- and post-release temporal intervals-corresponding software 
   trajectories based on the Cosine similarity applied to ranked vectors of discovered characteristic patterns.
   The right panel shows results of a cross-validation experiment where other pre- and post-release software trajectories 
   were classified by computing their NN similarity with previously discovered patterns.}
   \label{fig:STA2-results}
\end{figure}

\section{Mining Software Repositories}
As mentioned before, mining software repositories is a well established research direction since 
mid-1970's, when Meir Lehman pioneered the software evolution theory by studying historical records from 
software repositories \cite{citeulike:2739216}. 
For the last decade, researchers working in this field discuss their approaches and findings in a number of venues. 
Among these, are the Predictive Model in Software Engineering (PROMISE) workshop and the Working Conference on Mining 
Software Repositories (MSR) which are held within the annual International Conference on Software Engineering (ICSE).
In order to enable the comparison of proposed techniques performance, the both venues encourage researchers to 
apply them to reference datasets. 
While PROMISE maintains the same reference dataset over years \cite{promise12}, 
MSR offers a so-called MSR challenge dataset annually \cite{MSRChallenge2012} \cite{MSRChallenge2013}.
Note however, that the {PROMISE} research is mainly concerned with development of predictive models for 
Software Engineering \cite{Menzies13}, whereas MSR traditionally uses public software repositories stimulating 
the diversification of possible research directions \cite{citeulike:12550438} \cite{citeulike:2710928} \cite{citeulike:7853299}.

\section{Understanding Public Software Repositories}\label{section_understanding}
Traditionally, software repositories contain a variety of artifacts produced during the software life-cycle
and can be categorized by their purpose.
Previous research assigns software repositories into three main categories: source-control systems, 
defect-tracking systems, and archived communications \cite{citeulike:4534888}, but other types of repositories exist. 
These may contain various information, such as software system runtime logs, system testing logs, 
historical measurements, documentation, tutorial, and many other.
Recently, a novel type of repositories was proposed for MSR studies -- a historical information collected within the 
community-based question answering service Stack Overflow \cite{MSRChallenge2013}.

As pointed in previous review studies \cite{citeulike:12550438} \cite{citeulike:7853299} \cite{citeulike:7465518} there
is a number of issues associated with mining of \textit{public} repositories which not only create technical difficulties for 
the scientific research, but also affect its validity. 
The chief problem is that public project repositories are highly heterogeneous - each is managed and operated 
mostly in isolation serving a particular project and community needs, therefore having no explicit interactions with 
other projects. 
Moreover, within a project's repository, its SCM subsystems such as version control, defect-tracking, and mailing list, 
are rarely ``connected'  \cite{citeulike:13058334}. 
This issue of heterogeneity directly affects MSR studies generality since tools working for and results obtained from 
one repository, are rarely applicable to another.
Yet another issue is that while the public availability of software artifacts mitigates observability and privacy issues, 
the nature of these artifacts creates a number of other challenges, which limit the possible scope of the scientific 
research and significantly elevate its complexity. Among others, four issues are usually cited as the most significant:
\begin{itemize}
 \item First of all, the artifacts are created by developers and users not in order to enable the scientific research,
but merely to support software development activities. Therefore, the informational content of these artifacts is rather 
poor and additional evidence (i.e. metadata) is often needed \cite{citeulike:342840} \cite{citeulike:7954249} 
\cite{citeulike:7260421}.
 \item Secondly, the majority of these artifacts (change records, defect reports, assigned tasks, etc.) 
typically represent a snapshot of the software project state rather than reflect any of the performed actions.
Thus, it might be simply impossible to infer complete software development processes \cite{citeulike:1296888}.
Also, this fact effectively renders unusable (within the MSR domain) most if not all of previously developed 
event-based process and behavior discovery tools.
 \item Thirdly, the project's contributors not only create and submit to repositories artifacts on their own volition,
but most of the change management system (such as Git, Subversion, and Gerrit) encourage the asynchronous workflow 
where the locally created artifacts might never been committed and therefore remain unaccounted for 
\cite{citeulike:2280690} \cite{citeulike:9037939}. 
For the same reasons, it is often impossible to know exactly when the artifact's content was created.
 \item Finally, the vast volume of produced artifacts, their high dimensionality, and significant noise demand 
 for automated, high throughput and robust analysis techniques 
 \cite{citeulike:12550438}, \cite{citeulike:7853299}, \cite{citeulike:4534888}.
\end{itemize}

These data-related issues associated with mining of public software repositories not only create significant external 
threats to MSR research validity, but often impossible to resolve without altering the normal flow of OSS software 
process, for example by implementing a special measurement program, or by introducing instrumented source code 
editors and tools (as in Hackystat). 
Typically, MSR researchers deal with these issues by finding additional evidence in order to support their conclusions 
\cite{citeulike:5043664} \cite{citeulike:5128808}.

\subsection{Public software artifacts}
Public software repositories offer a wide range of software process and product artifacts for analyses.
Among others, these include source code change records, defect reports, feature requests, accepted, 
rejected and assigned tasks, developer communications, documentation, tutorials, etc. 
All these allow developers and users to instantly obtain a ``snapshot'' of the project, i.e. to retrieve the 
latest (or any previous) source code revision and a complete overview of the software project state 
along with the lists of open and closed issues, past and future plans, etc.

However, while being exceptionally convenient for the project participants, users, and management, this 
snapshot-oriented nature of public software artifacts creates numerous difficulties for the software process research 
as a ``snapshot'' rarely reflects finished, ongoing, or planned processes -- the issue that compromises the quality of 
performed studies as I have mentioned above. 

I acknowledge this software process observability problem when working with public software process artifacts and intentionally 
avoid discussing and concluding on software processes. Instead, what I shall focus on in this dissertation, is the 
validation of the proposed technique's ability to capture process-characteristic recurrent behaviors when snapshots 
are viewed in their dynamics. 

Nevertheless, I hypothesize, that the evolution of software measurements in time reflects recurrent development behaviors 
and that at least some of them are in fact characteristic to certain aspects of software processes. 
Therefore, by discovering recurrent patterns in the evolution of software measurements it shall be possible to at least 
partially infer and evaluate performed software development actions or processes.

Further in this section I review a number of common public software repositories and their artifacts to whose measurements 
STA already has been or potentially can be applied. 

\subsubsection{Source code management system}
Source code management system keeps track of the main output of a software project -- its source code, which is also the 
main subject of the scientific research. Metrics derived from the source code artifacts are predominant in studies concerned 
with software evolution, complexity, maintainability, and quality, as well as those that are concerned with productivity, 
project planning, and cost estimation (i.e. management) \cite{citeulike:4534888}. 

Typically, the evolution of source code is recorded as a sequence of consecutive change records, which are simple artifacts
tracking the change of each source code line. Despite to the artifact's simplicity, tracing the source code evolution through 
the analysis of change records can become increasingly difficult as developers branch the source code tree, merge it back, 
or abandon branches \cite{citeulike:13156191}.

While a large number of metrics can be derived through source code and change records analyses, it offers probably 
the most functional one -- the count of physical lines of code (LOC). Other source code metrics, such as the count of logical lines 
of code (LLOC), function points (FP), or software system complexity are much less used as they are language-dependent and their 
derivation involves significant data processing overhead.

\subsubsection{Defect tracking system}
Normally, the software project defect repository serves as a centralized system for managing all of software project 
Quality Assurance (QA) activities providing users and developers with means to report and to discuss an improper system behavior.
In some projects, the defect repository is also used to keep a track of requests for future system features and related
discussions.

Artifacts from defect repositories are numerous and complex, as they may contain system logs, input and output files, 
screen-shots etc. Their main purpose is to provide users with up to date information about system defects, their severity, 
and, if implemented in the system, with additional information about their technical nature, resolution plans, etc.

By studying defect records, researchers has shown that it is possible to build predictive models for future bugs by their association 
with source code change patterns \cite{citeulike:6055293} (i.e. activity) and with particular code fragments \cite{citeulike:393158}. 
In addition, it has been shown that it is possible to optimize software testing processes by identifying source code 
``hot spots'' through mining of the bug reports history \cite{ostrand2004tool}.

\subsubsection{Developer communications}
As OSS projects usually developed by distributed teams that typically lack the ability for face-to-face meetings, 
emails, mailing lists, and newsgroups are used as primary communication channels between project participants. 

Developer communications artifacts, such as email messages, mailing list posts, and newsgroup messages include 
agent identification, timestamps, topics, and other data, that provide information allowing for not only process agents 
identification, but understanding of their actions and related to the process coordination activities. 


For example Ying et al. in \cite{citeulike:1366052} proposed an interesting novel research direction of mining developer 
communications content for understanding of the software quality, while Huang et al. in \cite{citeulike:9495129} used developer 
communications to build a developer interaction network and to partition developers by levels of their involvement into the 
project and by technical expertize.

\subsubsection{Q\&A websites}
Frequently, professional software developers, amateur programmers, and computer hobbyists seek answers to various questions 
using the Internet. 
Among others resources, the Internet offers com\-munity-driven platforms, such as Stack Overflow (SO) website that explicitly 
targets programmers and is dedicated to software-, hardware-, and computer system administration-related issues.

While other types of artifacts available, the ones distributed by SO team are probably the most used in the MSR research. 
These are distributed monthly and contain the historical information about questions and answers along with their change 
history including voting data. In addition, SO team provides rich metadata about their service contributors. 
The public Stack Overflow dump was selected as the reference dataset in recent 2013 MSR Challenge \cite{MSRChallenge2013} 
which collected a number of submissions proposing interesting data analysis approaches.

While many of these are concerned with programming-related questions, such as identifying topics relevant to particular 
development communities \cite{kartik:msr14}, mining additional technical expertise \cite{VenkataramaniGAMB13} \cite{SaxeMG13}, 
or identifying problematic APIs \cite{KavalerPGCDF13} \cite{Linares2013Exploratory} and documentation \cite{Campbell2013Deficient},
some studies address broad phenomena such as collaborative problem solving \cite{Tausczik2014Collaborative},
knowledge sharing \cite{VasilescuCSCW14} \cite{Schenk2013Geo}, and contributor behaviors \cite{Bosu2013Building} \cite{GinscaP13}.

\subsubsection{Metadata}
Often, as reported by Begel et al. \cite{citeulike:7260421} who conducted a survey at Microsoft, in order to understand 
performed software processes, the quantitative information about source code change is not sufficient. 
Through the survey, the authors accounted for thirty one types of informational needs necessary for understanding and 
coordinating software processes, among which, the need for the software change metadata was clearly articulated. 
Among other reasons, it was found, that metadata allows developers to learn the rationale behind software change,
find responsible people, discover and track dependencies, and to learn about the status of items in progress. 
The authors concluded, that the majority of developer needs were concerned with people, not the code, and that the 
metadata is essential in meeting these requests.

Similarly, Kim et al. \cite{citeulike:4000311} who proposed a system for software repositories data collection, storage, 
and a universal data-exchange language, emphasized the metadata importance for the information management.
In addition, the authors has shown that it is possible to create a public, metadata-centric system for interfacing 
close-source software repositories to public.

Based on these and other results, I have designed STA DB in extensible metadata-centric manner. 
New types of metadata can be defined by the user and associated with existing and newly collected artifact entities 
and measurements. In turn, within the process of software trajectories definitions, the metadata allows for efficient 
data partitioning and retrieval.

\section{Data assimilation}
Currently, there is a voluminous amount of the research literature that deals with mining of public software repositories 
\cite{citeulike:2710928} which, in fact, extends probably even larger body of research work that covers studies based on mining 
private software repositories and databases \cite{citeulike:393158} \cite{citeulike:13125375} \cite{citeulike:13125481}.

While the majority of published work is concerned with analyses of a repository information 
for better understanding of software systems evolution \cite{citeulike:277045} \cite{citeulike:4000311}, 
understanding and improving software processes \cite{citeulike:5803126}, 
and with studying the impact of software tools on the processes and products \cite{citeulike:13125389},
some effort has been made towards automation of the historical data retrieval, measurements, and its representation. 
A number of the proposed solutions allows for the real-time interactive repositories exploration implemented by extending 
repository management tools such as CVS, SVN, etc. with a front-end engine, such as Bonsai \cite{bonsai},
or JReflex \cite{citeulike:3017440}, while others, such as CVSAnalY \cite{citeulike:6544724}, 
softChange \cite{citeulike:13125395}, and {TA}-{RE} \cite{citeulike:4000311} propose an approach based on 
the off-line artifacts retrieval, pre-processing, and on-demand analysis.

\begin{figure}[t]
   \centering
   \includegraphics[width=115mm]{figures/Flow.eps}
   \caption{Detailed overview of Software Trajectory Analysis data assimilation layer. 
    At first, software artifacts are collected from software repositories, measured, converted into 
    universal to STA format, and stored in a dedicated database.
    If Hackystat used as a data source, typically, no additional processing required and data can be used in the real-time.
    Stored in STA DB software artifact entities can be further enhanced by additional measurements and with metadata.
    Finally, the database allows for efficient data partitioning and its aggregation into software trajectories.}
   \label{fig:sta-assimilation}
\end{figure}

Similarly to the latter, STA relies on the off-line retrieval, mirroring, and pre-processing of public software artifacts as
shown at the Figure \ref{fig:sta-assimilation}. Note, that since STA has been initially designed as a Hackystat extension 
\cite{csdl2-10-09}, it does not need any specific parser and is capable of real-time collection of Hackystat data.

\section{Relevant MSR research on recurrent behaviors discovery}
As I have shown above, MSR is a very diverse research field concerned with a variety of problems. 
%Among others, researchers use information extracted from repositories to address research questions related to software system growth, 
%its understanding, software quality prediction, refactoring and change patterns, measuring individuals expertize and contribution,
%understanding development teams social structure, and with software processes understanding.
But in this section I focus on the previous MSR work that is specifically concerned with the application of analytical 
techniques to sequences of software artifact measurements -- the approach that STA builds upon.

\subsection{Itemset mining}
In data mining, frequently occurring items (actions, events) are often used in order to discover implicit knowledge from
large datasets. As I have mentioned earlier in Section \ref{section_software_process_design}, techniques based on frequent
items mining were previously applied for software process discovery from development event logs 
by Cook and Wolf \cite{citeulike:328044} \cite{citeulike:5120757} \cite{citeulike:5128143} 
and by Rubin et al. \cite{citeulike:1885717}. Unfortunately, since public software repositories do not offer 
development event logs, these techniques can not be adopted for mining software repositories in their proposed form.

Nevertheless, sequential item mining has found a number of applications in the MSR research.
For example Zimmermann et al. in \cite{citeulike:277045} developed a system called ROSE for the identification of 
co-occurring changes in a software system that aids in future change prediction. 
For the same purpose, Kagdi et al. \cite{citeulike:3929070} developed a sequential pattern mining technique capable of 
discovery of ordered sequences of frequently changed files. 
Livshits and Zimmermann \cite{citeulike:393158} developed DynaMine -- the system for bug prediction based on mining 
of frequent function call patterns.

Potentially, these techniques can be applied to STA results. For example it may be possible to discover ordered, or
unordered sequences of recurrent (frequent) behaviors which can be further associated with particular development actions.

\subsection{Time series analysis}\label{chapter2_section-tsanalysis}
Because the majority of software artifacts are time stamped, some MSR research seeks to quantitatively analyze ordered 
in time sequences of software artifacts or their measurements as these may carry useful information about software 
processes and recurrent behaviors. 

\begin{figure}[t!]
   \centering
   \includegraphics[width=145mm]{figures/FourrierMySQL.eps}
   \caption{}
   \label{fig:mysql-fourrier}
\end{figure}

For example Herraiz et al. \cite{citeulike:6544685} applied Autoregressive Integrated Moving Average (ARIMA) model 
to software evolution measurements for prediction of future changes. The authors has shown that it is possible to 
predict a number of future changes in Eclipse by means of the resulting non-explanatory statistical model. 

Similarly, Antoniol et al. \cite{citeulike:3378725} have explored the application of a common signal processing toolkit 
built upon Linear Predictive Coding (LPC) and Cepstrum coefficients to modeling of software artifact histories. 
In particular, the authors have shown that it is possible to identify files with very similar size change histories 
by using the proposed approach.

The temporal segmentation of time series has been applied to mining of Eclipse change log by Siy et al. \cite{citeulike:10896305}.
The authors have demonstrated that by partitioning of continuous development activities into the smaller segments whose duration 
is close to the software release cycle, it is possible to discover ``stronger trends'' (i.e. characteristic behaviors). 
For example they have found that developers tend to focus on a particular file subset within a release cycle duration. 
In addition, they were able to detect similar change activity patterns among developers.

Finally, Hindle et al. in \cite{citeulike:10377345} outlined an approach for discovery of recurrent behaviors from software 
measurements by Fourier analysis. 
The left panel of the Figure \ref{fig:mysql-fourrier} from their work indicates that the studied signal carries potentially 
distinguishable periodic behaviors, moreover, they were able to detect a promising smear of frequencies between 18 and 19 
[days] as it is shown at the right panel. Unfortunately this direction was not further investigated.

\epigraph{Without the right information, you are just another person with an opinion.}{Tracy O'Rourke, CEO of Allen-Bradley}