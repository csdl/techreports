\chapter{Introduction}

\textit{The research presented in this thesis concerns about recurrent behaviors discovery from
publicly available software process artifacts, and rests on knowledge discovery from
temporal data. Both topics are defined in sections \ref{section_background} - \ref{knowledge_discovery}. 
Section \ref{knowledge_discovery} introduces the research hypothesis.
Section \ref{section_contributions} enumerates main contributions of this thesis,
and Section \ref{section_organization} provides the thesis organization.
}

\section{Background.}\label{section_background}
Contemporary software projects typically have a considerably long life-cycle - well over decade.
Their development and maintenance usually carried out by geographically distributed teams 
and individuals. The development pace, the experience, and the structure of these development 
teams continuously change as developers are joining and leaving. When combined with
schedule and requirements changes, all these create numerous difficulties 
for developers, stakeholders, and community, ultimately affecting the project success. 

This software development complexity phenomena was identified in 1968 as ``Software crisis'' \cite{crisis}, 
and was addressed by bringing the research and the practice of software development 
(or as it was called ``programming'') under the umbrella of Engineering - in an effort to provide a 
control over the process of software development. 
Following the engineering paradigm, numerous methodologies and models of 
software design process, known as \textit{software processes} were proposed \cite{citeulike:10002165}.

A \textbf{\textit{software process}} is a set of activities performed in order to design, develop and maintain 
software systems. Examples of such activities include requirements collection and creation of UML diagrams;
requirements testing; code development, testing, and performance analysis. 
The intent behind a software process is to structure and coordinate human activities in order to achieve 
the goal - deliver a software system successfully in time and under a budget.

Since inclusion into Engineering, much work has been performed in software process research 
resulting in a number of industrial standards for software development processes models which were 
widely accepted in industry (CMM, ISO, PSP etc. \cite{citeulike:5043104}). 

In spite of this effort, industrial software development remains error-prone and
more than half of all commercial software
development projects ending up failing or being very poorly executed (Rubinstein, ``Chaos Reports'', 2006). 
Some of them are abandoned due to running over budget, some are delivered with such low quality or so late that
they are useless, and some, when delivered, are never used because they do not fulfill requirements. 

Through the analysis of this experience, it was understood, that software engineering is
very different from any other field of Engineering. There is almost no cost associated with materials 
and fabrication which dominate cost in all other engineering disciplines, but, ironically, 
software engineering is suffering from the costs and challenges associated with continuous 
re-design of the product and its design processes - the issue which is rarely seen at all 
in any of other engineering areas \cite{citeulike:5203446}. Addressing this issue, a number of 
alternative software processes emerged from academy, hobbyists, and practitioners. 
Among others, the Free/Libre/Open-Source Software model (FLOSS) and the software craftsmanship  
approaches gained a significant credibility. While the former \textit{holistic} software process paradigm emphasizes
loosely-organized collaboration, frequent releases, and removes the boundary between developers and customers, 
the latter is focusing on the roles of highly motivated, skilled individuals in a process of software creation
\cite{citeulike:262020} \cite{citeulike:2759198}. 

Nevertheless, alternative processes are plagued by the same complexity issues. 
As it was shown, most of FLOSS projects 
never reach a ``magic'' 1.0 version \cite{citeulike:12480029}. Among others, the great "infant mortality rate" of OSS
projects was related to a burnout, inability to acquire a critical mass of users, loss of leading developer(s), and
forking \cite{richter2007critique}. Software craftsmanship, from other hands, not only challenge developers
with technological advances requiring continuous education, but create cost and effort estimation difficulties for
stakeholders and project managers \cite{citeulike:11058784}.

%The recent study by Standish Group (Rubinstein, ``Chaos Reports'', 2006) indicates,
%that while `\textit{`Software development shops are doing a better job creating software than they were 
%12 years ago}'', still, only ``\textit{35\% of software projects in 2006 can be categorized 
%as successful meaning they were completed on time, on budget and met user requirements}‚Äù.
%Another, widely acknowledged problem with existing models is their rigidity - usually 
%once a project executed, the cost of incorporating a change in the process or a product 
%not only becomes significant, but also grows proportionally to the project execution time.

Currently, it is widely acknowledged, that there exists no single ``silver bullet'' process which can bring any 
software development project to success \cite{citeulike:1986013}. Processes are numerous, each has advantages 
and drawbacks, and each is accompanied with numerous application recommendations, success stories, and, 
sometimes, with failure experiences. 
The alarming rate of failing projects suggests, that currently available knowledge is not enough for
making a proper process choice in a particular situation \cite{citeulike:12550665}. 
The enormous cost of the lost effort, and the lack of understanding of software process ``mechanics'' 
continue to provide motivation for further research. 

\section{Software process design}\label{section_software_process}
Historically, there are two categories of approaches to software process design, analysis, and improvement. 
The first category consists of traditional to engineering \textit{top-down} approaches through \textit{proposing a
process based on specific patterns of software development}. 
For example, the Waterfall Model process proposes a sequential pattern in which developers first create a 
Requirements document, then create a Design, then create an Implementation, and finally develop Tests. 
The Test Driven Development process, from other hands, proposes an iterative behavioral pattern in which
the developer must first write a test case, then write the code to implement that test case, then refactor the 
system for maximum clarity and minimal code duplication \cite{citeulike:6086365}. 

While the top-down approach seems to be a natural extension of natural to humans creative processes - 
such as invention
and experimentation, and follows the usual path of trial and error, one of the problems is that ``inventing''
the adequate 
to the task software process is far from trivial \cite{citeulike:5043104} \cite{citeulike:1986013}. 
Moreover, it was shown, that the process inventors are often limited in their scope and tend to assume an idealized
versions of real processes, thus, likely to produce ``paper lions'' - process models which are likely to be
disruptive and unacceptable for end users, at least in their proposed form \cite{citeulike:9758924}. 
Finally, the evaluation cycle of an invented process is long and expensive. 

The second category consists of opposite, \textit{bottom-up} techniques for true \textit{process reconstruction 
through noticing of recurrent development event patterns of behaviors}. 
Within this category, the process inference problem is viewed as a two-levels process, 
where the first level consists of a patterns discovery (segmentation), and the 
second level consists of pattern recognition and a network analysis \cite{citeulike:2703162}.
One of the first works following this paradigm is by Cook and Wolf, where they show a
possibility of automated extraction of a process model through mining of recorded event logs
\cite{citeulike:328044} \cite{citeulike:5120757} \cite{citeulike:5128143}. 
Later work by Huo et al., shows that through event logs analyses it is also possible to 
improve existing processes \cite{citeulike:7691059} \cite{citeulike:7690766}. 

While the bottom-up approaches seem to be more systematic and less complex than invention, they 
also affected by a number of issues. A chief among these is the observability issue - 
it is usually very difficult to conduct a full depth study on a live project due to the privacy concerns. 
Moreover, it is expensive to observe a process performed by a team for a whole life-cycle of a project. 
Yet another issue is the capacity of the process discovery techniques - these often need to be supervised 
and fine tuned in order to reconstruct distributed and concurrent processes. 

\section{Public software repositories}\label{section_public_repositories}
Recently, however, the outlined above situation changed, and the interest for bottom-up process 
reconstruction has been revived. 
This change is driven by increase in public data that are made available by the proliferation of 
open source communities.
Currently, software artifacts are abundant: source code repositories, public bug/issue tracking systems, 
mailing list communications, social networks, Q\&A websites - all these are increasingly popular among
the software developers and easily accessible by community. 
This data availability effectively removes not only the high cost of observation, but most of the privacy 
concerns - both issues that previously made large-scale analysis of software projects unfeasible for 
most researchers.

Scientific community response on the availability of public artifacts was overwhelming, and a number of 
venues was established addressing increased interest. 
Since 2004, the International Conference on Software Engineering (ICSE) hosts a Working Conference on 
Mining Software Repositories (MSR). The original call for papers stated MSR's purpose as 
\textit{``... to use the data stored in these software repositories to further understanding of software 
development practices ... [and enable repositories to be] used by researchers to gain empirically based 
understanding of software development, and by software practitioners to predict and plan various aspects 
of their project''} \cite{msr2004} \cite{citeulike:7853299}. 
Several other venues: International Conference on Predictive Models in Software Engineering \cite{promise12}, 
International Conference on Open Source Systems, the Workshop on Public Data about Software Development, 
and the International Workshop on Emerging Trends in FLOSS Research have also played
an important role in shaping and advancing this research domain.

Some of the published work addresses the software process discovery. Among others, most notable, and 
relevant to my research is work by Jensen \& Scacchi. In their early work, they demonstrated, that 
information reflecting software processes can be gathered from public systems \cite{citeulike:12550640}. 
Later, in \cite{citeulike:5043664} and \cite{citeulike:5128808}, they show, that by manual mapping of 
collected process evidence to a pre-defined process meta-model it is possible to reconstruct some 
of the FLOSS processes. 
Another closely related to my research is work by Hindle et al. where they has shown that it is possible to 
discover software process evidence through partitioning \cite{citeulike:10377366}.

However, in these and other research work based on mining of publicly available software process artifacts 
it was shown, that while availability of artifacts partially solves problems of observability and privacy, 
at the same time, the nature of these artifacts creates a number of additional challenges, which significantly
elevate the complexity of process discovery problem:
\begin{itemize}
\item First of all, the artifacts created by developers and users not in order to enable the research,
but merely to support software development activities. Thus, the process-related informational content of these
artifacts is questionable.
\item Secondly, majority of these artifacts (change records, defect reports, assigned tasks, etc) represent the current 
state - a snapshot - of a software project state, rather than reflect an action, and it is simply impossible to
infer any of low-level software development events \cite{citeulike:1296888}.
This effectively render obsolete a majority of previously developed event-based process discovery tools.
\item Thirdly, developers and users not only create and submit to repositories artifacts on their own volition,
but most of the change management system (such as Git, Subversion, and Gerrit) offer an asynchronous workflow, 
where the locally created artifacts might never be committed \cite{citeulike:2280690} \cite{citeulike:9037939}. In other
words, artifacts are displaced in time and it is often impossible to know exactly when their content was created.
 \item Finally, the high volume of artifacts demands for automated, high throughput techniques robust to noise
 \cite{citeulike:12550438}, \cite{citeulike:7853299}, \cite{citeulike:4534888}.
\end{itemize}
Overall, it was well pronounced, that novel analysis and discovery techniques are needed for public software process
artifacts when ``\textit{... Going Beyond Code and Bugs...}'' \cite{citeulike:7853299}.

In this thesis, I advance current state of public software process artifacts analysis by showing a novel temporal data
mining technique capable to discover knowledge about performed processes.

\section{Knowledge discovery from time series, research hypothesis}\label{knowledge_discovery}
In data mining, time series are used as a proxy representing a vast variety of real-life phenomena 
in wide range of fields including, but not limited to physics, medicine, meteorology, 
music, motion capture, image recognition, signal processing, and text mining. 
While time series usually directly represent observed phenomenas by correlating data with time, the pseudo 
time series often used for representation of various data by combining data points into ordered sequences. 
For example in spectrography data values are ordered by component wavelengths \cite{citeulike:12550833};
in shape analysis the order is the clockwise walk direction starting from a
specific point in the outline \cite{citeulike:12550835}, in image classification the numbers of pixels
are sorted by color component values \cite{citeulike:2900542}.

Many important problems of knowledge discovery from time series reduce to the core task of finding 
characteristic, likely to be repeated, sub-sequences in a longer time series. 
In the early work these were called as 
\textit{frequent patterns} \cite{citeulike:5159615}, 
\textit{approximate periodic patterns} \cite{citeulike:1959582},
\textit{primitive shapes} \cite{citeulike:5898869}, 
\textit{class prototypes} \cite{citeulike:4406444}, 
or \textit{understandable patterns} \cite{citeulike:3978076}. 
Later, similarly to Bioinformatics, these were unified under the term \textit{motif} \cite{citeulike:3977965}.
Once found, these motifs can be used as an exploratory tool for hypothesis generation through finding 
associations with known, or unknown phenomena \cite{citeulike:3977965}.

Recent advances in semi-supervised and unsupervised finding of such characteristic sub-sequences, 
in particular work based on \textit{shapelets} \cite{citeulike:7344347} \cite{citeulike:11957982}
\cite{citeulike:12552293}, combined with previous work by Johnson et al. \cite{citeulike:12550871}, 
where they used knowledge extracted by experts through visual analysis of software metrics 
telemetry streams in order to improve software development management, 
prompted a \textbf{\textit{research hypothesis}}, that \textit{it might be possible to discover 
knowledge about performed software processes from publicly available software process artifacts in 
\textbf{automated, unsupervised fashion}.}

\section{Software trajectory analysis}\label{section_trajectory_definition}
In this thesis, I will test stated above hypothesis through an exploratory study. In order to proceed, 
I introduce \textit{\textbf{software trajectory}} - a time-series based representation of a yet another 
phenomena - a software development process. 

In a number of scientific disciplines, the term \textit{trajectory} usually represents 
an ordered progression of snapshots, or reduced in complexity sequence of states of a dynamic 
system (such as Poincare' maps). 
Thus, it describes a data type, which is somewhat different from time series, that are usually precise
measurements of known origin, obtained through direct observation; 
or mentioned above telemetry streams, that are precise measurements obtained by remote 
sensing. Since not all of the software development process metrics can be extracted from publicly 
available software artifacts \ref{section_public_repositories}, and the true state of a software system 
cannot be assessed exactly, the term \textit{software trajectory} exactly describes
\textbf{\textit{an ordered 
sequence of measurements extracted from software process artifacts}}.

In later chapters of this thesis I will show the novel, interpretable time series classification algorithm, which,
when applied to two or more sets of software trajectories, is capable to discover set-characteristic subsequences.
Further, through case studies, I will show, that by selecting specific sets of trajectories (a ``supervision''),
it is possible to associate these characteristic patterns with recurrent behaviors, which are the building blocks of 
software processes. For example, in PostgreSQL case study I will show that characteristic patterns discovered 
in Churn metric trajectories are related to the specific software process - PostgreSQL' CommitFest.

% Some of the previous research, especially in MSR field  \cite{citeulike:9114115, citeulike:7853299}, 
% indicates, that by application of a variety of a techniques it is not only possible to discover evidence of 
% software process \cite{citeulike:9007622}, but, at least partially, to infer the process as a 
% whole \cite{citeulike:5128808}. However, in the mentioned research, software process artifact features 
% were mostly captured by using of an expert knowledge of measurable discriminative properties of the 
% event classes, which effectively limits analyses to The feature selection
% process entails manual expert involvement and repeated experiments. Automatic feature
% selection is necessary when (i) expert knowledge is unavailable, (ii) distinguishing features
% among classes cannot be quantified, or (iii) when a fixed length feature description cannot
% faithfully reflect all possible variations of the classes as in the case of sequential patterns
% (e.g. time series data).


\section{Contributions}\label{section_contributions}
Main contributions of my work can be summarized as follows: 
\begin{itemize}
\item I propose a novel, generic algorithm for interpretable time series classification: SAX-VSM. 
While the classification performance of this algorithm is at the level of current state of the art, 
it offers an outstanding feature - discovery, generalization, and ranking of class-characteristic features. 
This, in turn, enables knowledge discovery by offering much clearer insight into classification results than any of 
competing techniques.
In addition, SAX-VSM is very fast in classification and has a small memory footprint. 
Overall, I expect this algorithm to play an important role in future because of the growing ubiquity of time series and 
a growing interest in behaviors.
\item Powered by SAX-VSM, I show a design Software Trajectory Analysis (STA) framework, 
and through case-studies show its capacity for recurrent behaviors discovery from publicly available software process
artifacts. While case studies are 
obviously limited, I argue that STA is a useful knowledge discovery tool applicable for a variety of software process 
artifacts and metrics. 
\item Finally, I provide SAX-VSM and STA implementations to community.
\end{itemize}

\section{Outline}\label{section_organization}