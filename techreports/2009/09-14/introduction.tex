\chapter{Introduction}\label{chapter_introduction}
\textit{A central issue addressed in this dissertation is the possibility of recurrent 
behaviors discovery from publicly available software process artifacts. 
I have explored an approach to this problem based on the transformation of artifact trails into 
time-series by their measurements and the application of a proposed time-series classification 
technique that enables finding of characteristic patterns. 
The dissertation presents results of this exploratory study and consisting of two parts: 
the algorithm proposal and results of its performance evaluation, and the results of a 
use case-based assessment of the algorithm's applicability to the problem of recurrent behaviors 
discovery from public software artifacts.}

\textit{The terms used throughout the thesis are defined in the Section \ref{sec_terminology}. 
Problem's background is given in Section \ref{sec_background}. 
Section \ref{section_software_process_design} discusses traditional approaches to software process 
design and contrasts them with open-source software processes design.
Sections \ref{sec_software_trajectory} reviews a previously developed technique to software process 
and product improvement based on continuous measurements and introduces software trajectory.
Section \ref{sec_research_hypothesis} presents the research hypothesis.
Section \ref{sec_knowledge_discovery} provides introduction into the problem of knowledge discovery 
from time-series.
Section \ref{section_contributions} enumerates main contributions of the thesis, 
while section \ref{section_organization} explains the thesis organization.}

\section{Basic terminology}\label{sec_terminology}
\begin{defn}\label{def_process}
A \textbf{\textit{Software Process}} defines a way the software development goes. It enumerates
resources and artifacts, but most importantly, it defines a set of activities that need to be are performed 
in order to design, to develop, and to maintain software systems.
\end{defn}
Examples of such activities include requirements collection and creation of UML diagrams, 
source code writing, system testing, and others. The intent behind a software process is to provide 
the control over software development effort by implementing a global strategy and by structuring
and coordinating human activities in order to achieve the \textit{goal} - to deliver a functional
software system on time and under the budget. 

\begin{defn}\label{def_process_desc}
A \textbf{\textit{Process Description}} is any sort of written software process description defining 
some or all of needed resources, artifacts, actions, activities and intended process behavior.
\end{defn}

\begin{defn}\label{def_process_model}
A \textbf{\textit{Process Model}} is a complete, unambiguous process description that guarantees 
a rigorous specification ready to be executed.
\end{defn}

\begin{defn}\label{def_metric}
A \textbf{\textit{Software Metric}} is a characteristic of a software system or a software process that can be 
objectively measured.
\end{defn}
Similarly to other sciences, measurements in Software Engineering are essential for establishing of systematic 
research. Examples of quantitative measurements of a software include the size of a software system measured 
in lines of code (LOC) or in functional points (FP), and the number of defects discovered in a delivered system. 
Examples of software process metrics include the velocity of a software process called ``churn'' that 
measures the amount of LOC changed per day, the response time to fix an issue, and the ``technical debt'', 
that measures deterioration of the code quality over time. 
Product and process metrics also used in order to derive high-level software project metrics including cost, 
schedule, and productivity.

\begin{defn}\label{def_artifact}
A \textbf{\textit{Software Artifact}} is one of numerous byproducts of a software process - a use-case, 
an UML class diagram, a change record, or a bug report. 
\end{defn}
Artifacts are not only produced by a process, but often re-used within the process to support 
software development activities and to document the resulting software.

\begin{defn}\label{def_behavior}
A \textbf{\textit{recurrent behavior}}, in context of software process is a \textit{frequent mannerism} 
in which a developer or a team perform the software development. 
\end{defn}
Recurrent behaviors are the basic building blocks of any human-driven process that reflect habits - 
a more or less fixed way of dealing with tasks.
For example, if one developer frequently runs unit tests before committing changes, while another typically 
commits without running the tests, the first developer's habit of preceding to the commit code testing 
is a recurrent behavior that may reflect the developer's process, or a development discipline.

\newpage
%
% >> section
%
\section{Background}\label{sec_background}
Contemporary software projects concern with development of complex software systems and typically have 
a considerably long life cycle - well over decade.
A project's development and maintenance activities are usually carried out by geographically 
distributed teams and individuals. The development pace, the experience, and the structure of the 
development team continuously change with project progression and as developers joining and leaving. 
When combined with schedule and requirements adjustments, these create numerous difficulties 
for stakeholders, developers, and users, ultimately affecting the project success \cite{citeulike:2207657}. 

This software development complexity phenomena was identified in 1968 as ``Software crisis'' 
\cite{naur_crisis_68}, and was addressed by bringing the research and the practice of software development 
(or as it was called ``programming'') under the umbrella of Engineering - in an effort to provide 
the control over the process of software development. 
Following the engineering paradigm, numerous methodologies of software design and development 
processes, known as \textit{Software Processes}, were proposed \cite{citeulike:10002165}.
Some of these were formalized and evolved into Software Process Models - industrial standards for 
software development such as CMM \cite{citeulike:9962021}, ISO \cite{iso-standard}, 
PSP \cite{citeulike:8347315}, and others \cite{citeulike:5043104}. 

In spite of this effort, industrial software  development remains error-prone and more than half of all 
commercial software development projects ending up failing or being very poorly executed 
(``Chaos Reports'', 2006 \cite{chaos2006}). Some of them are abandoned due to running 
over budget, some are delivered with such low quality, or so late, that they are useless, and some, 
when delivered, are never used because they do not fulfill requirements. 

By the analysis of software project failures, it was acknowledged, that the Engineering paradigm 
may not be an adequate way to provide the control over software development processes 
due to the fact that Software engineering is dealing with significantly different from other engineering 
fields problems \cite{citeulike:3729379} \cite{citeulike:5203446} \cite{citeulike:2207657}.
The chief argument supporting this point of view is the drastic difference in the cost model:
while in Software Engineering there is almost no cost associated with materials and 
fabrication, these usually dominate cost in all other Engineering disciplines, but, 
ironically, Software Engineering is suffering from cost and challenges associated with 
continuous re-design of the product and its design processes - the issue that is 
hardly seen at all in other Engineering areas. 
In addition, as pointed by numerous studies, engineering-like models are typically prescriptive and rigid,
they are difficult to adapt to the particular organizational structure, to project specificities, and to 
changing requirements \cite{citeulike:113403}. 
Thus, the degree to which an adopted Process Model structures software processes varies greatly 
between teams and projects and does not guarantee the success \cite{sacchi_2001}. 
Finally, an increasing understanding and appreciation of human factors in software development 
processes over tools, technologies, and standards, suggests that the human-driven software 
process aspects are likely to be defining in a software project fate \cite{citeulike:6580825} 
\cite{citeulike:149387} \cite{1605185} \cite{citeulike:113403} \cite{citeulike:12743107}. 

However, alternative to Software Engineering developer- and user-centric software processes that are 
popular in open-source, academic, and hobbyists communities were also found to be affected by the 
same complexity issues. Among others, inability to acquire a critical mass of users, loss of leading 
developers, and forking were cited as the main reasons for open-source project 
failures \cite{richter2007critique}. 
Agile and software craftsmanship approaches, that also address aforementioned engineering issues,
were shown to create significant difficulties with software project cost and effort 
estimation \cite{citeulike:12933080} \cite{citeulike:9928907}.

Currently, it is widely acknowledged, that there exists no ``silver bullet'' process which 
guarantees to bring a software project to the success \cite{citeulike:1986013}. 
Processes are numerous, each has advantages and drawbacks, and each accompanied with 
success stories and failure experiences, making the process selection difficult 
and the results of its application unpredictable.
This uncertainty, and the alarming rate of software projects failures suggest, that our understanding 
of software development ``mechanics'' is limited and insufficient \cite{citeulike:12550665}. 
The enormous cost of the lost effort, measured in hundreds of billions of US dollars 
\cite{citeulike:2207657} \cite{citeulike:2207653} \cite{citeulike:2207655}, 
continues to provide motivation for further research on software processes. 

%
% >> section
%
\section{Software process design and improvement}\label{section_software_process_design}
Traditionally, it was assumed that the software development is apriori performed for a profit in 
corporate, government, or military settings by people that are mostly collocated together. 
This assumption yielded industrial software process models describing ``on-site, software manufacturing'' 
which were researched and discussed for decades in software  engineering. 
Currently, we see the rise of alternative software processes - people are coming 
together over the Internet and create high quality software which they distribute openly 
promoting its modification and re-distribution. Surprisingly, they provide a very little if any of
guidance on their software processes. This not only challenges traditional school of software 
engineering, but creates a number of research directions that can address long standing issues 
in software development.

In this section I would like to re-visit traditional approaches to software process design and improvement, 
to contrast the differences of open-source processes, and to show a potential of the research based 
on public software artifacts.

\subsection{Traditional approaches}\label{sec_floss_processes}
Traditional approaches to software process design and improvement can be divided into two distinct categories. 

The first category consists of \textit{top-down} techniques through \textit{proposing a process} based 
on specific patterns of software development. 
For example, Waterfall Model process proposes a sequential pattern in which developers first create a 
Requirements document, then create a Design, then create an Implementation, and finally develop Tests 
\cite{citeulike:9982731}. 
The Test Driven Development process, from other hands, proposes an iterative behavioral pattern in which
the developer must first write a test case, then write the code to implement that test case, then re-factor the 
system for maximum clarity and minimal code duplication \cite{citeulike:6086365}. 

While the top-down approach follows the usual path of trials and errors, and seems to be an 
extension of natural to humans creative processes of invention and experimentation, 
the ``invention'' of an adequate to the task software process is far from trivial 
\cite{citeulike:5043104} \cite{citeulike:1986013}. 
Moreover, the evaluation cycle of an invented process is usually very expensive and considerably long.
In addition, it was shown that the process inventors are usually limited in their scope and tend to 
assume idealized versions of real processes, thus, often produce ``paper lions'' - process models which are 
likely to be disruptive and unacceptable for end users, at least in their proposed form \cite{citeulike:9758924}.

The second category of software design approaches consists of \textit{bottom-up} techniques that focus 
on the \textit{performed process reconstruction} through noticing of recurrent development events. 
Usually, the process reconstruction task is viewed as a two-levels problem where the first level 
consists of a event discovery (process segmentation) while the second level consists of process 
reconstruction through the events network analysis \cite{citeulike:2703162}.
One of the first works in this category by Cook and Wolf shows a possibility of automated extraction 
of a process model through the mining of recorded process event logs 
\cite{citeulike:328044} \cite{citeulike:5120757} \cite{citeulike:5128143}. 
Later work by Huo et al. shows that it is also possible to improve an existing process
through the event logs analysis \cite{citeulike:7691059} \cite{citeulike:7690766}. 

The bottom-up approaches, while appearing to be systematic and potentially less challenging than invention, 
are also affected by a number of issues, among which the observability is the most significant: 
while a live project observations are technically challenging to implement due to the high cost and 
privacy concerns, the post-process data collection affects its reconstruction due to 
discrepancies between actually performed and reported actions \cite{citeulike:7691059}. 
Yet another significant issue is the capacity of currently available process discovery and representation 
techniques - typically these need to be supervised by experts and finely tuned in order to reconstruct 
distributed and concurrent software processes. 

While both traditional approaches are opposite in their nature, they yield similar process models that 
effectively are sequences of actions (states) that must be performed (visited) successively or iteratively 
in order to deliver a software. The ``process inventors'' put the best of their knowledge, experience, 
and logical reasoning into the proposed sequence of steps, similarly, the process re-constructors 
strive to eliminate the noise and to converge to a concise sequence of steps that is supported by the 
majority of observations. 

This particular attention to the synthesis of sequential steps, leaves human factors, such as team's 
structure, work schedule, developer's discipline, behaviors, and motivation behind -- 
the issue that has been widely recognized \cite{citeulike:149387} \cite{citeulike:113403} 
\cite{citeulike:205322} \cite{citeulike:12798652} but still largely ignored in industrial practices 
mostly due to the  difficulties with human component benefits estimation 
\cite{citeulike:12798659} \cite{citeulike:12798662} \cite{csdl2-12-11}.

%
% >> section
%
\subsection{Free/Libre/Open Source processes}\label{sec_floss_processes}
Another phenomenon generating novel software processes is the social movement inspired by the philosophy 
of source code sharing and its collaborative improvement, that is called called ``free-software movement''. 
This social phenomenon originating from 1960s was partially formalized in 1983 by Richard Stallman,
who launched GNU Project and later, in 1985, founded the Free Software Foundation in order to support 
it. The ``open-source'' term, that is commonly used for description of free and libre open source software, 
was coined later, in 1998 at the very first Open Source Initiative (OSI) meeting \cite{osi-history}.

The open-source software development community, consists of self-organized individuals and teams of 
mostly non-professional programmers - amateurs, hobbyists, students, and academics. 
Bu using Internet, they collaborate and develop software that is distributed freely along with its 
source code and is usually called free/libre open-source software (FLOSS). 
The characteristic freedom of open-source development provides a thriving human-centric environment 
for creative individuals and teams where novel software processes can be invented and tested immediately.

Over the years, this software development model has proven its ability to deliver increasingly complex 
and surprisingly popular software in a ``global'' scale - when thousands of non-professional developers 
and users are scattered all over the world. A number of open source projects such as Linux and its 
derivatives, Gnome, Apache HTTP Server, MySQL database, and others, succeeded to develop efficient 
distributed software processes that provide control over the large development team and code-base.
Moreover, these processes allowed to deliver the state of the art software whose quality is similar 
or exceeding that of industrial projects \cite{coverity2012}. 
In turn, this attracted a considerable attention not only from industrial companies that seek to emulate 
successful open source software processes in traditional closed-source commercial environment 
\cite{oss_virtual_organizations} \cite{oss_balance} \cite{oss_hp} \cite{oss_4industry}, 
but from software process research community, that is fond of finding of novel software processes
\cite{citeulike:12550640} \cite{citeulike:5043664} \cite{citeulike:5128808} \cite{citeulike:10377366}.

\begin{figure}[ht!]
   \centering
   \includegraphics[width=140mm]{figures/Linus.Kernel.ps}
   \caption{A Torvald's response in the mailing list suggesting that practical reasons, the ``real-life'', 
   should be always considered over specifications.
   Excerpt from Linux mailing list. \url{http://lkml.indiana.edu/hypermail/linux/kernel/0509.3/1441.html}}
   \label{fig:kernel}
\end{figure}This freedom provides a thriving human-centric 
environment for creative individuals and teams where novel software processes can be invented and tested 
immediately.

A number of studies conducted on open source processes discovered, that they are significantly 
different from the traditional software development at many levels. In particular, their inherent capacity 
to adapt to changing requirements is often cited as the most prominent. 
For example, the most significant document shaping industrial software development - the specification - is rarely 
considered in open-source projects. Even in the Linux kernel development, which is probably one of the few strictly 
moderated development processes, developers prise practical reasons over specifications \ref{fig:kernel}.

The overall lack of FLOSS processes governance, high degrees of their distribution, and its inherent flexibility make 
it difficult to study open-source processes by using traditional approaches - through the process proposal or through 
its observation \cite{citeulike:12550640} \cite{citeulike:10377366}.
However, the vital open-source process characteristic - an extensive use of the efficient software change control 
system enables scientific research. 

In order to provide flexibility many FLOSS processes consider the final software ``look and feel'' and its functionality 
as open-ended questions, they highly encourage contributors to make only small incremental 
improvements and to synchronize code changes as often as possible \cite{so-checkin} \cite{git-best-practices1}
in order to enable the small change visibility allowing all developers to closely follow up with the project evolution.
Another way to manage the distributed software development complexity is the restriction of the write access to 
a project's source code tree. This approach establishes a workflow based on code forking by project's contributors 
and code ``patching'' (change merging) by core developers. Code patches are typically discussed among the project's
development team and users, and once merging decision is made, all of the contributor's changes are merged 
with main tree and become visible to all project participants.

Following the philosophy of source code sharing and its collaborative improvement, open-source software 
change control systems are usually publicly accessible and called ``\textit{public software repositories}''.
Currently, the free code hosting sites such as SourceForge, GoogleCode, and GitHub host thousands of FLOSS 
projects offering free public access to numerous software process artifacts, such as design documents, source code
change records, bug reports, and developers communications.

\subsection{Public software repositories}
Scientific community response on the availability of public artifacts was overwhelming, and a number of 
venues was established addressing the increased interest. 
Since 2004, the International Conference on Software Engineering (ICSE) hosts a Working Conference on 
Mining Software Repositories (MSR). The original call for papers stated MSR's purpose as 
\textit{``... to use the data stored in these software repositories to further understanding of software 
development practices ... [and enable repositories to be] used by researchers to gain empirically based 
understanding of software development, and by software practitioners to predict and plan various aspects 
of their project''} \cite{msr2004} \cite{citeulike:7853299}. 
Several other venues: International Conference on Predictive Models in Software Engineering \cite{promise12}, 
International Conference on Open Source Systems, the Workshop on Public Data about Software Development, 
and the International Workshop on Emerging Trends in FLOSS Research have also played
an important role in shaping and advancing this research domain.

Some of the published work addresses the software process discovery. Among others, most notable and 
relevant to my research is work by Jensen \& Scacchi. In their early work, they demonstrated, that 
information reflecting software processes can be gathered from public systems \cite{citeulike:12550640}. 
Later, in \cite{citeulike:5043664} and \cite{citeulike:5128808}, they show, that by manual mapping of 
collected process evidence to a pre-defined process meta-model it is possible to reconstruct some 
of the FLOSS processes. 
Another closely related to my research is work by Hindle et al. where they has shown that it is possible to 
discover software process evidence through partitioning \cite{citeulike:10377366}.

However, the research work based on mining of public software process artifacts shows, that while public 
artifact availability is minimizing observability and privacy issues, the nature of these artifacts creates a number of 
challenges which limit the possible scope of the applicable research and significantly elevate its complexity, 
effectively rendering previously designed techniques inefficient.
Thus, novel software process analysis and discovery techniques are needed to be developed for public software 
process artifacts analysis \cite{citeulike:7853299}.

%
% >> section
%
\section{Software Trajectory}\label{sec_software_trajectory}
In addition to the establishing of engineering-like software process paradigm, the acknowledgement of 
the software crisis led to the development of similar to Engineering project management techniques based on 
software measurements.
% The software measurement is also essential in software engineering research.

\subsection{Software measurement}\label{sec_software_metrics}
The goal of software measurement is to make objective judgments about software process and product quality. 
It has been shown that an effective measurement programs help organizations understand their capacities and 
capabilities - so that they can develop achievable plans for producing and delivering of software products. 
Furthermore, a continuous measurement effort provides an effective foundation for managing process 
improvement activities, such as PSP \cite{citeulike:8347315}, \cite{citeulike:5090131} 
\cite{citeulike:12929216}, CMM \cite{citeulike:9962021}, ISO 9001 \cite{iso-standard}, 
and SPICE \cite{spice-standard}.

In addition to practical applications, software measurement is extensively used in the research - it is the basis of 
Empirical Software Engineering research area where researchers base their conclusions on concrete evidence collected 
through experimentation and measurement of software systems and software processes \cite{citeulike:766768}.

\subsection{Software telemetry}\label{section_software_telemetry}
Ideally, by using measurements, a software process and product can be assessed in real-time, allowing efficient 
in-process decision making.
In in \cite{citeulike:557296}, Johnson et al. pioneered this approach by defining software project telemetry as a 
particular style of software process and product metrics collection and analysis based on 
\textit{automated measurements over a specified time interval}. 
They implemented an in-process software engineering measurement and analysis system called Hackystat 
\cite{citeulike:12929227}, that is capable of metrics collection, processing, and telemetry streams visualization. 
The authors showed, that the visual analysis of multiple telemetry streams aids in the in-process decision making 
and it is possible to improve existing software processes by using the knowledge extracted by visual 
analysis of these streams. At the same time, they acknowledged, that it is impossible to extract an analytical model 
that is capable to automate that decision making process.

Later, Kou et al. extended Hackystat by implementing a Software Development Stream Analysis Framework (SDSA) 
that is capable of partitioning of telemetry streams into sequences of development ``episodes'' by using pre-defined 
boundary conditions \cite{citeulike:6180831} \cite{citeulike:11538873}.
Further, by designing ``operational definitions'' of test-driven development (TDD) as sets of specific rules for 
episodes, they showed that it is possible to assign TDD compliance to episodes.
They implemented their approach in Zorro, that is a software system capable of software process measurement, 
development episodes inference, categorization, and their classification by the conformance of TDD. While Zorro is 
based on pre-defined partitioning and classification rules, the authors acknowledged that the application of machine 
learning techniques is desirable.

\subsection{Software trajectory}
One of the research directions, preceding this thesis work, was to explore the possibility of extension of Hackystat/Zorro
system with a layer that would be capable of development episodes discovery from telemetry streams obtained 
by measurement of publicly available software process artifacts. 
Unfortunately, it turned out that the granularity level of public artifacts is too high to be able to infer development 
episodes consistently.
Keeping the research direction, I have proposed a similar to software telemetry approach to software process analysis 
based on software artifacts measurements and mining called \textit{Software Trajectory} \cite{csdl2-10-09}. 

Software Trajectory is an automated approach to software process discovery based on software process artifacts 
measurements and mining. In contrast with previously proposed systems, that were built upon quantitative analyses
of atomic development entities such as actions or episodes, or were relying on some reference model, 
Software Trajectory focuses on a discovery of naturally occurring phenomena - recurrent behaviors. 

The design of Software Trajectory and its philosophy address a number of known issues that previously complicated 
and limited large scale studies on software processes.
First of all, Software Trajectory addresses process observability-associated problems by removing in-process 
measurement costs and privacy concerns - it relies on off-line measurements of publicly available artifacts. 
Secondly, Software Trajectory is designed not to depend on any prior knowledge about software processes - 
unsupervised data mining techniques intended to be used in order to bootstrap knowledge. 

Intuitively, Software Trajectory can be viewed as an analysis of curve that only approximately describes 
a path that a software or a process draws in the chosen metric space. 
Then, this curve is the \textit{trajectory} by definition, as it is known in Physics or Mathematics - an approximate 
path of the object in a physical space, or a reduced in complexity sequence of states of a dynamic system 
(Poincare' maps).

The exploratory study of software trajectory properties presented in this dissertation.

%
% >> section
%
\section{Research hypothesis, scope of the dissertation}\label{sec_research_hypothesis}
In previous sections, I have outlined an evidence of a limited performance of traditional engineering-like 
software processes, as well as the oversight by traditional approaches to software process 
design of a variety of human factors that fall beyond a typical sequence of development actions.
In contrast, I have identified a number of key differences in FLOSS software development that foster 
developer- and user-centric processes and, if systematically studied, can potentially shed light on 
human-driven aspects of software development. 
Then, I have pointed out a growing wealth of publicly available software process artifacts that may 
enable quantitative and qualitative FLOSS processes analysis and highlighted the need for new 
knowledge discovery techniques capable of processing of these datasets.
Finally I have proposed a Software Trajectory, an approach designed for the analysis of public 
software artifact trails targeting the discovery of recurrent behaviors.

In order to evaluate the performance of of Software Trajectory, I hypothesized, 
that \textbf{\textit{it is possible to discover recurrent behaviors from publicly available software 
process artifacts}}. 

Previously, it was shown that it is possible to discover recurrent behaviors on all levels of software 
development hierarchy \cite{citeulike:8347315}, whether in open-source \cite{citeulike:200721} 
or industrial \cite{citeulike:5090131} settings. Thus, if Software Trajectory will be capable to discover
recurrent behaviors, it will lay a foundation for further studies whose purpose will be the association 
of discovered behaviors with certain project or process traits, such as pace, agility, size, complexity, 
code quality and others. If hypothesis fails, then this study experiences can be used for designin of
an alternative software process analysis system.

Following the hypothesis, I have investigated a number of publicly available software repositories,
their artifacts, and a number of applicable data-mining techniques in a preliminary exploratory study 
\cite{csdl2-10-09}. However, similarly to other studies in the field, I have discovered two issues 
significantly affecting efficiency of currently available process mining techniques. 
The first issue is that while FLOSS process artifacts are numerous and readily accessible, 
they are irregular and represent not sequences of development actions, but rather series of software 
state snapshots. 
The second issue is that there is no baseline process exists for any given software project, making the 
assessment of its specificity difficult.

Addressing this issues, I cast the initial problem of event-based recurrent behaviors discovery into more 
generic problem of knowledge discovery from temporally ordered software measurements 
represented as time series, and approached this by developing of a novel technique for interpretable 
comparative analysis of time series that allows class-characteristic patterns discovery and ranking.
I have implemented this approach in the software artifacts analysis framework called 
Software Trajectory Analysis (STA) that automates public software artifacts collection, 
their measurements, software trajectories creation, and their characteristic patterns discovery. 

This dissertation proposes SAX-VSM, a novel algorithm for time-series classification, presents
results of its performance evaluation, details Software Trajectory Analysis framework implementation, 
and discusses its performance evaluation based on three case-studies.

%
% >> section
%
\section{Interpretable time-series classification}\label{sec_knowledge_discovery}
In data mining, time series are used as a proxy representing a large variety of real-life phenomena 
in a wide range of fields including, but not limited to physics, medicine, meteorology, music, 
motion capture, image recognition, signal processing, and text mining. 
While time series usually directly represent observed phenomenas by recording their measurable evolution 
in time, the pseudo time series often used for representation of various high-dimensional data 
by combining data points into ordered sequences. 
For example in spectrography data values are ordered by the component wavelengths \cite{citeulike:12550833};
in shape analysis the order is the clockwise walk direction starting from a specific point in the outline 
\cite{citeulike:12550835}, in image classification the numbers of pixels are sorted by color component 
values \cite{citeulike:2900542}.

Many important problems of knowledge discovery from time series reduce to the core task of finding 
characteristic, likely to be repeated, sub-sequences in a longer time series. 
In the early work these were called as 
\textit{frequent patterns} \cite{citeulike:5159615}, 
\textit{approximate periodic patterns} \cite{citeulike:1959582},
\textit{primitive shapes} \cite{citeulike:5898869}, 
\textit{class prototypes} \cite{citeulike:4406444}, 
or \textit{understandable patterns} \cite{citeulike:3978076}. 
Later, similarly to Bioinformatics, these were unified under the term \textit{motif} \cite{citeulike:3977965}.
Once found, motifs can be used for a research hypothesis generation by their association with known
or possible phenomena \cite{citeulike:3977965}. 

The recent advances in semi-supervised and unsupervised finding of such characteristic sub-sequences, 
in particular work based on \textit{shapelets} \cite{citeulike:7344347} \cite{citeulike:11957982}
\cite{citeulike:12552293} and \textit{bag of patterns} \cite{citeulike:10525778}, show a great potential 
of application of time series data-mining techniques to a wide variety of high-dimensional data.

Unfortunately, both techniques provide a limited insight into the data and suffer from performance issues. 
While exact shapelet techniques allow discovery of class-characteristic patterns and facilitate classification,
algorithm is almost quadratic and provides limited insight into class specificities. 
The bag of patterns algorithm, while performs in a linear time, requires a previous knowledge for input parameters 
selection and does not offer the class generalization.

In this thesis I propose an alternative solution to class-characteristic patterns discovery from time series called 
SAX-VSM. As I shall show, the proposed algorithm not only facilitates classification, but provides insights into 
the both: classification results and class specificities. These, in turn, enable the discovery of novel phenomenas.

\section{Contributions}\label{section_contributions}
Main contributions of my work can be summarized as follows: 
\begin{itemize}
\item I propose a novel, generic algorithm for interpretable time series classification: SAX-VSM. 
While the classification performance of this algorithm is at the level of current state of the art, 
it offers an outstanding feature - discovery, generalization, and ranking of class-characteristic features. 
This, in turn, enables knowledge discovery by offering much clearer insight into classification results than any of 
competing techniques.
In addition, SAX-VSM is very fast in classification and has a small memory footprint. 
Overall, I expect this algorithm to play an important role in future because of the growing ubiquity of time series and 
a growing interest in behaviors.
\item Powered by SAX-VSM, I design a Software Trajectory Analysis (STA) framework, and through case-studies 
show its capacity for recurrent behaviors discovery from publicly available software process
artifacts. While case studies are obviously limited, I argue that STA is a useful knowledge discovery tool applicable for a 
variety of software process artifacts and metrics. 
\item Finally, I provide SAX-VSM and STA implementations to community.
\end{itemize}

\section{Dissertation Outline}\label{section_organization}
The rest of this dissertation is organized as follows. Chapter \ref{chapter_background_work} discusses the history 
of Software Engineering, previous work in software process discovery, mining of software repositories, and current 
state of the art in time series mining. Chapter \ref{chapter_sax_vsm} proposes an algorithm for interpretable 
time series classification. Chapter \ref{chapter_sta} discusses the design of STA framework and presents case studies.
Chapter \ref{chapter_conclusions} concludes and discusses several directions for future study.