\section{Graveyard}

For Hackystat to be of use as a testbed for projects in the Science of
Design program, it must be adapted to collect the process and product data
of importance to the project under study.  In addition, the project must be
carried out in such a way that important data is actually amenable to
collection and analysis via Hackystat.  Some of the research to be
performed in this proposal includes dialogue with currently funded Science
of Design participants to articulate their testbed requirements and to
extend Hackystat to support them.  In the related work section, we will use
project descriptions from previous SoD awards to illustrate some of the
issues that might arise in this process.


The Program is looking for new and creative ways of thinking about a
Science of Design discipline for software-intensive systems. Incremental
research will not be funded. Project Descriptions must present a vision for
a Science of Design that forms the context for the contributions of the
proposal.  The research advances we propose for the Hackystat Framework to
support the Science of Design program are admittedly incremental in nature,
and thus would seem to violate at least one of the stated goals of the
program.  We wish to argue at the outset of this proposal that when
evaluating testbed technology for Science of Design, the goals of the
program are better supported by supporting a mature testbed technology with
demonstrated capabilities, leaving the novelty for the design innovations
themselves.

Issue: how can we lower the cost of comparability?  Several SoD proposals
talk about use of design verification techniques and their ability to
improve correctness, lower cost, etc.  If we can provide data within a
common framework, we can compare their effectiveness.

Emergent categories: Design verification/validation/automation through
formal techniques; end-user programming; economic/value-based design

An important research issue for Hackystat is to figure out how to
incorporate value-based costing in a way that makes sense for other
projects.

For example, some projects are quite explicit about going beyond
traditional cost models that are code-centric (economic approaches, end
user approaches) while other projects seem to maintain a much more
traditional approach to cost (design verification, etc.)

Things to think about:

- There are tools to support the development of designs

- There are (hopefully) mechanisms to support evaluation of designs

- Big questions: what are the process impacts of these tools; for example, 
do they change the way other aspects of development (coding, testing, 
maintenance) are performed?   Are the amenable to Agile or Plan-based 
development? 

In the project ``Monitoring in support of design science principles'',
Robinson proposes the use of a tool called ReqMon to provide guidance on
when and what aspects of software should be evolved by dynamically
monitoring individual user goals. The ReqMon Framework defines a language
for requirements and monitor definitions, plus monitoring tools to
instrument running systems for compliance with their requirements. By 
attaching sensors to both the ReqMon tool and other development tools, 
TfSoD can be used to simplify the ability of these researchers to generate
comparable data concerning: 
(a) the time spent defining and modifying requirements definitions;
(b) how that activity is interleaved with other development activities; and 
(c) how often the monitoring tools actually find a violation

In ``Teaching Creativity and Conceptual Representation, 
a Design Toolkit'', Amoussou proposes the creation of a new curriculum for
Science of Design, which includes  the creation of design artifacts.  If
sensors can be created for the tools that create these artifacts, then 
TfSoD can be used to better understand the way that the introduction of 
these new techniques change the way students generate designs, the level
of collaboration that takes place around design, and so forth. 

In ``Tools and Techniques for On-the-fly Design of Business
Process Integration'', Su proposes to develop a formal framework and
collection of tools that aid in the design (or re-design) and management of
business processes and business process integration. Su hypothesizes that
this targeted declarative framework can (a) increase precision and reduce
ambiguity in process specification, and (b) create flexibility though
support for incomplete specifications, leading to reduced development times
and higher quality code.  Use of TfSoD can provide the infrastructure required
to measure development time, and can facilitate evaluation of quality through 
use of automated quality assurance tools. By monitoring the use of other tools,
the testbed can also help reveal the way the declarative framework is integrated
into the overall development process.

In ``Design for Adaptivity and Reliable Operation of Software
Intensive Systems'', Abdelwahed proposes a semi-automated process based
upon mathematical models and control-theoretic techniques and supported by
tools for design, verification, and analysis. The intended outcome of this
research is to simplify the development of a large class of distributed
real-time and embedded software intensive systems.  TfSoD can provide 
infrastructure for instrumenting the design, verification, and analysis tools
produced through this research, along with the other tools (compilers, 
debuggers, run-time environments, etc.) required for development.  Such 
infrastructure can support understanding the time spent using the tools, 
the manner in which these design, analysis, and verification activities
are interleaved with other development activities, and the process impacts
of this approach. 

In ``Robust System Design Under Weak Component Assumptions'', Zhou proposes
a formal theory, language, and toolset for design and specification of
systems composed of imperfect components.  The language and toolset
supports analysis of a system's functional limits under different assumptions, 
and supports trade-off analysis between system assurance and component choice.
This research would appear to generate an operational definition of 
design ``quality'' in terms of a system's functional limits. A case study
using TfSoD instrumentation would reveal insights into the time spent
using the tool, and how developers interleave tool usage with other
development tasks (coding, compilation, debugging, etc.) 

In ``Comprehensibility as a Design Criterion'', Gamboa proposes to 
modify two program analysis tools (Daikon and AbsInt) for use as an 
empirical measure for design quality in terms of its ``comprehensibility''. 



- Some people equate ``scientific'' with formality--i.e.  the ability to
prove properties of a system.  I look at ``scientific'' as a method---a way
to derive evidence and interpret it.
