<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="GENERATOR" content="Microsoft FrontPage 4.0">
<meta name="ProgId" content="FrontPage.Editor.Document">
<title>Eclipse Innovation Grant 2004</title>
</head>

<body>

<h2 align="center"><b>The Review Game: Teaching&nbsp; Asynchronous Distributed
Software Review using EclipseView</b></h2>
<p align="center">Philip Johnson<br>
Collaborative Software Development Laboratory<br>
Department of Information and Computer Sciences<br>
University of Hawaii<br>
<a href="mailto:johnson@hawaii.edu">johnson@hawaii.edu
</a></p>
<p align="center">CSDL-03-09</p>
<p align="center"><a href="http://csdl.ics.hawaii.edu/techreports/03-09/03-09.html">http://csdl.ics.hawaii.edu/techreports/03-09/03-09.html</a></p>
<p align="center"><b>Last update: <!--webbot bot="Timestamp" S-Type="EDITED"
S-Format="%m/%d/%Y %I:%M:%S %p" startspan -->11/04/2003 10:18:02 AM<!--webbot bot="Timestamp" i-CheckSum="30111" endspan -->
&nbsp;</b></p>
<p align="center">A proposal to the <a href="http://www-3.ibm.com/software/info/university/products/eclipse/eig.html">Eclipse
Innovation Grant Program</a></p>
<p align="center">&nbsp;</p>
<h3 align="left"><b>Background and Objectives</b></h3>

Collaborative review of software designs and code has been shown repeatedly to be an efficient and effective approach to defect removal and quality
assurance. Beginning with the ground-breaking work of Michael Fagan on Software Inspection at IBM in the
1970's [1], a diverse body of knowledge has
been accumulated regarding different review procedures and their
properties.  The majority of review techniques consist of four general phases:
initiation, preparation, meeting, and rework.&nbsp; During the initiation phase, a review leader invites a group
of technical
personnel to participate in a review, and arranges for them to receive the
artifacts to be reviewed. During the preparation phase, the reviewers read
through the artifacts and record issues (i.e. potential defects).&nbsp; During
the meeting phase, the review participants meet to validate the set of issues to
be addressed by the author. During the rework phase, the author uses the information gathered from the review
meeting to improve
the quality of the artifacts. Perhaps the most rigorous application of
software review occurs in the Cleanroom method [2], in which software review is
made so effective that it replaces unit testing. Review also provides other
collateral benefits outside of defect removal: it disperses knowledge about the product to
team members; it helps junior developers to learn techniques and best practices
from more senior developers, and it provides management with information regarding
the state of product development.&nbsp;&nbsp;<p>
Agile methods, such as Extreme Programming, take a different path to
achieving the benefits of software review. Instead of alternating periods
of development with periods of review, XP blends review into the
development activity itself through the practice of pair programming.  The
approach yields a kind of "continuous" review of the software as it is
developed.&nbsp; Like more conventional review techniques, the pair programming
approach to review also disperses product knowledge and development techniques
across the group, but without the process overhead involved in organizing and
carrying out the review.&nbsp; Pair programming also tends to result in thorough
review, in the sense that it essentially guarantees that all code
is reviewed at the time it is written as well as during any enhancement
activities.<p>Our experience teaching both pair programming and software review
in a classroom setting has revealed that both techniques should be taught, and
that the techniques are actually complementary.&nbsp; While we can enforce the
use of pair programming in a classroom lab setting where students are required
to meet together, it is simply infeasible to require that students to pair program
all the time, including outside of class when working on larger projects.&nbsp;
Furthermore, a well rounded software engineer should be skilled in software
review as well as pair programming, since not all organizations and projects are
structured in a manner that enables pair programming to replace software review.
For example, open source software development, or any geographically distributed
organizational context, makes pair programming problematic.&nbsp;<p>Teaching
software review in a classroom setting raises its own set of problems. First,
each review creates a significant information management problem: if 20 students
review a software system and raise 10 issues each on average, that creates 200
issues to collate together, order, evaluate, and decide upon the appropriate action to
take.&nbsp; Second, this information management overhead encourages software review to be treated as an
exotic technique:
something to be &quot;experienced&quot; once or twice and then put aside for the remainder of
the course. Third, there is a wide variation in the usefulness and insight of
review issues: certain students focus on superficial formatting or syntactic
issues as a way to simply fulfill the assignment constraints, while others
analyze more deeply and discover subtle but important design and execution issues.&nbsp;In
some semesters, we have found this &quot;signal-to-noise&quot; ratio in the set
of issues to be quite
poor.&nbsp; Fourth, the information management
overhead associated with review, combined with the signal-to-noise ratio, can
create significant barriers to adoption beyond the classroom: some students come away
thinking that the benefits of review are not worth the costs of doing it.&nbsp;<p>The objective of this project is to design, implement, and evaluate
EclipseView, an Eclipse-based review system and associated curriculum materials that
teaches &quot;sustainable&quot; software review practice. The EclipseView
project is designed to address each of
the four problems in teaching software review identified above. First, to address the
information management problem, we will employ an Eclipse-based plug-in we
have designed that dramatically simplifies collection and management of review data.&nbsp; Second,
by use of tool support to lower the overhead
associated with information management for both students and instructor, we will
develop a software engineering curriculum in which distributed, asynchronous review is performed
regularly throughout the semester without incurring excessive resources and time.&nbsp;
Third, to improve the signal-to-noise ratio and to make the review experience
more fun, we have designed a &quot;Review Game&quot; for use in teaching review
of Java code.&nbsp; EclipseView introduces the concept of &quot;defect weights&quot;,
in which the first student to discover a (validated) defect wins a certain
number of points depending upon the type of defect uncovered.&nbsp; The sum of
all weighted issue points for a student becomes their &quot;score&quot;
for the review session.&nbsp; For example, being the first to discover a concurrency-related defect, such as the
need to make a method synchronized, gains the student significantly more points
than discovering that a line of code was not indented correctly. Each issue is
timestamped so that points can be awarded to the first student recording a given
defect. Students can view not only their issues and (pre-validation)
score, but also all of the issues generated by other students (and their
pre-validation scores).&nbsp; Making the scores and the combined issue set visible
to all participants during the review reduces spurious generation of duplicate
issues and thus decreases the information management overhead.&nbsp; It also
increases the game-like quality of the review by enabling students to see how
many additional points they need to discover to &quot;take the lead&quot;.&nbsp;<p>The EclipseView project builds upon
and integrates together two streams of research by our research group: automated software review and
automated software metrics collection and analysis.&nbsp;&nbsp; For the past ten
years, our research group, the Collaborative Software Development Laboratory (http://csdl.ics.hawaii.edu/)
has investigated tools and techniques for software review.&nbsp; CSRS, a system
based upon C++ and Emacs, provides a declarative high level language for
defining software review processes as well as instrumented facilities for
gathering and analyzing review data [3].&nbsp; A second research initiative on
review resulted
in the JavaJam system, which was built as an extension to JavaDoc and allowed
annotation of both Java designs and code [4].&nbsp;<p>Another stream of research
in our lab involves automated software metrics collection and
analysis. Our first research project in this area, LEAP [5], provided tool support for collection
and analysis of measurement data similar to that specified in the Personal
Software Process. However, we found that the PSP is quite
&quot;heavyweight&quot; and that the context-switching between work and
collecting-data-about-work, even with tool support, created substantial barriers
to adoption.&nbsp; To address these issues while preserving the benefits of
measurement, we began work in 2001 on Hackystat, a framework for unobtrusive
metrics collection and analysis [6]. To use Hackystat, developers download and
install &quot;sensors&quot; in development tools such as editors, testing tools,
configuration management systems, and so forth. These sensors collect data and
send it via Soap and XML to a server, which analyzes the data to provide
insights back to developers to support improvements in their work products and
processes.&nbsp; We currently support an Eclipse plug-in for Hackystat that
collects effort, JUnit test invocation, and object-oriented size metric data for
Java code.&nbsp;<p>The Hackystat architecture is modular, and we currently build and
maintain separate configurations to support: (1) project effort and size
analysis in a classroom setting, (2) build process analysis for the Mission
Data System development group at Jet Propulsion Lab,&nbsp; and (3) a
constructive cost modeling research project involving COCOMO.&nbsp;&nbsp; To
support EclipseView, we will implement a new Hackystat configuration for
software review that will collect and analyze sensor data sent from the Eclipse
review plug-in. The analyses will provide traditional review metrics, such
as defects per KLOC, defects per hour, and (usually Pareto) distributions of
defect types, in addition to analyses oriented toward the Review Game, including
the current score values for each reviewer and the ability for the instructor to
validate issues.&nbsp;<p>Our prior research and
development efforts make us well positioned to achieve the objectives of the
EclipseView project. The next section provides a summary of our methods and
intermediate milestones.
<h3>
Methodology and Milestones</h3>
<p>To evaluate our success in achieving our objective, we will implement an
initial version of EclipseView, deploy it in our software engineering classroom,
and gather quantitative and qualitative metrics on the usability of the tool,
the effectiveness of the pedagogy, the effectiveness of the review process, and
post-class adoption.&nbsp; Our project is organized according to the following
milestones.&nbsp;</p>
<p>Our first milestone will be the initial public release of a stand-alone Eclipse plug-in
for Java-based software review. This plug-in will form the basis of our curriculum and provides support for review that is tightly
integrated with the Eclipse platform. We will gather feedback from the Eclipse
community regarding the plug-in and use it to enhance its usability. We have
already implemented a &quot;spike solution&quot; with a Task View style
interface. Reviewers view Java files using an Eclipse editor, then use keyboard
shortcuts to associate an issue with a class, method, and/or line number. Issues
are timestamped, structured artifacts including an issue type, the reviewer
name, issue location, issue status, and a description. All issues associated with a given
project are automatically stored by the plug-in as XML files in a project folder named &quot;review&quot;. Each file
name includes a timestamp to enforce uniqueness..&nbsp; Issues are made visible
to other developers through standard mechanisms: either by locating the
project in a shared file system or through the use of a configuration management
system such as CVS.&nbsp; The Task View enables reviewers to sort and filter
issues in a variety of ways to facilitate review evaluation and
management.&nbsp; Achieving the first milestone provides a simple, generic review tool that
will be of use to any Eclipse-based project in which developers want to collect
and organize comments regarding source code.&nbsp;</p>
<p>For our second milestone, we will implement and publicly release
Hackystat-based collection and analysis facilities. These will include sensors
for the review plug-in that collect data regarding the time each reviewer spends
performing the review in Eclipse, the initial data regarding an issue, and
updates to an issue (such as changes to its status, or its deletion,
etc.)&nbsp;&nbsp; From this data, analysis facilities on the server side can
provide developers with standard analyses regarding the efficiency of the review
(issues raised per hour of review, number of false positives), effectiveness
(issues raised per KLOC), and defect type distribution.&nbsp;</p>
<p>In addition to the standard review analyses, we will also provide analyses to
support the review game.&nbsp; During the initiation phase, the instructor will
use their Hackystat account to configure the point values to be associated with
each defect type.&nbsp; During the preparation phase, while students are
inspecting the source code and generating issues in Eclipse, they can also log
into their Hackystat account to see the status of the Review Game, including
their current (pre-validated) score and how it compares to others in the class.
They can also use the standard analyses to see, for example, the current defect
type distribution.&nbsp; We will use the alert facilities in Hackystat to allow
students to have the system send them an email when their standing in the Review
Game changes.&nbsp; During the meeting phase, the instructor uses the Eclipse
plug-in to review the generated issues and set the status field to validated if
they are true defects. This data is also sent to Hackystat and used to generate
the validated scores and assign final rankings to participants.&nbsp;</p>
<p>Our third milestone is a technical report presenting the case study use and evaluation of the EclipseView
system as part of a unit on software review in our graduate and undergraduate
software engineering courses at the University of Hawaii.&nbsp; Laurie Williams,
at North Carolina State University, has also expressed interest in evaluating
EclipseView for possible use in teaching software review in her courses.&nbsp;
We will gather qualitative and quantitative data regarding the effectiveness of
the Eclipse plug-in for collecting and managing issue data, the effectiveness of
the Hackystat standard review analyses, and of course, the effectiveness of the
Review Game for stimulating interest and seriousness in performing the review.
In addition, we will perform a post-course survey approximately six months after
the semester ends to see which students are still performing software
review.&nbsp; The results will help us improve the Eclipse plug-in, the
Hackystat sensors and analyses, and the Review Game parameters including the set
of defect types and point values.&nbsp; These three milestones are designed to
provide the Eclipse and software engineering community with tools and techniques
to support sustainable software review.&nbsp;</p>
<h3>References&nbsp;</h3>
<p>[1]&nbsp; Design and code inspections to reduce errors in program
development, M. Fagan, IBM Systems Journal, 1976.</p>
<p>[2] The Cleanroom approach to quality software development, Michael Dyer,
John Wiley and Sons, 1992.</p>
<p>[3] An instrumented approach to improving software quality through formal
technical review, Philip Johnson, Proceedings of the 16th International
Conference on Software Engineering, 1994.</p>
<p>[4] JavaJam: Supporting collaborative review and improvement of open source
software, Monir Hodges, M.S. Thesis, Department of Information and Computer
Sciences, University of Hawaii, 2000.</p>
<p>[5] Empirically guided software effort guesstimation, Philip Johnson,
Carleton Moore, Joseph Dane, Robert Brewer, IEEE Software, December, 2000.</p>
<p>[6] Beyond the Personal Software Process: Metrics collection and analysis for
the differently disciplined, Philip Johnson, Hongbing Kou, Joy Agustin,
Christopher Chan, Carleton Moore, Jitender Miglani, Shenyan Zhen, William Doane,
Proceedings of the 25th International Conference on Software Engineering, 2003.</p>
<h3>
Personnel and Collaborators</h3>
<p>This project will be managed by Philip Johnson, Professor of Information and
Computer Sciences at the University of Hawaii. Two graduate students will also
be involved in this project. Laurie Williams at North Carolina State University
is a potential collaborator on assessing the use of EclipseView for teaching
software review.&nbsp;</p>
<h3>
Current and Requested Funding&nbsp;</h3>
<p>
We request $30,000 for this project. There is no current funding for the
EclipseView project. Support for the Hackystat project is provided in part by a
grant from the National Science Foundation.
<h3>
Long term impact to the information/computing industry and Eclipse</h3>
<p>The long term of impact of EclipseView on the IT industry is to support
improved software quality by providing new approaches to tools and pedagogy for
software review.&nbsp; Current software review support tends to be proprietary,
heavyweight, unintegrated, and/or costly. Providing an open source software
review tool that is tightly integrated with the Eclipse platform will provide a
new low-cost and lightweight approach for developers wishing to integrate
software review into their development activities.&nbsp; The Review Game
pedagogy provides a fun and useful way for students to practice review, with the
desirable side-effect of improving the signal-to-noise ratio in the set of
issues.&nbsp; This can provide students with a more positive initial experience
with review, motivating them to continue their practice after the semester
ends.&nbsp;&nbsp;</p>
<p>The long term impact of EclipseView on the Eclipse community involves all
those factors discussed for the IT industry, of course. In addition, there are
two impacts specific to Eclipse. The first is to provide Eclipse with a new
competitive advantage in the IDE marketplace: we do not believe any other IDE (Emacs,
JBuilder, Together/J, Netbeans, or Forte) provides tightly integrated,
lightweight support for review.&nbsp;&nbsp; Second, the Eclipse tool development
community forms a very natural user community for this technology.&nbsp; After
initial classroom deployment and evaluation, our next step will be to publicize
it within this community and solicit trial use.&nbsp; Over time, we hope that
this tool will become a valuable resource to the Eclipse developer community in
their efforts to sustain and improve the quality of the Eclipse implementation
itself.&nbsp;</p>
<h3>
Planned use for cash

</h3>
<p>Research assistantships for the two graduate students at $15,000 per year
each. </p>
<p>&nbsp;</p>

</body>

</html>
