\documentstyle[nftimes,11pt,/group/csdl/tex/lmacros,/group/csdl/tex/definemargins]{article}
\input{/group/csdl/tex/psfig/psfig}

\definemargins{1in}{1in}{1in}{1in}{0.3in}{0.3in}

\begin{document}


\title{{\bf Does every inspection {\em really} need a meeting?}}

\author{
        Philip M. Johnson\\ 
        Danu Tjahjono\\
        Department of Information and Computer Sciences\\
        University of Hawaii\\
        Honolulu, HI, 96822 USA\\
        johnson@hawaii.edu
      }

\maketitle


{\bf Keywords: } Formal technical review, inspection, meetings, controlled
experiments, CSRS.

\begin{abstract}
  
  Software review is a fundamental component of the software quality
  assurance process, yet significant controversies surround the most
  efficient and effective review method. A central question surrounds the
  use of meetings; traditional review practice views them as essential,
  while more recent findings question their utility.  To provide insight
  into this question, we conducted a controlled experiment to
  assess several measures of cost and effectiveness for a meeting and
  non-meeting-based review method.  The experiment used CSRS, a computer
  mediated collaborative software review environment, and 24 three person
  groups.  We found that the meeting-based review method studied was
  significantly more costly than the non-meeting-based method, but that
  meeting-based review did not find significantly more defects than the
  non-meeting-based method. However, the meeting-based review method was
  significantly better at reducing the level of false positives, and
  subjects subjectively preferred meeting-based review over
  non-meeting-based review.  This paper presents the motivation for this
  experiment, its design and implementation, our empirical findings,
  pointers to Internet repositories for replication or additional analysis
  of this experiment, conclusions, and future directions.

\end{abstract}

\newpage

\ls{1.5}

\section{Introduction}

Formal technical review (FTR) is an umbrella term for a variety of structured
group processes designed to assess and improve the quality of a software
work product.  While the value of formal technical review (and its most
popular form, inspection) to software quality improvement is undisputed,
debate about the most effective review procedure is increasing.  Such
controversy is recent; until a few years ago, structured group review of
software work products was virtually equated with the inspection method
invented by Michael Fagan \cite{Fagan76,Fagan86}.  As the potential of
formal technical review has become better understood, a plethora of
alternatives have been proposed by researchers and practitioners,
including Active Design Review \cite{Parnas87}, 
Two-Person Reviews \cite{Bisant89}, Verification-Based Inspection
\cite{Dyer92}, Phased Inspections \cite{Knight93}, and N-fold Inspection
\cite{Martin90}.

Perhaps the most fundamental procedural constant of Fagan inspection and
its many variants is the review meeting, where the review team, after some
preparation, discusses the work product in a face-to-face manner and notes
as many defects as possible.  Review meetings are often considered
essential to the effectiveness of formal technical review, primarily
because they make possible a ``synergism'' among the review team that can
lead to the discovery of defects not found by any of the participants working
individually \cite{Ackerman89,Weller93}.  Fagan refers to this as the ``Phantom Inspector,'' and some
review forms actually provide a check box to indicate whether or not the
``Phantom'' attended the meeting.  Other reasons for holding a review
meeting include education, clarification, and an imposed deadline
\cite{Gilb93,Strauss94}.

However, meetings are a costly component of an already costly
process which has been shown to add 15-20\% overhead onto development
\cite{Russell91}. Meetings are costly because they require the simultaneous
attendance of all team members, and their effectiveness requires
satisfaction of many conditions, including adequate preparation, efficient
moderation, readiness of the work product for review, and cooperation among
group members.  Furthermore, simply scheduling a time for a meeting of the
review team has been shown to lengthen the start-to-finish time for
inspection by almost a third in one development group \cite{Votta93}.
Presumably, long inspection intervals lead to longer overall development
intervals, with potential costs when time-to-market for a
product is a critical factor.

As a result, some researchers and practitioners have proposed a fundamental
change to the formal technical review process: the radical restructuring or
elimination of meetings altogether.  The proponents of this position claim
that the benefits of meetings have been exaggerated and that alternatives
(such as two person ``depositions'') are more cost effective.  Although the
evidence presented is substantial, far more research is required to truly
understand the implications of such a recommendation. Specifically, the
case studies published so far present only half the story: discussing
either the strengths of a non-meeting-based method \cite{Parnas87} or the
weaknesses of meeting-based methods \cite{Votta93}. Until now, no study has
provided a controlled, side-by-side comparison designed to compare meeting-based and
non-meeting-based software review and assess both their costs and
benefits directly.

This paper presents the results of such a controlled experiment designed
to provide insight into the strengths and weaknesses of meetings for
formal technical review. The study compared various measures of cost and
effectiveness for a meeting-based review method and a non-meeting-based
review method.  Analysis of the data determined that the meeting-based
method was significantly more costly, but was also significantly more effective
in filtering out false positives.  However, we were unable to observe a
significant difference in defect detection effectiveness between the two
methods.  Finally, subjects strongly preferred the meeting-based method
and perceived it as enabling higher review quality, even though the empirical
data did not support this conclusion.  These findings suggest that 
the decision to use or discard meetings is more complicated than either 
traditional proponents, who believe it mandatory, or the 
recent review revisionists, who would eliminate it altogether.

The remainder of this paper is organized as follows.  The next section
motivates our study with a discussion of prior research related to the
costs and benefits of review meetings. The following section presents the
experiment, including the design, instrumentation, and procedures. The
final sections present the results, our conclusions, and proposals for
future research.

\section{Related work} 

Although reviewing software is as old as programming itself, the first
structured, measurement-based group process for software review was
Michael Fagan's inspection method \cite{Fagan76}. Fagan
inspection essentially consists of a five step process:

\begin{itemize}
\item {\em Overview:} the author presents an overview of the scope and
purpose of the work product.

\item {\em Preparation:} reviewers analyze the work product with the goal
of understanding it thoroughly. 

\item {\em Inspection meeting:} the inspection team assembles and the 
reader paraphrases the work product. Reviewers raise issues that are 
subsequently recorded by the scribe. 

\item {\em Rework:} the author revises the work product. 

\item {\em Followup:} The moderator verifies the quality of rework and 
decides if reinspection is required. 

\end{itemize}

Two aspects of Fagan inspection are especially relevant to the question of
the effectiveness of meetings in formal technical review. First, the goal
of the preparation phase is to gain a thorough understanding of ``intent
and logic'' the work product, {\em not} to identify defects. It is only
during the inspection meeting phase that defect identification becomes an
explicit goal.  Fagan notes that ``sometimes flagrant errors are found
during [preparation], but in general, the number of errors found is not
nearly as high as in the [inspection meeting]'' \cite{Fagan76}.  Second,
the inspection meeting involves a specific technique, paraphrasing, which
generates an in-depth analysis of the entire document in real-time by the
review team during the meeting.

These two factors, the preparation goal and the meeting technique, have
been manipulated extensively in the design of new FTR methods by other
researchers and practitioners.  One common modification is to introduce
defect detection as an explicit goal in preparation. In these cases, the
reviewers note defects on a preparation form or on the work product itself
prior to the inspection meeting. Thus, reviewers now have two goals to
satisfy during preparation: comprehension of the work product and detection
of defects.

A second modification is to change the meeting technique from paraphrasing
to defect collection \cite{Gilb93,Humphrey90}.  This shifts the focus of the
meeting away from the work product and onto the issues raised during
preparation. 

The Active Design Review technique \cite{Parnas87} invented by David Parnas
and David Weiss makes even more radical modifications to the preparation
goals and meeting technique. Active Design Reviews were designed to address
several perceived weaknesses of inspection methods, among them:

\begin{itemize}
\item Despite Fagan's desire that preparation to focus on comprehension,
Parnas and Weiss observed that many reviewers in Inspection meetings did
not always understand the review document. If reviewers do not adequately comprehend the document, then they are
      unlikely to discover the important defects. 
\item Each reviewer should have a specialized area of concern to minimize 
      overlap and maximize coverage of the work product.
\item A meeting of the whole group is unnecessary for defect collection. 
\end{itemize}

Active Design Reviews address these issues by requiring reviewers to fill
out individually customized questionnaires during preparation that assess
their comprehension of the work product and point them toward areas prone
to defects.  The group meeting is eliminated. Instead, the author meets
with each reviewer individually to go over their questionnaires and gather
feedback on the work product. Parnas and Weiss deployed this method for the design
of a military flight navigation system with favorable results, although he
did not report any quantitative measures of effectiveness.

Larry Votta built upon Parnas' research in a study of development groups at 
Bell Labs of Lucent Technologies Inc. (formerly AT\&T Bell Laboratories)
\cite{Votta93}.  He collected data on the perceived utility of
meetings by developers as well as several statistics on their outcome. His
data showed that within the development environment studied, scheduling
conflicts appeared to lengthen the total time of an inspection by
approximately 30\%.  Furthermore, he was unable to demonstrate the presence
of ``synergism'' within the inspection meetings---the number of new issues
raised during the meeting appeared to be cancelled out by the number of issues
found during preparation that were ``lost'' (i.e.,~not recorded) during the
subsequent meeting.  A related study by Votta and others found that 90\% of
the defects were found during the preparation phase, leaving only 10\%
discovered during the meeting \cite{Eick92}.  These results appear to
support Parnas' claim that whole group meetings are unnecessary for 
high quality, cost effective defect detection.

Parnas' claim and Votta's results stand in direct contradiction to those of
Fagan.  While Fagan observed that many more errors are found at the
inspection meeting, Votta and his colleagues observed the opposite.  This
conflict in findings might be due to differences in the goals and
techniques for preparation and meeting between the two methods.  In Fagan
inspection, the objective of preparation is comprehension, and defect
discovery does not become an explicit goal until the inspection meeting. In
both Active Design Reviews and the inspection method as practiced by
development groups at Lucent, the objective of preparation is both
comprehension and defect discovery. Defect collection, not discovery, is
the primary goal of the inspection meeting.  Given this difference in
objectives, it is not surprising that Fagan found the inspection meeting so
productive for defect discovery, while Votta et al.~did not.

Thus, Votta's work provides evidence that group meetings may not be
necessary for an inspection method whose meeting goal focuses on defect
collection. However, it does not show that meetings are not useful when
their goal is defect discovery through paraphrasing. Furthermore, Fagan
asserts that ``a team is most effective if it operates with only one
objective at a time.''  If Fagan is right, then perhaps the mixing of
comprehension and defect discovery during preparation by Lucent
developers leads to decreased review effectiveness.

If Votta's study indicates that whole group meetings do not contribute
significantly to review effectiveness when the meeting goal is defect {\em
  collection}, the next step is to determine if whole group meetings
contribute significantly to review effectiveness when the meeting goal is
defect {\em detection}.  To provide insight into this issue, we designed
and carried out a controlled experiment to assess whether or not a
``real group'' software review method that used paraphrasing for the
purpose of defect detection can outperform a ``nominal group'' software
review method that also used paraphrasing for the purpose of defect
detection.  

A ``real group'' review method is one in which the group members meet
face-to-face and interact with each other in order to detect defects in the
work product. A ``nominal group'' review method is one in which the group
members work individually without interacting with each other, and the
results of their individual efforts toward defect detection are simply
pooled together as the group's result.  Although there is prior research on
real vs. nominal group performance, these studies have focussed on idea
generation, not software review \cite{Diehl87,Mullen91}.

If Fagan's results generalize, then the real group review method should
outperform the nominal group review method.  If Votta's results generalize,
then the opposite outcome should occur. The experiment, and our results,
are presented next.

\section{The experiment}

Our experiment compared the performance of a real group software review
method and a nominal group software review method.  The main research
question was:

\ls{1.0}
\begin{quotation}
  
%  Are there differences in detection effectiveness (the number of program
%  defects detected) and detection cost (the amount of effort/time to find a
%  defect) between subjects who review source code using a synchronous,
%  same-place same-time interactive method (a ``real group'' method) and subjects
%  who review source code using an asynchronous, same-place same-time
%  non-interactive method (a ``nominal group'' method)?
  
  Are there differences in defect detection effectiveness and defect detection cost
  between subjects who review source code using a synchronous, same-place
  same-time interactive method and subjects who review source code using an
  asynchronous, same-place same-time non-interactive method?

\end{quotation}
\ls{1.5}

We also explored several other questions, including whether the two
methods differ in their ability to detect certain classes of defects;
whether the two methods differ in their ability to detect ``false
positives'' (issues raised as defects that are not actual defects); the level of
defect discovery duplication in nominal groups; the level of synergism in
real groups; the levels of reviewer satisfaction with each method; and the
levels of reviewer confidence in the outcome with each method.  (Additional
research questions considered in this experiment are discussed in Danu
Tjahjono's Ph.D. thesis \cite{csdl-95-08}.)

The subjects were 27 undergraduate students enrolled in ICS-313
(Programming Language Theory) and 45 undergraduate students enrolled in
ICS-411 (System Programming) classes at the University of Hawaii in the
Spring of 1995. The subjects were assigned to groups of size 3, for a total
of 24 different groups.  Each group performed two reviews, once using the
real group review method and once using the nominal group review method.

To help obtain a controlled experiment, we implemented the real group and
nominal group review methods using CSRS, the Collaborative Software Review
System \cite{Johnson94}.  CSRS helped us to ensure that each review method
was carried out in a consistent and standard fashion by each review group.
CSRS also helped us to minimize the differences between the two review
methods apart from those considered in the experimental design. The real
group method and corresponding software support is called EGSM
(Experimental Group Synchronous Method) and the nominal group method and
software is called EIAM (Experimental Individual Asynchronous Method).


\subsection{Experimental design}

The experimental design consists of one factor (group interaction) with two
treatments: real group interaction and nominal group interaction.  The
design is balanced: each group reviews two sets of source code using one of
two different group interactions.

We carried out the experimental design in two rounds, the first round using
the ICS-313 students and the second round using the ICS-411 students. The
source code reviewed by the students was based upon an amalgam of recently
completed exercises in the two classes, so that the students were 
familiar with both the problem domain and the software's design, though not
the actual source code itself. The ICS-313 groups reviewed two portions of
an Employee database application written in C++, which they developed 
to learn about object orientation in C++.  The ICS-411 groups
reviewed two portions of a two-pass assembler written in C as part of the 
normal curriculum in systems programming.

Figure \ref{design} illustrates the experimental design 
for each of the two rounds. 

%\begin{figure}[htb]
%    \small
%  \begin{center}
%    \begin{tabular} {|l|p{1.2in}|p{1.2in}|}
%      \multicolumn{3}{c}{{\bf Round 1: ICS-313 Groups}}\\
%      \hline
%      & {\bf Employee1} & {\bf Employee2}\\
%      \hline
%      & & \\
%      {\bf EGSM} & G1$^1$, G6$^1$, G8$^1$, G9$^1$ & G2$^2$, G3$^2$, G4$^2$, G5$^2$, G7$^2$ \\
%      & & \\
%      \hline
%      & & \\
%      {\bf EIAM} & G2$^1$, G3$^1$, G4$^1$, G5$^1$, G7$^1$ & G1$^2$, G6$^2$, G8$^2$, G9$^2$ \\
%      & &  \\
%      \hline
%      \multicolumn{3}{c}{{\bf Round 2: ICS-411 Groups}}\\
%      \hline
%      & {\bf Pass-1} & {\bf Pass-2}\\
%      \hline
%      & & \\
%      {\bf EGSM} & G3$^2$,G4$^2$,G9$^2$,G10$^2$, G11$^1$,G12$^2$,G13$^1$ & G1$^2$,G2$^2$,G5$^2$,G6$^1$, G7$^2$,G8$^1$,G14$^1$,G15$^2$\\
%      & & \\
%      \hline
%      & & \\
%      {\bf EIAM} & G1$^1$,G2$^1$,G5$^1$,G6$^2$,
%      G7$^1$,G8$^2$,G14$^2$,G15$^1$ & G3$^1$,G4$^1$,G9$^1$,G10$^1$, G11$^2$,G12$^1$,G13$^2$ \\
%      & &  \\
%      \hline
%     \end{tabular}
%  \end{center}
%  \caption{Source code and group assignments for the two rounds. ``G{\em n}''
%  indicates a group. The
%  superscript indicates the order in which the source code was reviewed.}
%  \label{design}
%  \normalsize
%\end{figure}

\begin{figure}[htb]
    \small
  \begin{center}
    \begin{tabular} {|l|p{2.5in}|p{2.5in}|}
      \multicolumn{3}{c}{{\bf Round 1: ICS-313 Groups}}\\
      \hline
      & {\bf Employee1} & {\bf Employee2}\\
      \hline
      & & \\
      {\bf EGSM} & G1$^1$, G6$^1$, G8$^1$, G9$^1$ & G2$^2$, G3$^2$, G4$^2$, G5$^2$, G7$^2$ \\
      & & \\
      \hline
      & & \\
      {\bf EIAM} & G2$^1$, G3$^1$, G4$^1$, G5$^1$, G7$^1$ & G1$^2$, G6$^2$, G8$^2$, G9$^2$ \\
      & &  \\
      \hline
      \multicolumn{3}{c}{{\bf Round 2: ICS-411 Groups}}\\
      \hline
      & {\bf Pass-1} & {\bf Pass-2}\\
      \hline
      & & \\
      {\bf EGSM} & G12$^2$,G13$^2$,G18$^2$,G19$^2$, G20$^1$,G21$^2$,G22$^1$ & G10$^2$,G11$^2$,G14$^2$,G15$^1$, G16$^2$,G17$^1$,G23$^1$,G24$^2$\\
      & & \\
      \hline
      & & \\
      {\bf EIAM} & G10$^1$,G11$^1$,G14$^1$,G15$^2$,
      G16$^1$,G17$^2$,G23$^2$,G24$^1$ & G12$^1$,G13$^1$,G18$^1$,G19$^1$, G20$^2$,G21$^1$,G22$^2$ \\
      & &  \\
      \hline
     \end{tabular}
  \end{center}
  \caption{{\em Source code and group assignments for the two rounds. 
           EGSM supports the real group method and EIAM supports the 
           nominal group method.
           Employee1, Employee2, Pass-1, and Pass-2 are the work products reviewed.
           G{\em n}  represents one of the 24 three-person groups. 
           The group superscript indicates the order in which the source code was
           reviewed. For example, G10$^1$ in the table above indicates that
           Group 10 reviewed the Pass-1 source code first. }
         }


  \label{design}
  \normalsize
\end{figure}

\subsection{Variables}

The independent variable in this experiment is the {\em type of group interaction},
with two treatments, a real group method (EGSM) and a nominal group method
(EIAM). 

For the main research question, the experiment obtained data on two
dependent variables, or review measures: {\em defects}, the total number of
distinct, valid defects detected by a group; and {\em effort}, the total
review time spent by a group.  For other research questions, we obtained
values for several additional dependent variables, including: {\em false
  positives}, the number of invalid defects recorded by the group; {\em
  duplicates}, the number of duplicate defects found during nominal group
review; and {\em synergism}, the number of defects found solely through interaction
of two or more people during real group review.

\subsection {Threats}

\subsubsection{Threats to internal validity}


Threats to internal validity are those factors that may affect the values
of the dependent variables apart from the setting of the independent
variable.  We considered three types of threats to internal validity in the
design of this experiment: selection effects, learning effects, and
instrumentation effects. A selection effect is any difference between
individuals in the different treatment groups such that differences in
dependent variables ensue. A learning effect (or maturation effect) is any
naturally occurring process within the subjects that could cause a change
in their performance, and thus affect the dependent variables.  An
instrumentation effect is any change in the measurement devices or tasks
being measured that leads to a change in subject performance, thus 
affecting the dependent variables. 

To reduce selection effects in the ICS-313 round, we rated each
individual's skill as low, medium, or high, based upon their grades in
prior assignments. We then selected a member at random from each category
to form groups of three. To reduce selection effects in the ICS-411
round, we chose individuals at random to form groups of three. 
These techniques were used to create groups with approximately equal 
aggregate programming skill.

We randomized the order in which the two review methods were presented to
groups to reduce learning effects. These effects were also reduced through
a training session prior to the experiment in which subjects practiced the
use of CSRS and the software review methods. The goal of the training
session was to ensure that groups did not become dramatically better at
review simply because of the experimental process itself. The goal of
randomizing the presentation of treatments was to ensure that, if such
learning effects still did occur, the effect on the dependent variables for
each treatment would tend to cancel each other out.

Finally, we reduced instrumentation effects by minimizing the differences
between the two documents inspected. In both rounds, we ensured that the
two documents were of approximately the same size and had approximately the
same numbers of defects of the same types. We also reduced instrumentation
effects by having all groups inspect both documents.  In addition, in the
ICS-411 round, we also ensured that defects of the same type occurred in
the same relative location.

\subsubsection{Threats to external validity}

Threats to external validity are those factors that limit the applicability
of the experimental results to industry practice. Such threats include: the
student reviewers may not be representative of professional programmers;
the software reviewed may not be representative of professional software;
and the inspection process may not be representative of industrial
practice. 

These threats are valid, and limit the direct application of these results
to industry practice.  Overcoming the first two threats is best
accomplished by replication of this study using industrial programmers with
real work products. To support this replication, our experimental materials
and apparatus are freely available via the Internet. Section
\ref{sec:replication} provides a pointer to these materials.  

The third threat is clearly significant, since few industrial inspection
processes are computer-mediated, much less use the CSRS software package
employed in this study.  To help lessen the impact of this threat, we based
our experimental review methods on descriptions of industrial practice of
software review, such as Fagan inspection \cite {Fagan76} and more recent
methods such as Gilb's Inspection \cite{Gilb93}. Despite the presence of
an on-line system, subjects still carried out review in the traditional
fashion: paraphrasing the document, meeting in a group or working
individually, and noting discovered defects on a form.  The methods did not
employ mechanisms (such as electronic voting and asynchronous issue
resolution discussion) that are found in more ``purely'' computer-mediated
review methods like FTArm \cite{Johnson94}.

Nevertheless, significant differences remain between common
industrial inspection practice and the computer-mediated review methods
used in this study. For example, since our focus was on differences between
individuals and groups with respect to defect detection effectiveness, we
eliminated phases such as rework from the review methods. Since we
developed the instruments, there was no author role. Section 
\ref{sec:conclusions} revisits these threats in light of the 
data we collected and analyzed. 

\subsection{Analysis strategy}

Most of the research questions were tested using the Wilcoxon signed rank
test \cite{Ferguson89}. This non-parametric test of significance does not
make any assumption regarding the underlying distribution of the data.  It
is based on the rank of differences between each pair of observations in
the dataset.

The data analysis proceeds in the following way. Assume that the data are a
set of N paired observations on X and Y. (As an example, the number of
defects found by Group G1 using EIAM along with the number of defects found
by group G1 using EGSM would generate one paired observation.)  The
difference, d, between each pair is calculated.  If the two observations in
a pair are the same, then d = 0 and the pair is deleted from the analysis.
The remaining d's are then ranked without regard to sign; that is, the absolute
values $|X_{i} - Y_{i}|$ are ranked. A rank of 1 is assigned to the
smallest d, of 2 to the next smallest, and so on. The sign of the
difference d is then attached to each rank. Denote the sum of the positive
ranks by $W_{+}$ and the sum of the negative ranks by $W_{-}$. The normal
deviate z (z-value) is given by

\small
 \( z = \frac{W - \frac{N(N+1)/4}{4}}{\sqrt{\frac{N(N+1)(2N+1)}{24}}} \),
where $W = W_{+}$ if $W_{+} \leq W_{-}$ else $W_{-}$.

\normalsize

The p-value of z is then used to test the null hypothesis concerning X and
Y, that is, that there is no significant differences between X and Y.  If
the p-value is less than the significance level $\alpha$ = 0.05, then we
reject the null hypothesis, and can conclude that there is a significant
difference between X and Y.

\subsection{Experimental instrumentation}

We developed two basic instruments for this experiment: the source
code materials reviewed by the subjects, and the experimental apparatus
using CSRS. 

\subsubsection{Source code review materials}

The experimental review materials were based on programs recently
implemented by the students themselves.  Two sets of software of
approximately the same size were selected.  We revised the code extensively
to introduce defects and recompiled it to ensure that it had no syntax
errors.  For both studies, we attempted to seed sufficient numbers of
defects to prevent a ceiling effect where groups would detect all the
defects in the materials.  As the results show, this ceiling effect was not
present, since no group found more than 60\% of the defects present. We
provided natural language specifications for each procedure and function.

In both rounds, the defects concerned logic, computation, and data handling
problems, such as missing or incorrect condition tests, forgotten cases or
steps, or incorrect data access. Some of these defects were specific to
the C/C++ languages, such as memory leaks.  None of the defects, however,
involved an incorrect specification. The subjects were instructed that when
the code did not conform to the specification, then the specification
should be assumed correct, and the code was therefore incorrect.

For the ICS-313 round, the programs implemented an Employee database using
the C++ programming language. One program (Employee1) used an array
implementation of the database, and the other (Employee2) used a
linked-list implementation.  We seeded both sets of source code with
natural defects, in other words, defects previously made by the students
themselves.  We obtained these defects by asking students to submit their
programs right after the first successful compilation, which occurred in
class several weeks prior to the experiment.  We then extracted defects
from all of the programs submitted, and created two composite
implementations that each included 20 defects. By the end of the
experiment, to our surprise, the groups had detected three previously
undiscovered defects in the array implementation and five previously
undiscovered defects in the linked list implementation.

For the ICS-411 round, the programs implemented Pass-1 and Pass-2 of a
two-pass assembler using the C programming language. As in the ICS-313
round, we obtained first compilation implementations from the students,
extracted defects and then created composite implementations with seeded
defects.  Unlike the ICS-313 round, these defects were also seeded in the
same relative location.  For example, when a defect of type ``uninitialized
variable'' was seeded in the top-level function for the Pass-1
implementation, another defect of type ``uninitialized variable'' was also
seeded in the top-level function for the Pass-2 implementation.  We
initially seeded 19 defects in each of the two programs, but the end of the
experiment, we documented one additional defect in the Pass-1 source code.



\subsubsection{Experimental apparatus}

To help ensure that all groups carried out review the same way, and to 
facilitate data collection, we used the CSRS computer-mediated software
review environment as the experimental apparatus for this study.  The
CSRS data and process modeling languages facilitated the implementation of
two review methods whose aim was to differ only with respect to the type of 
group interaction.

Figure \ref{fig:egsm-screen} shows a screen image from the EGSM review from the
Pass-2 assembler source.  In both methods, CSRS provides subjects with this
three window user interface, where the set of functions/procedures to be
reviewed are shown in the upper right screen, the function or procedure
currently under review is shown in the left screen, and defects raised by
reviewers are entered in a commentary window in the lower right screen.


\begin{figure*}[htb]
{\centerline {\psfig{figure=egsm.ps}}}
 \caption{{\em An EGSM screen image from the ICS-411 round.}}
 \label{fig:egsm-screen}
\end{figure*}


The EIAM interface shown in Figure \ref{fig:eiam-screen} differs only slightly
from the EGSM interface in Figure \ref{fig:egsm-screen}.  While all issues
generated by any reviewer are visible to the entire review group
in the EGSM method, each EIAM reviewer has access only to the issues she
generates herself. This affects the set of
links displayed at the bottom of the left screen. Similarly, the
Criticality and Confidence-level fields (in the lower right screen) are
public in EGSM, and private in EIAM. Thus, these fields display the values
for all members in the group in EGSM, and only the single reviewer's value
in EIAM.  Finally, EGSM includes a field called ``Suggested-by'' (in the
lower right screen) that allows each reviewer to indicate who suggested the
issue, and is used to measure group synergism. This field is not included in
EIAM, since synergism is not present by definition.


\begin{figure}[htb]
 {\centerline{\psfig{figure=eiam.ps,width=6in}}}
 \caption{{\em An EIAM screen image from the ICS-411 round.}}
 \label{fig:eiam-screen}
\end{figure}


\subsection{Experimental procedures}

\subsubsection{Training}

All subjects attended a set of lectures on formal technical review. This
lecture explained the goals of formal technical review and the specific
procedures to be used in this study.  The training was based upon software
review tutorial materials we developed for use in industry.

The subjects were then assembled into three person teams according to the
procedures specified above. They next attended a two hour training session
to familiarize themselves with the CSRS review environment and the EGSM and
EIAM review methods. During this session, they practiced review on sample
source code implementing a ``BigInteger'' data abstraction. They also
practiced the use of paraphrasing as a mechanism to analyze software and
discover defects.

\subsubsection{General review procedures}

Both EGSM and EIAM consist of a single review phase, whose objective is
defect detection.  Subjects were told to simply note the presence of any
defect found, and to not attempt to determine how to correct the defect.


Since all subjects had recently completed the implementation of a program
quite similar to the review materials, there was no need for a
``preparation'' phase (typical in real-world review methods) to acquaint
reviewers with the review materials. Their prior work with the
program also meant that they would not have to spend time during the review
itself to acquire an understanding of problem domain, programming language,
or program design. Following Fagan's approach, this design enabled the
subjects to devote their entire attention to defect detection, as opposed
to a mixture of comprehension and defect detection.

Due to the experimental objectives, we could also safely eliminate the
rework and followup phases typical in real-world review methods.

Both methods used the paraphrasing method from Fagan inspection as the
analysis technique.  For EGSM, one of the three subjects in each group was
assigned to the role of Presenter, and she orally summarized the
source code in a line-by-line fashion.  The presenter also acted as a
reviewer and was free to discover defects. For EIAM, subjects were 
trained to paraphrase the source code silently to themselves.

In EGSM, the subjects collaborated fully with each other. As the Presenter
paraphrased the code aloud, any of the review team members were free to interrupt
with suggestions of potential defects. Others would then confirm or reject
the suggestion.  If disagreement continued, the team would vote on whether
or not to include the issue as a defect.  The Presenter was the only one
who could enter issues, and all review screens were kept synchronized. This
prevented reviewers from ``wandering off'' into the code and kept the
reviewers together.

In EIAM, subjects worked entirely independently, raising issues and noting
them by themselves. For administrative purposes, each EIAM team did meet in
the same room at the same time, but no interaction between members was
allowed.

In all review sessions, one of the authors (Danu Tjahjono) acted as an
``external moderator.''  The purpose of this role was to guarantee
consistent and correct execution of the review methods. The external
moderator performed such tasks as ensuring that paraphrasing was used in
EGSM, that the subjects remained focussed on the review task, and that any
questions about the CSRS user interface could be answered quickly and
correctly.

The experimental design limited the review time for each session to a
maximum of three hours.  However, no data were affected by this ceiling,
since every review team or reviewer completed the task within 2.75 hours.

\subsubsection{Review environment}

Prior to the experiment, the source code for each program under review was
divided into separate hypertext nodes for each structural element (function
definition, class declaration, variable declaration, etc.).  These nodes
were stored a separate CSRS database for each group as a set of ``source''
nodes. The subjects inspected these source nodes for defects.  When defects
were discovered, they were recorded and stored in the CSRS database as
``issue'' nodes. The CSRS system also stored additional metric data in
these databases, such as the time spent on review.  

For each review method, the typical procedure for using CSRS 
involved the following.

\begin{enumerate}
  
\item After connecting to the database, the system displays three screens:
  Source, Summary and Commentary.  The Source and the Commentary screens
  are initially empty.  The Summary screen displays all source nodes that
  needed to be reviewed with their status, which is initially set to
  ``unread.''
  
\item The presenter then selects a source node to be reviewed. (The
  ``presenter,'' in the EGSM method, was one member of the review group.
  The ``presenter'' in the EIAM method was the reviewer.)  We did
  not attempt to control the order in which source nodes were reviewed:
  participants were able to review the program in any order, and to revisit
  previously reviewed nodes for further review at any time.  The default
  order presented in the Summary screen follows the natural calling
  hierarchy of the program, and was the order typically used by the groups.

  
  A source node, once selected from the Summary screen, appears in the
  Source screen. The presenter then uses the paraphrasing technique
  to find as many defects as possible in the source node.
  
  
\item When a defect is discovered in the source node, the presenter creates
  an issue node, fills out every field in the node, and saves the node.
  
  In EGSM, a subject who spots a potential defect interrupts the
  presenter. The group then discusses the validity of the defect.  If a
  majority of the group members agree on the validity, the presenter
  creates an issue node and describes the defect in the ``description''
  field. The entire group then individually fills out the remaining fields
  (Criticality, Suggested-by, and Confidence-level).  As with source nodes,
  issue nodes are also displayed on all subjects' screens synchronously.

  In EIAM, the subject creates an issue node and fills out all the fields.
  
\item When no more issues can be detected in the source node, the presenter
  then marks the status of the source node to ``reviewed,'' and selects
  another source node for review.

\item When all source nodes are reviewed, the review session ends. 

\end{enumerate}



The precise differences between the behavior of CSRS when enacting the 
EGSM method and the EIAM method are as follows:

\begin{itemize}
\item The EGSM Source screen displays the line number where the cursor is
  located in the mode line. (The ``mode line'' is Emacs terminology for a
  status line displayed in reverse video at the bottom of a window.) This
  line number is used by the presenter in EGSM to direct the attention of
  reviewers to a specific location in the source code. This line number is
  not displayed in the EIAM method.

\item All source nodes include an ``Issues'' field, which contains links to
  the issue nodes raised during review of that source node. In EIAM,
  these references are private to each reviewer. In EGSM, all reviewers see
  all issues created during the trial.
  
\item The ``Criticality'' field in issue nodes describes the degree of
  significance of each issue. In EIAM, the values provided
  are  high, medium, or low. In EGSM, the values provided are high, medium,
  low, or none. This final value is present in EGSM in order to allow a 
  reviewer to indicate her disagreement with the group's decision to
  include this issue as a defect. In EIAM, the ``none'' value is not needed
  since an individual reviewer doesn't create an issue node unless she has
  decided that the issue represents a defect.
  
\item The ``Confidence-level'' field in issue nodes describes the subject's
  confidence in their assessment of the issue. For the same reasons
  provided above for the criticality field, the EIAM method supports high,
  medium, and low, while the EGSM method supports high, medium, low, and
  none.

  
\item The ``Suggested-by'' field in issue nodes is only provided for the
  EGSM method.
  This field represents each reviewer's perception of who 
  in the group initially suggested the issue. The values provided are:
  \begin{enumerate}
  \item Me
  \item Me but inspired by others
  \item Other but also occurred to  me
  \item Other and had not occurred to me
  \end{enumerate}
  
  This field is private to each member.  The suggested-by field is used to
  identify occurrences of group synergism in defect detection. 
  
\item The ``Subject,'' ``Lines,'' and ``Description'' fields describe the
  nature of the defect. In EGSM only the presenter can fill out these
  fields.

\end{itemize}




\subsubsection{Round 1: ICS-313}

For the ICS-313 round, 27 students participated and were split into 9
groups. Four of the groups were randomly chosen to use the EGSM method to
review the array implementation of the employee database, while the remaining
five groups used EIAM to review the same source materials.  All groups then
switched methods and reviewed the linked list version of the employee
database. The ICS-313 round was completed within two weeks. 

\subsubsection{Round 2: ICS-411}

For the ICS-411 round, 45 students participated and were split into 15
groups. Seven of the groups were randomly chosen to use the EGSM method
first, while the other eight groups used the EIAM method first.  The 
ordering of review material was also randomly assigned.  All groups
then switched both review method and source material for their second
review session. The ICS-411 round was also completed within two weeks. 

\subsection{Data collection}

We collected data through two mechanisms: CSRS and questionnaires filled
out by all subjects at the end of each review session.  CSRS stored
defect and metric data for each group in a separate internal database.
Access to the complete raw data set is provided at the web site 
for this article. 

For each real group, the value of the dependent variable {\em defects} was
calculated as the total number of defects entered into CSRS by the group,
minus those that we manually determined to be {\em false positives}.

For each nominal group, the value of {\em defects} was calculated by summing
all of the defects found by the individuals in a particular group, then
subtracting both those defects that we manually determined to be {\em false
  positives} as well as any defects we determined to be {\em duplicates},
i.e., found by more than one member of the group. 

For all groups, the value of {\em effort} was calculated as the sum of the
total time spent on review by each member of the group.  CSRS used a
timestamp-based mechanism to automatically record the time spent 
using the system by each reviewer. 

For each real group, the value of {\em synergism} was determined by analysis
of the value of the ``Suggested-by'' field for each recorded defect. The
Suggested-by field had four possible values: ``Me,'' ``Me, but inspired by
others,'' ``Other but also occurred to me,'' and ``Other and had not
occurred to me.''  We decided upon a conservative operational definition
for synergism: Synergism occurs only when none of the reviewers record
``Me'' as the value of the ``Suggested-by'' field.

Finally, each subject filled out three questionnaires during the study.  A
questionnaire evaluating the subject's attitudes towards the EGSM method
and the EGSM review group experience was administered after the EGSM
review. A similar questionnaire on EIAM was administered after the EIAM
review. A final questionnaire evaluating subject preference for EIAM or
EGSM, and their satisfaction with CSRS was administered at the end of the
study.  Most of the questions required subjects to respond by circling
one number on a five point scale, although a few questions were open ended
and asked for explicit commentary. 


\section{Experimental results}


\subsection{Cost and effort}

Figure \ref{fig:wilcoxon-results} summarizes the results of comparing the
performance of the real group review method (EGSM) to the nominal group
review method (EIAM) for the major dependent variables using the Wilcoxon
signed rank test.  We show the results from analyzing the data for each
round separately and when grouped together.

A ``-'' in a column indicates that we were not able to detect a significant
difference between EGSM and EIAM for the review measure. A
``$>$'' indicates that EGSM performance was significantly
higher (p$<$.05) than EIAM performance for the corresponding
review metric, while a ``$<$'' indicates that EGSM performance was
significantly lower (p$<$.05) than EIAM performance.


\begin{figure}[ht]
  \small
  \begin{center}
  \begin{tabular}{|l|c|c|c|}
    \hline
  \multicolumn{4}{|c|}{EGSM vs. EIAM}\\
   \hline
Review Measure    & ICS-313 & ICS-411 & All \\
   \hline
{\em \% Defects Found}         & -       & -       & - \\
{\em Cost}     & $>$     & -       & $>$ \\  
{\em Effort}          & $>$     & -       & $>$ \\
{\em Issues Raised}          & $<$     & $<$     & $<$ \\
{\em \% False positives} & $<$     & $<$     & $<$ \\
  \hline
   \end{tabular}
  \end{center}
 \caption{{\em Summary of major hypothesis outcomes using the
Wilcoxon signed rank test. Figure \ref{fig:main-hypothesis} provides
a detailed breakdown of individual group values for each of these hypotheses.}}
 \label{fig:wilcoxon-results}
\normalsize
\end{figure}

The significance results in Figure \ref{fig:wilcoxon-results} were
generated from analysis of the raw statistics presented in Figure
\ref{fig:main-hypothesis}. For the 313 analyses, nine paired observations
were used, while for the 411 observations, 15 paired observations were
used.  Combined analyses used 24 paired observations.  The review time for
the EIAM method is the sum of the group members' review time, while the
review time for the EGSM method is the presenter's time multiplied by the
number of subjects (since the subjects were synchronized with the
presenter).

\begin{figure}[htpb]
 {\centerline{\psfig{figure=jese97-main-hyp2.xls.eps}}}
 \caption{{\em Detailed breakdown of individual group values used to test
major hypotheses, as summarized in Figure \ref{fig:wilcoxon-results}.}}
 \label{fig:main-hypothesis}
\end{figure}


As Figures \ref{fig:wilcoxon-results} and \ref{fig:main-hypothesis} reveal,
we were unable to detect a significant difference between the real and
nominal group review methods with respect to the number of valid defects
discovered. In other words, this study was unable to demonstrate that
review meetings for the purpose of defect discovery outperformed
individuals working independently.  The EGSM method found an average
of 43\% of all known defects, while the EIAM group method found an
average of 46\% of all known defects.

On the other hand, we found that the real group method was significantly
more costly than the nominal group method.  The average effort required per
defect was 41 minutes for EGSM and 34 minutes for EIAM, and the average
aggregate effort for a review session was 5:57 hours for EGSM and 5:11
hours for EIAM.

We also found that individuals working independently using nominal group
review method raised significantly more total issues (an average of 14)
than those working together using the real group method (an average of 
37 for the entire group, or 10 per individual).
However, the nominal group method resulted in a significantly greater
percentage of false positives (an average of 22\% of all raised issues)
than the real group method (an average of 5.3\% of all raised issues).

\subsection{Synergism and duplicates}

Figure \ref{fig:individual-results} presents two of the results from
analysis of measures that apply to a single review method only.
{\em Synergism} indicates the percentage of defects in which
synergism played a role for the set of EGSM review sessions. (Recall that
synergism is defined as occurring when no reviewer indicated that they were
solely responsible for raising the issue.) We found that
synergism participated in the process of defect discovery about a third
of the time overall. 

{\em Duplicates} indicates the average percentage of defects that were
discovered by more than one reviewer in a given EIAM group for the set of
review sessions. Again, about a third of the defects were discovered by 
more than one reviewer during individual review. 

\begin{figure}[ht]
\small
  \begin{center}
  \begin{tabular}{|l|l|c|c|c|}
   \hline
Data                       & ICS-313 & ICS-411 & All \\
   \hline
{\em Synergism (EGSM only)}         & 42\%    & 21\%     & 29\% \\
{\em Duplicates (EIAM only)}      & 29\%    & 31\%     & 30\% \\
  \hline
   \end{tabular}
  \end{center}
 \caption{{\em Selected method-specific measurements}}
 \label{fig:individual-results}
\normalsize
\end{figure}

\subsection{Differences in defect type detection}

In the ICS-411 round, we seeded the two review programs with 19 defects of
the same type and in the same relative location.  This allowed us do a
comparative analysis of the ability of EGSM and EIAM review methods to
detect errors of a specific type.  Figure \ref{fig:defect-class} shows how
we classified the 19 seeded defects in the ICS-411 round into six defect
types.


\begin{figure}[t]
  \begin{center}
    \ls{1.0}
  \begin{tabular}{|c|c|p{3in}|}
   \hline
    ID & Type & Definition \\
   \hline
   C1 & Missing initialization &
        Missing or incorrect initialization of  variables
        (within or across functions).  \\
   C2 & Incorrect operators &
        Incorrect use of C Language operators (for example, using = 
        instead of ==, or using $!$ to check for a non-zero value instead
        of a zero value).\\
   C3 & Incorrect condition &
        Missing or incorrect condition tests (for example, using $<$
        instead of $>$, or using $||$ instead of \&\&).\\
   C4 & Mistyped statements &
        Incorrect conditions or  steps due to mistyped
        statements (for example, a missing keyword ELSE, or incorrect block
        delimiters \{,\}, or repetition of the same statement twice).\\
   C5 & Incorrect algorithm &
        Incorrect use of data structure/algorithm (for example,
        an incorrect binary search algorithm). \\
   C6 & Incorrect computation &
        Incorrect computation or deviation from the specification
        (for example, using + instead of * in an expression, or reading an incorrect
        data item from the file).\\
  \hline
   \end{tabular}
   \ls{1.5}
  \end{center}
 \caption{{\em Defect classifications for the ICS-411 round}}
 \label{fig:defect-class}
\end{figure}



Figure \ref{fig:class-results} shows which defects were caught by the
ICS-411 groups using EGSM and EIAM. 


\begin{figure}[t]
 {\centerline{\psfig{figure=jese97-types.draw.eps}}}
 \caption{{\em Detection frequency for EGSM and EIAM groups in ICS 411
 round classified by defect 
          defect type. For each review (Pass-1, Pass-2), the table lists the number of
          defects found for each of the 19 seeded defects. Each of the 19
          seeded defects are also classified by their type.  The table also
          lists the aggregate frequencies for each type.
 }}
 \label{fig:class-results}
\end{figure}


Using the Wilcoxon test, only the detection of defect type C4 was found to
differ significantly between the two groups, with EIAM groups finding
defects of this type more often than EGSM groups (p$<$0.05).
Defect type C4 is a mistyped statement, which was seeded in defect 7 and
defect 17.  13 of the EIAM groups found defect 17, compared to only 9 of
the EGSM groups. Both groups found defect 7 with approximately equal
frequency (14 EGSM groups versus 15 EIAM groups).

An example of defect 17 is a mistyped keyword in the
following code fragment:

\small
\ls{1.0}
\begin{verbatim}
     Access_Symtab(SEARCH, &symtabret, source.labl, &address);
     if (symtabret == NOTFOUND) {
       address = locctr;
       Access_Symtab(SEARCH, &symtabret, source.labl, &address);
\end{verbatim}
\ls{1.5}
\normalsize

This code is supposed to add a label (source.labl) into a symbol table.
The first call to Access\_Symtab searches the label in the symbol table
to ensure that the label is not already there, then it calls
Access\_Symtab for the second time to store the label.  Instead of
requesting STORE, the second call incorrectly requests a SEARCH again.


Observation during the EGSM session revealed that this defect was often
missed due to poor paraphrasing by the presenter. If the presenter
incorrectly paraphrased the statement with the correct logic, the reviewers
were inclined to follow along with the presenter's interpretation.
                               
An example of defect 7 is a missing ``else'' keyword in an 
if-then-else code fragment:

\small
\ls{1.0}
\begin{verbatim}
     if (ch == 'T')
        source->comline = true;
     source->comline = false; 
\end{verbatim}
\ls{1.5}
\normalsize

Observation indicated that the group did not catch this
defect because the presenter unconsciously added the word ``else'' when
paraphrasing the statement, and again, the reviewers went along
with the presenter.  


\subsection{Qualitative reactions}

After each review method, subjects were asked to fill out a questionnaire
that obtained subjective information concerning their experience with the 
method. At the end of the study, participants filled out a final post-test
questionnaire that asked them questions concerning the comparison of the
EGSM and EIAM methods. 

Figure \ref{fig:quest-results} summarizes some of the
results from analysis of the questionnaire data.  The questionnaire results
are shown pooled for all subjects.  The numbers shown indicate the
percentage of subjects agreeing with the focus of the question by
responding with a 4 or 5 on a five point scale.  Depending upon the way the
question was worded, a response of 4 or 5 corresponded to ``high'' or
``very high,'' ``true'' or ``very true,'' or ``somewhat EGSM'' or
``strongly EGSM.''

\begin{figure}[ht]
\small
  \begin{center}
  \begin{tabular}{|l|c|}
   \hline
Question Focus                   & \% Agreement \\
   \hline
{\em Confident in EGSM review quality} & 78\% \\
{\em Confident in EIAM review quality} & 42\% \\
{\em Satisfied with EGSM method} & 81\% \\
{\em Satisfied with EIAM method} & 61\% \\
{\em More productive using EGSM than EIAM} & 72\% \\
{\em Prefer EGSM over EIAM} & 63\% \\
{\em Prefer EIAM over EGSM} & 7\% \\
{\em No preference between EGSM and EIAM} & 30\% \\
  \hline
   \end{tabular}
  \end{center}
 \caption{{\em Selected questionnaire results}}
 \label{fig:quest-results}
\normalsize
\end{figure}

These results show that over three quarters (78\%) of the subjects felt
confident or very confident in the quality of their review using EGSM,
while less than half (42\%) felt confident or very confident in the quality
of their EIAM review. 81\% were satisfied or very satisfied with the
EGSM
review method. This percentage drops to 61\% for the EIAM review method.
Finally, most subjects felt they were more productive using EGSM than EIAM
(72\%) and most preferred to use EGSM over EIAM (63\%). Only 7\% preferred
EIAM to EGSM.  The comments made by participants support this view:

\ls{1.0}
\begin{itemize}
\item ``Working in a group helped me find errors faster and better.''
\item ``Working in a group helped me understand the code better.''
\item ``You got analysis from  different angles that
others might not see it. If one missed the error, others might find it.''
\item ``Working in a group was better because I was not too familiar with
the code and/or programming language myself; others could contribute
more errors than me.''
\item ``When working alone, I had to rely on my own knowledge, which was
rather limited.''
\item ``Sometimes you got ideas from others that you had never thought
of, and you felt less stress.''
\item ``Working in a group made you felt more confident; when you raised
an issue, others might support you.''
\item ``Working in a group could stimulate more ideas and was far more
effective than working alone. If one raised an issue, others could
correct it immediately to avoid unnecessary mistakes.''
\end{itemize}
\ls{1.5}

On the other hand, the few students who preferred working individually
expressed comments such as the following:

\ls{1.0}
\begin{itemize}
\item ``Working alone provided me with a better concentration.''
\item ``In a group, more time was spent for discussion and less time was
used to find the errors.''
\item ``You spent less time arguing the issue validity when working alone.''
\item ``I could get a lot more done in a given amount of time when working
by myself.''
\end{itemize}
\ls{1.5}


\section{Conclusions and limitations}
\label{sec:conclusions}

In this study, we used a real group vs. nominal group approach to gain
insight into the contributions and importance of meetings in software
review. Real groups reflect a meeting-based approach to defect discovery,
closely similar to Fagan inspection. Nominal groups reflect an
individual-based approach to defect discovery, closely similar to review
methods such as those studied by Votta et~al.~in which defect discovery is
an objective of the individual preparation phase.

Our goal was to provide additional insight into the question of whether
meetings provide an essential contribution to the review meeting process,
as Fagan asserts, or whether meetings are simply costly without
corresponding benefits, as Votta and his colleagues assert.  Interestingly,
our data appear to provide partial support for each of these assertions.
It is important, when interpreting these conclusions, to remember
that these data were collected using student programmers. Care should
be taken when generalizing these results to professional programming 
groups.

\subsection{Support for non-meeting-based review}

We were unable to find any significant differences between meeting-based
groups and individuals in the number of valid defects
discovered\footnote{It is important to recognize that this does not mean
  that we found the two techniques to be equally effective. There may be
  differences that we were unable to detect due to the design or conduct of
  the experiment.}. However, we did find a significant difference in the
cost of review: meeting-based review required more total effort and more
effort per defect.

Furthermore, the data suggest that changes to the review method could
potentially lead to individuals significantly outperforming groups in
defect discovery.  This is because the nominal groups in this study did
generate significantly more {\em issues} than the real groups, but their overall
defect discovery was reduced by the presence of duplicates.  In research by
Adam Porter and colleagues \cite{Porter95}, they hypothesized that
``systematic'' defect discovery techniques (such as their scenario-based
technique) would outperform ``ad-hoc'' techniques (such as paraphrasing) in
part because of ``reduced reviewer overlap'' (duplication of defects). In
their experiments, systematic defect discovery techniques did indeed
significantly outperform ad-hoc techniques.  Therefore, it is plausible
that individual reviewers could employ a systematic defect discovery
technique that reduces defect duplication to the point where individuals
would both outperform groups and have lower overall cost. 

In summary, the fact that we were able to observe significantly more cost for
meeting-based review, and that we were unable to observe significantly more
defects discovered by meeting-based review, and that related research
suggests a mechanism for improving non-meeting-based review effectiveness, 
all suggest support for the observations of Votta and his colleagues.

\subsection{Support for meeting-based review}

Our data do not unequivocally support a non-meeting-based review style,
however. Real groups using meetings generated significantly less false
positives (issues that were not valid defects) than individuals in nominal
groups. Meetings were thus more effective than individuals alone at
filtering issues.

The real group review method also produced a substantial amount of synergism:
almost 30\% of the defects discovered through this method appeared to
result from some form of synergism.

Furthermore, most subjects indicated a personal preference for the
meeting-based review method. Most felt more confident in the quality of
meeting-based review, and more satisfied with it as a review process.
Indeed, almost three quarters of the subjects believed that meeting-based
review was more productive than individual review, even though primary
review measures suggest the opposite!

Why did the subjects feel more confident in meeting-based review, and even
believe it to be more productive?  In the meeting-based review sessions,
reviewers ended the session with an awareness of all the defects found by
the group.  In the nominal groups, they left the session knowing only about
their own (potential) defects. (The average group found about 50\% more
defects during a session than the average individual.)  Furthermore, in
meeting-based review, all of the potential defects raised were subject to
immediate verification by other members present, providing everyone with
some confidence that the defects raised were valid. In the nominal groups,
no such external validation took place.  Finally, subjects felt the
presence of synergism during meeting-based review: they clearly discovered
defects through interaction with each other, even though this did not lead
to a measurable difference in defects found between the two review methods.

Thus, meetings appear to provide benefits, but these benefits are not
necessarily to increase the total number of defects discovered.  Rather, we
conjecture that the meeting serves as a mechanism for participants to share
their review experiences with each other, to obtain insight into the
overall effectiveness of review, to gain additional insight into the work
product and its quality, and to foster a sense of collective ownership and
responsibility for the review outcome.

\subsection{Application to industrial settings}

What do these results imply for software development organizations?  First,
as with many controlled experiments, these are clearly ``laboratory''
findings: we employed a student population, implemented review methods that
differ in significant ways from common industrial practice, and employed
computer support for review far more advanced than that commonly found in
industry. Given these differences, what insights apply to practitioners in
the ``field''?

Organizations can assess their similarity to the context of this experiment
as one way of determining the validity of these findings to their own
situation.  For example, organizations staffed with highly experienced and
skilled programmers, with a stable and mature development and review
process, may find this research useful as a source of ideas for potential
variations to their established procedures, including meeting modifications
and/or introduction of computer tool support. For such organizations,
differences between their organizational environment and that of this
experiment may make direct application of the data problematic. For
example, such organizations might find that review meetings do little to improve
developer skill or domain knowledge.

Alternatively, organizations with a more youthful and less experienced
development staff and less defined development and review process may find
these results more directly applicable.  In such a case, the pros and cons
of meeting-based review observed in this study may well reflect their
organization. Such organizations might find that the additional insight
into the review process and products gained through meeting-based review is
well worth the extra time investment.

Finally, it is important to note that organizations are not static, and
that initially youthful and inexperienced programmers can mature into
highly experienced and skilled software engineers.  It may be appropriate
for organizations to employ a meeting-based technique as their initial
review method while most participants are novices and can thus benefit from
the benefits of meetings.  Then, as developer skills mature, the
organization might consider a transition to a non-meeting-based technique
with little loss in defect-detection effectiveness and potential savings in
cost.

\section{Future directions}

\subsection{Replication}
\label{sec:replication}

An essential future direction is to assess the validity of these results,
our conclusions, and the threats to external validity through replication.
In particular, replication can help establish if meeting-based review for
the purpose of defect detection is truly more costly than individual review
without also detecting significantly more defects. (It is interesting to
note that our findings are not consistent with those of the behavioral
science researchers, who find that nominal groups generally outperform real
groups \cite{Diehl87,Mullen91}.) Also, replication can also provide more
insight into the issue of synergism, in which our positive findings appear
to contradict the negative findings reported by Votta.

To support this process, the CSRS system, the EGSM and EIAM methods, 
the experimental procedures, and the raw data gathered from
this experiment are all available on the World Wide Web at:

\small
\begin{verbatim}
http://www.ics.hawaii.edu/~csdl/csrs/experiment/
\end{verbatim}
\normalsize

A related approach to validation is comparative analysis, in which the
results of an experiment are compared to a similar, but not identically
designed experiment.  We performed one such comparative analysis
with an experiment performed at the University of Maryland
\cite{Porter97}.  After carrying out a process of ``reconciliation'' in
  which we established a common framework for the comparison of the two
  experiments, we reanalyzed the data with respect to this common framework
  and compared the results.  Through this process we found many striking
  similarities between the two experiments, which increases our confidence
  in the results obtained for the experiment and the one conducted at the
  University of Maryland.

\subsection{Whither review meetings?}

This study was designed to help resolve a central question in modern
software review practice: when are meetings worth their cost?  As a first step,
we identified the importance of the objectives for preparation and the
meeting. In Fagan inspection, where defect discovery is restricted to the
meeting, meetings obviously account for most defect discovery. In 
Lucent's inspection, where defect discovery occurs during preparation and the
primary objective of the meeting is defect collection, meetings do not
account for much defect discovery.

Our study found that a meeting-based method is significantly more costly
than its non-meeting-based alternative, but it did not demonstrate that
meetings find significantly more defects to justify this cost. However,
meetings do appear to provide other benefits, such as filtering false
positives, improving review skills, and increasing awareness of
and confidence in the outcome of review.  Other authors have noted that
review meetings can serve such functions as team building and domain
education \cite{Freedman90}.

We hypothesized above that review meetings may be worthwhile when
introducing review into an organization, and that after participants become
skilled at review, a transition to a meetingless method could potentially
reduce review costs with no reduction in defect detection effectiveness.  A
useful future direction is to perform a longitudinal, case study experiment
in industry that tests this hypothesis by studying transition from a
traditional meeting-based review method to a non-meeting-based review
method.

\subsection{Computer-mediated software review}

This study also demonstrates the potential of computer-mediated software
review environments, both as an experimental platform and as practical
support of software review. CSRS provided substantial help in 
supporting and standardizing the review practice by both real and nominal
groups.  Reviewer satisfaction with CSRS was very high: 72\% of the 
subjects indicated they would prefer to use CSRS over manual review.

Computer-mediated software review may also play a key role in overcoming
the costs of software review meetings without losing their most important
benefits. In prior research \cite{Johnson94}, we implemented a method
called FTArm using CSRS that reduced or eliminated 
face-to-face review meetings while preserving the ability of participants
to filter false positives, retain group awareness of the state of review,
enable educational opportunities, and so forth.  The success of our
previous research, in combination with the outcome of this research,
strongly supports increased research into computer-mediated
software review within an industrial setting. 


\section{Acknowledgments} 

We gratefully acknowledge the efforts of the ICS 313 and ICS 411 classes,
who graciously served as subjects for this study.  Richard Halverson,
instructor of ICS 411, also helped with aspects of the ICS 411 round design
and implementation.  We also acknowledge the many
contributions to this research from the other members of the Collaborative
Software Development Laboratory, including: Cam Moore, Rosemary Andrada,
Dadong Wan, Jennifer Geis, Julio Polo, and Russ Tokuyama. Finally, we 
thank the anonymous reviewers whose suggestions and comments substantially
improved the presentation of this research.
This research was supported
in part by a grant  from the National Science Foundation (CCR-9403475).

%\appendix

%\section{ICS 411 Defect List}
%\label{app:411-defects}

%\subsection{Defect Classes}
%\label{app:defect-classes}

% We classified
%the ICS-411 defects as follows:

%\begin{figure}[h]
% \caption{Defect Class}
%  \begin{center}
%  \begin{tabular}{|c|p{3in}|}
%   \hline
%    Defect Type  & Definition \\
%   \hline
%   C1 & %Missing initialization &
%        Missing or incorrect initialization of  variables
%        (within or across functions).  \\
%   C2 & %Incorrect operators &
%        Incorrect use of C Language operators (for example, using = 
%        instead of ==, or using $!$ to check for a non-zero value instead
%        of a zero value).\\
%   C3 & %Incorrect condition &
%        Missing or incorrect condition tests (for example, using $<$
%        instead of $>$, or using $||$ instead of \&\&).\\
%   C4 & %Mistyped statements &
%        Incorrect conditions or  steps due to mistyped
%        statements (for example, a missing keyword ELSE, or incorrect block
%        delimiters \{,\}, or repetition of the same statement twice).\\
%   C5 & %Incorrect algorithm &
%        Incorrect use of data structure/algorithm (for example,
%        an incorrect binary search algorithm). \\
%   C6 & %Incorrect computation &
%        Incorrect computation or deviation from the specification
%        (for example, using + instead of * in an expression, or reading an incorrect
%        data item from the file).\\
%  \hline
%   \end{tabular}
%  \end{center}
% \label{tb:defect-class}
%\end{figure}

%\newpage
%\subsection{Pass-1}


%\begin{figure}[h]
%\begin{center}
%\begin{tabular}{|l|l|l|l|p{3in}|}
%\hline
%No & Function & Type & Line & Description \\
%\hline
%1 & hextonum & C3 & 17 & Need to check for blanks (i.e., less than 4 digits)\\
%2 &          & C3 & 25 & Return partially converted value when error occurred. \\ 
%3 & Access\_Symtab & C1 &  11 & hash is uninitialized. \\ 
%4 &               & C5 & 12,28,53 & Should have been SYMTABLIMIT+1.\\
%5 &               & C2 & 22 & Missing ! in strncmp.\\
%6 &               & C3 & 54-55 & Does not exit the loop when table is full.\\
%7 & Write\_Int\_File & C4 & 27 & Missing else. \\ 
%8 & P1\_Read\_Source  & C3 &  20 & Should have been $<$. \\
%9 &               & C3 & 29 & Should have been \&\&. \\
%10 &              & C6 & 40 & Should return when source$->$comline is true.\\
%11 &              & C1 & 43 & i is uninitialized. \\ 
%12 &              & C6 & 57 & Should have been 14 (not 13) \\
%13 &              & C6 &73 & source$->$operand are not initialized.\\
%14 & P1\_Proc\_RESW & C6 & 30 & Should have been nwords*3.\\ 
%15 & P1\_Assign\_Loc & C6 & 28 & Should have been locctr+3.\\
%16 & P1\_Assign\_Sym & C2 & 12 & Extra character ! in strncmp.\\
%17 &                 & C4 &16 & Should have been STORE.\\ 
%18 &                 & C2 & 17 & Should have been ==.\\ 
%19 & Pass\_1 & C1 & 10  & Missing initialization of endofinput = false.\\
%\hline
%\end{tabular}
%\caption{Defect List for Pass-1}
%\end{center}
%\end{figure}

%\newpage
%\subsection {Pass-2}
%\begin{figure}[h]
%\begin{center}
%\begin{tabular}{|l|l|l|l|p{3in}|}
%\hline
%No & Function & Type & Line & Description \\
%\hline
%1 & dectonum & C3 & 18 & Check for 4 digits max.\\
%2 & dectonum & C3 & 20 & Return partially converted value when error occurred. \\ 
%3 & dectonum & C1 & 11 & i is uninitialized. \\ 
%4 & P2\_Search..  & C5 & 13,15 & Should have been high=mid-1 and low=mid+1.\\
%5 & P2\_Search..  & C2 &18 & Missing ! in strncmp.\\
%6 & dectonum & C3 & 16 & Does not exit the loop after *converror is set to true.\\
%7 & Read\_Int\_File  & C4 & 18 & Missing else. \\ 
%8 & P2\_Write\_Obj  & C3 & 46 & Should have been $<$. \\
%9 & P2\_Write\_Obj & C3 & 32 & Should have been $||$. \\
%10 &P2\_Write\_Obj & C6 & 52  & Should return when objct.rectype!=ENDREC.\\
%11 & P2\_Proc\_BYTE & C1 & 35 & i is uninitialized. \\ 
%12 & Read\_Int\_File  & C6 & 31  & Should have been 5 (not 4) \\
%13 & Read\_Int\_File & C6 & 19 & locctr is not read.\\
%14 & P2\_Proc\_BYTE & C6 & 45 & Should have been (i-1)*2.\\ 
%15 & P2\_Write\_Obj & C6 & 36  & Should have been /2.\\
%16 & P2\_Assemble..& C2 & 37 & Extra character ! in strncmp.\\
%17 & P2\_Write\_Obj & C4 & 16 & Should have been PROGLENGTH.\\ 
%18 & P2\_Proc\_BYTE & C2 & 19 & Should have been ==.\\ 
%19 & P2\_Proc\_START & C1 & 19  & Missing FIRSTSTMT = false.\\
%\hline
%\end{tabular}
%\caption{Defect List for Pass-2}
%\end{center}
%\end{figure}


\newpage

\ls{1.0}
\bibliography{/group/csdl/bib/csdl-trs,/group/csdl/bib/ftr,/group/csdl/techreports/96-06/96-06}
\bibliographystyle{plain}



\end{document}

