<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html>
  <head>
    <title>BRIE: The Benchmark Inspection Experiment</title>
  </head>

  <body>

<center>
      <h1>BRIE: The Benchmark Inspection Experiment</h1>
Philip Johnson <br>
Department of Information and Computer Sciences <br>
University of Hawaii<br>
http://www.ics.hawaii.edu/~csdl/techreports/96-13/96-13.html<p>
  Last Modified On: Tue May  5 11:49:04 1998
</center>

<h2>Introduction</h2>

The Sydney ISERN meeting generated the concept of a "benchmark
inspection experiment"--- a "physics lab" style of experiment
in software engineering. Ideally, such a benchmark experiment
would have the following properties:

<ul>

<p><li> A benchmark experiment is easily carried out by ISERN members as
well as other members of the software engineering community in both
academia and industry.

<p><li> The expected results from a benchmark experiment, or at least their
general characteristics, are well-known and stable across replications. It
should be straightforward to determine when experimental error
has significantly influenced the outcome.

<p><li> A benchmark experiment provides direct and immediate 
value to the organization.

<p><li> A benchmark experiment should not require extensive equipment
or experimental expertise.  In addition to  the "direct and immediate
value" noted above, the experiment should provide introductory
training in experimental software engineering.

</ul>

<p>

The BenchmaRk Inspection Experiment (BRIE) is an attempt to 
design and package a simple experimental design that satisfies
the goals of a benchmark experiment as defined above.

<p> 

The BRIE acronym has a second expansion: Basic RevIew Education.  BRIE is
designed to have a second, complementary goal: a high quality training
package for a simple formal technical review method.  Thus, BRIE is also a
curriculum module that can be used in either an industry or academic
setting to teach aspects of review and provide participants with 
the essence of a review experience.

<p>

<h2>Outline of the BRIE Experimental Procedure and Design </h2>

The basic experimental procedure is the following:

<table border>
<tr><th> Setup
<tr><th> Pre-Test
<tr><th> Review Training
<tr><th> Post-Test
</table>
<p>

<ul>
<p><li> <em>Setup</em> During Setup, the teacher/experimenter
creates two documents to be reviewed during the experiment, plus
a third training document.  The two experimental documents should 
be matched in approximate length and complexity.  The teacher/experimenter 
should ensure that each of the three documents contains at least
25 defects. The documents are chosen to be relevant and 
comprehensible to the participants undergoing training, and can
be any software engineering artifact: requirements, designs, 
code, planning documents, etc. 

<p> <li> <em>Pre-Test</em> In the Pre-Test phase, 
the teacher/experimenter provides each 
participant in the class with one of the two 
experimental documents to review. Half of the class
analyzes one of the documents, and the rest 
analyze the other. They are asked to discover 
and record as many defects as possible in the 
document in one hour. This Pre-Test precedes any
training and is thus the first activity encountered
by the participants in the Basic Review Education.
Parcipants work individually.

<p><li> <em>Review Training</em> After the Pre-Test, the participants
receive training in one or both of the reviewer and moderator roles for a
simple version of inspection.  An example of such a training is available
as a set of Microsoft PowerPoint and Word documents at the BRIE Inspection
Home Page (http://www.ics.hawaii.edu/~csdl/brie/). These materials
were originally developed for review training at Tektronix, Inc. but
can serve as a good basis for BRIE training materials.

<p>

The review training involves an introduction to review concepts and
motivation for one simple, form-based review method. The training
includes an in-class inspection exercise with the third, "training" review
document.

<p><li> <em>Post-Test</em>  The Post-Test phase can also be viewed
as the "Final Exam" for the class. The teacher/experimenter splits
the class into groups of three.  Each group of three consists of 
participants who reviewed the same document in the pre-test, and 
will now review the other document using the review method presented
to them during the review training phase. 

</ul>

<p>
If we assume 24 participants, labelled {a, b, .. x}, then 
the experimental design is the following:

<table border>
<tr> <th> <th> Document 1 <th> Document 2
<tr> <th> Pre-Test <td> a b c d e f <br> g h i j k l
                   <td> m n o p q r <br> s t u v w x
<tr> <th> Post-test <td> [m n o] [p q r] <br> [s t u] [v w x]
                    <td> [a b c] [d e f] <br> [g h i] [j k l]
</table>
<p>

<h2>Data Collection </h2>

The following data is collected during the experiment:
<ul>
<li> Pre-Test:
    <ul>
    <li> True defects discovered by each individual.
    <li> False positives recorded by each individual.
    <li> Duplicate defects discovered (when individuals are
         formed into "nominal groups" according to the Post-Test
         group memberships).
    <li> Frequency of defect discovery. For each known defect, this metric indicates
         the percentage of individual reviews during which it was
         discovered.
    <li>
    </ul>
<li> Post-Test: 
    <ul>
    <li>The same data is collected as during the Pre-Test, but collected
on a group, not individual basis.
    <li> Meeting gains and meeting losses. 
    </ul>
</ul>

<h2> Expected Outcomes </h2>

The defect detection effectiveness of the post-test condition, i.e.
group inspection-based defect detection, should outperform 
the individual analyses in the unstructured pre-test condition.
This is predicted for the following reasons:
<ul>
<li> A structured approach, including forms and checklists, should
 improve performance.
<li> Inspection requires two encounters with the document (preparation
and meeting) while individual analysis has only one.
<li> Enhanced sensitivity to defects due to training should improve 
motivation and performance.
<li> Learning my contribute to post-test performance, depending
upon the backgrounds of participants.
<li> The combined number of defects detected by three people should
be greater than those of any single individual.
</ul>
<p>

This expected outcome is unsurprising theoretically, since it 
merely suggests that untrained individuals are not as good
at defect detection as trained groups. It also does not demonstrate
that group-based inspection is an optimal form of software
review---there may be other methods that outperform it. However, 
the outcome is still quite useful pedagogically, since it provides
empirical evidence that the training "worked": that the defect
detection performance at the end of training exceeded the 
performance at the beginning. This can be a non-trivial 
learning experience for software engineers who are unfamiliar
with the possibilities of group-based software review. 
<p>
After further experience with 
the BRIE inspection method, it is expected that many groups
will move beyond the simple BRIE review method to a more
advanced method that improves their productivity and/or
efficiency.

<h2>Analyses of interest to ISERN participants </h2>

The BRIE experiment provides opportunities for data analysis
of interest to ISERN members in at least the following ways:
<ul>
<p><li> Meeting gains vs. meeting losses. The BRIE experiment allows us 
     to collect additional data concerning the relative 
     contributions of meeting gains and losses, which can validate
     the data collected previously by Porter and Votta.
<li> Defect obscurity.  All review experiments in which the same document
is searched for defects by multiple individuals or groups provides insight
into the "obscurity" of a defect---how easy or difficult it is to detect. 
Gathering empirical, case-by-case data on a wide range of defects and their
frequency of detection is an important first step toward the formation of 
a "theory" of defect obscurity which would explain/predict how/why 
some defects are harder to find than others. 
<p><li> Meta-analysis.  Replications of this study will differ in at least
the following major ways:
  <ul>
  <li> Backgrounds and skill levels of participants.
   <li> Numbers of participants in the experiment.
    <li> Experimental documents.
   <li> Experimental document type (code, designs, requirements, etc.)
   <li> Numbers and types of defects introduced.
   </ul>
However, other important aspects of the design are controlled, such 
as the pre-test/post-test process, the blocked experimental design, 
and the data collected.
</ul>

<h2> Evaluation of the potential of BRIE as a benchmark experiment </h2>

Let me revisit the primary features of a benchmark experiment as
I've defined them:

<p><li> <em>A benchmark experiment is easily carried out by ISERN members as
well as other members of the software engineering community in both
academia and industry.</em> BRIE appears to be a very accessable
experimental design, as it is wrapped inside of a training course
on software review for quality improvement that is of interest
to both academia and industry.

<p><li> <em>The expected results from a benchmark experiment, or at least
their general characteristics, are well-known and stable across
replications. It should be straightforward to determine when experimental
error has significantly influenced the outcome.</em> The primary expected
outcome of BRIE is that the post-test group-based defect detection
performance is greater than pre-test individual performance.
If this does not occur, then the outcome strongly 
indicates that the training was ineffective in some 
way.

<p><li> <em> A benchmark experiment provides direct and immediate value to
the organization.</em> BRIE teaches a useful group-based approach to
software review that incorporates concepts proven over the past twenty
years. For organizations with little prior exposure to software review,
successful implementation of the training should have a positive impact
upon their software quality improvement efforts.

<p><li> <em>A benchmark experiment should not require extensive equipment
or experimental expertise.</em>  BRIE is decidedly low-tech. 
Nothing more than an overhead projector, a copying machine, and 
a few pencils are required to deliver the training. 

</ul>

<h2>Next Steps </h2>

I would like to develop the BRIE concept in the following way:
<ul>
<li> Review and critique of the BRIE experiment and training
materials. What are the obvious holes in BRIE? (Does it look
more like Swiss Cheese than a French fromage? :-) How can
the problems in BRIE be addressed while retaining the
characteristics of a benchmark experiment?  What needs to 
be elaborated?
<li> Beta-testing.  Who would like to try out the BRIE 
experiment?  Can we get some initial experience by the 
Italy ISERN meeting? 
<li> What opportunities for analysis are missing?  Can
we disseminate the BRIE data effectively?  Can we develop
an on-line repository for BRIE data?
</ul>






      <hr>
      <address><a href="mailto:johnson@natasha.ics.Hawaii.Edu">Philip Johnson</a></address>
<!-- Created: Fri Sep 27 09:02:39 HST 1996 -->
<!-- hhmts start -->
Last modified: Tue May  5 11:49:04 HST 1998
<!-- hhmts end -->
  </body>
</html>
