%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 04-18.tex -- Improving review process quality with Jupiter-Hackystat paradigm
%% Author          : Takuya Yamashita
%% Created On      : Mon Sep 23 11:52:28 2002
%% Last Modified By:
%% Last Modified On: Fri Sep 24 16:22:32 2004
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 2002 Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%

%% This is a sample file showing how to produce CSDL TechReports in ICSE
%% conference style using LaTeX.  It can be adapted to thesis structure
%% with very minor changes.

\documentclass[11pt,twocolumn]{article}
\input{/export/home/csdl/tex/psfig/psfig}
\usepackage{/export/home/csdl/tex/icse2003/latex8}
\usepackage{times}
%% A verbatim-like environment which allows font changes
%%\usepackage{alltt}
%% New LaTeX2e graphics support
\usepackage[final]{graphicx}
% uncomment the % away on next line to produce the final camera-ready version
% and uncomment the \thispagestyle{empty} following \maketitle
\pagestyle{empty}

\begin{document}

\title{Improving Team Review Process \\ with Jupiter and Hackystat tools}

\author{\protect\begin{tabular}{ccc}
Takuya Yamashita  \\
\end{tabular}\\
\em  Collaborative Software Development Laboratory \\
\em  Department of Information and Computer Sciences \\
\em  University of Hawai'i \\
\em  Honolulu, HI 96822 \\
\em  takuyay@hawaii.edu} \maketitle \thispagestyle{empty}

\begin{abstract}  % 200 words

Over many years, there is general agreement that review reduces
development costs and improves product quality. Even though this is
the common fact, it is still hard for reviewers and team to conduct
review process efficiently.

Inefficient preparation time reduces the number of detection of a
certain type of severity review issues or the content of the fault
detections However, there is no threshold for the amount of the
preparation to generate a kind of severities and contents.

I claim that understanding the preparation time will provide the
significant change of the review process quality in such a way that
the higher severity level for review issues  tends to more decline
along with the less predation time.

To investigate my research questions, I designed and implemented a
new software review tool called Jupiter, which is a plug-in for
Eclipse and used the Collaborative Software Development Laboratory's
metric collection system called Hackystat. Jupiter provides features
including a structured sequence of review phases and structured
review data representations. Hackystat and its sensor provide the
feature that collects review metrics and analyze the questions based
upon the metrics. By using these tools, the experimental evaluation
would be conducted in class room setting and industrial settings.

From my analysis of the evaluation data, I believe that I could find
the evidence that understanding preparation time based upon
Hackystat data does give better insight on the content of defects
for each severity level, and then help the review process to
determine the efficient preparation time.

Finally, this thesis is going to be done by not later than May, 2005
with implementation of evaluation tools, collections of the review
data from the organizations, their experimental results, and the
conclusion.

\end{abstract}

\Section{Introduction} \label{sec:intro}

For the past 30 years, software researchers have been establishing
theories and best practices to improve software quality
\cite{Parnas2003}. However, no matter how hard the industry tries,
software is still released with many bugs. This is not because
developers have neglected removing bugs. Most developers have
strived removing them by means of exercising unit testing and
coverage before release. Existence of bugs in a system in spite of
the effort is because developing or developed software is so
complicated that they can not recognize the bugs until software is
released.

Software review, or code inspection, is one of approach to reduce
bugs before release. Software review is effective in that it can
identify most faults during design and code inspection. Here are
some specific examples. 93 percent of all faults in a 6000-line
business application were found by inspections \cite{Pfleeger2001}.
Seven thousand inspection meetings included information about 11,557
faults \cite{Pfleeger2001}. In addition, there are many other
studies sharing that software review identifies and removes faults.
Thurs, no one doubts that the software review can identify and
reduce faults somehow.

However, a review is not necessary to be conducted efficiently all
the time. In a review process, a team experiences common problems
that can undermine the effectiveness of software review. For
example, overwhelm materials may be scheduled for a single review
because participants are not aware of realistic review rates
\cite{Wiegers1998}. On the contrary, tiny preparation time may be
scheduled for a review because reviewers are not prepared before the
team meeting. Even though reviewers are well prepared, the team may
not inspect all raised issues because it runs out of time. As a
result, the rest of the untouched issues are marked as validated
without careful examination

I claim that determining a proper preparation rate (time) brings the
improvement of the team meeting quality, that is to say, the
confirmation rate to deal with most raised issues carefully and
efficiently.

Before examining my claim, the next section introduces the related
work and how my claim is rational enough to investigate the problem.

\Section{Related Work} \label{sec:relatedWork}

The concept of the preparation rate and inspection rate is discussed
in the context of the improvement of inspections \cite{Barnard1994}.
Preparation rate is defined as "the speed with which the inspectors
cover the inspection material before the inspection meeting,
including the time it takes to study the design specifications and
review the code" \cite{Barnard1994}.  Inspection rate is defined as
"the speed with which the inspectors cover the material in the
inspection meeting, and it includes the time to paraphrase each line
of code, record the faults, and discuss the material
\cite{Barnard1994}." Since it is hard for reviewers to cover
overwhelm materials in a time and jeopardize the quality of the
delivered software, it is said that it would be better to keep
inspection rate around 100 to 150 line of code per hour
\cite{Barnard1994}.  In other words, it might be said that
inspection ratio of 100 to 150 LOC / hours can keep the quality of
the software without jeopardy.

However, there are different kinds of projects, reviewers,
inspections in all over the world so that applying the 100 to 150
inspection ratio to the universe of inspection seems brute force
approach.

The next section brings up the problems potentially hidden in the
preparation and inspection rate derived from our experiences.

\Section{Problems} \label{sec:problems}

Our Collaborative Software Development Laboratory has conducted
review process many times. The review guideline says that review
preparation should not exceed more than one hour and limit the
amount of the review materials to less than half a dozen classes.
Sometimes a class contains more than 100 to 150 line of code so that
it is obvious that this guideline exceeds the AT\&T Bell
Laboratories' threshold. As a result, the inspection rate also
exceeds the threshold. On the contrary, if the AT\&T Bell
Laboratories' threshold was held, and reviewers can not spend more
than fifteen minutes before meeting, and then allocation of one hour
meeting would be in vain because the team might confirm all raised
issues within a half hour.

Next section brings up my research question about this problem.

\Section{Research Questions} \label{sec:researchQuestions}

I claim that the efficient preparation time and materials can be
determined in such away that the issue confirmation rate is
examined, and as a result, the team review process can be
efficiently improved.

Inefficiency of the review process quality in team meeting would be
caused by issue confirmation rate, which is the ratio of the number
of issue confirmed to the total number of issue raised. The
confirmation rate would be affected by the inadequate preparation
time and materials.

If a preparation rate is meaningful, namely data-analyzed
preparation time and data-analyzed amount of materials are provided,
and then the confirmation rate would be close to one (i.e. the
number of confirmed issues in the team phase is close to the number
of raised issues in preparation phase). As a result, the team could
handle all raised issues, which mean that team could conduct
efficiently review process.

\Section{Experimental Plan}
\label{sec:experimentalResearchEvaluation}


In order to investigate a proper preparation rate (i.e. preparation
time and line of code) against the quality of the review process in
team phase (meeting), I designed and implemented a new review tool
called Jupiter. It provides the individual, team, and rework phase
and facilitate the structural sequence of the review process.
Jupiter can store and record review issues in xml files. Our CSDL
designed and implemented a metric collection system called
Hackystat. It provides an infrastructure of collecting metric data,
and activity data, and analyzing the data. I also designed and
implemented the Hackystat sensor for Jupiter to collect the review
related data.


Using these three tools, I will collect the data about review
process:
\begin{itemize}
    \item Review Active time for individual and team phase
    \item Number of review issues posted in the individual phase
    \item Number of review issues confirmed in the team phase
    \item Line of Code reviewed or covered in individual and team phase
\end{itemize}
Review active time is defined as the time in which review process is
conducted through Jupiter, including comment for a code,
manipulation of the review tool. Note that the review active time
should be distinguished from review effort so that review active
time does not contain the time to read and think about the review
materials such as source code and design documents. It is because it
is hard to automatically determine if a person is reading, thinking,
or being apart from the computer based upon the tool-embedded
sensor. So I need to some complimented methods to determine the
actual review time. One approach to imitate the actual time is that
the author of review session declares the preparation time and the
declared time is used as a review time. In this case, reviewers are
supposed to prepare reviews punctually. In the team meeting,
moderator is supposed to keep review meeting punctual too.

The number of review issues posted and covered gives the
confirmation rate, which is the ratio of the review issues covered
to the total review issues posted. The ratio is used to determine
the efficiency of the team review process. For example, 40 review
issues are posted by reviewers in a preparation phase, and 20 review
issues are confirmed by team members in the team phase. In this
case, the confirmation rate is one half (20 /40). This rate would
not be efficient rather than the perfect confirmation rate (1/1)
because the half of the issues have not been unconfirmed even though
reviewers strived to post forty issues. Since the meeting time is
precious in team collaboration, and there is a fact in organization
that team can not spend enough time for meeting, the unconfirmed
issues are postponed to the next review team meeting, marked as
temporary validated without careful examination, or even thrown away
at that time.

The amount of the review materials can be measured by the line of
code. The author of a review session is supposed to specify the
actual amount of code rather than ambiguous amount of code. This
helps to determine the inspection rate too.

To gather review data, I will ask several organizations including
our research lab to see the results. The Data will be collected
depending upon the material size of review. The preparation time
should vary for the experimental period to see the significant
difference for the confirmation rate. After a couple of pilot
studies for them, I will set the several preparation times to
analyze the data.

To analyze the row data, I will use Hackystat. System. One approach
to determine the efficiency of the team review process is to make 1)
the analysis of the correlation between the confirmation rate and
the amount of source codes, 2) the analysis of the correlation
between the confirmation rate and review time.

\Section{Expected Results} \label{sec:results}

The expected result for the experimentation is that given inspection
time in team meeting, confirmation rate will vary depending upon the
preparation time and preparation material size. By adjusting the two
variables, I believe that there is an efficient pairs for two
variables to optimize the confirmation rate, which would be close to
the perfect confirmation rate.

\Section{Acknowledgment} \label{sec:acknowledgment}

We gracefully acknowledge Dr. Philip M. Johnson (CSDL director), and
other CSDL colleagues who contributed ideas for code review tool,
its evaluion, and helped the structure of this thesis idea. We
also thank all persons who spared precious their time to participate
in the evaluation questionnaire.

\bibliographystyle{/export/home/csdl/tex/icse2003/latex8}
\bibliography{/export/home/csdl/bib/review}
\end{document}
