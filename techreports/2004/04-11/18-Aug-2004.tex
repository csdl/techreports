%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 18-Aug-2004.tex -- Hackystat-UH ISESE Paper
%% Author          : Philip Johnson
%% Created On      : Mon Sep 23 11:52:28 2002
%% Last Modified By: Philip M. Johnson
%% Last Modified On: Wed Aug 18 14:31:56 2004
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 2002 Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 

\documentclass[11pt,twocolumn]{article} 
\input{/export/home/csdl/tex/psfig/psfig}
\usepackage{/export/home/csdl/tex/icse2003/latex8}
\usepackage{times}
%% A verbatim-like environment which allows font changes
%%\usepackage{alltt}
%% New LaTeX2e graphics support
\usepackage[final]{graphicx}
% uncomment the % away on next line to produce the final camera-ready version
% and uncomment the \thispagestyle{empty} following \maketitle
\pagestyle{empty}

\begin{document}

\title{Improving Software Development Management through Software Project Telemetry}


\author{\protect\begin{tabular}{ccc}
Philip M. Johnson & Hongbing Kou & Mike Paulding  \\
Qin Zhang & Aaron Kagawa & Takuya Yamashita \\
\end{tabular}\\
\em  Collaborative Software Development Laboratory \\
\em  Department of Information and Computer Sciences \\
\em  University of Hawai'i \\
\em  Honolulu, HI 96822 \\
\em  johnson@hawaii.edu}
\maketitle
\thispagestyle{empty}

\begin{abstract}  % 200 words

Abstract goes here.  

\end{abstract}

\Section{Introduction}
\label{sec:intro}

It is conventional wisdom in the software engineering research community
that metrics can improve the effectiveness of project management.
Proponents of software metrics quote theorists and practitioners from
Galileo's ``What is not measurable, make measurable'' \cite{Finkelstein82}
to DeMarco's ``You can neither predict nor control what you cannot
measure'' \cite{DeMarco82}.  Software metrics range from internal product
attributes, such as size, complexity, and modularity, to external process
attributes, such as effort, productivity, testing quality, and reliability
\cite{Fenton}. 

Despite the potential of metrics in theory, their effective application
appears to be far from mainstream in practice. For example, a recent case
study of over 600 software professionals revealed that only 27\% viewed
metrics as ``very'' or ``extremely'' important to their software project
decision making process \cite{Kulik2003}. The study also revealed that cost
and schedule estimation was the only metrics application attempted
by a majority of respondents.

At least two hypotheses could account for this chasm separating the theory
from the practice of software metrics. The first is that three quarters of
practitioners are simply ignorant: if they would subscribe to the journals
and/or partake of the many educational opportunities regarding software
metrics, they would find that implementing current ``best practices'' would
make a substantial, positive impact on their project management decision
making.

While all of us, theorists and practitioners alike, can always benefit from
additional reading and education, an alternative hypothesis also explains
this chasm between metrics theory and practice. Perhaps the metrics
methodology used by theorists yields results not easily translated into
practice?  Consider that much metrics research involves the following basic
methodology: (1) collect a set of process and product measures (such as
size, effort, known defects, complexity, etc.) for a set of completed
software projects; (2) generate a model that fits this data; (3) claim that
this model can now be used to predict characteristics of future
projects. For example, that a future project of size X will require Y
person-months of effort, or that the future implementation of a module with
complexity S will be prone to defects with probability T.

There are several barriers that practitioners face when attempting to adopt
metrics research findings using this approach. First, to use the model
directly for management decision-making, one must confirm that the set of
projects used to calibrate the model are similar. This is the {\em Context
Problem}: unless the context associated with the software process and
project data in the repository is similar to the context associated with
the practitioner's projects, then the validity of the model is unclear.
Second, one must also confirm that future projects will also have a context
similar to the repository projects.  If one cannot confirm these
similarities, then we confront a third barrier: the cost and time required
to build a custom repository of historical project data of sufficient
completeness and quality to generate a new model calibrated to the
practitioner's own context. This generates yet another barrier: the risk
that changes in one's organization or development context could invalidate
the predictive power of the model for which so much time and effort was
spent in creation and calibration.

Faced with these barriers, it is no wonder that many practitioners find it
daunting to apply measurement ``best practices'' to their own
situation. Indeed, the agile community generally argues against model-based
metrics applications, promoting much ``softer'' metrics for decision-making
\cite{agile}.

Fortunately, creation of predictive models based upon historical project
data is not the only possible way to apply software metrics to project
management.  In this paper, we present an alternative method
based upon the notion of ``telemetry''.

\Section {Software Project Telemetry}

According to Encyclopedia Brittanica, telemetry is a ``highly automated
communications process by which measurements are made and other data
collected at remote or inaccessible points and transmitted to receiving
equipment for monitoring, display, and recording.''  Perhaps the most high
profile user of telemetry is NASA's Mission Control Center in Houston,
Texas, where telemetry has been used since 1965 to monitor the Gemini,
Apollo, SkyLab, and Space Shuttle flights.  At Mission Control Center,
dozens of specialists monitor telemetry data sent from sensors attached to
a space vehicle and its occupants.  This data is used for many purposes,
including early warning of anomalies indicating problems, for better
insight into the current status of the mission, and for the impact of
making incremental course or mission adjustments.

For the past several years, we have been designing, implementing, and
evaluating tools and techniques to support a telemetry-based approach to
software project management. Rather than measure past projects and hope
future projects are similar, software project telemetry uses measures from
the current project for decision-making, thus guaranteeing that the context
holds.  Rather than build a model from old projects that attempts to
predict the exact threshold of allowable complexity for a new class,
telemetry allows the manager to monitor whether complexity is increasing,
decreasing, or staying the same as the project progresses.  

Telemetry data can lead to a more incremental, experiential approach to
project decision-making. For example, if the manager finds that complexity
telemetry values are increasing, {\em and} that defect density telemetry
values are also increasing, then they could decide to take corrective
action and see if that results in a decrease in defect density telemetry
values.     

   aslfa

ksadflaksjd aslfa

We define "software project telemetry" as a style of software metrics
definition, collection, and analysis with the following three essential
properties:

\begin{enumerate}

\item {\em Software project telemetry data is collected automatically by tools
   that are unobtrusively monitoring some form of state in the project
   development environment.}  In other words, the software developers are
   working in a ``remote or inaccessable location'' from the perspective of
   metrics collection activities. This contrasts with software metrics data
   that requires human intervention or developer effort to collect, such as
   PSP/TSP metrics.
        
\item {\em Software project telemetry data consists of a stream of time-stamped
   events, where the time-stamp is significant for analysis.} Software
   project telemetry data is thus focused on evolutionary processes in
   development.  This contrasts, for example, with Cocomo, where the time
   at which the calibration data was collected about the organization is
   not particularly significant (or, at least, the time scale of
   significance would be measured in years or decades, as opposed to
   minutes, days or weeks.)
        
\item {\em Software project telemetry is useful for in-process, local
   monitoring, control, and short-term prediction.} Telemetry analyses
   provide representations of current project state and how it is changing
   at the grain sizes of days, weeks, or months.  Control thresholds, if
   set correctly, can enable us to differentiate between random and
   non-random variation.  The simultaneous display of multiple project
   state values and how they change over the same time periods allow
   opportunistic analyses---the emergent knowledge that one state variable
   appears to co-vary with another in the context of the current project.

\end{enumerate}

We have identified several forms of software project telemetry.  {\em
Developer telemetry} is data gathered by observing the behavior of
developers, and includes information about the files they edit, the time
they spend, the changes they make to code, the sequences of tool or command
invocations, and so forth. Developer telemetry can be gathered by attaching
sensors to editors, such as Eclipse or Emacs, to configuration management
tools such as CVS, to code review tools, and so forth.  {\em Build
telemetry} is data gathered by observing the results of tools invoked to
compile, link, and test the system. Build telemetry can be gathered from
build tools like Ant or Make, testing tools like JUnit, and so forth.  {\em
Execution telemetry} is data gathered by observing the behavior of the
system as it executes. Execution telemetry can be gathered by instrumenting
the run-time environment of the system to collect data about its internal
state (heap size, occurrence of exceptions, etc.) as well as by tools that
perform load or stress testing of the system, such as JMeter.





\Section{Software Project Telemetry in Action}

Examples of Software Project Telemetry.

\Section{Hackystat: Infrastructure support for Software Telemetry}

Explanation of hackystat, issues that need to be solved, etc.

\Section{Future directions}

Does software project telemetry provide a ``silver bullet'' that solves the
problem of using metrics for software project management? Of course not.
While software project telemetry does address certain problems inherent in
traditional measurement, and provide new support for a more local,
in-process level of decision-making, it raises some new barriers to
adoption that future research must address.

First, the decision-making value of telemetry data is only as good as the
quality and diversity of data that can be obtained by sensors.  If the
project does not have a defect tracking database, then the utility of the
remaining telemetry data is reduced.  If the project also does not do
automated daily (or continuous) builds, then the utility of telemetry is
reduced further.   


   1. What are useful kinds of software telemetry data? Utility involves
   two dimensions: the effectiveness of the data in supporting software
   development, and its generality.  Effectiveness is measured by the
   ability of the data to help managers and developers identify positive
   changes in development. Generality is measured by the constraints that
   the type of data places on the development environment and approach. For
   example, a telemetry stream that monitors the success or failure of a
   daily build imposes the constraint that the development project do
   builds on a daily basis.
       
   2. How can software telemetry data be made usable? One thing is
   abundantly clear about software telemetry data: it is easy to collect an
   overwhelming amount of it.  To make it usable, we must determine
   appropriate ways to abstract raw telemetry data into higher-level
   representations, including synthetic measures that combine multiple raw
   data streams. We must also develop methods (perhaps based upon control
   charts) to separate normal from unusual telemetry data values. In
   addition, we must determine the appropriate grain-size for different
   telemetry streams.  Some telemetry analyses may be most appropriate when
   sampled on a daily basis, others might require a grain-size of a week or
   a month in order to effectively distinguish signal from noise. Usability
   of telemetry data also includes the design of presentation and
   notification strategies.  What form of visual presentation enables
   software developers and management to interpret software telemetry data
   appropriately?  When is it appropriate for the system to generate a
   notification about telemetry data, and to whom should the notification
   be sent?

Availability, future work.


\bibliographystyle{/export/home/csdl/tex/icse2003/latex8}
\bibliography{/export/home/csdl/techreports/04-11/04-11,/export/home/csdl/bib/csdl-trs,/export/home/csdl/bib/hackystat,/export/home/csdl/bib/psp}
\end{document}
 










