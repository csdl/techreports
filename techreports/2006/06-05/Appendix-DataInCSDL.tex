\chapter{CSDL Data Summary}
\label{Appendix:DataInCSDL}

This appendix provides a summary of the data I gathered in the CSDL study. The raw data themselves cannot be published because of privacy issues. The summary is intended to provide a mechanism for the reader to ``audit'' my conclusions in some sense. The CSDL setting, the context, and the result of the study are reported in Chapter \ref{Chapter:EvaluationInCSDL}.


\setlength{\parindent}{0mm} %indentation of paragraphs
\setlength{\parskip}{3mm}   %gap between paragraphs
  
\textbf{2006-01-05-1:}
I discussed the existing analysis of issue metrics with the project manager in an interview. He did not utilize the metrics in project management, because the analysis was inadequate for release cycle planning and tracking. 

\textbf{2006-01-05-2:}
I interviewed two developers on their opinions about the utility of the metrics currently collected in the lab. One of them thought the issue related metrics were not that useful, because they were quite different from what he had anticipated.

\textbf{2006-01-06-1:}
I discussed the current status of issue management with a developer. He told me that most of his development activities were not recorded in the issue database. He suggested that we could put an issue number in commit log to link the issue tracking system and code repository together. He wanted telemetry analysis to provide him information about how much time he spent on each issue.

\textbf{2006-01-09-1:}
I held a discussion with a developer, who estimated that only 20\% - 30\% of his development activities were tracked by the issue database. None of the Hackystat v6 to v7 transition issues were recorded, they were assigned in weekly meetings. None of Zorro related issues were recorded. He never followed the issue priority in resolving issues. He had five open issues on average. 
When asked about his perception of the utility of the metrics currently collected in the lab, he told me that in general the metrics were useful, because it was a big leap from knowing nothing to know something. However, FindBugs reported too many problems, and that a lot of them were false-positive.

\textbf{2006-01-11-1:}
I asked a developer about his perceptions about the utility of the metrics currently collected in the lab. He told me that metrics from FindBugs were not useful, but they could be made useful if the false-positive problem could be resolved. He also told me that he did not really understand how issue metrics were computed. 
When asked about issue management status in CSDL, he estimated that less than 15\% of his development activities were tracked by the issue database. He told me that most of his issues were assigned through emails instead of the issue management system.

\textbf{2006-01-12-1:}
Integration build failed. A developer forgot to test the changes before committing the code, causing JUnit failure in Core\_Installer module.

\textbf{2006-01-16-1:}
A number of topics were covered in the weekly status meeting.
The project manager discussed possible changes to improve the issue management practice with the developers.
The developers discussed the metrics from FindBugs and PMD, and they all appeared to agree that there were too many false-positive warnings.
The developers noted that one could generate a lot of telemetry charts using the telemetry language, but they raised the question: \textit{``do we really care about all those charts?''}

\textbf{2006-01-19-1:}
Integration build failed. A developer modified the build script but forgot to update the server build environment, causing JUnit failures in multiple modules.

\textbf{2006-01-19-2:}
I interview the project manager. He told me that as part of corrective measures, he went through all Jira issues and tagged them with realistic fix version numbers to get ready for the new release cycle (i.e., Release 7.3).
He was identified as responsible for a recent integration build failure. I asked him how it might impact his local quality assurance practice. He told me that it was \textit{``very effective''} in making him think more about the integration build result when committing changes.%P

\textbf{2006-01-19-3:}
As part of corrective measures, the project manager sent out an email requiring that all future commits should have issue Id in commit log comment field.

\textbf{2006-01-20-1:}
The project manager sent out an email imagining new insights could be gained after the changes made to CSDL issue management practices.

\textbf{2006-01-21-1:}
A developer installed a Jira plug-in to enable one way link from Jira issues to SVN commits. 

\textbf{2006-01-23-1:}
The project manager sent out an email stating that he was committing in branches. This was the first time a branch was used in CSDL and was officially acknowledged.

\textbf{2006-01-23-2:}
CSDL had a weekly status meeting. This was the first time in my observation that the new issues assigned in the meeting were recorded in the issue management system.

\textbf{2006-01-24-1:}
Integration build failed. A developer updated the build script to invoke a newly developed sensor but forgot to install the sensor on the build server.

\textbf{2006-01-26-1:}
Integration build failed. A developer changed code in Sdt\_FileMetric module, causing JUnit failure in dependent App\_Pri module. 

\textbf{2006-01-29-1:}
I enhanced the Jira sensor to collect missing information required for issue tracking telemetry analyses.

\textbf{2006-01-31-1:}
Integration build failed. A developer updated the build script to invoke a newly developed sensor but forgot to set the proper environment variable on the build server.

\textbf{2006-01-31-2:}
I interviewed a developer on the impact of the integration build failure alert mechanism. He commented that it made him \textit{``a little bit more cautious''} when committing changes.

\textbf{2006-01-31-3:}
I interviewed a developer on the impact of the integration build failure email alert mechanism. He commented: \textit{``you might be identified as a culprit, (which) tends to make you try a little harder to think about when you actually do the thing (i.e., committing changes).''} 

\textbf{2006-01-31-4:}
I noticed an inconsistency in telemetry charts: there was no data from one of the developers. It turned out it was caused by a bad server-side project configuration.

\textbf{2006-02-01-1:}
The same developer told me he had fixed the problem, but the inconsistency still existed in telemetry charts. The project was still mis-configured despite the developer's effort to fix it.

\textbf{2006-02-02-2:}
I showed some module level coverage charts to a developer. He noted that the automatically-scaled y-axis made comparison across related charts difficult.

\textbf{2006-02-02-3:}
I interviewed a developer on the impact of the integration build failure alert mechanism. He told me that his behavior was changed significantly from \textit{``I just build the module to see if it works''} to \textit{``I build the entire system and test every time before commit.''}

\textbf{2006-02-02-4:}
I interviewed a developer on the impact of the integration build failure alert mechanism. He told me that it had not changed his behavior because he was always careful about local quality assurance.

\textbf{2006-02-03-1:}
I made the issue tracking charts available on the telemetry wall, and showed them to a developer. He commented that they could be used not only to track issue status but also to predict system release date.

\textbf{2006-02-05-1:}
I gathered a list of the types of metrics collected in CSDL, and generated telemetry charts to show how these metrics changed over time in different grain size. One scene was devoted to project level and individual level release cycle issue tracking. The telemetry wall was up.

\textbf{2006-02-06-1:}
CSDL had a weekly status meeting. I introduced the telemetry wall.
The project level issue tracking charts were found \textit{``highly useful''} by the project manager, but the individual level issue tracking charts were completely useless. One developer commented: \textit{``I would rather look into \textit{Jira} directly, because I don't know which issues remain.''} The project manager commented: \textit{``I would not be interested in those individual charts.''}
I noticed that the developers were using some of the charts to assess whether the underlying sensors data seemed correct or not. Further discussion identified two common causes for incorrect sensor data: (1) sensor not working correctly, and (2) bad server-side project configuration.
However, in general, most of the charts were not useful. The project manager and the developers gave me a list of questions that they wished telemetry charts could help shed light on, such as some notion of quality indicators for each module in the project.
There also appeared to be a performance problem with the telemetry wall. Some of the charts covering long-term metrics trends could take several minutes to show up, which was more than the developers were willing to wait.

\textbf{2006-02-07-1:}
Integration build failed. A developer forgot to check code format before committing the changes, causing Checkstyle failure in Sdt\_WorkspaceMap module.

\textbf{2006-02-09-1:}
Integration build failed. A developer changed code in Core\_Kernel module, causing JUnit failure in dependent Sdt\_Activity module. 

\textbf{2006-02-09-2:}
CSDL deployed FindBugs and PMD sensors, even though the false-positive issue had not been addressed.

\textbf{2006-02-10-1:}
Integration build failed. A third party tool (FindBug) invoked during the integration build failure terminated abnormally.

\textbf{2006-02-13-1:}
Integration build failed. A developer changed code in Core\_Kernel module, causing compilation failure in dependent App\_Ggqm module. 

\textbf{2006-02-13-2:}
I wanted to display some telemetry charts on the telemetry wall in the middle of the weekly status meeting, but encountered severe server timeout.

\textbf{2006-02-14-1:}
Integration build failed. A developer changed code in Core\_Kernel module, causing JUnit failure in dependent App\_Course module. 

\textbf{2006-02-14-2:}
I discussed with a developer about the current status of dependency metrics collected in the lab. He commented that the metrics was not useful. He also mentioned that a visiting scholar once tried to visualize the dependency information by using a graph, but nobody found the graph useful. 

\textbf{2006-02-18-1:}
Integration build failed. A developer forgot to test the changes before committing the code, causing JUnit failure in Sensor\_XmlData module.

\textbf{2006-02-20-1:}
While redesigning telemetry charts for the telemetry wall, I noticed several charts for module-level telemetry trends were overly cluttered.

\textbf{2006-02-21-1:}
Integration build failed. A developer changed code in Core\_Kernel module, causing compilation failure in dependent App\_Mds module. 

\textbf{2006-02-21-2:}
I discussed the top-down designed charts with three of the developers, and they thought the charts displayed useful information.
However, they commented that some of the charts displaying module information were too cluttered to be useful. One of them also noted that most of the lines were ``uninteresting'' because they represented inactive modules.

\textbf{2006-02-22-1:}
I discussed the top-down designed telemetry wall, especially the module level quality indicator telemetry scenes, with the project manager. He liked them. However, one minor inconvenience was that we had to check the vertical axis while comparing coverage trends in different modules.  

\textbf{2006-02-23-1:}
One of the developers toggled the telemetry wall in auto-update mode. The server stopped responding under the heavy load, and its CPU usage stayed at 100\%. I had to restart the server.

\textbf{2006-02-24-1:}
Integration build failed. I changed code in Core\_Telemetry module, causing JUnit failure in dependent App\_Cgqm module. 

\textbf{2006-02-25-1:}
The project size measure in CSDL was changed from SLOC to LOC. Telemetry charts were used to show the trends of the two measures, indicating that apart from absolute number changes there was no change in the trends using either measure.

\textbf{2006-02-26-1:}
Telemetry charts showed missing coverage data. It turned out that the sensor configuration file was not updated when a developer added a new module.

\textbf{2006-03-01-1:}
I reviewed the ``DailyProjectCoverage'' code, and found it that it never released references to sensor data instances, which was temporary data structure as far as ``DailyProjectCoverage'' is concerned. 

\textbf{2006-03-03-1:}
I generated Structure 101 reports on Hackystat modules, and discussed the metrics with the project manager. When asked whether the metrics appeared to be consistent with his intuition about the complexity of individual module, he responded \textit{``I don't know what to think. If it's a metric for easiness of maintenance, then I think the module I wrote is the easiest one to maintain.''}

\textbf{2006-03-05-1:}
Telemetry charts showed missing issue metrics. It turned out that a developer forgot to update the project configuration when adding a new module.

\textbf{2006-03-05-2:}
The project manager created a Jira issue, requesting me to perform an audit on all daily project code to ensure references to temporary data structures were released.

\textbf{2006-03-07-1:}
After receiving complaints about missing coverage data, the project manager sent me an email asking whether it would be possible to design charts to help detect sensor malfunction.

\textbf{2006-03-08-1:}
I interviewed a developer on the impact of the integration build failure alert mechanism. He told me that he had spent more time on local quality assurance than before. There was overhead, but it was acceptable since it would be much more troublesome to have integration build failures. 

\textbf{2006-03-08-2:}
I interviewed a developer. He seemed to controll the local quality assurance overhead by reducing the number of commits.

\textbf{2006-03-09-1:}
I added additional telemetry charts on the telemetry wall, designed to verify developer-side process metrics. Immediately, I detected that one developer had missing active time data. It turned out that the developer reinstalled the IDE but forgot to reattach the sensor.

\textbf{2006-03-10-1:}
I started the code audit, and found several classes followed the same design pattern of ``DailyProjectCoverage,'' failing to release temporary data structures.

\textbf{2006-03-11-1:}
Integration build failed. A developer changed code in Sdt\_Coverage module, causing compilation failure in dependent App\_Pri module. 

\textbf{2006-03-11-2:}
After I sent out my findings, the project manager elaborated the distinction between ``cache'' and ``temporary data structure'' in an email to call to the attention of all the developers.

\textbf{2006-03-13-1:}
I interviewed a developer. He confirmed that almost all his work was tracked by the issue tracking system now.  

\textbf{2006-03-14-1:}
The server performance improved after the ``temporary data structures'' were properly disposed, but the improvement was not significant enough to reduce the telemetry wall response time to an acceptable range. I profiled the code with another developer, and we had a number of surprising findings. (1) When sensor data were cached in memory, over 90\% of processor time was spent on string comparison in workspace code. The case-sensitive comparison was 10 times more expensive than case-insensitive comparison. (2) The sensor data evolution mechanism in some classes was implemented very inefficiently. For example, the ``FileMetric'' class spent 80\% of processor time in the ``recognizeData()'' method, even if the data were already stored in the newest format. (3) The cost of reading sensor data from XML files was negligible.

\textbf{2006-03-14-2:}
I interviewed a developer. He commented that the overhead on local quality assurance was acceptable give the consequence of integration build failures. He controlled the overhead by reducing the number of commits.

\textbf{2006-03-15-1:}
Integration build failed. I was unable to determine the cause.

\textbf{2006-03-15-2:}
I interviewed two more developers. They all confirmed that most of their work was tracked by the issue tracking system.

\textbf{2005-03-15-3:}
I sent out an email to the developer mailing list reporting the performance profiling result.

\textbf{2006-03-15-4:}
After reporting the performance profiling findings, I received an email from a former developer. He had experimented with a Berkeley XML DB back-end and found no performance improvement at all.

\textbf{2005-03-15-5:}
In an email request for comments, I brought up the idea of introducing nested function calls to the telemetry language.

\textbf{2006-03-16-1:}
During a discussion of telemetry charts with the project manager, I mentioned the idea of making relating charts more comparable by augmenting the telemetry language to allow a user to specify the vertical axis manually. He agreed that it would improve the usability of telemetry charts. 

\textbf{2006-03-16-2:}
A Jira issue (HACK-612) was created to implement filter functions to solve the usability and scalability problem of telemetry charts for large projects.

\textbf{2006-03-17-1:}
The project manager was so impressed with the utility of those top-down designed telemetry charts on the telemetry wall, that he decided to devote an entire page on the Hackystat website to publish the results.

\textbf{2006-03-17-2:}
The project manager used the telemetry wall to show the status of Hackystat development to outsider developers. For those overly-cluttered charts, he had to enlarge them to occupy all the nine screens to show the details.

\textbf{2006-03-17-3:}
A Jira issue (HACK-616) was created to enhance the telemetry language to allow manually specified vertical axis.

\textbf{2006-03-20-1:}
Integration build failed. A developer missed one file while committing his changes, causing compilation failure in Core\_Installer module.

\textbf{2006-03-20-2:}
The project manager reviewed the issue tracking chart after release 7.3 was finished, and reflected that the chart helped him determine whether more issues could be added to that release. 

\textbf{2006-03-21-1:}
Integration build failed. A developer committed local diagnostic code which should not be committed at all, causing JUnit failure in Sensor\_CppUnit module.

\textbf{2006-03-21-2:}
An external user, who managed his own server, reported degrading telemetry analysis performance proportional to server up-time. He had not picked up the recent fixes, but the problem reported was consistent with the effect of not releasing temporary data structure. 

\textbf{2006-03-22-1:} 
I gave the project manager a list of recent failed integration builds together with their causes, and asked him to determine which ones were acceptable and which ones were not from his point of view.

\textbf{2006-03-23-1:}
I discussed the telemetry charts on integration build failures and various software development process metrics with two of the developers. They confirmed that integration build failure was a complex phenomenon, and that it would be very hard, if not impossible, to predict the probability from the process metrics. 
The code issue density charts, which were computed from FindBugs and PMD metrics, were also available on the telemetry wall. The two developers told me that the charts failed to provide clue about Hackystat code quality, because they did not know the rules used by FindBugs and PMD to generate warnings.
When asked about whether they invoked analyses themselves, one developer said: \textit{``The language is the last thing I want to use. It looks complex.''}

\textbf{2006-03-28-1:}
Integration build failed. I changed code in Core\_Telemetry module, causing compilation failure in dependent App\_Cgqm module. 

\textbf{2006-03-29-1:}
Integration build failed. A developer changed code in Sdt\_Activity module, causing compilation failure in dependent App\_PrjSize module. 

\textbf{2006-03-30-1:}
Integration build failed. It was caused by the same error as the previous day. The developer responsible for the error did not fix it in time.

\textbf{2006-03-30-2:}
The project manager sent out an email giving statistics of Jira issues in recent release cycles.

\textbf{2006-04-03-1:}
I held a discussion with the project manager, and we formalized the change to the telemetry language in order to allow a user to specify the vertical axis manually.

\textbf{2006-04-05-1:}
Integration build failed. A developer changed one single line in App\_Cgqm and did not test the change, causing JUnit failure in that module.

\textbf{2006-04-05-2:}
There was a performance complaint on the daily project details analysis.

\textbf{2006-04-06-1:}
Integration build failed. A developer changed code in Sdt\_UnitTest module, causing compilation failure in dependent App\_Pri module. 

\textbf{2006-04-06-2:}
In an email, one of the developers noted that there were many redundant computations in the daily project details analysis.

\textbf{2006-04-07-1:}
Integration build failed. It was caused by the same error as the previous day. The developer responsible for the error did not fix it in time.

\textbf{2006-04-07-2:}
One of the developers modified the ``DailyProjectUnitTest'' code taking advantage of its data access locality, and the result was remarkable: the analysis time for 2006-04-05 unit test metrics was reduced from 124 seconds to 6 seconds.

\textbf{2006-04-07-3:}
I discussed the charts on the telemetry wall with the project manager. He was comparing release 7.4 issue tracking chart with the chart from the previous release cycle to make short term predictions.

\textbf{2006-04-08-1:}
Integration build failed. It was caused by the same error as the previous two days. The developer responsible for the error did not fix it in time.

\textbf{2006-04-09-1:}
I finished the implementation of filter functions and closed the Jira issue (HACK-612).

\textbf{2006-04-09-2:}
I enhanced the telemetry language with \textit{``y-axis''} construct, and closed the Jira issue (HACK-616). 

\textbf{2006-04-10-1:}
I did the same thing with the ``DailyProjectFileMetric'' code, and reduced the analysis time for 2006-04-05 file metrics from 180 seconds to just 1 second.

\textbf{2006-04-10-2:}
The project manager was happy with the results. He wanted to formally document the design pattern as a best practice in the Hackystat developer guide in the summer.

\textbf{2006-04-11-1:}
I update the telemetry wall, converting some charts to use manually-specified y-axises, and modifying the module-level coverage charts to use filter functions to show only the top 5 and bottom 5 covered modules. I showed the changes to the project manager and the developers. For the y-axis enhancement, they thought the change had improved the usability a lot, because comparisons could be made more intuitively with fixed vertical axises. For the filtered charts, they liked the changes, but requested an additional chart to show modules with coverage that changed most.

\textbf{2006-04-13-1:}
Integration build failed. A developer changed one single line in Sensor\_Office and did not test the change, causing Checkstyle failure in that module.

\textbf{2006-04-14-1:}
Integration build failed. A developer changed code in Sdt\_Commit module, causing compilation failure in dependent App\_Cgqm module. 

\textbf{2006-04-16-1:}
Integration build failed. A developer changed code in Sdt\_Issue module, causing compilation failure in dependent App\_Cgqm module. 

\textbf{2006-04-17-1:}
Integration build failed. It was caused by the same error as the previous day. The developer responsible for the error did not fix it in time.


\textbf{2006-04-17-2:}
A quick poll indicated that, since three months ago the FindBugs and PMD reports were made available, none of the developers had spent over 1 hour in total reading the reports. This number was a little bit higher for the project manager, but it was only 2 - 3 hours.  

\textbf{2006-04-17-3:}
I discussed with the developers treatment options for each of the 17 types of FindBugs warnings found in hackyCore\_Kernel module in the weekly status meeting. The comments from the developers indicated that they had learned a lot by going over their own code that generated the warnings.

\textbf{2006-04-17-4:}
I revised the filter function implantation, and made the chart showing modules with coverage that changed most available on the telemetry wall. One of the developers commented that filter functions made the chart not only much cleaner but also much useful. 

\textbf{2006-04-19-1:}
Integration build failed. A developer imported Java code with wrong package names, causing the build failure.

\textbf{2006-04-20-1:}
A developer modified Jira sensor code. Telemetry charts showed missing issue metrics. It turned out it was caused by a bug in the code.

\textbf{2006-04-22-1:}
Integration build failed. A developer changed code in Sdt\_Dependency module, causing compilation failure in dependent App\_Cgqm module. 

\textbf{2006-04-22-2:}
An email from the project manager indicated he detected the same Jira sensor problem using the real-time sensor verification charts on the public Hackystat website.    

\textbf{2006-04-24-1:}
I discussed with the developers treatment options for the remaining types of FindBugs warnings found in the Hackystat source in the weekly status meeting.

\textbf{2006-04-25-1:}
I modified the code issue telemetry chart to track the number of warnings falling into ``fail'' and ``monitor'' categories. The chart was primarily designed to be used by the project manager.
I enhanced FindBugs report. Warnings in the ``fail'' category were highlighted in red color, and warnings in the ``monitor'' category were highlighted in blue color. I also modified the build script so that the developers could generate the report with single command on their workstations. This was primarily designed to be used by the developers to fix the warnings.

\textbf{2006-04-26-1:}
During an interview, the project manager told me that his project management skill had improved a lot with respect to release cycle issue tracking and planning.

\textbf{2006-04-26-2:}
The project manager assigned tasks for the developers to get rid of the warnings that fell into the ``fail'' category.

\textbf{2006-05-06-1:}
Telemetry analysis indicated that all FindBugs warnings in the ``fail'' category had been eliminated, and the warnings in the ``monitor'' category had been reduced by more than a half. I had not received any complaint from the developers about the enhanced FindBugs report.



\setlength{\parindent}{6mm} %indentation of paragraphs
\setlength{\parskip}{3mm}   %gap between paragraphs
  