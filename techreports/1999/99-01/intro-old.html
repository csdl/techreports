<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html>
  <head>
    <title>Introduction to Project Leap</title>
  </head>

<body>
<center>
<h4>Reflective Software Engineering with the Leap toolkit</h4>
<h1>Introduction to Project Leap</h1>
<p>
<!-- Created: Tue Jan 19 10:52:57 HST 1999 -->
<!-- hhmts start -->
Last modified: Fri Jul 16 11:29:24 HST 1999
<!-- hhmts end -->
<p>
</center>

<p><hr>

<h2>Table of Contents</h2>
<ul>
<li> <A HREF="intro.html#motivation">Motivation: Software development on
Internet Time</A>
<li> <A HREF="intro.html#method">The Leap method</A>
<li> <A HREF="intro.html#quicktour">A quick tour of the Leap toolkit</A>
<li> <A HREF="intro.html#references">References</A>
<li> <A HREF="intro.html#acknowledgements">Acknowledgements</A>
</ul>

<p><hr><p><h2><A NAME = "motivation">Motivation: Software development on
Internet Time</a></h2>    

The demands of software development on "Internet Time" include shortened
time to market, reduced development budgets, and faster release cycles.
The pace of technical and economic innovation in Internet Time industries
tends to result in increased organizational volatility, including frequent
restructuring and high employee turn-over.  These combined pressures can
wreak havoc with traditional, top-down process improvement initiatives,
which typically require sustained commitment from top-level management for
years at a time, "champions" who remain within the organization with stable
responsibilities, and a stable developer, product, and market focus so that
any process improvement opportunities identified during one product or
development cycle remain relevant during the next product and development
cycle.  Traditional, top-down process improvement initiatives tend
to incur significant financial and administrative costs to administer the
program, report its findings, and justify its continued existance.

<p>

<em>Empirical</em> top-down process improvement initiatives must combat an
additional problem: measurement dysfunction.  Research by Robert Austin [Austin96]
on software development organizations identifies measurement dysfunction as
a significant obstacle to process improvement. Measurement dysfunction
refers to a situation in which the act of measurement affects the
organization in a counter-productive fashion, and which leads to results
directly counter to those intended by the organization for the measurement.
Such dysfunction occurs because many process measures have two potential
applications: (1) to provide <em>information</em> about the organization
and (2) to support <em>performance evaluation</em> of individuals and
groups.  Since it is impossible for an organization to guarantee that a
measure, once collected by the organization, will never be used for
performance evaluation, process measures may be skewed to reflect what the
organization (or process improvement team) wants or needs to hear, rather
than what is actually occuring in the organization. 

<p>

Despite these concerns, traditional top-down process improvement
initiatives remain an important and valuable component of a high quality
software development organization.  However, in conjunction with top-down
approaches, it is also possible to pursue a "bottom up", developer-centered
approach. In a bottom-up approach, the focus is on providing individual
developers with the insights necessary to acquire and improve their
technical skills.  Management buy-in and support is secondary to the
developers' self-interest in their own professional development.
Management reports on the progress and success of the individual's skill
acquisition efforts are no longer required and in fact counter-indicated,
since preserving the privacy of personal measurements and insights
is important to preventing measurement dysfunction.  Finally, given the
increasing tendency of modern software developers to change jobs every
couple of years, which can render their interest in and commitment to top-down
process improvement initiatives as marginal at best, a bottom-up approach
represents a "portable" activity that the developer can maintain across
jobs and companies.

<p>

Since 1997, we have pursued a research initiative regarding bottom-up
technical skill acquisition and improvement called Project Leap.  We hope
through this research initiative to uncover some of the principles
underlying successful bottom-up process improvement.  Project Leap
leverages our prior research experiences in formal technical review [Johnson98] and
the Personal Software Process [Disney98].  Based upon these experiences, we
conjecture that approaches to bottom-up process improvement are made more
effective by obeying the four "Leap" design constraints:

<ul>
  
<p><li><b>L</b>ightweight.  Bottom-up methods should be ``lightweight''.
  In other words, they must involve a minimum of process constraints, be
  easy to learn, easy to integrate with existing methods and tools, and
  require minimal investment and commitment from management. If a bottom-up
  method imposes new overhead on the developer, then that effort must yield
  a short-term, positive return-on-investment to that same developer.
  
<p><li><b>E</b>mpirical. Bottom-up methods should have a quantitative, as
well as qualitative dimension.  A lightweight orientation cannot be gained
at the expense of high quality collection and analysis of data. Developer
improvement should be observable over time through measurements of effort,
defects, size, and so forth.
  
<p><li><b>A</b>nti-measurement dysfunction.   Measurement, while an
  integral part of effective bottom-up methods, should be carefully
  designed to minimize dysfunction.  Yet the most simple solution to
  dysfunction, making all data totally private, is incompatible with the
  benefits to the organization of sharing certain kinds of data and process
  improvements. A goal of Project LEAP is to find a suitable balance between
  totally public and totally private measurement data.
  
<p><li><b>P</b>ortable. 
  Useful developer improvement support cannot be locked to a particular
  organization such that the developer must ``give up'' the data and tools
  when they leave the organization. Rather, this support should be akin to a
  developer's address book; a kind of ``personal information assistant''
  for their software engineering skill set that they can take with them
  across projects and companies.

</ul>
<p>

These four criteria, when composed together, create additional
requirements. For example, we believe that extensive automation is
required within any method that is simultaneously lightweight, empirical,
and anti-measurement dysfunction.  On the other hand, automation clearly
does not guarantee lightweight processes or meaningful empirical evidence
of improvement. As an example, a repeated criticism of our CSRS automated
review system was that its extensive measurement system would lead to
dysfunctional behavior in an industrial setting.  

<p>

Our efforts in Project Leap have produced a toolkit which has been in
public release for approximately one year, and in active classroom and
research use for approximately six months. The Leap toolkit is also under
small scale evaluation at two of our industrial affiliate sites, and we
intend to pursue broader industrial evaluation over the coming year. 
Indeed, our motivation for this research demo presentation is to 
introduce the toolkit to a broad audience and solicit increased 
involvement in its evaluation from the research and industry communities. 

<p>

The Leap toolkit is implemented in 100% Java and runs on all major
platforms. The most recent release is available for download from the Leap
Toolkit Home Page at <A
HREF="http://csdl.ics.hawaii.edu/Tools/LEAP/LEAP.html">http://csdl.ics.hawaii.edu/Tools/LEAP/LEAP.html</A>.


<p><hr><p><h2><A NAME = "method">The Leap method</a></h2>    

The tools in the Leap toolkit are designed to support the following general
cycles of improvement as illustrated in the following figure:

<p>
<img src="screens/intro-cycle.gif" width=100%>
<p>

While all of the paths in this figure are important, two paths are
highlighted for conceptual clarity.  The "green" path represents the
personal improvement cycle of technical skill acquisition and improvement,
which is augmented by the "blue" path representing group-based
review.  The following provides a bit more detail on the illustration:

<p>
<ul>
<p><li><em> Generate or refine goals for technical skill acquisition and
improvement. </em> Example goals could include improved estimation of
size or time, improved skill at upstream design, increased direct
hours on major tasks, decreased incidence of certain classes of defects,
etc.
<p><li><em> Generate or refine definitions of projects, defect types,
document types, etc.</em> Leap definitions reduce the overhead of 
data collection (by providing pull-down or pop-up menu support for
common data types) and aid in analysis (by ensuring common terminology
between projects).

<p><li><em> Collect primary data on size, time, and defects.</em> The Leap
toolkit includes specialized tools for collection of size and time. 
We have released another tool, called <A HREF="http://csdl.ics.hawaii.edu/Tools/LOCC/LOCC.html">LOCC</A>, to support collection of
size data. 

<p><li><em> Obtain additional defect data via group review.</em> Leap
builds in support for asynchronous review and dissemination of 
review artifacts via the web and/or email.

<p><li><em> Perform analyses on primary data.</em> Leap builds in dozens of
analyses accessible through pull-down menus, which provide charts and
tables concerning time, size, defects, defect types, rates (size/time,
defects/time), densities (defects/time, defects/size), trends, and
planning/estimation. 

<p><li><em> Evaluate progress toward goals.</em> Historical trend charts
can help developers identify whether they are making progress toward
their stated goals, and static charts indicate whether their actual
data meets their estimates.

<p><li><em> Generate checklists and patterns. </em>  To support defect
prevention and encode "best practice", Leap enables developers
to generate checklist items and simple "patterns". 


</ul>
<p>

The Leap approach follows earlier formulations for process improvement,
such as GQM [Basili84]. It is the attempt to satisfy the Leap constraints
in a bottom-up context that produces several differences between the Leap
toolkit and other top-down approaches.  

<ol>
<li> Leap does not record
authorship or other identification; all data collected and manipulated in
Leap is unattributed.  In a personal environment, authorship is
unnecessary, and lack of attribution is a small but important step toward decreased
measurement dysfunction.  
<li> While Leap provides various definition
mechanisms to enable developers to describe what kind of procedures they
used to develop a work product or perform a review, Leap makes no attempt
to enforce or assess compliance with a particular procedure. Indeed, Leap
recognizes that useful definitions must be "bootstrapped" over time through
the use of the tool.  
<li>Leap enables developers complete control over
what kinds of Leap information is shared with others. While developers
typically are happy to share certain kinds of insights, such as checklists
and patterns, we have found that time data is especially susceptible to
measurement dysfunction.  Thus, Leap makes it easy for developers to keep
track of time they devote to a review activity, but provide to others only
the set of defects they uncovered during review.  
<li> Leap provides an
integration mechanism for both personal software engineering data and data
generated through the process of group review.
</ol>
<p>

Leap is similar in many ways to the Personal Software Process [Humphrey95].
The essential differences between Leap and the PSP are as follows.  First,
the PSP views automated support as helpful but optional.  Leap views
automation as essential to reducing the overhead of process data collection
and analysis to an acceptable level. Our prior research also indicates that
automation may be necessary (though not sufficient) for collection of
accurate personal process data [Disney98].  Second, the PSP involves an
essentially "heavyweight" orientation toward process definition and
adherence: in the PSP, one is instructed to follow quite rigidly defined
process scripts which sometimes involve practices quite unfamiliar to most
developers (such as to completely code all system definitions before
compiling for the first time).  Leap allows a more "lightweight"
orientation, in which one can begin collecting and analyzing data without a
great deal of process definition, adding such definitions incrementally
when deemed useful.  Third, unlike PSP, Leap integrates support for
asynchronous review as an essential service in the toolkit.  Fourth, the
PSP requires you to collect data on your defects---what you do wrong. In
addition to defects, Leap also helps you to collect data on your
patterns---what you do right.

<p>

<p>

<p><hr><p><h2><A NAME = "quicktour">A Quick Tour of the Leap Toolkit</a></h2>    

This User Guide goes into great detail on each of the Leap tools and their
applications.  This introductory chapter concludes with a quick tour of
selected portions of the Leap toolkit, using screen shots taken from
several different developers working on different projects. These screens
are intended to provide a general overview of what you can expect using
Leap.


<p>
<img src="screens/intro-tool.gif">
<br>
This LeapTool window is what first appears after invoking the toolkit.
Its menus provide access to 14 separate window-based tools and viewers,
plus any drop-in "extensions" provided by third parties.


<p>
<img src="screens/intro-projects.gif" width=100%>
<br>
Most Leap data is organized into "projects" which we
recommend to be relatively 
short-term (1-3 week) increments of work. 

<p>
<img src="screens/intro-timing.gif" width=100%>
<br>
Leap provides a timing tool that allows you to dynamically record the time
you spend on a project. 

<p>
<img src="screens/intro-sizes.gif" width=100%>
<br>
Leap provides a mechanism for recording hierarchical size measurements for
work products. In addition, more than one type of size measure can be 
provided for any given work product. (For example, both lines of code and
function point measures might be provided for a code work product.) 
To simplify size counting, we provide a specialized size measurement tool
called <A HREF="http://csdl.ics.hawaii.edu/Tools/LOCC/LOCC.html">LOCC</A>
(not shown).

<p>
<img src="screens/intro-defects.gif">
<br>
This window shows one tool for recording defect data for use in Leap.
Several fields provide popup menus to facilitate data entry. In addition,
we have developed other tools, such as <A
HREF="http://csdl.ics.hawaii.edu/Tools/Defmacs/Defmacs.html">Defmacs</A>
(not shown), to support defect collection in more specialized contexts and
environments.

<p>
<img src="screens/intro-summary.gif">
<br>

This image shows some of the summary statistics provided by Leap about
projects, which can be calculated at any time during the course of the project.

<p>
<img src="screens/intro-timetrend.gif" width=100%>
<br>
This analysis chart shows the number of direct hours applied to projects at
the grain size of weeks. Leap can also display direct hour trends on
a daily or monthly basis.

<p>
<img src="screens/intro-planning.gif" width=100%>
<br>
This interactive analysis tool shows a graph of projects with respect
to their size and direct hours of effort. Using simple average/max/min
values, Leap can provide an estimate of the direct hours for a future 
project given an estimate of its size.  The size measure used can
be configured by the user. 


<p>
<img src="screens/intro-email.gif">
<br>
This window illustrates Leap support for distributing review and other
process data. Developers have complete control over which kinds of 
data they share with others. Leap also enables developers to download
Leap data files directly from the Internet by entering an URL.


<p>
<img src="screens/intro-checklists.gif" width=100%>
<br>
This final screen shot illustrates how Leap represents checklists.
It is possible to note with each defect whether a checklist was
used to find it, and thus determine the effectiveness of various
checklist items over time.  

<p>

<p><hr><p><h2><A NAME = "references">References</a></h2>    

<ul>
<li> [Austin96] Robert D. Austin, Measuring and Managing Performance in
Organizations, Dorset House Publishing, 1996.

<li> [Basili84] Victor Basili and David Weiss, "A Methodology for Collecting
Valid Software Engineering Data", IEEE Transactions on Software
Engineering,
Vol. SE-10, no. 6, November 1984. 

<li> [Disney98] Anne M. Disney and Philip M. Johnson, Investigating Data
Quality Problems in the PSP, Proceedings of the ACM SIGSOFT Sixth
International Symposium on the Foundations of Software Engineering, Lake
Buena Vista, FL, November, 1998.

<li> [Humphrey95] Watts S. Humphrey, A Discipline for Software Engineering,
Addison-Wesley, 1995.

<li> [Johnson98] Philip M. Johnson and Danu Tjahjono, 
"Does Every Inspection Really Need A Meeting?",
Journal of Empirical Software Engineering, 
Volume 4, No. 1, January, 1998.

</ul>
<p>    
<p><hr><p><h2><A NAME = "acknowledgements">Acknowledgements</a></h2>    

Project Leap and the Leap toolkit is supported by grant CCR-9804010
from the National Science Foundation, and from grants by Tektronix,
Inc. and Digital Equipment Corporation/Compaq.



</body>
</html>
