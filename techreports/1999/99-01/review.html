<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html>
  <head>
    <title>Introduction to Software Review using Leap</title>
  </head>

<body>
<center>
<h4>Reflective Software Engineering with the Leap toolkit</h4>
<h1>Introduction to Software Review using Leap</h1>
<p>
<!-- Created: Tue Jan 19 10:52:57 HST 1999 -->
<!-- hhmts start -->
Last modified: Fri Jul 16 11:30:32 HST 1999
<!-- hhmts end -->
<p>
</center>

<p><hr>

<h2>Table of Contents</h2>
<ul>
<li> <A HREF="review.html#motivation">Why do review?</A>
<li> <A HREF="review.html#overview">Overview of Leap-based review </A>

<li> <A HREF="review.html#planning">Phase 1: Planning</A>
  <ul>
  <li> <A HREF="review.html#planningWorkspace">Creating the review workspace</A>
  <li> <A HREF="review.html#planningDefs">Setting up review_defs.leap</A>
  <li> <A HREF="review.html#planningPrefs">Setting up review_prefs.leap</A>
  <li> <A HREF="review.html#planningBatch">Setting up the RunReview script</A>
  </ul>

<li> <A HREF="review.html#orientation">Phase 2: Orientation</A>
  <ul>
  <li> <A HREF="review.html#orientationEmail">An orientation email</A>
  </ul>



<li> <A HREF="review.html#preparation">Phase 3: Preparation</A>
  <ul>
  <li> <A HREF="review.html#preparationRecording">Recording defects</A>
  <li> <A HREF="review.html#preparationDistributing">Distributing defects</A>
  </ul>

<li> <A HREF="review.html#meeting">Phase 4: Meeting/Consolidation</A>

<li> <A HREF="review.html#rework">Phase 5: Rework</A>

<li> <A HREF="review.html#postmortem">Phase 6: Postmortem</A>
  <ul>
  <li> <A HREF="review.html#postmortemForms">Forms of postmortem analysis</A>
  </ul>

</ul>

<p><hr><p><h2><A NAME = "motivation">Why do review?</a></h2>

Formal technical review (FTR) is acknowledged as one of the most efficient and
effective approaches to improving the quality of software work products,
including specifications, designs, plans, and test cases as well as
code. Although code review was the focus of early approaches to software
review, modern practice focuses instead on more upstream documents because
specification and design defects are typically the most expensive kinds of
defects to remove later on in the development process.

<p>

Formal technical review is defined as:   
<BLOCKQUOTE> 
<EM>
  A method involving a structured encounter in which a group of technical
  personnel analyzes an artifact according to a well-defined process.
  The outcome is a structured artifact that assesses or improves
  the quality of the artifact as well as the quality of the method.
</EM>
</BLOCKQUOTE>
While there are many variants to FTR, most obey a five step process:
<ol>
<li> Planning, involving the setup of the review by a leader.
<li> Orientation, involving team formation and introduction to the work
product(s) to be reviewed.
<li> Preparation, in which reviewers individually analyze the work product
and look for defects.
<li> Meeting/Consolidation, in which the 
results of the reviewers' individual preparations are consolidated 
into a single, consensual report on the work product. Traditionally, this
consolidation and consensus-building
process is performed in a meeting, although asynchronous, non-meeting based
approaches have also been investigated. 
<li> Rework, in which the author of the work product removes the defects
found.
</ol>
<p>

Originally, FTR was an entirely manual process involving paper printouts
and pencilled comments. In the past decade, several different
computer-supported approaches to formal technical review have been 
implemented.  The <A HREF="http://www.ics.hawaii.edu/~johnson/FTR/">WWW FTR
Archive</A> provides links to most of these systems. In addition, some
of the assumptions underlying traditional review, such as the need for 
a face-to-face meeting, have started to come under question.

<p>

Despite a broadening in scope and process, it remains clear that bringing a
group of technical personnel together to focus attention on a work product
has an undeniably positive effect upon its quality.  The debate tends to
center on how to perform this activity most cost-effectively.  Here, it is
also clear that cost-effectiveness depends to a great extent on the
characteristics of the organization.  For example, some organizations (such
as certain development groups at Lucent, Inc.)  have determined that
face-to-face meetings add little to the effectiveness of reviews while
adding substantially to their cost.  However, in other, less mature
development organizations, the face-to-face meeting can provide an
opportunity for high quality education about domain knowledge and
organizational development practices.  It is wise for an organization
beginning review to explore the variety of approaches available to help
determine which is most useful and effective for their situation.
The FTR Archive contains links to literature and training organizations
to support organizations in this process.

<p><hr><p><h2><A NAME = "overview">Overview of Leap-based review</a></h2>

We designed review support in the Leap toolkit to satisfy the following
objectives:

<ul>
<p><li> <em>Low clerical overhead.</em>  Manual forms of software review can 
incur substantial clerical overhead, including distribution of
work documents, collation of issues collected by reviewers, 
record taking during the review meeting, documentation
of rework activities, and collection and analysis of review
metrics.  A primary goal of Leap support for software review
is to reduce the overhead involved with distribution and
management of issue data and review measurements.

<p><li> <em>Process neutrality.</em>  Review is not a one-size-fits-all 
approach to software quality improvement.  The Leap tools
attempt to provide generally useful mechanisms for entering
and disseminating review data and measures that can be 
adapted to a wide variety of specific review procedures. As
a specific example, Leap tools for review are suited to
both methods including and excluding the review meeting.

<p><li> <em>Geographic and temporal neutrality.</em>  Leap support for review does
not require reviewers to be the same location or to perform
review at the same time.

<p><li> <em>Review measurement and analysis at the individual level.</em>
Unlike traditional review measurement systems, which focus exclusively on
measurement and analysis for organizational process improvement, Leap
supports individual measurement and analysis.  For example, Leap analyses
help individual authors to reflect upon the kinds of defects found in their
work products over time and to design checklists to aid them in preventing
those kinds of defects from arising in their work products before
review. Leap also helps reviewers become more skilled at review, by
enabling them to track the kinds of defects others find in work products
that they miss.

</ul>

The next six sections provide examples of using Leap to support six
basic stages in software review.  These stages can and should be
modified to suit the context of your specific organization.  

<p><hr><p><h2><A NAME = "planning">Phase 1: Planning</a></h2>

In Leap, as in most forms of FTR, the planning phase requires the review
leader to accomplish two major goals: (1) establish the review process, and
(2) organize the artifacts to be used in review.  Review planning in Leap
involves the review leader in three major activities: creating the review
workspace, setting up the leap definitions and preferences files, and
setting up the RunReview batch file.

<h3><A NAME = "planningWorkspace">Creating the review workspace</a></h3>

Although it is not strictly necessary, I highly recommend that the review
leader define a new subdirectory or folder to serve as the central
"workspace" for each review. This workspace should be accessable to 
all members of the review team, either via a shared file system or
the Web.  This shared workspace will be initially populated with four
things:
<ol>
<li> The work product or products to be reviewed.
<li> A batch file used to invoke Leap for this review (typically called
RunReview).
<li> A configuration file containing the review process definitions (typically called
review_defs.leap).
<li> A configuration file of settings to customize various Leap tools for review
(typically called review_prefs.leap). 
</ol>

Leap comes with "shrink-wrapped" versions of the batch and configuration
files for review in the Leap/demo directory that you should use as a basis
for defining your own customized review process.  The following
illustration shows an example subdirectory after the initial setup with a
sample document to review and copies (<em>not</em> shortcuts, aliases, or
symbolic links!) of the shrink-wrapped versions of the batch and
configuration files taken from the Leap/demo directory.

<p>
<img src="screens/review-workspace.gif">
<p>

<h3><A NAME = "planningDefs">Setting up review_defs.leap</a></h3>

Having created the workspace, the next step is to customize the
review_defs.leap file now located in your review workspace.  To begin,
bring up a fresh version of Leap and load in this file. The
review_defs.leap file provides sample entries for many of the Leap objects
under the "definitions" menu, including a sample project, document type,
defect types, severities, phases, docID, size definitions, and checklists.
At least initially, you'll be able to do review using this file
as a base with just a few minor changes. However, it's important to 
understand exactly how the definitions are used even if you don't 
need to change them, so the next several paragraphs discuss all
of the definitions in turn.

<ul>

<p><li> <em>Project. </em> To facilitate later analysis of review data, it is
helpful to define each review as a separate project in Leap. To define a
project, bring up Ilio and fill in the name and a brief description of the
review.  You should also select a phase set by right clicking in the
PhaseSet field to pop up a menu of PhaseSets (a "Review" phase set is
provided as a default.) If you desire to compute review measures involving
size (such as defect density, review rate, etc), then you need to specify
the file name of the work product or products in the "FileList" field of
the Project table. If you do not require these size-related measures, then
you can leave the FileList field blank.
<p>

Here's an excerpt from the project table in review_defs.leap:
<p>
<TABLE BORDER>
<TR>
<TH>Project</TH><TH>Project Description</TH><TH>FileList</TH><TH>PhaseSet</TH><TH>Start</TH><TH>Stop</TH></TR>
<TR>
<TD>98-13-review</TD>
<TD>Review of tech report 98-13</TD>
<TD>98-13.tex</TD>
<TD>Review</TD>
<TD>Thu Feb 11 10:43:00 HST 1999</TD>
<TD></TD>
</TR>
</TABLE>



<p><li><em>Document Type.</em> Defining the type of document for this work
product is also quite useful, since it allows you to organize all of the
defects you've collected on all of projects and analyze them separately by
document type. To define a document type, bring up Puaa and add the 
document type name and description.  The review_defs.leap file provides
five document types by default. 

<p>
Here's an excerpt from the document types table in review_defs.leap:
<p>
<TABLE BORDER>
<TR>
<TH>Document Type</TH><TH>Description</TH></TR>
<TR>
<TD>Java Source</TD>
<TD>Java source code</TD>
</TR>
<TR>
<TD>JavaDoc</TD>
<TD>JavaDoc design</TD>
</TR>
<TR>
<TD>TechReport</TD>
<TD>General research publication</TD>
</TR>
<TR>
<TD>SRS</TD>
<TD>Software Requirements Specification</TD>
</TR>
<TR>
<TD>Presentation</TD>
<TD>Research presentation (typically Powerpoint)</TD>
</TABLE>





<p><li><em>Defect Types.</em> Each document type can be associated with a
set of defect types, which help reviewers to categorize the issues that
they find and can aid later analysis and interpretation of review measures.
Pueo defines a set of default defect types for each of the default document
types. These default types are provided merely for illustrative purposes,
and you can and should create an organization-specific set of defect types
for each of your document types.  In most cases, defect type categories
should be created or refined from analysis of previous defects uncovered in
the organization's work products.  Although the default defect type 
categories use a numeric prefix, and we find this to be helpful, it is 
not required by Leap.

<p>
Here's an excerpt from the defect types table in review_defs.leap:
<p>
<TABLE BORDER>
<TR>
<TH>Defect Type</TH><TH>Description</TH><TH>DocType</TH></TR>
<TR>
<TD>10: Syntax</TD>
<TD>General syntax errors</TD>
<TD>TechReport</TD>
</TR>
<TR>
<TD>20: Structure</TD>
<TD>Errors in the section/subsection organization</TD>
<TD>TechReport</TD>
</TR>
<TR>
<TD>30: Motivation</TD>
<TD>Error in motivating the report</TD>
<TD>TechReport</TD>
</TR>
<TR>
<TD>40: Thesis/claim</TD>
<TD>Missing/problematic thesis or claim</TD>
<TD>TechReport</TD>
</TR>
<TR>
<TD>50: Evaluation</TD>
<TD>Missing/problematic evaluation mechanism</TD>
<TD>TechReport</TD>
</TR>
<TR>
<TD>60: Interpretation</TD>
<TD>Errors in interpreting results</TD>
<TD>TechReport</TD>
</TR>
<TR>
<TD>70: Future directions</TD>
<TD>Errors in extrapolating results</TD>
<TD>TechReport</TD>
</TR>
<TR>
<TD>80: Lucidity</TD>
<TD>Vague or unclear areas</TD>
<TD>TechReport</TD>
</TR>
<TR>
<TD>99: Misc. Error</TD>
<TD>Miscellaneous error</TD>
<TD>TechReport</TD>
</TR>
</TABLE>


<p><li><em>Severities.</em> It can be very useful for reviewers to provide
an estimate of the severity of the particular issue. The Noio tool allows
entry of severity levels, and three default severities (High, Medium, Low)
are provided in review_defs.leap. Severities must be assigned an integer
"level" which is used to order them from the most severe severity (of
level "1") on downward.

<p>
Here's an excerpt from the severities table in review_defs.leap:
<p>
<TABLE BORDER>
<TR>
<TH>Severity</TH><TH>Level</TH><TH>Description</TH></TR>
<TR>
<TD>High</TD>
<TD>1</TD>
<TD>Highest severity error, precludes use of system</TD>
</TR>
<TR>
<TD>Medium</TD>
<TD>2</TD>
<TD>Moderate severity error, precludes use in many cases.</TD>
</TR>
<TR>
<TD>Low</TD>
<TD>3</TD>
<TD>Low severity error, only rarely effects use.</TD>
</TR>
<TR>
<TD>NonDefect</TD>
<TD>4</TD>
<TD>An issue that does not appear to actually be a defect.</TD>
</TR>
</TABLE>


<p><li><em>Phases.</em> The review method outlined in this chapter consists
of six phases: Planning, Orientation, Preparation, Meeting/Consolidation, Rework, and
Postmortem. These six phases are defined in the review_def.leap file, and
collected together into the "Review" phase set.  An integer field called
"Order" is used to define the sequence of phases.  You can modify the 
contents of this table in the Ulua tool to change the existing phase set
or define additional phase sets for alternative review methods.

<p>
Here's an excerpt from the phases table in review_defs.leap:
<p>
<TABLE BORDER>
<TR>
<TH>Phase</TH><TH>Order</TH><TH>PhaseSet</TH><TH>Description</TH></TR>
<TR>
<TD>Planning</TD>
<TD>1</TD>
<TD>Review</TD>
<TD>Planning phase, including setting up the workspace and customizing
the process.</TD>
</TR>
<TR>
<TD>Orientation</TD>
<TD>2</TD>
<TD>Review</TD>
<TD>Getting the review team notified and prepared for review.</TD>
</TR>
<TR>
<TD>Preparation</TD>
<TD>3</TD>
<TD>Review</TD>
<TD>Individual reviewer analysis of the work product</TD>
</TR>
<TR>
<TD>Meeting/Consolidation</TD>
<TD>4</TD>
<TD>Review</TD>
<TD>Face to face or asynchronous consolidation of issues, verification
that they are defects, and assignment/refinement of their severity level</TD>
</TR>
<TR>
<TD>Rework</TD>
<TD>5</TD>
<TD>Review</TD>
<TD>Correction of defects found. Typically removed by the author.</TD>
</TR>
<TR>
<TD>Postmortem</TD>
<TD>6</TD>
<TD>Review</TD>
<TD>Analysis of review metrics and generation/refinement of checklists,
 patterns, and definitions.</TD>
</TR>
</TABLE>


<p><li><em>DocIDs.</em> Defining a DocID for each work product inspected
during a review is useful for keeping track of which issues were 
associated with which work product.  In addition, if the same file
is inspected more than once over a period of time, using a DocID
(rather than simply the file name) can help keep track of which issues
were discovered during which review.
<p>
Here's an excerpt from the DocIDs table in review_defs.leap:
<p>
<TABLE BORDER>
<TR>
<TH>Document ID</TH><TH>Description</TH><TH>DocType</TH></TR>
<TR>
<TD>98-13.pdf</TD>
<TD>The 98-13 technical report</TD>
<TD>TechReport</TD>
</TR>
</TABLE>


<p><li><em>Size Definitions.</em> The size definitions table, manipulated
using the Aama tool, is needed in review only if you intend to 
produce size-related measures.  In this case, you need to define
a size definition corresponding to the size measurements you 
produce on each document type. (A given document type can have
more than one possible size definition, incidentally).  The 
review_defs.leap file includes two example size definitions. Size
definition and measurement in Leap is a complex topic and a
separate chapter in this User Manual is devoted to that topic.
<p>
Here's an excerpt from the size definitions table in review_defs.leap:
<p>
<TABLE BORDER>
<TR>
<TH>Size Type</TH><TH>DocType</TH><TH>FirstLevel</TH><TH>SecondLevel</TH><TH>ThirdLevel</TH><TH>FourthLevel</TH></TR>
<TR>
<TD>JavaSize</TD>
<TD>Java Source</TD>
<TD>LOC</TD>
<TD>Method</TD>
<TD>Class</TD>
<TD>Package</TD>
</TR>
<TR>
<TD>WordSize</TD>
<TD>TechReport</TD>
<TD>Words</TD>
<TD></TD>
<TD></TD>
<TD></TD>
</TR>
</TABLE>


<p><li><em>Checklists.</em> Checklist items can be quite useful in
review. The Kala tool supports entry and modification of checklists.  Sets
of checklist items are organized by document type. The review_defs.leap
file includes over 30 example checklist items.  Most groups will 
eventually want to evolve this initial list into one
containing checklists customized to their organization and 
work products. 
<p>
Here's an excerpt from the checklists table in review_defs.leap:
<p>

<TABLE BORDER>
<TR>
<TH>Checklist</TH><TH>DocType</TH><TH>Phase</TH><TH>Description</TH></TR>
<TR>
<TD>10: Passive Voice</TD>
<TD>TechReport</TD>
<TD></TD>
<TD>Are there any occurrences of passive voice construction?</TD>
</TR>
<TR>
<TD>20: Abstract</TD>
<TD>TechReport</TD>
<TD></TD>
<TD>Does the abstract concisely summarize the hypothesis and 
contributions of the technical report?</TD>
</TR>
<TR>
<TD>30: Thesis</TD>
<TD>TechReport</TD>
<TD></TD>
<TD>Is the thesis or main purpose of the technical report clearly stated?</TD>
</TR>
<TR>
<TD>40: Structure</TD>
<TD>TechReport</TD>
<TD></TD>
<TD>Do the section and subsection titles alone provide a clear
representation of a lucid and well organized presentation?</TD>
</TR>
<TR>
<TD>50: Evidence</TD>
<TD>TechReport</TD>
<TD></TD>
<TD>Are all claims backed up with evidence?</TD>
</TR>
<TR>
<TD>60: Transitions</TD>
<TD>TechReport</TD>
<TD></TD>
<TD>Does each section transition smoothly into the next?</TD>
</TR>
</TABLE>


<p><li> <em>Work product size data.</em> While not located under the
"definitions" menu, if you desire to calculate size-based review measures,
you will need to define the size of the work product(s) under review.  To do
this, you use the Iole tool that is accessable from the "Data" menu. The
example review_defs.leap file shows one sample size data entry for the file
98-13.tex (the same file name that was listed in the "FileList" field for
the Project called "98-13-review").


<p>
Here's an excerpt from the sizes table in review_defs.leap:
<p>

<TABLE BORDER>
<TR>
<TH>Date</TH><TH>SizeType</TH><TH>Path</TH><TH>File</TH><TH>Project</TH><TH># of Units</TH></TR>
<TR>
<TD>Thu Feb 11 12:36:00 HST 1999</TD>
<TD>WordSize</TD>
<TD></TD>
<TD>98-13.tex</TD>
<TD>98-13-review</TD>
<TD>13128</TD>
</TR>
</TABLE>


</ul>

Once you are satisfied with your definitions, you should save out the 
modified version of review_defs.leap into the review worksapce 


<h3><A NAME = "planningPrefs">Setting up review_prefs.leap</a></h3>

The review_prefs.leap file contains configuration information useful for
review. The two most important configuration items are the MailServer and
the Hidden Column Preferences.  When all members of the review group use
the same mail server, setting the MailServer preference once in
review_prefs.leap saves each reviewer from setting it individually.  
Of course, in distributed review settings, reviewers can simply
override the default setting with their own Mail Server setting. 
To customize the Mail Server setting, bring up Ahi (located under
the File menu in the Leap command tool), and change the 
Mail Server hostname value. Then select the "Export" menu item to 
save these configurations into your review workspace with the 
name review_prefs.leap.  It will ask you if it's OK to overwrite
the current file, which you should do.

<p>

The Hidden Column preference setting is useful for configuring 
Honu for the purposes of review.  Honu, the defect recording tool,
has many fields, some of which are more useful for personal defect
collection, and some of which are more useful for group review.
The default review_prefs.leap file contains settings for Honu and
other tools that hide and show the fields that are well suited to
review. To customize the displayed fields, you should go into 
Honu and use the Hide/Show menu to set Honu's display the way
you want it. Then, bring up Ahi and press Export as noted above
to save the current window configuration. 

<p>

<h3><A NAME = "planningBatch">Setting up the RunReview script</a></h3>

The final file to modify is the script file used to invoke Leap 
for the purpose of review.  This syntax of this script file depends
upon the operating system, but the approach is the same in all
cases: the script simply invokes Leap, passing it the files
review_defs.leap and review_prefs.leap, and instructing it to 
open Honu automatically.  On Windows platforms, we supply a file
named RunReview.bat to do this, which looks similar to the 
following, with the jar files omitted for brevity's sake:

<pre>
rem RunReview.bat: Invokes Honu with review_defs.leap and review_prefs.leap

rem Edit if LEAP is not installed in default location.
set LEAP=c:\PROGRA~1\Leap

rem Change the directory location of review_defs.leap and review_prefs.leap! 
%LEAP%\jre\bin\jre -cp <em>&lt;jars&gt;</em> csdl.java.leap.tool.LeapTool %LEAP%\demo\review_defs.leap %LEAP%\demo\review_prefs.leap  -honu
</pre>

Customization of this file is extremely simple. All you have to do is edit
the final line to change the subdirectory location of review_defs.leap and
review_prefs.leap from the demo directory to your review workspace
directory, then save it back into your review workspace.  Note that if 
you did not install LEAP in the default location, you'll need to edit the 
first 'set' command in this script as well.

<p><hr><p><h2><A NAME = "orientation">Phase 2: Orientation</a></h2>

During the orientation phase, the review leader informs the review team of
the task to be performed and the location of the review workspace.  For an
experienced team that is well acquainted with both review and the work
product, the orientation phase might consist of a 
simple email message, such as the one illustrated below:


<p>
<img src="screens/review-email.gif" width=100%>
<p>

For inexperienced review teams, it may be more preferable to schedule a
face to face meeting to talk about the purpose and method of review,
potentially including a demonstration of how the Leap tool is used to
record defects.

<p><hr><p><h2><A NAME = "preparation">Phase 3: Preparation</a></h2>

<p><h3><A NAME = "preparationRecording">Recording defects</a></h3>

The essential goal of the preparation phase is for each reviewer to analyze
the work product(s) and discover and record any major issues they find.
Many approaches to preparation have been investigated, including:
<em>ad-hoc</em> approaches where the reviewers simply read and record
issues; <em>checklist-based</em> approaches where reviewers are guided by
one or more checklists, <em>perspective-based</em> approaches where each
reviewer analyzes the work product from a particular point of view;
<em>active</em> approaches where the reviewers "exercise" the work product
by solving problems and/or constructing other work products; and
<em>statistical</em> approaches where reviewers "sample" portions of a
large work product in order to obtain an estimate of the overall quality,
and so forth.  Leap can provide helpful infrastructure support for 
any of these approaches to preparation. 

<p>

To begin the preparation phase, reviewers obtain a copy of the work
products from the workspace and double click on the Leap review script
(i.e. RunReview.bat in Windows).  RunReview.bat will automatically
load the review_defs.leap and review_prefs.leap files and bring up
the Honu tool for defect recording: as illustrated here:

<p>
<img src="screens/review-honu.gif" width=100%>
<p>

As you can see, Honu has been configured to show seven fields for each
defect: the document ID, project, location, defect type, severity, description,
and the checklist used to uncover the defect, if any.  

<p>

If reviewers wish to record the time they spend in preparation, then 
they can also bring up the Io time recording tool from the "Data"
menu in the Leap command window.

<p>
Next, reviewers begin the analyze the document, looking for issues.  
Here's a full screen image of one reviewer during analysis of 
the 98-13 work product. The screen image illustrates the work product,
the use of Honu to record defects, and the use of Io to record
the time spent during preparation:

<p>
<img src="screens/review-fullscreen.gif" width=100%>
<p>

Honu is an example of a table viewer, and so includes all of the
functionality described in the chapter on <A HREF="tables.html">Generic
Table Functionality</A>.  For example, the docID, defect type, severity,
and checklist fields are all supported with popup menus to ease entry of
predefined values for these fields.

<p><h3><A NAME = "preparationDistributing">Distributing defects</a></h3>

The defect information recorded into Honu can be distributed in several ways.
First, the Save button in Honu can be used to save the defect data in the
review workspace.  You can use the Load button to reload the file of defect
data for further work during a later session.  The Export
Tables commands can also be used to save defect data and/or other 
data (such as time data) into the review workspace for shared use.

<p>

A second way to distribute Leap data is via email.  Pressing the email
button in Honu brings up the following window:

<p>
<img src="screens/review-emailwindow.gif">
<p>

This window allows you to select which of your currently loaded
data to include in an email message to the recipients.  This email
mechanism is particularly useful in distributed review situations
where no common shared filespace is available.

<p>

In our example review, by the end of the preparation phase, four
reviewers have placed Leap data files containing their 
proposed defects into the review workspace, as shown below:

<p>
<img src="screens/review-prepfolder.gif">
<p>


<p><hr><p><h2><A NAME = "meeting">Phase 4: Meeting/Consolidation</a></h2>

The primary goal of this phase is to create a consolidated list of
defects based upon the results of the preparation phase.  Traditionally,
this is done via a meeting of the review team.  Using Leap, the 
review team can:
<ul>
<li> Load all of the individual defect files into Honu and view them
simultaneously.
<li> Sort the combined defect list by location, type, or severity. This
enables the review team to process the document in a  sequential, 
functional, or priority-driven manner.
<li> Validate or amend the proposed severity level of each defect, 
including a change to the status of non-defect.
<li> Edit the description of any defect to improve clarity.
<li> Generate and enter new defects.
<li> Generate and enter new checklists, patterns, or other definitions
for review_defs.leap to improve the overall review process.
</ul>

<p>
Details on sorting and editing field descriptions are provided
in the Chapter on <A HREF="tables.html">Generic Table Functionality</A>.

<p>

The consolidated list of defects resulting from the meeting/consolidation
phase should be stored in a new file in the review workspace.  To store
just the combined defect data, use the Export Tables menu item
in the Leap command tool and specify
the new file name to be used.  

<p>

Note that all of these functions of the Meeting/Consolidation stage
can be performed asynchronously without a meeting. In the simplest
case, the review leader or author simply collects together 
the results from preparation and analyzes it alone. It is also
possible for the review leader to create the combined file, then 
pass it via email to each reviewer in turn to review in a round
robin style.  Other approaches to asynchronous review can be
supported by Leap. 

<p><hr><p><h2><A NAME = "rework">Phase 5: Rework</a></h2>

The goal of the rework phase is to improve the quality of the 
work product by removing the defects found during the preceding
phases of review. The author typically uses the consolidated 
file of defects as a basis for rework activities.  Leap supports
rework in the following ways:

<ul>
<p><li> Sorting and filtering of defect data allows the reviewer to 
easily organize the defects by functional area or by location in 
the work product.

<p><li> Io helps the author accumulate time data on the effort required for
rework. If the author wishes to collect more fine-grained data, she can
enter individual fix times on a per defect basis into Honu by invoking the
Hide/Show Columns operation and showing the "Fix Time" field.

<p><li> The Hide/Show Columns menu item can also be used to show two
additional columns: injected and removed.  These columns allow the author
to specify during what phase of development the particular defect was
injected, and during what phase of development it was removed.  This 
data can be useful in identifying design stage defects vs. implementation
stage defects and so forth. 

</ul>

The following screen image shows Honu with these additional columns 
displayed for use during rework:

<p>
<img src="screens/review-rework.gif" width=100%>
<p>

<p><hr><p><h2><A NAME = "postmortem">Phase 6: Postmortem</a></h2>

Many review method descriptions end with "rework", because at that point
the first order goal of review---improving the quality of the work
product---has typically been achieved.  Some review methods briefly mention
a "third hour meeting", typically held a day or so after the rework is
completed, in which the review team performs a debriefing intended to
improve the quality of the review <em>team and process</em>, rather than the review
work product.  Unlike many forms of automated support for review, Leap
provides explicit support for this second order goal as well
as for the first order goal.   Leap is relatively unique in its focus
on improving the skills of authors and reviewers as the primary means
to enact review process improvement. 


<p>

<p><h3><A NAME = "postmortemForms">Forms of postmortem analysis</a></h3>

Many review method descriptions provide only vague details concerning
improvement in the review process. This can lead to the third hour
meeting being "optimized away" after a few unsuccessful attempts at
review process improvement.  
<p>

In contrast, the Leap toolkit provides concrete analyses that reviewers and
review teams can use during postmortem to guide improvement in the review
process.  It is useful to distinguish among the several kinds of
improvement that are possible based upon who is doing the analysis and the
data that must be made available:
<p>
<ul>

<p><li> <em>Individual reviewers, personal data only.</em> One kind of
postmortem analysis focusses on each reviewer's individual behavior.
Here, each reviewer analyzes their own defect data, and/or time data,
and/or size data from the current review and from any prior reviews that
they have participated in.  These analyzes can help reviewers to gain
insight into the kinds of defects that they are effective in finding, the
rate of review, time/size data useful for predicting the amount of effort
required for future reviews, and potential new checklist items and
patterns.

<p>
For example, the following screen image shows a plot of the time
spent by a reviewer on a half dozen prior reviews.  The current
work product to review is similar the previous ones, and is 2500
words long. Using historical average/max/min data as a guideline,
the planner indicates that the preparation activity may take between
68 minutes and 215 minutes, with 103 minutes being the best estimate
given the average prior rates of preparation. 

<p>
<img src="screens/review-planning.gif" width=100%>
<p>

While there are some limitations to each reviewer analyzing only their
own data, the advantage of this form of improvement is that this data
is intrinsically available to the reviewer, and the resulting analyses are
of specific relevance to the individual reviewer.

<p><li> <em>Individual reviewers, personal data and other reviewer's data
from joint review projects.</em>  A closely related postmortem analysis
results when reviewers compare their own defect data for a given review 
with the defect data generated by other reviewers.  This allows them
to gain insight into the kinds of defects that other reviewers find
that they missed in the work product, leading to checklists and
patterns useful to improving their future review practice.

<p>
For example, the following screen lists defects in Honu where
the "UserFound" field is visible and used to distinguish between
those defects that this user discovered and those that other
reviewers supplied. 

<p>
<img src="screens/review-userfound.gif" width=100%>
<p>



This data is usually intrinsically available, since the 
meeting/consolidation stage usually results in joint discussion
of all defects generated by all reviewers.  The resulting
analyses are of specific relevance to the individual reviewer.




<p><li> <em>Individual authors, personal data and group defect data.</em>
This form of postmortem analysis is possible for authors of the work
products, who will have access not just to their own personal defect data,
but also to the defects discovered in their work products by the reviewers.
Information about these externally discovered defects can help the author
to generate checklist items and patterns that they can use to prevent those
defects from occuring in future work products they create of that type.
Recording when the defect appeared to be injected can help the author
to establish when in the development process to check for that particular
kind of defect.  Finally, trend-based analyses can help the author to
determine if these process improvement strategies are in fact leading to
a reduction in those types of defects.

<p>

Once again, while this form of process improvement is limited to
individuals, the advantage is that it utilizes data intrinsically available
to the author, and the resulting analyses are of specific relevance to the
individual author.


<p><li> <em>Development groups, combined checklist and pattern data.</em>

This form of postmortem analysis is possible when individuals provide
their checklist items and patterns, generated through individual analysis,
to someone else in the organization who will combine them together and make
them publically available to others in the organization.  Such aggregate
results of individual process improvement activities can be quite useful to
others in the organization, exposing them to checklist items and patterns
that they might find extremely helpful although they had not yet generated
defect data appropriate to uncovering the checklist items or patterns on
their own.

<p>
For example, following each review, a file containing organizational
checklist data could be updated and made available to all members
of the organization. The following screen shows an example of part
of such a collective "best checklist practice" file.

<p>
<img src="screens/review-checklists.gif" width=100%>
<p>




This form of analysis requires someone to put effort into
analysis for the good of the group, rather than for their own direct
benefit, and to set up a publically available folder containing this data.
It thus requires organizational, rather than individual, investment and
effort.

<p>

This form of analysis utilizes data that, although not
intrinsically available, is typically possible for reviewers and authors to
contribute to the group without adverse effect.

<p><li> <em>Development groups, combined defect, time, size data.</em>  

This final form of postmortem analysis involves each individual providing
their raw data concerning defects, time, and size to another
person in the organization who will aggregate the data together. 

<p>

Although this form of postmortem analysis is possible in Leap, we do not
recommend it. First, we believe that the production and sharing of
checklist items and patterns by individuals is where the real benefits of
individual insights to the organization lies.  We believe that individual
time, size, and defect data has little to no utility to process improvement
efforts of the organization as a whole, and that this "raw" data can only
be usefully interpreted by the individuals who generated it.  Finally, we
are also aware of many organizations in which the act of requesting this
data from individuals will lead to measurement dysfunction---the need for
individuals to modify the data to suit the desires of the organization.
The results of process improvement in such a situation could be highly
counter-productive, given the inaccuracy of the raw data.

</ul>













</body>
</html>
