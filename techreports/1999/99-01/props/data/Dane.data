<html>
<head>
<title>LEAP Data File</title>
</head>
<body>
<table border>
<tr>
<td>DRL Plain 2.0.0
<tr>
<th>CreateDate<th>DocID<th>Project<th>FixTime<th>Location<th>Occurrences<th>DefectType<th>Severity<th>Description<th>Valid<th>UserCreated<th>UserFound<th>Injected<th>Removed<th>Checklist
<tr>
<td type=CreateDate>11:33 09/24/1998
<td type=DocID>Dane.html
<td type=Project>Rev-Dane
<td type=FixTime>1
<td type=Location>02
<td type=Occurrences>1
<td type=DefectType>30: Content
<td type=Severity>Med,1
<td type=Description>The SEI did not conclude that the PSP improved
productivity.

<td type=Valid>
<td type=UserCreated>
<td type=UserFound>
<td type=Injected>
<td type=Removed>
<td type=Checklist>
<tr>
<td type=CreateDate>11:35 09/24/1998
<td type=DocID>Dane.html
<td type=Project>Rev-Dane
<td type=FixTime>1
<td type=Location>03
<td type=Occurrences>1
<td type=DefectType>30: Content
<td type=Severity>High,0
<td type=Description>It is inaccurate to say that "errors in the analysis phase itself 
often distorts results", when we had only 10 students
worth of data.  What Anne's thesis shows is that it
is *possible* for errors in the analysis phase to 
distort results in a significant manner. It's an
existance proof argument only, empirically speaking.
<td type=Valid>
<td type=UserCreated>
<td type=UserFound>
<td type=Injected>
<td type=Removed>
<td type=Checklist>
<tr>
<td type=CreateDate>11:38 09/24/1998
<td type=DocID>Dane.html
<td type=Project>Rev-Dane
<td type=FixTime>1
<td type=Location>04
<td type=Occurrences>1
<td type=DefectType>30: Content
<td type=Severity>Low,2
<td type=Description>89 *projects*, not 89 students.
<td type=Valid>
<td type=UserCreated>
<td type=UserFound>
<td type=Injected>
<td type=Removed>
<td type=Checklist>
<tr>
<td type=CreateDate>11:39 09/24/1998
<td type=DocID>Dane.html
<td type=Project>Rev-Dane
<td type=FixTime>1
<td type=Location>06
<td type=Occurrences>1
<td type=DefectType>30: Content
<td type=Severity>High,0
<td type=Description>I think "what people can say about any study which uses PSP" is 
mostly based upon whether they use measures
*external* to the PSP for its evaluation.  Regardless of 
the presence or absence of tools, if you use external measures,
you are likely to be able to make some judgement 
about the PSP, either pro or con. 
<td type=Valid>
<td type=UserCreated>
<td type=UserFound>
<td type=Injected>
<td type=Removed>
<td type=Checklist>
<tr>
<td type=CreateDate>11:42 09/24/1998
<td type=DocID>Dane.html
<td type=Project>Rev-Dane
<td type=FixTime>1
<td type=Location>20-22
<td type=Occurrences>1
<td type=DefectType>30: Content
<td type=Severity>Med,1
<td type=Description>This section's response seemed to be a little superficial, but maybe
the question is too hard for students to address at 
this point.  However, it is certainly not the case that 
the LEAP toolset is designed to support the SEI's
evaluation of the PSP toolset.  The design of the 
LEAP toolkit consciously rejects some essential aspects 
of the PSP (i.e. heavyweight process), and least 
in the instantiation of the PSP in the Discipline for
Software Engineering. 
<td type=Valid>
<td type=UserCreated>
<td type=UserFound>
<td type=Injected>
<td type=Removed>
<td type=Checklist>
</table>

<table border>
<tr>
<td>TRL Plain 2.0.0
<tr>
<th>Project<th>Phase<th>DocType<th>Start<th>Stop<th>Interrupt<th>Delta<th>Description
<tr>
<td type=Project>Rev-Dane
<td type=Phase>Reviewing,0
<td type=DocType>LitReview
<td type=Start>11:33 09/24/1998
<td type=Stop>11:52 09/24/1998
<td type=Interrupt>0
<td type=Delta>19
<td type=Description>
</table>

</body>
</html>
