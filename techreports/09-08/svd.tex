\section{Singular Value decomposition (SVD)}
The Singular Value Decomposition for ad-hoc query support in the time-series databases was proposed by Korn et. al. in \cite{citeulike:4373332}. They started with a case study of the typical warehouse dataset (AT\&T) of sale patterns representing the data as a matrix $X$ of size $N \times M$ where rows correspond to distinct customers and columns to the time intervals. One of the observations is that the amount of customers is much larger than the length of the time-series: $N>>M$. Another observation is that the size of such a matrix could easily reach the unmanageable in order to process ``random access'' queries size. The proposed solution for this problem is to design a ``compression'' algorithm which by trading in some of the precision will allow to process significant amount of the data with random access queries. 

While being aware of some convenient compression techniques such as LZ and Huffman, as well as the spectral decomposition (DFT, DCT and DWT) and clustering techniques, Korn et. al. are arguing that their Singular Vale Decomposition approach is the only solution which does not suffer from the CPU-time complexity and performs spectral decomposition in the optimal manner.

The SVD decomposition method is based on the theorem stating that the real-valued matrix $X$ can be expressed as 
\begin{equation}
X = U \times \Lambda \times V^{t}
\label{eq:svd_transform}
\end{equation}
where $U$ is a column-orthonormal $N \times r$ matrix ($r$ is rank of the matrix $X$), $\Lambda$ is a diagonal $r \times r$ matrix of eigenvalues $\lambda_{i}$ of $X$ and $V$ is a column-orthonormal $M \times r$ matrix. Which essentially is the spectral decomposition:
\begin{equation}
X = \lambda_{1} u_{1} \times v_{1}^{t} + \lambda_{2} u_{2} \times v_{2}^{t} + \ldots + \lambda_{r} u_{r} \times v_{r}^{t}
\label{eq:svd_spectrum}
\end{equation}
of the matrix $X$ where $u_{i}$ and $v_{i}$ are column vectors of the $U$ and $V$ and $\lambda_{i}$ is a diagonal elements of $\Lambda$.

Once $X$ is decomposed (two passes over the matrix) into the form of spectrum \ref{eq:svd_spectrum}, authors suggesting to truncate the sum in equation to the first $k$ terms, $k \leq r \leq M$:
\begin{equation}
\hat{X} = \sum_{i=1}^{k} \lambda_{i} u_{i} \times v_{i}
\label{eq:svd}
\end{equation}
where $k$ depends on the space restriction. Note, that these $k$ terms also known as the ``principal components'' in the PCA analysis. 

The compression ratio after applying the SVD transform is 
\begin{equation}
s = \frac{N*k + k + k + k*M}{N*M} \approx \frac{k}{M}
\label{eq:svd_compression}
\end{equation}
since $N >> M \geq k$. The reconstruction of the any cell of the original matrix $\hat{X}$ takes only $O(k)$ time:
\begin{equation}
\hat{x}_{i,j}  = \sum_{m=1}^{k} \lambda_{m} * u_{i,m} * v_{j,m},\; i=1,...,N;\; j=1,...,M
\label{eq:svd_reconstruct}
\end{equation}

Since the SVD approximation could yield considerable errors when restoring some of the original data, authors suggesting to keep correcting information in the form of tuples $(row,column,delta)$ for the values which approximated poorly (outliers) calling this method as ``SVD with Details'' or SVDD. 

\begin{figure}[tbp]
   \centering
   \includegraphics[height=55mm]{svd.eps}
   %%{seriesheatmap}
   \caption{The Reconstruction Error (RMSPE) versus disk storage space (\%) for clustering, DCT, SVD and SVDD.}
   \label{fig:svd_benchmark}
\end{figure} 

As per performance of both SVD and SVDD methods, they were compared with DCT (discrete cosine transform) and the S-PLUS embedded clustering method and found to be significantly superior \ref{fig:svd_benchmark}, especially SVDD with the growth of the space allowed to use for corrective information.