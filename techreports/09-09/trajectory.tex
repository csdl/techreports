\chapter{Hackystat-Trajectory - software process mining framework.} \label{trajectory}
As we seen in the Chapter \ref{related.work} it is possible to infer and successively formalize a software process by observing its artifacts and particularly recurrent behavioral patterns. The problem of finding such patterns is the cornerstone of my research. My approach to this problem rests on the application of data-mining techniques to symbolic time-point and time-interval series constructed directly from the real-valued telemetry streams provided by Hackystat.

Aiming validation of my approach, I am designing and developing a ``Hackystat Trajectory'' framework which provides an one-stop shop for recurrent behavior patterns mining from the software-process data. The high-level overview of the framework I am working on is shown at the Figure \ref{fig:system_overview} and resembles the flow of the Knowledge Discovery in Database process discussed in Han et al \cite{citeulike:709476}. As shown, the data collected by Hackystat is getting transformed into symbolic format and indexed for further use in the data mining process. The tools designed for data-mining have a specific restriction in the search space by domain and context knowledge in order to limit the amount of reported patterns to the useful ones. I am planning to design a GUI in the way which will allow easy access and modification of such rules. 

Note, that there was a research done, which is highly related to expressed here ideas. Hongbing Kou, in his thesis \cite{citeulike:2703162}, was able to infer TDD behaviors using automatically collected in-process software metrics through developed by him technique called Software Development Stream Analysis (SDSA).

\section{Current state of the development}

\begin{figure}[tbp]
   \centering
   \includegraphics[height=130mm]{trajectory_progress.eps}
   \caption{Screenshots of three versions of TrajectoryBrowser (panels $a$, $b$ and $d$) and the software process simulation (panel $b$). Simulated data was used for validation in early stages.}
   \label{fig:trajectory_progress}
\end{figure}

I have started development of the Hackystat Trajectory framework in early 2008 by developing a user interface for visual comparison of multi-variate time series. This was done by following the idea expressed by Philip Johnson which he titled it as ``From Telemetry to Trajectory''. Borrowing the term ``trajectory'' I called the software package as ``TrajectoryBrowser'' and titled performed analyzes as ``Trajectory analyzes''. The idea was to visualize software project metrics as ``trajectories'' in 3D space as opposite to the classical 2D representation. The first \textit{TrajectoryBrowser} was an ad-hoc application based on the two technologies: Java3D for the visualization and JADE multi-agent framework \cite{citeulike:1230319} for the data generation (see Figure \ref{fig:trajectory_progress} panels $a$ and $b$). While this first version fulfilled basic requirements for visualization allowing ``eyeballing'', it did not provide any means for quantifying similarities between trajectories or finding similar trajectories autonomously.

In order to resolve these issues with similarity measurement and implement an indexing of temporal features, I have started experimenting with direct application of Euclidean distance and later with spectral decomposition of time series through DFT. Both methods were found inconsistent in results and sometimes even misleading due to the noisy and aperiodic temporal data generated by the software process. 

At the next iteration, I have implemented Dynamic Time Warping (DTW) algorithm inspired by its success in many application and robustness to noise. This code was wrapped it into the second, web-based version of TrajectoryBrowser (see Figure \ref{fig:trajectory_progress} panels $c$). Second version provided user with ability to visualize time-series intervals and quantify the similarity, nevertheless, there was no implementation of unsupervised similarity search provided.

While working on the temporal data indexing by using sliding window and DTW, I found another promising approach for the very same task: PAA and SAX (see section \ref{paa} and \ref{sax}) approximations. The simplicity of these two methods allowed me to integrate them with existed code base almost instantly, delivering the third version of TrajectoryBrowser which I am currently using in my research. Next two sub-section will present the indexing mechanism I am using along with the index database design. simplicity of these two methods allowed me to integrate them with existed code base almost instantly, delivering the third version of TrajectoryBrowser which I am currently using in my research. Next two sub-section will present the indexing mechanism I am using along with the index database design.

\subsection{Temporal data indexing}
The Figure \ref{fig:data_flow} explains the data abstraction process within the Hackystat Trajectory framework in a greater detail. Collected and aggregated by Hackystat, \textit{raw sensor data} and \textit{Hackystat Telemetry streams} are used as the data sources. By using a user-defined taxonomy mapping, streams of individual events retrieved from Hackystat Sensorbase getting sorted by the activities, tokenized and converted into symbolic time point (Events) and time-interval (Episodes) series. By performing a user-configured PAA and successive SAX approximations Hackystat telemetry streams are getting converted into the same temporal symbolic format. This data, in turn, is getting indexed and stored for future use in the unsupervised pattern mining.

\begin{figure}[tbp]
   \centering
   \includegraphics[height=85mm]{data_flow.eps}
   \caption{The overview of the data abstraction from the the low-level process and product artifacts collected by Hackystat (left side) to the high-level symbolic time-point and time-interval series stored in the Trajectory data repository.}
   \label{fig:data_flow}
\end{figure}

I have not experimented with symbolic abstraction and mining of the raw sensor data yet, but as mentioned before, it was shown in previous work by Hongbing Kou that such data can be used in the software process inference. The indexing and mining of the Telemetry streams was implemented with PAA and SAX. For indexing I am using a sliding window approach and storing index data in the relational database leveraging SQL. Figure \ref{fig:indexer} presents the current software system overview.

\subsection{Index database design}
The Figure \ref{fig:trajectory_db} presents a database schema in details. This schema was designed with two main requirements in mind. First of all, it must be able to hold a local copy of telemetry streams due to the high time cost of querying Telemetry service. Secondly, it must support implemented KDD algorithms through optimized for speed SQL queries. Both goals were archived resulting in the extremely high turn-around speed for both, indexing and querying.

First requirement appeared due to the Service-Oriented nature of Hackystat. The network lag for my remote location and the amount of data needed to be retrieved during each of the indexing sessions resulted in the very slow indexing. To overcome this issue I have developed a software module which is ``caching'' Telemetry streams data locally. The \textit{Project, User, Member, Chart, Chartvalue} and \textit{Download} tables (located at the upper part of the Figure \ref{fig:trajectory_db}) are designed to store the Telemetry data. The system performs incremental update of the streams over the time by reading the \textit{Download} table, which keeps track of updates.

SAX indexing is performed on demand using a user-specified alphabet, sliding window size and PAA size. Following Lin \& Keogh \cite{citeulike:2821475}, in order to investigate the sensitivity and selectivity of approximation level, I have conducted a number of experiments with various alphabets. Individual alphabets for each of the data streams (\textit{Build, Coverage} etc.) and \textit{Universal Telemetry Alphabet} (see example for five letters alphabet at the Figure \ref{fig:distribution}, panel) were built and tested for indexing. These custom alphabets and the original SAX alphabet based on the Normal distribution are kept in the \textit{Sax\_alphabet} and \textit{Alphabet\_cut} tables. 

The \textit{Sax\_top\_index} table is a ``binder'' which keeps information about all indexes built and the \textit{Sax\_index\_chart} table keeps track of all indexes built for a particular chart and a timeframe. \textit{Sax\_motif} and \textit{Sax\_motif\_entry} tables separate heavyweight symbolic motifs and lightweight offset ``information'' reducing the database size. 

This database schema was found optimal for easy retrieval of any kind of information needed for streams comparisons or clustering. By running a single query it is possible to get a vector of most frequent motifs for each of the streams or find a set of motifs shared between streams.

\begin{figure}[tbp]
   \centering
   \includegraphics[height=60mm]{indexer.eps}
   \caption{The database schema used for the Hackystat Telemetry data retrieval and indexing in the pilot project.}
   \label{fig:indexer}
\end{figure}

\section{Preliminary evaluation}
In order to demonstrate the ability of the current system implementation to perform telemetry indexing and temporal recurrent patterns extraction I have conducted two experiments. For the first experiment, aiming an unsupervised classification of Telemetry streams, I was using the real data collected during the Spring'09 software engineering class. The second experiment, aiming a discovery of sequential patterns was also conducted by using a real data from my own concurrent development of two software projects.

In order to compare and classify behavior of telemetry streams and individual developers I have built SAX indexes for 

\section{Future development roadmap}
