\chapter{Hackystat-Trajectory - software process mining framework.} \label{trajectory}
As we have seen in the Chapter \ref{related.work} it is possible to infer and successively formalize a software process by observing its artifacts and particularly recurrent behavioral patterns. The problem of finding such patterns is the cornerstone of my research. My approach to this problem rests on the application of data-mining techniques to symbolic time-point and time-interval series constructed directly from the real-valued telemetry streams provided by Hackystat.

Aiming the delivery of a software tool aiding in discovery of recurrent behavioral patterns in the software process, I am designing and developing a ``Hackystat Trajectory'' framework which provides an one-stop shop for recurrent behavior patterns mining from the software-process data. The high-level overview of the framework is shown at the Figure \ref{fig:system_overview} and resembles the flow of the Knowledge Discovery in Database process discussed by Han et al. in \cite{citeulike:709476}. As shown, the data collected by Hackystat is getting transformed into the symbolic format and indexed for further use in the data mining process. The tools designed for data-mining have a specific restrictions placed on the search space by domain and context knowledge in order to limit the amount of reported patterns to the useful ones. I am planning to design a GUI in the way which will allow easy access and modification of such rules. 

\section{Current state of the development}

\begin{figure}[tbp]
   \centering
   \includegraphics[height=130mm]{trajectory_progress.eps}
   \caption{Screenshots of three versions of TrajectoryBrowser (panels $a$, $b$ and $d$) and the software process simulation (panel $b$). Simulated data was used for validation in early stages.}
   \label{fig:trajectory_progress}
\end{figure}

I have started development of the Hackystat Trajectory framework in early 2008 by developing a user interface for visual comparison of multi-variate time series. This was done by following the idea expressed by Philip Johnson which he titled as ``From Telemetry to Trajectory''. Borrowing the term ``trajectory'' I called the software package as ``TrajectoryBrowser'' and titled performed analyzes as ``Trajectory analyzes''. The idea was to visualize software project metrics as ``trajectories'' in 3D space as opposite to the classical 2D representation. The first \textit{TrajectoryBrowser} was an ad-hoc application based on the two technologies: Java3D for visualization and JADE multi-agent framework \cite{citeulike:1230319} for the data generation (see Figure \ref{fig:trajectory_progress} panels $a$ and $b$). While this first version fulfilled basic requirements for visualization, allowing ``eyeballing'', it did not provide any means for quantifying similarities between trajectories or finding similar trajectories autonomously.

In order to resolve these issues with similarity measurement and implement an indexing of temporal features, I have started experimenting with a direct application of Euclidean distance and later with spectral decomposition of time series through DFT. Both methods were found inconsistent in results and sometimes even misleading due to the noisy and aperiodic temporal data generated by the software process. 

At the next iteration, I have implemented Dynamic Time Warping (DTW) algorithm inspired by its success in many application and robustness to noise. This code was wrapped into the second, web-based version of TrajectoryBrowser (see Figure \ref{fig:trajectory_progress} panels $c$). Second version provided user with ability to visualize time-series intervals and quantify the similarity, nevertheless, there was no implementation of unsupervised similarity search provided.

I have started developing indexing module by using sliding window and DTW. While working on this, I found another promising approach for the very same task: PAA and SAX (see section \ref{paa} and \ref{sax}) approximations. The simplicity of these two methods allowed me to integrate them with existed code base almost instantly, delivering the third version of TrajectoryBrowser which I am currently using in my research. Next two sub-section will present the indexing mechanism I am using along with the index database design. 

\subsection{Temporal data indexing}
The Figure \ref{fig:data_flow} explains the data abstraction process which happens before indexing in a greater detail. Collected and aggregated by Hackystat, \textit{raw sensor data} and \textit{Hackystat Telemetry streams} are used as the data sources. By using a user-defined taxonomy mapping, streams of individual events constituting raw data are retrieved from Hackystat Sensorbase and getting sorted by the activities, tokenized and converted into symbolic time point (\textit{Events}) and time-interval (\textit{Episodes}) series. By performing a user-configured PAA and successive SAX approximations, Hackystat Telemetry streams are getting converted into the same temporal symbolic format. This data, in turn, is getting indexed and stored in the dedicated relational database for future use in the data mining.

\begin{figure}[tbp]
   \centering
   \includegraphics[height=85mm]{data_flow.eps}
   \caption{The overview of the data abstraction from the the low-level process and product artifacts collected by Hackystat (left side) to the high-level symbolic time-point and time-interval series stored in the Trajectory data repository.}
   \label{fig:data_flow}
\end{figure}

I have not experimented with symbolic abstraction and mining of the raw sensor data yet, but this approach has a solid foundation provided by Hongbing Kou, in his thesis \cite{citeulike:2703162}. He was able to infer TDD behaviors using the very similar to mine technique called Software Development Stream Analysis or SDSA. Within SDSA, the low-level software process data was converted into symbolic Episodes first. Secondly, Episodes were matched to the known TDD patterns, and if they were found to satisfy TDD rules (having sufficient support), the generative process was found as TDD.

The indexing and mining of the Telemetry streams is implemented with PAA and SAX. For indexing I am using a sliding window approach and storing index data in the relational database leveraging SQL. Figure \ref{fig:indexer} presents the current software system overview.

\subsection{Index database design}
The Figure \ref{fig:trajectory_db} presents the TrajectoryDB database schema in details. This schema was designed with two main requirements in mind. First of all, it must be able to hold a local copy of telemetry streams due to the high time cost of querying Telemetry service remotely. Secondly, it must support implemented KDD algorithms through optimized for speed SQL queries. Both goals were archived resulting in the extremely high turn-around speed for both, indexing and querying.

\begin{figure}[tbp]
   \centering
   \includegraphics[height=60mm]{indexer.eps}
   \caption{Current design Trajectory framework allowing Hackystat Telemetry data retrieval, indexing, browsing and mining.}
   \label{fig:indexer}
\end{figure}

Hackystat is implementing a Service-Oriented architecture, and in order to retrieve a Telemetry data one must query Telemetry service over network. Due to the network lag for my remote location and the amount of data needed to be retrieved during each of the indexing sessions resulted in the very slow real-time indexing. To overcome this issue I have developed a software module which is ``caching'' and incrementally updating Telemetry streams data locally. The \textit{Project, User, Member, Chart, Chartvalue} and \textit{Download} tables (located at the upper part of the Figure \ref{fig:trajectory_db}) are designed to store the Telemetry data. The system performs incremental update of the streams over the time by reading the \textit{Download} table, which keeps track of updates.

SAX indexing is performed on demand using a user-specified alphabet, sliding window size and PAA size. Following Lin \& Keogh \cite{citeulike:2821475}, in order to investigate the sensitivity and selectivity of different approximation levels, I have conducted a number of experiments with various alphabets. Individual alphabets for each of the data streams (\textit{Build, Coverage} etc.) and \textit{Universal Telemetry Alphabet} (see example for five letters alphabet at the Figure \ref{fig:distribution}, panel $a$) were built and tested for indexing. These custom alphabets and the original SAX alphabet based on the Normal distribution are kept in the \textit{Sax\_alphabet} and \textit{Alphabet\_cut} tables. 

The \textit{Sax\_top\_index} table is a ``binder'' which keeps information about all indexes built and the \textit{Sax\_index\_chart} table keeps track of all indexes built for a particular chart and a timeframe. \textit{Sax\_motif} and \textit{Sax\_motif\_entry} tables separate heavyweight symbolic motifs and lightweight offset ``information'' reducing the database size. 

This database schema was found optimal for easy retrieval of any kind of information needed for streams comparisons or clustering. By running a single query it is possible to get a vector of most frequent motifs for each of the streams or find a set of motifs shared between streams.

\section{Future development roadmap}
As I pointed before, the current version of the TrajectoryBrowser and analyses do not support processing of the low-level raw Hakystat data and based on it analyses. I have already started the development of such a module along with extending a database schema for storing raw data, its approximation and taxonomy. The schema will be presented and discussed within the proposal defense presentation.

Once all the base components will be in place, I will start developing data mining algorithms for Events and Episodes using discussed temporal grammars, relations and algorithms.