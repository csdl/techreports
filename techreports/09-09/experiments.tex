\section{Planned evaluation in the classroom}
In order to evaluate Trajectory performance through the discovery of well known and novel software process recurrent behavioral patterns, I am planning to conduct a classroom case study. The approach I am taking is based on the continuous collection of the software process artifacts by Hackystat and its daily re-indexing and mining with Trajectory. This software process dataset will be collected from the classroom software project development during the Fall'09 and Spring'10 Software Engineering classes. Performing analyses daily within the data collection interval provides an advantage of a real-time communications with students. Once I will be able to identify emerging, strongly supported patterns, I will communicate via e-mail with students in order to conform observed phenomena. I am planning to perform two types of analysis: the individual developer software process analysis (personal software process discovery) and software product process analysis.

\subsection{Personal software process discovery}
\begin{table}
\begin{center}
    \begin{tabular}{ | c | l | }
    \hline
    Symbolic abbreviation & Description \\ 
     of Events 						& 	  \\ 
    \hline
    $CI$                  & code implementation event, \\
    											& this corresponds to the new code entry \\
    \hline    											
    $CR$                  & code refactoring event, this includes renaming \\
    											& or deleting of functional units, and buffer transactions \\
    \hline
    $CD$                  & debugging event \\
		\hline
		$CCS$                 & successful compilation event \\
		\hline
		$CCF$                 & unsuccessful compilation event \\
    \hline
		$UTS$                 & successful unit test run \\
		\hline
		$UTF$                 & unsuccessful unit test run \\
		\hline
		$CM$                  & code commit to SCM \\
		\hline
		$CU$                  & code update from SCM \\
		\hline
    $CAS$                 & code analysis success event, corresponds to a \\
                          & successful invocation of one of the code analysis tools \\
    \hline
		$CAS$                 & code analysis failure event, corresponds to a \\
                          & unsuccessful invocation of one of the code analysis tools \\
    \hline    
    $CCP$                 & positive delta in the code size (churn) \\
    \hline
    $CCN$                 & negative delta in the code size (churn) \\
    \hline
    \end{tabular}
    \caption{Taxonomy of the Symbolic Events to be used in the classroom evaluation of Trajectory analyses.}
    \label{fig:data_collected_points}
    \end{center}
\end{table}
In this experimental validation, I am planning to perform Trajectory analyses using Hackystat data collected from individual developers. The data collected by Hackystat from each of developers will be aggregated into symbolic streams of two types: symbolic Event series and symbolic Episodes series:
\begin{itemize}
	\item Symbolic Events series. This dataset will consist of the thirteen types of Events listed in the Table \ref{fig:data_collected_points}. These Events represent a set of essential code-development activities which will constitute a multivariate symbolic time-point series for further analyses through data mining. The goal of these analyses will be to discover recurrent behavioral patterns in the sequence of activities among the developers. 
	\item Symbolic Episodes data. This dataset will consist of the thirteen types of Episodes listed in the table \ref{fig:data_collected_intervals}. These Events represent a set of code-development activities which intended to capture dynamic recurrent behavioral patterns.
\end{itemize}

The goal of the Event series analysis is in the building of a taxonomy of the symbolic software process patterns corresponding to one of the behaviors such as TDD, or code-first.

As opposite to the analysis of symbolic Events series, which threats the process as a set of sequential activities, the analysis of symbolic Episodes series aims a discovery of overlapping, dynamic patterns. For example I might find that unsuccessful unit testing episode is usually happening within a code refactoring episode only, whether during code implementation, unit tests are usually successful and accompanying by decreasing coverage. Also, I expect to infer a developers' ``reactions'' patterns. For example as a reaction on the continuous failure of unit tests, one might start broad refactoring of the code with purpose of reduce it's complexity. 
\begin{table}
\begin{center}
    \begin{tabular}{ | c | l | }
    \hline
    Symbolic abbreviation 	& Description \\ 
    of Episodes 						& of metric	  \\ 
    \hline
    $CI$ 									& code implementation episode, \\ 
    											& this corresponds to the new code entry events \\
	  \hline
    $CR$ 									& code refactoring episode, this includes renaming \\
    											& or deleting of functional units, and buffer transactions \\
    \hline
    $CD$ 									& code debugging episode, \\
    											& corresponds to debugging events \\
		\hline
		$CCS$ 								& successful code compilation episode, corresponds \\
													& to two or more successful compilation events \\
		\hline
		$CCF$ 								& unsuccessful code compilation episode, corresponds \\
													& to two or more successful compilation events \\
    \hline
		$UTS$ 								& successful unit test episode, corresponds \\
													& to two or more successful unit-test events \\
		\hline
		$UTF$ 							  & unsuccessful unit test episode, corresponds \\
													& to two or more unsuccessful unit-test events \\
		\hline
		$TCG$ 								& code test coverage growth, corresponds to a positive \\
													& delta between at least three code coverage analysis \\
													& tool invocations \\
		\hline
		$TCD$ 								& code test coverage decrease, corresponds to a negative \\
													& delta between at least three code coverage analysis \\
													& tool invocations \\
		\hline
		$CSG$ 								& code size growth, corresponds to a positive \\
													& delta between at least three code size analysis \\
													& tool invocations or three commits \\
		\hline
		$CSD$ 								& code size decrease, corresponds to a negative \\
													& delta between at least three code size analysis \\
													& tool invocations or three commits \\																										
		\hline
		$CCG$ 								& code complexity growth, corresponds to a positive \\
													& delta between at least three complexity analysis \\
													& tool invocations \\
		\hline
		$CCD$ 								& code complexity decrease, corresponds to a negative \\
													& code complexity delta between at least three complexity \\
													& analysis tool invocations \\													
		\hline
		$CAS$ 								& ``clean code development'' episode, corresponds to \\
													& at least three successful code analysis tools invocations \\
		\hline
		$CAF$ 								& ``unclean code development'' episode, corresponds to \\
													& at least three unsuccessful code analysis tools invocations \\
		\hline		
	  \end{tabular}
    \caption{Taxonomy of the Symbolic Episodes to be used in the classroom evaluation of Trajectory analyses.}
    \label{fig:data_collected_intervals}
    \end{center}
\end{table}

As pointed before, once I will collect a certain amount of data, I expect to see some recurrent patterns gaining enough support to be considered as ``candidate patterns''. The support function, I am considering to use, as a support, will be a product of two metrics: one is based on the support function from AprioriAll algorithm and quantifies the fraction of the developers demonstrating the pattern, and the second one is based on the total frequency of the pattern appearance. The reason for two-components is that the sampling space is limited to only eight to ten students (``developers'') in each of the classroom studies, and it is likely that there will be ``non-shared'' strong patterns - ones that observed only within a data collected from the single person.

Once a new pattern emerges in the ``candidate patterns'' pool, it will be critically reviewed and classified as a ``meaningful'' or a ``unmeaningful'' one on the basis of the process it is inferring to. By performing these reviews, I am planning to fulfill two goals: first is to prune the candidate patterns pool to truly useful and interesting patterns, and second is to enhance a knowledge base of the data-mining algorithms limiting the amount of reported unmeaningful or trivial patterns. The ability to communicate with students will provide additional feedback I will use in the classification resolving complex cases.

\section{Planned evaluation using public data}
In addition to the classroom experiments, I am planning to evaluate my research through the use of publicly available software configuration management repositories. The goal of this experiments is to assess the ability of my approach to reproduce already published results indicating correctness and similar to other tools performance. If I will be able to find novel patterns within this data with my approach, it will provide additional utility of my research.

As the main source of the data, I am planning to use public SCM repositories whose mining is traditionally used in the ``MSR Challenges'' \cite{citeulike:5043676} and published in proceedings of ``IEEE International Conference on Mining Software Repositories''. The GNOME project SCM data will be used in the 2009 challenge, Eclipse repositories were used in 2008 and 2007; PostgreSQL and ArgoUML in 2006. Two primary temporal data sets are offered for use: one is the CVS repository audit trail and second one is the bugs and issues tracking information. The following MSR challenge categories I found relevant to my research:
\begin{itemize}
  \item Approaches, applications, and tools for software repository mining.
  \item Analysis of change patterns to assist in future development.
	\item Prediction, characterization, and classification of software defects based on analysis of software repositories.
	\item Techniques to model reliability and defect occurrences.
\end{itemize}
My approach will be based on the approximation of the artifacts from SCM repositories into symbolic series using ``SCM trail taxonomy'' and successful indexing of this symbolic data. 

For example, in order to find ``ripple effect'' patterns in software change, I will construct a set of symbolic time-points and time-interval series which will represent all code changes in time by pre-processing change requests data and CVS events using issues numbering. Usually, once a change request has been submitted, it is discussed by developers and users, and once it found useful, it is assigned a number and handed to a developer who will be responsible for coding and code submission to CVS. I will develop a pre-processing engine which will find all the change requests and associated CVS events and will store this data in the local database following \cite{citeulike:5333719}. Once data will be mirrored locally, for time-points Trajectory data I will construct a series for each of the software packages (like \texttt{eclipse.swt.layout.*}, \texttt{org.eclipse.swt.widgets.*}, etc.) where symbolic data represnts a change occured in the package. For time-interval Trajectory data I will use the duration while change requests was ``open'' to construct series representing concurrent order and dynamics of changes. By applying Trajectory framework I expect to discover something like ``the change requested in package $A$ class $b$ usually results in changes in package $A$ class $c$ and package $B$ class $d$'' and it takes $Ab_{t} + Ac_{t} + Bd_{t}$ time to finish these changes. This information can be used to estimate the time and effort of newly requested software changes as well to identify changes that more likely to induce bugs and fixes and should be avoided. By adding the information about developers working on fixes, it is also possible to design an expert system which will suggest assignment of newly reported bug reports or feature request to a particular developer(s).

In addition to the public SCM, there are various public data hosted by the ``PROMISE software engineering repository'' \cite{Sayyad:2005} which contains two kinds of data: the software product datasets used for the building of the predictive software models, and some ``universal'' data sets representing SACM repository transaction. Most of these datasets were used for a number of publications and I am planning to evaluate my framework by attempting to reproduce these results. 