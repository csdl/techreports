\section{Planned evaluation through mining software repositories}\label{public.evaluation}
I am planning to evaluate my research and it's results through the mining of publicly available software configuration management (SCM) repositories. The goal of this experiments is to assess the ability of my approach to reproduce already published results, which will indicate a correctness of approach taken, along with acessing Software Trajectory performance comparing it with other tools. If I will be able to find novel patterns within this data with my approach, it will provide additional utility of my research.

As the main source of the data, I am planning to use public SCM repositories whose mining is traditionally used in the ``MSR Challenges'' \cite{citeulike:5043676} and published in proceedings of ``IEEE International Conference on Mining Software Repositories''. The GNOME project SCM data will be used in the 2009 challenge, Eclipse repositories were used in 2008 and 2007; PostgreSQL and ArgoUML in 2006. Two primary temporal data sets are offered for use: one is the CVS repository audit trail and second one is the bugs and issues tracking information. The following MSR challenge categories I found relevant to my research:
\begin{itemize}
  \item Approaches, applications, and tools for software repository mining.
  \item Analysis of change patterns to assist in future development.
	\item Prediction, characterization, and classification of software defects based on analysis of software repositories.
	\item Techniques to model reliability and defect occurrences.
\end{itemize}
My approach will be based on the approximation of the artifacts from SCM repositories into symbolic series using ``SCM trail taxonomy'' and successful indexing of this symbolic data. 

For example, in order to find ``ripple effect'' patterns in software change, I will construct a set of symbolic time-points and time-interval series which will represent all code changes in time by pre-processing change requests data and CVS events using issues numbering. Usually, once a change request has been submitted, it is discussed by developers and users, and once it found useful, it is assigned a number and handed to a developer who will be responsible for coding and code submission to CVS. I will develop a pre-processing engine which will find all the change requests and associated CVS events and will store this data in the local database following \cite{citeulike:5333719}. Once data will be mirrored locally, for time-points Trajectory data I will construct a series for each of the software packages (like \texttt{eclipse.swt.layout.*}, \texttt{org.eclipse.swt.widgets.*}, etc.) where symbolic data represents a change occurred in the package. For time-interval Trajectory data I will use the duration while change requests was ``open'' to construct series representing concurrent order and dynamics of changes. By applying Trajectory framework I expect to discover something like ``the change requested in package $A$ class $b$ usually results in changes in package $A$ class $c$ and package $B$ class $d$'' and it takes $Ab_{t} + Ac_{t} + Bd_{t}$ time to finish these changes. This information can be used to estimate the time and effort of newly requested software changes as well to identify changes that more likely to induce bugs and fixes and should be avoided. By adding the information about developers working on fixes, it is also possible to design an expert system which will suggest assignment of newly reported bug reports or feature request to a particular developer(s).

In addition to the public SCM, there are various public data hosted by the ``PROMISE software engineering repository'' \cite{Sayyad:2005} which contains two kinds of data: the software product datasets used for the building of the predictive software models, and some ``universal'' data sets representing SACM repository transaction. Most of these datasets were used for a number of publications and I am planning to evaluate my framework by attempting to reproduce these results. 
