\chapter{Experimental evaluation} \label{experiments}
I propose to conduct two case studies: \textit{Public data case study} (Section \ref{public.evaluation}), and \textit{Classroom case study} (Section \ref{classroom}) in order to empirically evaluate the capabilities and performance of Software Trajectory framework. These studies differ in the granularity of data used, and in the approaches for evaluation. 

During my work on the pilot version of Software Trajectory framework, I began a set of small experiments in order to aid in the architectural design and algorithms implementation. In addition, these experiments helped me to outline the boundaries of applicability of my approach to certain problems in software engineering. I call these experiments the \textit{Pilot study} and Section \ref{pilot.evaluation} discusses some of the insights yielded by this study.

My intent behind these empirical studies is to assess the ability of Software Trajectory framework to recognize well known recurrent behavioral patterns and software processes (for example Test Driven Development), as well as its ability to discover new ones. In addition, these studies will support a classification and extension of the current Hackystat sensor family in order to improve Software Trajectory's performance. It is quite possible that some of the currently collected sensor data will be excluded from the Software Trajectory datasets, while some new ones will be designed and developed in order to capture important features from the studied software development data streams. 

Before proceeding with the presentation of design of these studies and approaches for evaluation, I will discuss my evaluation paradigm.

\section{Review of evaluation strategies}
In contemporary literature, research methods are categorized into three paradigms: quantitative, qualitative and mixed-methods  \cite{citeulike:5410262}. Despite the arguments presented in the past for integrating first two methods \cite{GubaLincoln-CompetingParadigms}, combining qualitative and quantitative methods in a single, mixed-method study is currently widely practiced and accepted in many areas of research, and in particular, in empirical software engineering \cite{citeulike:5361927} \cite{citeulike:5361791}. 

\subsection{The two paradigms}
The quantitative paradigm is based on positivism. The positivism philosophy presumes that there exists only one truth, which is an objective reality independent from human perception, which, in turn, means that a researcher and a researched phenomena is unable to influence this truth or be influenced by it \cite{GubaLincoln-CompetingParadigms}. Quantitative studies are focused on finding of this truth through extensive study of the phenomena, and often characterized with very large sample sizes, control groups etc. in order to ensure proper statistical analysis.

In contrast, qualitative study is based on constructivism \cite{citeulike:209817} \cite{GubaLincoln-CompetingParadigms} and interpretivism \cite{citeulike:447180}. Both presume the existence of multiple truths based on the one's construction of reality, as well as non-existence of such a reality prior to the investigation. Moreover, once such a reality is solely or socially constructed, it is not fixed, - it is changing over time, shaped by new findings and experiences of researcher. Techniques of qualitative studies are not meant to be applied to a large population. They rather focus on small, rich in features samples which can provide valuable information. In other words, subjects in a qualitative study are picked not because they represent large groups, but because they can articulate important high-quality information.

The assumptions behind the two approaches extend beyond just methodological or philosophical differences to the one's perceptions of reality. According to Guba et al. \cite{GubaLincoln-CompetingParadigms}, qualitative and quantitative methods do not study the same phenomena. Applying the authors' statement to empirical software engineering, we can infer the limitations of qualitative studies by their inability to access some of the phenomena that researchers are interested in, such as prior experiences, social interactions, and the behavioral variations of individuals performing software process.  

\subsection{Mixed methods and Exploratory research}
Surprisingly, the fundamental differences between the two paradigms are rarely discussed in the mixed-methods research which is based on pragmatism. The finding of truth is more important in pragmatist paradigm than the question of methods or philosophy. Researchers practicing the mixed-method approach are free to use methods, techniques and protocols which meet their purposes, and which, in the end, provide the best understanding of the research problem. This unifying logic in understanding of studied phenomena regardless of approach, is central to mixed methods.

While the three discussed paradigms at least assume the existence of a problem and provide the outlines for methods of data collection and design of a study, sometimes a clear statement of a problem is missing from research. In such a case, exploratory research must be conducted in order to clarify the detailed nature of the problem.

\section{Software Trajectory approach evolution}
When I started exploring Hackystat telemetry streams during the Software Engineering class taught by my present adviser Philip Johnson, I realized the ``coolness'' of software process metrics visualization for providing insights into software process. By using the web interface of Hackystat, I was able to pull and visualize various telemetry streams reflecting my development. Working in the class, I compared metrics of my software process to ones from my team members. Performing such analyses, and discussing their results enabled us to improve our individual and team software processes, which resulted in excellent grades. 

After joining CSDL, I started my research by exploring the boundaries of telemetry visualization and telemetry analyses, and finding possibilities for improvements. At that point of time, I was working on two problems: improving the visualization of telemetry streams and introduction of metrics for quantitative comparison of telemetry streams (trajectories). While working on these two problems, I realized that the real problem I am trying to solve lies beyond the reach of these two. The problem is that the user is not provided with enough details about their software process to be able to perform comparative analyses of two projects (two software processes). I am envisioning the solution of this problem through the extension of Hackystat analyses with the ability to discover detailed features from software process.

In other words, by conducting \textit{exploratory research} of visualization tools and techniques as well as designing ``flexible enough'' metrics appropriate for telemetry streams comparison, I have realized the importance of understanding of the generative software process. This knowledge about performed software process, inferred from the set of collected artifacts, is crucial in the process improvement and in the comparative analysis of software projects.

Once I was able to clearly formulate this problem, I have started another exploratory study - my pilot study. By performing this exploration of the tools and techniques for software process mining and inference, I am trying to build my own toolkit which will allow me to approach the next problem - the problem of quantifying of software process through the discovery of recurrent behaviors. 

\section{Software Trajectory case studies and evaluation design}
The proposed public data case study is based on the use of publicly available Software Configuration Management (SCM) audit trails of the big, ongoing software projects such as Eclipse, GNOME etc. Mining of SCM repositories is a well-developed area of research with much work published \cite{citeulike:5043676}. SCM repositories contain coarse software product artifacts which are usually mined with a purpose of discovering of various characteristics of software evolution and software process. I am using a mixed-method approach in this study. In the first phase of this study, I plan to perform SCM audit trail data mining following published work and using Software Trajectory as a tool in order to discover confirmed patterns in software process artifacts, and thus quantitatively evaluate Software Trajectory's performance when compared to existing tools. In the second phase, I will develop my own pre-processing and taxonomy mapping of software process artifacts into temporal symbolic series. By using this data and Software Trajectory framework, I plan to develop a new approach for SCM audit trail mining and possibly discover new evolutionary behaviors within software process. These discovered knowledge will be evaluated through the peer-reviewed publication submitted for the annual MSR challenge \cite{citeulike:5043676}.

The classroom case study is based on a more comprehensive data set. This data will be collected by Hackystat from Continuous Integration and from individual developers and will contain fine-grained information about performed software process. The approach I am taking in this study is very similar to the public data case study. I will develop my own taxonomy for mapping of software process artifacts into symbolic temporal data and will apply Software Trajectory analyses to this data in order to discover recurrent behaviors. In turn, these discovered knowledge will be evaluated through interviewing for usefulness and meaningfulness. Results of interviewing will be used to improve Software Trajectory and will constitute part of my thesis and following publication.

Both case studies are exploratory in nature. At this point of my research, I can only see that the properties of my approach and its current implementation in the Software Trajectory framework appear to be very promising. The wealth of developed techniques for temporal symbolic data mining and recent development of SAX approximation allow me to overcome many computational limitations in existing approaches for mining of software process artifacts. The current implementation of Hackystat provides the ability to capture fine-grain software product and process metrics providing a richness of data, which, potentially, might reveal new insights. Current research in software process discovery indicates the overall feasibility of proposed goals in the discovery of unknown recurrent behaviors in software process. 

Nevertheless, there is no prior knowledge about application of these techniques to software process mining. Moreover, at this stage of my research, it is impossible to foresee if new recurrent behaviors will be discovered or their meaningfulness or usefulness for real world applications. For this reason I am undertaking a constructivism paradigm in my research \cite{citeulike:209817}, and will develop knowledge about the applicability of my approach to software process mining during the development of Software Trajectory framework and its empirical evaluation. By designing, developing, deploying and observing a software system, and by conducting interviews and surveys, I will gain the desired experience and knowledge.

But it is possible that this part of my research will fail, and I will not be able to discover any meaningful novel knowledge about software process. If so, I will apply every effort to investigate and explain the pitfalls of my approach to the domain of software process data mining. It may be that failure is due to the specificity of domain, or due to the insufficiency of the information enclosed in the collected artifacts, or maybe due to inefficiency of augmented methods. Through thorough analyses of failed experiments and collection of feedback, I will outline boundaries of the approach taken in this work and appropriate avenues for future development.
