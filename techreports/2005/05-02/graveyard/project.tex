%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% project.tex -- 
%% Author          : Philip Johnson
%% Created On      : Thu Oct  4 08:05:31 2001
%% Last Modified By: Philip M. Johnson
%% Last Modified On: Wed May 11 10:55:10 2005
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 2001 Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
\nocite{*}

\section{Overview}

\subsection{Motivation}

The NSF Next Generation Cybertools program has the ambitious goal of
producing technologies that ``not only change ways in which social and
behavioral scientists research the behavior of organizations and
individuals, but also serve sciences more broadly.''

To make the research issues related to this goal more concrete, and to frame our
approach to addressing them, we begin this proposal by describing an
organization with a host of interesting research opportunities and
challenges: the Defense Advanced Research Projects Agency (DARPA) High
Productivity Computing Systems (HPCS) program \cite{hpcs}.

The mission of the HPCS program involves the development of next
generation, peta-scale high performance computing platforms for commercial
availability by 2010.  In a radical break with past high performance
computing initiatives, the focus of this program is not just on the
development of new and faster hardware. In addition, an explicit objective
of this program is to radically decrease the cost and time required by
organizations to perform their science and engineering activities that
require these high performance computing environments.  For example, the
development of a new climate model might currently require a team of dozens
of scientists and engineers several years to implement.  Next generation
HPC environments should simultaneously halve the size of the team and the
time required to implement such a system. DARPA is currently funding
research and development by IBM, Sun Microsystems, and Cray to better
understand the hardware, software, and organizational requirements to
achieve up to 10x productivity improvements.  

%% The HPCS ``organization'' thus
%% consists the program management group at DARPA, research and development
%% groups at IBM, Sun, and Cray, end-user organizations such as Lawrence
%% Livermore Laboratory and Los Alamos National Laboratory, and affiliated
%% academic research organizations at universities such as the University of
%% Maryland and the University of Hawaii.

Two of the principal investigators on this proposal have been associated
with the HPCS program as academic researchers. This has given us insight
into the enormous challenges associated with understanding, assessing, and
improving organizational behavior in the largely unstudied domain of high
performance computing system application development.  While still in a
very early stage, research by the vendors and affiliated researchers has
begun to generate a body of quantitative and qualitative data
concerning the behavior of developers and others in HPC organizations.

For example, pilot studies have been performed in a classroom setting with
students developing simple high performance systems, resulting in
quantitative data on the tools they used, the times at which they
invoked the tools and the results, and properties (such as the size) of the
software they produced \cite{Funk05}. Examples of qualitative data range from interviews
with administrative staff of high performance computing centers to journals
kept by professional developers as they work on HPC software \cite{Votta05}.

Initial analyses of the raw data have included formal models, such as Timed
Markov Models fit to classroom data \cite{Smith05}. Timed Markov Models are
used to characterize workflows and the effort associated with workflow
steps.  Other case study data has been used to generate semi-formal models,
such as ``telemetry'' based analyses \cite{csdl2-04-11}.  Still other kinds
of data, such as the qualitative journal data, has been best suited to
qualitative encoding techniques \cite{Votta05}.  Research has also led to
proposals for new ways to assess high performance productivity, such as
Purpose-Based Benchmarks \cite{Gustafson04}.

So far, dissemination of research data and results have been via HPCS program
meetings \cite{hpcs-meeting} academic workshops \cite{pphec05,sehpcs05}, and 
themed journal issues \cite{ijhpca04}.

As the HPCS program builds momentum, a variety of organizational research
challenges are appearing.

First, the HPCS program is revealing the need for primary research on
organizations using high performance computing environments. Basic
questions need to be answered: How are high performance computing system
applications developed and maintained?  Where are the productivity
bottlenecks? What are the organizational constraints on innovation in
technology or methods? What is the most appropriate research methodology,
or combination of methodologies, for gaining insight into these questions?

Second, the answers to these basic question must support the design of new
technologies and organizational procedures that will yield an order of
magnitude productivity improvement in high performance computing
applications.  This requires, of course, an operational definition of
``productivity'' that can be measured in both current and future
environments.  Interestingly, no such measure has yet been agreed upon by
this community, even though its definition has profound implications for
the evaluation of the technologies under development and the future
processes and products of the end-user organizations.

Third, the HPCS program serves as an umbrella over many different types of
organizations, generating substantial challenges regarding the publication
and/or protection of information.  The three HPCS vendor awardees, Sun,
IBM, and Cray, are motivated to openly publish certain types of research
results regarding productivity in order to (for example) influence the
ultimate definition of the productivity measure used to evaluate their
systems. On the other hand, each organization also generates research
results that constitute proprietary information. The ultimate end-users of
these systems (government and military laboratories, automobile companies,
financial service institutions, etc.)  form another set of organizations.
The academic and corporate researchers form a third set of
organizations. Each of these organizational layers have privacy issues
related to the information they collect, manage, and disseminate to others.

Fourth, the HPCS program is distributed geographically and involves a large
number of constituent organizations and concurrent research activities.  A
major challenge to the program involves ensuring alignment among the many
approaches to qualitative and quantitative data gathering and research
methods.  A true ``alignment'' will enable replication, in which data
gathered to test a hypothesis at one site can be gathered in a similar
manner at another site in order to see if the hypothesis is similarly
supported.  Alignment will also enable meta-analysis, in which data from
multiple sites can be validly composed together into a larger dataset for
the purpose of certain analyses.

Having set the stage, we now present the fundamental objective of this
proposal, followed by an overview of the information infrastructure
we will use to achieve it.

\subsection{Objective}

The objective of our proposed research is to produce an open source
information infrastructure architecture and data management policies that
support scalable, collaborative, distributed, integrated, qualitative and
quantitative organizational research data collection, analysis,
dissemination, and archiving.

By ``open source'', we mean not only source code released under a license
that allows access and modification by others, but also the creation of a
community of developers willing and able to maintain and enhance this 
infrastructure beyond the period of this grant. 

By ``information infrastructure architecture'', we mean the creation of a
software framework that allows integration and interoperability of tools
developed by us and by others.  

By ``data management policies'', we mean procedures and mechanisms that
support context-sensitive publication or protection of raw or processed
qualitative or quantitative data.  The management policies will not only
address privacy issues, but also ``lifecycle'' issues related to data
repositories. 

By ``scalable, collaborative, distributed, integrated, qualitative and
quantitative organizational research data'', we mean an infrastructure that
can support hundreds to thousands of concurrent data collection and
analysis activities, allowing analysis and annotation of data by many
researchers across many institutions, combining both qualitative and
quantitative data.

Finally, by ``collection, analysis, dissemination, and archiving'', we mean
an  infrastucture that  can  support data  management  policies across  the
entire lifecycle of qualitative and quantitative data.

Our approach is intended to address the requirements for Testbed I (Organizations).

\subsection{GarageLab/Datalla}

Our information infrastructure consists of two fundamental components:
GarageLab, a front-end system to support the display and analysis of
qualitative and quantitative information, and Datalla, a back-end
peer-to-peer network to support controlled dissemination of the collected
data.

The basic function of GarageLab is to support display and analysis of data.
First, it enables the researcher to visualize multiple streams of raw
qualitative and quantitative data by organizing each as ``tracks'' along a
timeline.  Similar to multi-track editors for music (such as GarageBand
\cite{GarageBand}), GarageLab allows the user to ``zoom in'' or ``zoom
out'' of the chosen data streams, and ``cut and paste'' data streams from
one timeline to another.  GarageLab will also allow annotation of timelines
with additional information, such as for encoding episodes with
classifiers. Finally, GarageLab will allow plug-ins to support
``processing'' of the raw data in various ways.  For example, one plug-in
might produce a timed markov model, while another might produce a network
representation.

Datalla, the back-end system, provides several services.  First, each
Datalla server provides storage for raw qualitative and quantitative
organizational data.  A user with an account on a Datalla server can login
via GarageLab to access data on that Datalla server.  Second, each Datalla
server can receive qualitative and quantitative data from Datalla
``sensors'', which are small software programs that can be used to collect
and send raw data to a server.  Finally, each Datalla server can
communicate with other Datalla servers, forming a peer-to-peer network.
The kinds of data that can be communicated to other servers is controlled
by the privacy policy in effect.


\subsection{Research Approach}

Achieving our objective using the GarageLab/Datalla infrastructure will require us to 
carry out the following research and development activities. 

(1) {\em Infrastructure technology research and development.} Our prior
experience with Hackystat \cite{Hackystat} provides us with expertise in
open source development of client-server systems for automated collection
and analysis of quantitative data. We will leverage this experience in the
development of the GarageLab and Datalla software infrastructure. Research
challenges include successful application of the GarageBand multitrack
metaphor to display and manipulation of qualitative and quantitative
organizational data, and the development of suitable APIs to allow
'plug-ins' with appropriate access to internal data.

(2) {\em Research on and development of policies and procedures for data
  privacy and dissemination.} While the infrastructure can make
  context-dependent privacy policies possible, we must perform research to
  understand what appropriate privacy policies would be. Such policies will
  influence the design of publication/protection mechanisms within the
  infrastructure.  Research challenges include an appropriate means to
  classify data with respect to its privacy policy, appropriate safeguards
  to prevent unauthorized dissemination, and evaluation mechanisms to
  determine the effectiveness of a privacy policy once in place. 

(3) {\em Research on and development of models and mechanisms for
  integrating qualitative and quantitative information.}  Our basic
infrastructure can ``integrate'' qualitative and quantitative information
only in a fairly superficial sense: the data can be stored in together in a
repository, and the raw data can be displayed together along a timeline.
True integration goes much deeper: how do the qualitative and quantitative
data come together to tell us something new about the organization that we
could not have known from either kind of data by itself?  We will pursue
network models \cite{Pentland05} as one approach to this ``deeper''
integration.  Research challenges include the dependencies between the
integration models and the raw data required to successfully connect the
two types of data.

(4) {\em Case study evaluation.}  To test the validity of our approach, we
  will perform a case study of infrastructure deployment with selected
  partners in the HPCS domain. Through this case study, we will evaluate
  how well we have accomplished each component of the objective stated
  above. Note that there is a ``meta'' level in this case study. At one
  level, HPCS researchers will be collecting, integrating, analyzing,
  disseminating, and archiving qualitative and quantitative data about high
  performance computing.  At the meta level, we will be collecting
  qualitative and quantitative data about this usage of the GarageLab
  infrastructure and policies in order to evaluate our approach.  Research
  challenges include ensuring that the technology is robust enough for use
  in a live environment, and gaining buy-in from the case study
  participants necessary to evaluate the deployment effectively.


\section{Related Work}

The research related to this proposal can be organized into the areas of
infrastructure and data repositories, privacy policies and technology,
qualitative data and methods, and quantitative data and methods.

\subsection{Infrastructure and data repositories}

An important component of empirical study is the generation of data,
artifacts, and the use of experimental testbeds of various
kinds. Infrastructure, such as online data repositories, that makes this
information available to others in the community enable experimental
replication, meta-analysis, and facilitating the growth of the empirical
research community.  However, it is also useful to control access to these
artifacts so that the original researchers know how they are being used, to
prevent them from being misused, and so that the results of their use are
also available to the original researchers.  Maintenance and technical
support for these data repositories is also an important issue. In this
section, we present some examples of prior work on infrastructure and data
repositories.

{\bf NASA SEL database.} Since its inception in 1976, Principle
Investigator (PI) Basili has been affiliated with the NASA Software
Engineering Laboratory (SEL).  The original mission of SEL was to study
software development for Ground Support Software at NASA/GSFC with the goal
of improving the quality of the software developed \cite{Basili95}. It
collected data resources expended, changes and defects, product
characteristics, and processes applied as well the conformance to those
process on several hundred projects. Various project characteristics were
also collected, e.g., context variables. The artifacts developed where real
ground support systems.  Models of cost, schedule, and quality were built
using this data.  For years the data was given away freely and all
artifacts were shared with whoever asked for them. It is therefore hard to
estimate how many people used the data as it was available from both NASA
and the Rome Air Development Center repository. Problems from the owner's
point of view were (1) the data was often misused and misinterpreted
because there were not a sufficient number of context variables, (2) the
results of the analysis were often not known/shown to the original data
owners, (3) the data owners often had to spend time in organizing and
making the data available to requesters and answering questions, and (4)
the opportunities for feedback and meta-analysis were often lost. Misuse of
the data (item 1) was also a problem for the requestors as well as the
community since questionable results were published. Late in the life cycle
of the project, it was recommended that anyone who wanted to use the data
must first spend some time at the SEL in order to acquire an understanding
of the nature of the data and its appropriate use.

{\bf CeBASE.} More recently, PI Basili (along with Barry Boehm of USC)
developed the Center for Empirically-based Software Engineering
(CeBASE). CeBASE was an NSF sponsored project with the role of acting as a
repository of "experience" associated with the application of various
software development methods, techniques in order to create hypotheses,
qualitative and quantitative models of, and any other form in aggregated
experiences. The goal of the project was to create a shared repository of
empirical information on the effects of applying a variety of techniques,
methods and life-cycle models to software development. Areas of interest
for sharing have been defect detection methods \cite{Boehm01}, COTS
developments \cite{Basili01}, and Agile methods. The emphasis in CeBASE
has been to get the collaborators to provide "experience" in the form of
qualitative information on the effects of applying technologies.  The
approach has been to ask CeBASE collaborators to sign up, so they are
publicly affiliated with the project. However, there is no monitoring so
non-affiliated people can use the experience base and members can use it
without the owners knowing about it and providing feedback to the
experience base. Other issues are: Who pays for the cost of maintaining the
experience base?  Who has access to analyze and synthesize and create new
knowledge?  How do we assure they provide quality data, analysis, and new
knowledge?

{\bf Hackystat.} Since 2001, PI Johnson has been leading the Hackystat
Project, an NSF funded project to explore automated collection and analysis
of software engineering metrics.  In Hackystat, sensors are attached to
individual development tools, which unobtrusively collect data about
product and process and send them to a central server where analyses over
these data can be performed.  Examples of sensor data include: activities
(compilation, file editing, etc.) within an editor (such as Emacs, Eclipse,
JBuilder, etc.), invocation of unit tests and their results (using a test
framework such as JUnit), size/complexity of the system (in LOC, methods,
classes, operators, etc. using a tool such as LOCC), configuration
management events (such as commits and lines added/deleted using a tool
such as CVS), test case coverage (using a tool such as JBlanket), software
review process and products (time spent doing review, issues generated,
etc. using a tool such as Jupiter), and defect management (using a tool
such as Jira).  From this raw sensor data, higher level abstractions can be
built regarding the trajectory of development, and analyses can be
performed to look for trends in sensor data and co-variance over time.

{\bf TheDataWeb.} TheDataWeb is an online information repository for
demographic, economic, environmental, health, and other datasets.
Developed through a collaboration between the U.S. Census Bureau and the
Centers for Disease Control, TheDataWeb provides unified access to data
housed in different systems in 16 different federal agencies. Example
datasets include American Housing Survey, the Behavioral Risk Factor
Surveillance System, the Consumer Expenditure Survey, the Current
Population Survey, and the National CEnter for Health Statistics Mortality
Survey.  From an implementation perspective, TheDataWeb consists of two
parts: a ``DataWeb Servlet System'', which providers of data can use to
make their datasets accessable to users of the TheDataWeb, and the
``DataFerrett'', a client-side application that enables users to browse and
query these datasets and extract data from them.



\subsection{Privacy policies}

This section will cover research ethics in privacy, internet ecommerce findings, and privacy technologies 
such as P3P and Shibboleth.

\subsection{Qualitative data and research methods}

This section will talk about what is traditionally meant by qualitative research, what's been done, what 
are the issues.

\subsection{Quantitative data and research methods}

This section will talk about what is traditionally meant by quantitative research, what's been done, what 
are the issues.

\subsection{Results from prior NSF research}

Vic and Philip need to put in short paragraphs here. 


\section{Research Plan}

Our research plan involves the development of a new framework for
qualitative and quantitative data collection, analysis, and publication.
The framework will support research and evaluation of a new modeling
formalism based upon narrative theory for integrating qualitative and
quantitative data together, as well as a privacy policy mechanism for
controlling dissemination of data.  To understand the strengths and
weaknesses of our approach, we will conduct a set of case studies to
evaluate our infrastructure and modeling techniques. To support
dissemination of the research, we will create curriculum materials and use
them in a variety of classroom and professional settings.  Finally, we have
developed a coordination plan to facilitate efficient collaboration among the 
researchers in this project. Each of these components are discussed in more detail below.

\subsection{GarageBand/Datalla: Cyberinfrastructure for data collection and analysis}

Proposed framework description goes here.

\subsection{NarraNet: Integrating qualitative and quantitative information}

Narrative framework goes here.

\subsection{KnockFirst: A flexible privacy policy for scientific research data}

Privacy policy proposal goes here.

\subsection{Case study evaluation}

Case study description in HPC domain.

\subsection{Outreach and dissemination}

Classroom stuff goes here.

\subsection{Coordination Plan}

Work breakdown structure, etc. goes here. 


\section{Anticipated Contributions}

The end of the proposal.  Summarize and show the expected contributions.

\section{Excerpts from the NSF Solicitation}

We need to both ensure
that we are achieving the goals of this solicitation, as well as clearly
addressing the standard NSF review criteria.  To help us get there, the
next two sections provide copies of text from the solicitation that
we will need to keep in mind as we proceed.

\subsection{Solicitation Goals}
\label{sec:solicitation}

Some of the solicitation goals include:
\begin{enumerate}

\item The development of tools that facilitate the integration of
qualitative and quantitative information from heterogeneous sources,
multiple media, and/or multiple modes;

\item Investment in basic research that addresses the protection of the
confidentiality of respondents in computerized, widely accessible
databases; and

\item The development of incentives, standards and policies for collecting,
storing, archiving, accessing, and publishing research results using
organization-relevant information.

\item Testbed I. information collected on organizations from a variety of
heterogeneous, independently developed data sources, such as administrative
and survey data, temporal, spatial and image data or textual data. The goal
is to free users from having to locate the data sources, interact with each
data source in isolation, and manually combine data from multiple formats
and multiple sources. This could be achieved through the creation of new
and more accurate and efficient ways to collect, code and analyze
qualitative information from case studies, and other sources, and to enable
the linking of this information with repositories of quantitative data,
while protecting fundamental privacy and confidentiality concerns. The
research should be designed to show how appropriate cybertools can lead to
multiple advances in the empirical understanding of how organizations
emerge, develop, thrive or weaken.

\item Proposals must address the protection of data providers from
identification, exploitation, and other misuses of personal or
organizational information. Such misuses present a perpetual challenge to
the melding of data and media of different types in a tool for widespread
use. Proposals in response to this solicitation must show a sophisticated
understanding of this sociotechnical problem and must propose to advance
fundamental knowledge of effective privacy protections during the
development of the analytical tools and in their later use by various
research communities.

\item Proposals must demonstrate potential long-term sustainability,
usability, and impact. This could be achieved for the organizational
"testbed", for example, by documenting proposed collaboration with firms in
an industry, attracting support from foundations or developing replicable
incentive-compatible policies for collecting, storing, accessing, and
disseminating data while continuing to utilize and advance relevant
cybertechnology.

\item Unifying Data Models and System Descriptions: There is a need to
develop stronger theoretical foundations for the representation and
integration of information of various types from extant data models (e.g.,
temporal, spatial and image data, textual data, administrative and survey
data) as well as the scientific literature into conceptually coherent
views.

\item Reconciling heterogeneous formats schemas and ontologies: The
fundamental problem in any data sharing application is that systems are
heterogeneous in many different aspects, such as different ways of
representing data and/or knowledge about the world, different
representation mechanisms (e.g., relational databases, legacy systems, XML
schemas, ontologies), different access methods and policies. In order to
share data among heterogeneous sources, approaches to form a semantic
mapping of their respective representations are needed to avoid manual
intervention in each step of converting and merging data resources.

\item Web semantics: Data on the web needs to be defined and linked in a
way that it can be used by machines not just for display purposes, but also
for automation, integration and reuse of data across various
applications. Supported research topics will include frameworks for
describing resources, methods of automating inferences about web data and
resources, and the development of interoperable ontologies, mark up
languages and representations for specific social, behavioral and other
scientific domains.

\item Decentralized data-sharing: Traditional data integration systems use
a centralized mediation approach, in which a centralized mediator,
employing a mediated schema, accepts user queries and reformulates them
over the schemas of the different sources. However, mediated schemas are
often hard to agree upon, construct and maintain. For example, researchers
conducting social and behavioral research share their experimental results
with each other, but may do it in an ad hoc fashion. A similar scenario is
found in data sharing among government agencies. Architectures and
protocols that enable large-scale sharing of data with no central control
are needed.

\item On-the-fly integration: Currently, data integration systems rely on
relatively static configurations with a set of long-lived data
sources. On-the-fly integration refers to scenarios where one wants to
integrate data from a source immediately after discovering it. The
challenge is to significantly reduce the time and skill needed to integrate
data sources so that scientists can focus on domain problems instead of
information technology challenges.

\end{enumerate}

\subsection{NSF Review Guidelines}

The generic ones are:

\begin{enumerate}
\item What is the intellectual merit of the proposed activity?  How
important is the proposed activity to advancing knowledge and understanding
within its own field or across different fields? How well qualified is the
proposer (individual or team) to conduct the project? (If appropriate, the
reviewer will comment on the quality of the prior work.) To what extent
does the proposed activity suggest and explore creative and original
concepts? How well conceived and organized is the proposed activity? Is
there sufficient access to resources?

\item What are the broader impacts of the proposed activity?  How well does
the activity advance discovery and understanding while promoting teaching,
training, and learning? How well does the proposed activity broaden the
participation of underrepresented groups (e.g., gender, ethnicity,
disability, geographic, etc.)? To what extent will it enhance the
infrastructure for research and education, such as facilities,
instrumentation, networks, and partnerships? Will the results be
disseminated broadly to enhance scientific and technological understanding?
What may be the benefits of the proposed activity to society?

\item Integration of Research and Education One of the principal strategies
in support of NSF's goals is to foster integration of research and
education through the programs, projects, and activities it supports at
academic and research institutions. These institutions provide abundant
opportunities where individuals may concurrently assume responsibilities as
researchers, educators, and students and where all can engage in joint
efforts that infuse education with the excitement of discovery and enrich
research through the diversity of learning perspectives.

\item Integrating Diversity into NSF Programs, Projects, and Activities
Broadening opportunities and enabling the participation of all citizens --
women and men, underrepresented minorities, and persons with disabilities
-- is essential to the health and vitality of science and engineering. NSF
is committed to this principle of diversity and deems it central to the
programs, projects, and activities it considers and supports.

\end{enumerate}

There are several final review criteria specific to this solicitation:

\begin{enumerate}

\item  Possession of the scientific expertise and resources needed for tool development.
\item Possession of the scientific expertise and resources needed for the creation and analysis of databases on organizations and individuals.
\item  Cohesion of technology, tools and data within each "testbed".
\item  Documented outreach and dissemination plan.
\item Evidence of applicability to a broad range of sciences.
\item Quality of coordination plan.
\item Demonstration of scalability to, for example, additional organizations or other large-scale databases.
\item Evidence of long-term sustainability and impact.

\end{enumerate}














