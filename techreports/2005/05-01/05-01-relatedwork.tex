%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 05-01-relatedwork.tex -- Thesis white paper - software inspections
%% Author          : Aaron A. Kagawa
%% Created On      : Mon Sep 23 11:52:28 2004
%% Last Modified By: Aaron Kagawa
%% Last Modified On: Tue Jul  5 20:11:56 2005
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 2004 Aaron A. Kagawa
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Related Work}
\label{chapter:relatedwork}
This chapter presents previous research that is related to Priority Ranked
Inspection.  The initial invention of PRI can be attributed the current
traditional inspection literature's consistent lack of information on the
selection of documents for inspection. Previous research on software
inspection has focused on the process in which inspection is conducted.
Instead, my research focuses on the selection of documents for inspection.

Inspections are one of the oldest formal software development processes.
For more than 30 years, researchers have been studying many areas within
the inspection domain. In this chapter, I will discuss three research areas
that are most related to my research.

First, I will discuss the different types of inspection processes.
Throughout this thesis, I use the term ``inspection'' to encompass
processes defined as a static analysis technique that relies on visual
examination of development products to detect errors, violations of
development standards, and other problems \cite{IEEE-STD88}. However, since
Michael Fagan invented the inspection technique in 1976, there have been
many variations on the general concept. We now have Fagan Inspection
\cite{Fagan76}, Software Inspection \cite{Gilb93}, In-Process Inspection
\cite{Strauss94}, peer review \cite{Wiegers01}, software reviews, and code
walkthroughs just to name a few. These processes range from formal and
labor-intensive activities to informal and very cheap methods. Furthermore,
each of these processes claims to be the best inspection method for certain
circumstances. This area of research is very general and is almost entirely
focused on how to conduct inspections.

Second, I will discuss some research ideas that alter the traditional
inspection process in hopes of increasing its effectiveness.  These ideas
generally come from the understanding that inspections are too
labor-intensive and finding a more effective method would greatly affect
the results. For example, some argue that the inspection meeting is a waste
of time and resources \cite{Johnson97, Votta93}. Others argue that the
inspection meeting is critical for supporting social and educational
aspects of inspection \cite{Gilb98, Gilb99, Johnson98, Johnson98a}. Unlike,
the previous research area, alterations to an inspection process are more
specific to an individual organization's development process. For example,
deciding to alter an inspection process to remove meetings will work very
well for an organization that is geographically dispersed. It would have
less of an impact for an organization with three developers that work in
the same office. 

Third, I will discuss a very small research area focused on determining the
optimum software document to select for inspection. There are very few
publications that address this issue. Unlike the first two areas, this
research area primarily focuses on finding a set of measures that help the
selection process. Software measures vary tremendously from organization to
organization; therefore acceptance of this research has been relatively
low.


\section{Types of Inspection}
There are many different types of inspection processes. As previously
stated, they range from very formal to very informal activities.  This area
of research focuses on the inspection process and how inspections should be
conducted. In this section, I introduce a few of the most widely accepted
inspection processes.

There are two major problems that account for the large variations in
inspection research. First, there is no standard or accepted definition of
the inspection process. Second, even when processes are fairly well
designed they are extremely hard to follow. For example, a report in the
IEEE Transactions on Software Engineering found that 84 percent of
organizations performed inspections, but 0 percent performed them entirely
correct \cite{Bisant89}. Thus in practice there is many different
executions of the same inspection process. These problems indicate that
inspection processes have ambiguous or unclear definitions
\cite{csdl-95-08}. Another reason is that there is relatively little
knowledge and theory about inspection effectiveness factors \cite{Ebenau94,
  csdl-95-08}. Regardless of the problems listed above, it is generally
accepted that adopting any variation of the inspection process is much
better than not doing any.

\subsection{Informal Inspection Processes}
Informal inspections processes typically are loosely defined, not planned,
not structured, and not recorded. Ad hoc Review, Peer Deskcheck, Pair
Programming, and Walkthrough are all different types of informal inspection
processes \cite{Wiegers02}. Most inspection literature suggests that even
though informal inspections are proven to find less defects than formal
inspection, they are some times the best solutions for certain situations
\cite{Gilb93, Wiegers02}. For example, informal inspections can be used on
low-risk documents and formal inspections can be used on high-risk
documents \cite{Wiegers02}. The defining factor of informal inspection
processes is they generally require much less resources than formal
inspections.

Ad hoc review is by far the most informal form of inspection. It is a
spur-of-the moment request of asking a fellow developer to help look at a
piece of code to solve a problem. These reviews are not planned, not
measured, and have no long-term impact on the software development process.

Pair programming is a practice from Extreme Programming whereby two
programmers work side-by-side at one computer on the same design,
algorithm, code or test. This practice is often considered to be a
continuous informal inspection process, because the creation of documents is 
constantly under evaluation from the second programmer
\cite{Wiegers02}. Pair programming is a relatively new practice and is not
considered to be one of the traditional inspection processes.

Walkthroughs are informal presentations in which a developer, usually the
author of the source code, describes various aspects of the document under
review \cite{Wiegers02, Freedman90}. Walkthroughs do not have a defined
procedure and the results of the process are often not recorded. 


\subsection{Formal Inspection Processes}
Formal inspection processes typically are defined in detail, carefully
planned, very structured, multi-step, have assigned roles for participants,
and recorded. Like informal inspections, formal inspections are most
beneficial in certain situations. For example, formal inspections are
generally quite expensive as they require extensive training and up to 15
percent of the projects resources \cite{Gilb93}.

Michael E. Fagan invented inspections in 1976 while working at IBM.
``Inspection'', with a capital ``I'', or the term ``Fagan Inspection'' is
used when referring to his technique. Using Fagan Inspection, Bell Labs
reported 14 percent productivity increase, better tracking, early defect
detection, and more importantly the employees credited Fagan Inspection
with an ``important influence on quality and productivity'' \cite{Gilb93}.

One of the most widely accepted types of formal inspection is ``Software
Inspection'', which was developed by Tom Gilb and Dorothy Graham
\cite{Gilb93} in the book of the same title. Software Inspection is based
on the Fagan-style Inspection and is generally more robust and disciplined
than other techniques. Since Software Inspection is one of the most widely
accepted processes, I spend the next few paragraphs explaining the process
in more detail.

Software Inspection is defined as a two-part process, product Inspection
and process improvement. According to the Software Inspection literature,
product Inspection and process improvement cannot and should not exist
without one another.

\subsubsection{Project Inspection}
There are ten lengthy steps in the product Inspection portion of the
Software Inspection process. I have provided a short description of each of
the steps in the following sections.

\begin{flushleft}
  \textit{(1) Request: Initiating the Inspection Process} \\ The
  Inspection process begins with an author's voluntary request for an
  Inspection. The request is delegated to an Inspection leader. An
  Inspection leader is a trained-and-certified employee and is generally
  not a manager. It is the leader's responsibility to organize, plan and
  conduct the inspection.
\end{flushleft}

\begin{flushleft}
  \textit{(2) Entry: Making Sure 'Loser' Inspections don't Start} \\ The
  Inspection leader is required to check the volunteered document against
  an Entry Criteria. This criterion ensures that the document is worth
  inspecting. The leader conducts a quick look through the document to
  assess the initial quality of the document. For example, the author has
  spent an adequate amount of time working on the document, there are a
  minimal number of minor defects, etc. ``The purpose of having entry
  criteria to the Inspection process is to ensure that the time spent in
  Inspecting the product and associated documents is not wasted, but is
  well spent'' \cite{Gilb93}.
\end{flushleft}

\begin{flushleft}
  \textit{(3) Planning: Determining the Present Inspection's Objectives and
    Tactics} \\ If, and only, if the document has successfully passed the
  entry criteria, then the Inspection leader can begin to plan the
  Inspection. This includes many managerial tasks; inviting participants,
  scheduling an Inspection meeting, gathering supportive documentation,
  establishing average optimum checking rates, and suggesting areas of
  possible improvement in the document.
\end{flushleft}

\begin{flushleft}
  \textit{(4) Kickoff Meeting: Training and Motivating the Team} \\ The
  purpose of a kickoff meeting is to ensure that Inspection process begins
  correctly. This includes dispensing required documents and explaining the
  expectations of the participants. This meeting saves time by dispensing
  the necessary information, which is needed to conduct the Inspection.
  This meeting is also an opportunity to introduce process changes in the
  Inspection process.
\end{flushleft}

\begin{flushleft}
  \textit{(5) Individual Checking: The Search for Potential Defects} \\ The
  participants, or `` inspectors'', are required to work alone to find
  potential major defects in the documents provided. These defects are
  generally identified with the aid of rules, checklists, and other
  standards of the organization.
\end{flushleft}

\begin{flushleft}
  \textit{(6) Logging Meeting: Log Issues Found Earlier and Check for More
    Potential Defects} \\ This meeting has three purposes: log the issues
  generated in the individual checking phase, discover more major defects,
  and identify possible ways of improving the inspection process. This
  meeting is conducted and moderated by the Inspection leader.
\end{flushleft}

\begin{flushleft}
  \textit{(7) Edit: Improving the Product} \\ The overall goal of
  Inspection is to remove the defects that were found. During this phase,
  the author is given a list of the defects (issues become defects if they
  are deemed as valid) that were identified and is required to make the
  necessary improvements to remove any defects from the document.
\end{flushleft}

\begin{flushleft}
  \textit{(8) Follow up: Checking the Editing} \\ The purpose of this phase
  is to ensure that the author correctly executed the Edit phase. The
  Inspection leader must ensure that all issues are correctly classified;
  either as valid defects or invalid issues and that the author has
  corrected all known defects.
\end{flushleft}

\begin{flushleft}
  \textit{(9) Exit: Making Sure the Product is Ready to Release} \\ The
  Inspection leader consults the exit criteria to determine if the
  inspected document contains a certain level of quality as defined by the
  exit criteria. For example, the Exit criteria can contain rules that
  specify: successful Follow Up phase completion, certain metrics about
  this particular Inspection was recorded and within limits, and that the
  number of defects are below a certain threshold.
\end{flushleft}

\begin{flushleft}
  \textit{(10) Release: The Close of the Inspection Process} \\ This is the
  last phase of the Inspection process. At this point, the document can be
  officially released and the Inspection process is concluded. However, if
  it is determined that there are some acceptable and unavoidable defects
  remaining in the document, then such defects must be documented.
\end{flushleft}


\subsubsection{Process Improvement}
Equally important to the product Inspection portion of Software Inspection
is process improvement. Process improvement is the continuous improvement
of the entire software development process. The idea is simple; Software
Inspections can remove defects, but process improvement can prevent
defects.

In Software Inspection, process improvement can be accomplished in many
ways. A low-cost procedure could be as simple as discussing the cause of
the defects. This discussion takes place in a Process Brainstorming
Meeting. ``The purpose of the process brainstorming meeting is \textit{not}
to deal with the document and its defects. It is to deal with the
\textit{causes} of those defects'' \cite{Gilb93}.

On the other hand, process improvement can be very expensive, for example
Process Change Management Teams. Specialized teams can be formed to collect
and analyze the metrics that are obtained from the conducted Inspections.


\subsection{Software Inspection and Priority Ranked Inspection}
The different types of traditional inspection processes explained above are
quite different from the Priority Ranked Inspection (PRI) process. The
biggest difference is traditional inspection processes provide guidelines
on \textit{how to conduct inspections} and PRI provides guidelines on
\textit{what to inspect}. In fact, the PRI process does not provide any
guidance on how to inspect the documents once they are selected. Instead,
PRI is a document selection process that wraps around any traditional
inspection process.

There are three main areas where Software Inspection and Priority Ranked
Inspection differ. They are the selection of documents, cost cutting, and
volunteering. 

\subsubsection{Lack of Discussion about Selection of Documents}
In the book, ``Software Inspection'', Tom Gilb and Dorothy Graham
\cite{Gilb93} provide very few paragraphs on the subject of document
selection. The following is the entire paragraph that discusses document
selection.

\begin{quotation}
  \textit{The starting point for any Inspection is the request from the
    author of a document that the document be Inspected. Inspection is
    always voluntary, and authors must not be coerced into 'volunteering'
    documents against their will. \\ Authors are motivated to request
    Inspection for two reasons:
\begin{enumerate}
\item they will get help to upgrade their document before official release;
\item they must achieve exit status in order to claim that they have met a
  deadline, and that the quality of their work is really good enough.
\end{enumerate}
}
\end{quotation}

Like the Software Inspection process, most traditional inspection
literature fails to address several key areas of selecting a document for
inspection.

\begin{enumerate}
\item What happens when an organization does not have enough resources to
  inspect every document that is ready? Inspections are expensive. It can
  consume 15 percent of the projects budget \cite{Gilb93}. What happens to
  the volunteering process when an organization can inspect one in every
  five volunteered documents? 
\item What happens when two authors volunteer two different documents at
  the same time? Which document should be selected? Selecting what to
  inspect from two documents is not difficult. However, what if there are
  twenty or a hundred different documents that are waiting to be inspected?
\item Defects can occur in documents that already ``exited'' the
  development and inspection process; therefore can these documents be
  inspected? The current literature suggests a linear development process,
  which documents that have been inspected and completed the development
  process are never to be inspected again.
\end{enumerate}

I strongly believe that the selection of documents for inspection is a
complicated process that warrants much more attention than the traditional
inspection literature provides.

\subsubsection{Cost Cutting}
\begin{quotation}
  \textit{``The bottom line is that I [Tom Gilb] believe that it is more
    relevant to view Inspection as a way to control the economic aspects of
    software engineering rather than a way to get 'quality' by early defect
    removal'' \cite{Gilb99}.}
\end{quotation}

I believe Tom Gilb is correct. If inspections do not provide an economic
benefit, then why do them at all? However, with an estimated 10-15 percent
of a project's budget that is required to conduct successful inspections,
it is difficult for organizations with limited inspection resources to
correctly implement the suggested process. The bottom line seems to be that
not all organizations can invest 10-15 percent of their budget to
inspections.

In Software Inspection, there are three primary ways to reduce the
resources; sampling, inspecting up-stream documents, and focusing on major
defects. The practice of sampling suggest that instead of inspecting an
entire document, pick one to four representative portions of the document.
The practice of inspecting up-stream documents suggests that requirement
and design documents need to be correct before programming can begin.
Focusing on major defects suggests that minor defects, such as code
comments, are irrelevant to the customer-performance of the system and
should be ignored.

In my opinion, Software Inspection and other traditional inspection
literature does not address the most obvious way to save resources, which
is minimizing the number of documents that need to be inspected. The
current literature suggests that inspections are a ``gateway'' to complete
the document's development process \cite{Ebenau94, Wiegers98}. This process
works well for organizations that have the resources to treat it as such.
However, for organizations with limited inspection resources, inspecting
every document is quite impossible. In contrast to traditional approaches,
Priority Ranked Inspections embraces the notion of skipping the inspection
of some documents.

\subsubsection{Volunteering}
Another problem associated with the selection of documents for inspection
is the notion of volunteering. Notice the term 'voluntary' is emphasized in
Gilb and Graham's selection process. Yet, in the very same book, ``Software
Inspections'', contains a case study of Software Inspections used in a
company where documents were required to be inspected rather than
volunteered.

\begin{enumerate}
\item ``Most who tried inspections responded with enthusiasm, but only four
  groups were continuing to do inspections - not surprisingly, those groups
  in which the manager \emph{required} them. Most groups tried a few
  inspections, then interest waned as deadlines approached. A few managers
  ignored inspections altogether, citing schedule pressures as the
  reason.''
\item ``The vice president of marketing notified his department that
  inspections were \emph{required} for approval of all mandatory documents
  produced.''
\item ``Each year, development groups are \emph{required} to inspect
  more of their pre-code documents.''
\item ``Code inspections remain optional at least until \emph{100 percent}
  of the pre-code documents are inspected.''
\end{enumerate}

In my opinion, many of the problems associated with volunteering is that
most developers do not have a clear understanding of what documents are
best to volunteer \cite{Wiegers98}.


\section{Alterations to the Inspection Process}
This section presents some examples of related research on alterations to
traditional inspection processes to increase inspection effectiveness.
Research in this inspection area focuses on specific process changes and
the addition of tool support. I spend most of this section discussing an
area of research aimed at using automated tools to support the inspection
process, because this area is directly related to my PRI research. However,
there are many other different research findings and suggestions that are
not presented in this section. 


\subsection{Eliminating Steps in Inspection}
One of the first things organizations do to save resources is alter the
inspection process by changing the required steps within the process. I
present two of the many publications in this research area. The first is a
simple suggestion to eliminate the inspection meeting. The second suggests
that outsourcing the entire inspection process is beneficial.

\subsubsection{Inspections Without Meetings}
Lawrence Votta Jr. found that the inspection meeting, where the inspectors
meet and discuss the validity of the issues found in the individual phase,
is ineffective and can actually delay the inspection process days and even
weeks \cite{Votta93, Glass99}. Therefore, he proposed that the inspection
meeting be totally eliminated. Of course, other publications and
organizations disagree and would rather hold inspection meetings, because
they value the benefits of synergy and education over the increased
resources need to conduct inspections \cite{Gilb98, Gilb99, Johnson98,
  Johnson98a}. 

\subsubsection{Outsourcing Inspections}
An extreme process change is outsourcing the whole inspection process to a
third-party company. Jasper Kamperman states that outsourced software
inspections are cheaper, easier, and greatly beneficial \cite{Kamperman05}.
A company utilizing outsourced inspections would send out software code,
documentation, and a survey and receive back a list of potential defects.
However, Kamperman mentions that outsourced inspections will probably find
more superficial programmatic defects than deep design problems. In
addition, according to other research, Automated Software Inspection tools
are also an effective way to detect superficial programmatic defects. I
discuss these tools in the next section.

\subsection{Automated Software Inspection}
\label{subsection:automated-software-inspection}
Due to the rigorous and labor-intensive requirements of formal inspection
processes, a whole branch of research on inspection is aimed at automating
some part of the inspection process. There are two different ways
automation can be added to an inspection process. First, add tools that
automate the inspection process, which makes it less labor-intensive to
follow guidelines and record the results \cite{vanEmden02}. Second, add
tools that inspect code automatically, which replace some of the work
required of human inspectors \cite{vanEmden02}. Most of the current
research focuses on the second type of automatic software inspection. This
type of automation has a very low overhead with many potential benefits.
For example, tools that support automated software inspection are less
dependent on human factors \cite{NagappanWPSV04}. PRI implemented with
Hackystat is an example of a type of automated software inspection support.

\subsubsection{Automated Software Inspection Tools}
Automatic software inspection (ASI) tools are a relatively new way to
identify defects early in a development process. In addition, it is very
useful in identifying a subset of defects found in manual software
inspection, but is not as labor-intensive. ASI tools are also known as
static analysis tools that analyze source code and provide error and
warning messages similar to a compiler.

Using ASI does not replace manual inspections. Labor-intensive manual
inspections, for example Software Inspections, will find more complex,
functional, algorithmic design problems \cite{NagappanWPSV04}. However, ASI
tools will make manual inspections more effective by allowing the
inspectors to focus on these issues. In the related research field of
pre-release defect density, Nagappan and Ball \cite{NagappanB05} had very
successful findings in utilizing static analysis, which should also
translate to inspection:

\begin{enumerate}
\item Static analysis defect density can be used as early indicators of
  pre-release defect density;
\item Static analysis defect density can be used to predict pre-release
  defect density at statistically significant levels;
\item Static analysis defect density can be used to discriminate between
  components of high and low quality.
\end{enumerate}
  
Criticisms of ASI tools state that the defects found are generally
superficial programmatic errors and cannot replace manual inspections
\cite{vanEmden02}. In addition, a major problem with ASI tools is that they
can generate up to 50 false positives for every valid defect
\cite{NagappanWPSV04}.


\subsubsection{Code Smells}
A unique idea to improve the effectiveness of software inspection with
automation is ``code smells'' \cite{vanEmden02}. Code smells are a metaphor
to describe patterns that are generally associated with bad design and bad
programming practices \cite{vanEmden02}. Code smells are different from
other static analysis tools such as the ones mentioned in the previous
section. Instead, it is inspired by the ``code smells'' defined in
\cite{Fowler99}, which describe patterns of code that requires refactoring.
Like the human sense of smell, any group of source code has a smell; the
question is whether the smell is good or bad. The following are a few
examples of code smells discussed in \cite{vanEmden02}:

\begin{enumerate}
\item Duplicate Code
\item Methods that are too long
\item Classes that contain too much functionality
\item Classes that violate data hiding of encapsulation
\item Classes that delegate the majority of their functionality to other
  classes
\end{enumerate}

There are three defining characteristics of code smells; what smells are
detected, how the smells are detected, and how are the smells are
presented.

\paragraph{What smells are detected}
There are three general rules associated with code smells. First, like the
human sense of smell, there is no static list of all possible code smells.
Different projects and organizations can have a different set of code
smells that works best for their quality assurance. Second, code smells are
subjective measures, which are based on the organization and project's
previous experience. Code smells are parameterized to provide subjectivity
on whether specific smells are good or bad. Third, code smells do not have
to be precise. In other words, code smells do not give an absolute
decision about the quality of a software document. Instead, it provides a
possible indication of problems relative to the other documents in the same 
software system. 

These three rules are very similar to the rules in PRI. First, PRI is fully
extensible and provides the ability to add any PRI measure and PRI
indicator to the ranking function. Second, PRI supports subjectivity in the
calibration of PRI indicators. Third, PRI does not give an absolute
determination of a software document's MINI or LINI distinction. Instead,
the MINI and LINI distinction is relative to the other PRI rankings in the
same project.

\paragraph{How the smells are detected}
Code smells are detected with a software tool that statically analyzes
source code. As I previously stated, the static analysis tool makes
automated software inspection possible. Therefore, code smells provides the 
possibility of lowering inspection resources by automatically detecting code 
that has a bad smell. 

%%There are two different types of code smells; primitive and derived smells.
%%Derived smells are created with a combination of one or many primitive
%%smells. Primitive smells are detected directly from the source code.

Code smells are measures of a software product. PRI differs from this
approach, because it allows measures of software product and development
process activities. One interesting future enhancement to PRI is to
investigate the addition of code smells to the PRI measures. One would
think that if code smells are a useful measure, then the combination of
code smells with other software product and development process measures
would also be beneficial. 

\paragraph{How are the smells are presented}
Code smells are presented with structural graphs. These graphs allow the
user to interact with the graphs to find ``smelly'' code. 

Although, I haven't used the presentation user interface, it seems that
this particular choice of graphical presentation is a little difficult to
use effectively. In my opinion, users will not want to search for the
``smelly'' code. Therefore, I believe the most useful presentation of code
smells is a tabular ranking similar to the ranking provided in
PRI. \newline

\indent To conclude code smells are a unique automatic software inspection
technique and has many of the same problems and solutions that I have
addressed in my PRI research.




\section{Selection of Documents for Inspection}
This section presents some inspection research on increasing the
effectiveness of inspection by selecting the right documents to inspect.
This is a very small area of inspection research relative to the two
previous areas discussed in the proceeding sections. In fact, in some ways
it is a much smaller branch of the ``Alterations to the Inspection
Process'' domain. Research in this area is almost always focused on the
utilization of tool support to help aid the selection process. Since tool
support is very specific to an organization's development process, research
in this area is not as general as the two previous areas.

``Few organizations have the time and commitment to inspect everything they
create (unless contractually required to do so), so focus your inspection
resources where they will do the most good'' \cite{Wiegers02}. In this
point in my thesis, I've said this many different ways. And each of the
following sections contain research that try to find the ``best code to
inspect.'' Once again, I believe PRI is an example of research in this
area. 


\subsection{Code Smells}
As I already explained in Section
\ref{subsection:automated-software-inspection}, code smells are a unique
idea to improve the effectiveness by identifying software code that needs
to be refactored. In addition, code smells can aid the inspectors'
assessment of quality software code. The research on code smells not only
provides automated tool support, but it also provides a general process of
which any tool can support the basic theory of code smells.

Although, authors of the publication \cite{vanEmden02} do not explicitly
say, code smells could also be used to aid the document selection process
for inspection. Instead, they leave us with a general statement that code
smells can immediately show the maintainers if the system contains bad
smells, what parts are affected, and where the concentration of smells is
the highest \cite{vanEmden02}. In addition, they do not make any claims
that the inspection of code with ``really bad code smells'' will detect
more high-severity defects than the inspection of code with ``really good
code smells.''

\subsection{Crocodile}
Crocodile \cite{KohlerRS98} is another automatic software inspection tool
similar in nature to code smells. The research context of this tool is
large object-oriented software systems. The publication \cite{KohlerRS98}
plainly states that a large software system is often too large to be
entirely inspected by humans. Using Crocodile can concentrate inspection
resources to ``critical'' areas of the system where inspection is most
necessary. Therefore, unlike code smells, this research has explicitly
claimed that Crocodile can help pre-select suspicious modules for manual
inspection. In addition to building a tool, the publication
\cite{KohlerRS98} creates a process that utilizes the tool to support its
intended design of auditing large software systems.

The Crocodile tool quantitatively measures structural properties of an
object-oriented system. Like most automatic software inspection tools, this
is done by static analysis of source code. The measurements are then fed
into a database where meta-analysis is conducted to build a quality model
of the system. The quality model is very similar to a Goal-Question-Metric
graph and consists of a goal (which is identifying software quality),
factors (which are quality measures such as maintainability and
reusability), criteria (which help define and measure the factors), and
metrics (which are used to determine if the criteria is fulfilled). Each
level of the quality model contains threshold values to determine the
``good'' or ``bad'' meaning of the measures. For example, the allowed
threshold for the ``Number of Parents'' measure is 0 and 1. Although not
explicitly stated, I assume these thresholds are configurable.

The Crocodile tool and the process of its use are, in some ways, very
similar to PRI. First, they both have the same goal of trying to identify
areas of a software system where inspection is most necessary (MINI).
Second, the Crocodile's threshold limits to determine meaning of the
measures are very similar to the PRI indicators, which also uses threshold
limits to provide a numerical ranking. Third, Crocodile's process includes
a phase in which the tool can be adjusted based on any anomalies found in
the results. However, there are a few differences. First, Crocodile only
supports one type of measurement, which is object-oriented metrics from
static analysis. PRI supports any type of measurement, both product and
process measures. Second, like code smells, they do not make the claim that
the inspection of ``critical areas'' will detect more high-severity defects
than the inspection of ``non critical areas.''


\subsection{Risk Analysis}
Evaluating the risk of potential defects, usually called risk analysis,
within documents is another way to select documents for inspection. Risk
can be defined as the likelihood that a work product contains defects and
the potential for damage if it does \cite{Wiegers02}. Therefore, one
objective of using this approach is to reduce the risk associated with a
specific document.

Karl Wiegers mentions a few high-level selection criteria in his book,
``Peer Reviews in Software: A practical guide.'' \cite{Wiegers02}.
According to Wiegers, an organization should select documents to inspect
that have the following properties.

\begin{enumerate}
\item Code that could potentially contain errors that could propagate
  throughout your product and lead to expensive rework or to execution
  failures.
\item Code that traces back to safety- or security-related requirements.
\item Modules that has been changed many times.
\item Modules that has a history of containing many defects. 
\item Fundamental and early-stage documents, such as requirements and
  specifications. 
\item Documents on which critical decisions are based, such as
  architectural models that define the interfaces between major system
  components.
\item The parts you aren't sure how to do, such as modules that implement
  unfamiliar or complex algorithms or enforce complicated business rules
  and other areas in which the developers lack experience or knowledge.
\item Components that will be used repeatedly.
\end{enumerate}

To be able to repeatedly and reliably determine if work products contain
any of these properties, there must be a way to accurately measure the
properties that indicate documents in most need of inspection. PRI
implemented with Hackystat provides the possibility to be able to ``sense''
some, if not all, of these properties.

















