
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The Inevitable Pain of Software Development: Why there is no silver bullet
Daniel M. Berry

[15] "However, inspection is a double pain. First, the documents to be
inspected must be produced, and we know that documentation itself is a
pain. Second, inspection is one of these unpopular activities that are the
first to be scrubbed when the deadlines are looming \cite{Gilb93}."

[16] "Every shred of evidence indicates that formal technical reviews (for
example, inspections) result in fewer production bugs, lower maintenance
costs, and higher software success rates. Yet we're unwilling to plan the
effort required to recognize bugs in their early stages, even though bugs
found in the field cost as much as 200 times more to correct
\cite{Pressman95}."


%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Using multiple adaptive regression splines to understand trends in
inspection data and identify optimal inspection rates

Lionel C. Briand, Bernd Freimut, Ferdinand Vollei.

[1] "However, when on is faced with planning inspections, a number of
decissions have to be made. For example, the following questions are
considered relevant as they are deemed to have an impact on inspection
effectiveness, that is the capacity of inspections to uncover defects: 
- What overall effort to devote to the inspection?
- What should be the inspection rate?
- How many participants to involve?
- How should the material to be insepcted be broken down?
"

[2] "Because there is little knowledge and theory about inspection
effectiveness factors [cite]..."

[3] "In order to improve and control inspections, it is first necessary to
indentify the factors impacting inspection effectiveness, that is the
number of defects detected. Knowing and understanding these factors will
enable us to control them when planning and conducting an inspection, so
that a maximum defect detectiong effectiveness can be achieved. "

"For example, several studies showed that the effort spent on inspecting an 
artifact has a major impact on the inspection effectiveness
[13]. Christension et al. [2] reported the preparation effort of the
inspectors to be correlated with the densitiy of defects found. Ebenau [6]
identified the examination rate and the preparation rate as major drivers
of inspection effectiveness. In a context where defects are searching
during meetings (i.e., examination), spending more effort on preparation
(i.e., reading a document) yields a higher understanding of the documents
to be inspected and hence results in more detected defects during
inspection meetings. Spending more effort on examining the document simply
increases the thoroughness of the inspection and increases the chances of
detecting defects."

"Characteritices of the inspected product can have an impact on the
effectiveness of inspections as well. Some studies [2][6] reported the size 
of the inspected document to impact inspection effectiveness as a larger
document usually contains a larger number of defects. Additionally, the
"complexity" of the product [2][8][9] and its initial quality [10] can have 
an effect on inspection effectiveness as these factors relate to the defect 
context of the inspected product."

"The characteristics of the inspection team can also show some effect on
inspection effectiveness. Porter et al. [11] suggest that an inspection
team composed of several inspectors can enable the detection of a wide
variety of defects since each inspector is likely to rely on a different
expertise. The larger and more varied the team, the better the coverage of
the document, thus resulting in an increased inspection
effectiveness. Inspectors well versed in the application domain can already 
know about potential defects in the inspected product [11][15][13]."

"Finally, the organization of the inspection process and its infrastructure 
can have an impact on the effectiveness as well. Porter et al. [11]
indentify the number of inspection sessions as another factor influencing
inspection effectiveness. Additionally, the defect detection technique
chosen for an inspection may have an impact on effectiveness as well. For
example, more systematic techniques may help inexperienced inspectors [12].


%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Preliminary results on using static analysis tools for software inspection
Nachiappan Nagappan, Laurie Williams, John Hudepohl, Will Snipes, Mladen
Vouk

[1] "Automated software inspection tools are emerging for identifying a
subset of defects in a less labor-intensive manner than manual inspection."

"Recently, commercial automated software inspection (ASI) tools and
services have been emerging. ASI tools are also known as static analysis
tools, code standard analysis tools, and source code analysis tools. These
tools produce error messages similar to those of a compiler."

"ASI tools are intended to identify faults which allow the software
enginners to recode before they surface more publicly as manual inspection
faults or as test and/or field failures."

"However, ASI does not replace the manual inspection process. As will be
discussed, manual inspections have been show to identify a wider range of
defect types than ASI. However, the removal of the defects found by the ASI 
can allow the labor-intensive manual reviews to be more efficient and focus 
on more complex, functional and algorithmic defects." 

[2] "The effectiveness of inspections is dependant upon satisfying many
conditions, such as adequate preparation, readiness of the work product for 
review, high quality moderation, and cooperative interpersonal
relationships. The effectiveness of ASI is less dependant upon these human
factors."

"An important issue with the use of ASI tools is the inevitablility of
significant amounts of false positives. False positives are when the tool
identifies a fault but a deeper analysis of the context shows no
problem. There can be as many as 50 false positives for each true fault
found by automated inspection tools."

[3] "Code churn is a measure of the number of changed lines of code (LOC)
between two (not necessarily consecutive) releases of the code. Similar to
the LOC metric [17], code churn can also be used as an indicator of quality 
factors, such as fault-proneness [16]."

[7] "The ASI faults, code churn, deleted lines of code, and actual defects
of the product under study were analyzed. The main observations are as
follows: 
- A multiple regression approach for empirical analysis of actual field
failures using ASI faults, code churn and deleted lines of code leads to
statistically significant results, indicating these are effective
in-progress metric indicators of actual field qualityl and
- Discriminant analysis performed using automated inspection faults, code
churn and deleted lines of code is a feasible technique for detecting fault 
prone programs"

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Static analysis tools as early indicators of pre-release defect density
Nachiappan Nagappan, Thomas Ball

[1] "During software development it is helpful to obtain early estimates of 
the defect density of software components. Such estimates identify
fault-prone areas of code requiring further testing. We present an
empirical approach for early prediction of pre-release defect density based 
on the defects found using static analysis tools. ... We show that there
exists a strong positive correlation between the static analysis defect
density and the pre-release defect density determined by testing."

"Static analysis can find errors that occur on paths uncovered by
testing. However, static analysis may produce false errors.
Testing, on the other hand, has the ability to discover not only the
"shallow" errors exposed by static analysis tools but also to expose deep
functional and design errors. We cannot expect static analysis tools to
find such errors. Said another way, programmers make many different kinds
of errors. Static analysis tools find certain classes of these errors while 
testing must be used to find the other classes of errors."

"our results show that the static analysis defect density is correlated at
statistically significant levels to the pre-release defect density
determined by various testing activities. Further, the static analysis
defect density can be used to predict the pre-release defect density with a 
high degree of sensitivity."

[3] "These studies have used structural and complexity metrics of the source
code to act as predictors of system defect density and
fault-proneness. Instead, we leveraged the subset of defects found by
static analysis tools to estimate pre-release system defect density. While
static analysis tools find only a subset of all the actual defects, it is
highly likely that these defects would be indicative of the overall code
quality. Our basic hypothesis is that the more static analysis defects
found, the higher the probability of other defects being present."

[6] "Based on our results, we find that:
- Static analysis defect density can be used as early indicators of
pre-release defect density;
- Static analysis defect density can be used to predict pre-release defect
density at statistically significant levels;
- Static analysis defect density can be used to discriminate between
components of high and low quality."

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Predictive Quality Control with Software Inspections
Robert G. Ebenau

[1] "Software inspections are most successful when they are applied in a
development process that has clearly delineated checkpoints at key
development stages. Criteria can then be established that determine when an 
interim work product is ready to inspect and when it is successfully
completed. Software inspections thus provide a gating point, an opportunity 
to control the transfer of the product from development stage to the next."

[2] "As inspection data is collected during the inspection meetings, it is
analzed and the results are fed back into the inspection and development
processes. This enables a project to detect error prone patterns and
possible overall weaknesses in the product or project procedures. But,
what's more, the inspection data can also provide a "presumptive" measure
of product quality and permit statistical quality control."

[5] "The TACK results are very disapointing. The density of the major
defects is particularly low, while the preparation and examination rates
are very high. The inspection specifications called for preparation and
examination rates of 200 lines per hour or less, which were both greatly
exceeded. Because of these poor results, TACK did not persue any further
inspection data analysis, but rather sought to identify why the inspection
progress was allowed to operate uncontrolled"

[aaron] Looking at the data presented on the TACK project in
\cite{Ebenau94}, one can assume two causes of his findings. First, the
inspection team did not follow the inspection process well. Second, the
TACK project had very few defects and had very high software quality.
However, Ebenau does not mention the cause of his findings. Using the
second assumption, it could be reasonable to say that most of the TACK
project was LINI (least in need of inspection). Therefore, according to
PRI, I would claim that most of the project did not need to be inspected,
thus saving resources. Again, using the second assumption of high software
quality, it seems that Ebenau's statements that decreasing preparation and
examination rates would yield better results is contradicting. I would
claim that inspecting a very high quality (LINI) document with a slower
preparation rate would not uncover a significant amount of additional
defects. 

[10] "Manage inspection performance to achieve effective results by
maintaining reasonable work product sizes and preparation and examination
rates". 

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
An assessment of large object oriented software systems: a metric based
approach 
Gerd Kohler, Heinrich Rust, and Frank Simon

[1] "We define a metric based process for the audit of large software
systems and present first experiences of using it. This process respects
both the fact that human insight is necessary for a successful
identification of problems in a large system, and that these systems are
often too large to be reviewed by humans completely. In the process, an
automatic software measurement tool is used to focus the experts efforts to 
the modules of the system for which this inspection is most necessary. The
process includes a planning phase and an evaluation phase. The planning
phase considers management activities like effort estimation and time
schedules and takes into account the particular situation and needs of the
application context. In the evaluation phase, statistical information and
customer feedback is used to evaluate the quality of the process and to
improve it."

[2] "Our motivation for the development of such a process is our perception 
of the following problem in software development practice: There are large
software projects which tend to outgrow the capacities of the developers to 
keep an overview. After some time, it is necessary to rework parts of this
"legacy systems". But how should capacities for software reengineering be
applied? Finite resources have to be efficiently used. Our process should
help project teams to identify the most critical aspects regarding the
internal quality of large software systems. 

The following shaped our process:
- Human insight is necessary to check if a method, class or subsystem has
to be reworked.
- In very large systems, not all modules can be inspected manually.
- Automatic software measurements can help to preselect suspicious modules
for manual inspection. We assume good correlation between the manual
idenfication of anomalous structures and the identification of anomalies
based on automatic measurements.
- Thus, in very large systems, the efficient application of human resources 
should be guided by an automated analysis of the system.
"

[2] "A software metrics tool, called Crocodile, was developed at the
Technical Univeristy in Cottbus [LS98]. It is used to focus the attention
of an inspector to critical parts of the software. This focusing is based
on quantitative measurements of structural properties of the
object-oriented system. Crocodile does not deal with source code
details. It only considers packages (e.g. Java packages or subsystems),
classes with inheritances and associations, their methods/attributes and
their usage."

[3] Module Selection (2.2) 
"The revers engineering step creates an abstract description of the whole
software system. Within this abstraction the automatically calculated
critical values of phase 2.1.4 are identified and localised. Crocodile
supports the module selection with some visual tools. For example it plots
a diagram for every measure with the most critical outlier and the
thresholds. Another diagram for the module selection displays a weighted
combination of some metrics. Defining a combined metric on an ordinal scale 
makes it easy to select the most critical parts for the observation focus."

[4]
Source code review (2.3)
After selecting a module, source code review is done. 

[5] Crocodile Metrics
- Number of Parents
- Character of Code
- Weighted method count
- Afferent coupling between objects
- Efferent coupling between objects
- Number of getting and setting methods
- Number of public attributes
- Number of public methods

[6] "a lot more knowledge about the software can be used in the process,
and more specific consequences follow: The quality model can be tuned to
the specific demands at hand; reverse engineering is less an issue, some
modules are known to be more critical than others and thus deserve more
attention, whether their metrics values are conspicuous or not; evaluation
of the measurement is easier, since from practice, the modules which are
more difficult to use or which have more defects are known."

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Java quality assurance by detecting code smells 
Eva van Emden, Leon Moonen

[1] "Code smells are a metaphor to describe patterns that are generally
associated with bad design and bad programming practices. Originally, code
smells are used to find the places in software that could benefit from
refactoring. In this paper, we investigate how the quality of code can be
automatically assessed by checking for the presence of code smells and how
this approach can contribute to automatic code inspection."

"It is generally accpted that the cost of repairing a bug is much lower
when that bug is found early in the development cycle. One of the
advantages of software inspection is that the software in analysed before
it is tested. Thus, potential problems are identified in the beginning of
the cycle so that they can be solved early, when it's still cheap to fix
them." 

"Traditionally, software inspection is a formal process that involves
labor-intensive manual analysis techniques such as formal code reviews and
structured walk-throughs. Inspection is a systematic and disciplined
process that is guided by well-defined ruls. These strict requirements
often backfire, resulting in code inspections that are not performed well
or sometimes even not performed at all.

These problems are addressed by tools that automate the software inspection 
process. We distinguish two approaches:
1. Tools that automate the inspection process, making it easier to follow
the guidelines and record the results.
2. Tools that perform automatic code inspection, relieving the programmers
of the manual inspection burdon.

We concentrate on the second type: tools that perform automatic
inspection. Such tools are interesting since automatic inspection and
reportin on the code's quality and condformance to coding standards allow
early (and repeated) detection of signs of project deterioration. Early
feedback enables early corrections, thereby lowering the development costs
and increasing the chances for success."

[2] Code smells
"The existing tools that support automatic code inspection (for example, 
the well-known C analyzer LINT [15]) tend to focus on improveing code
quality from a technical perspective. the fewer bugs (or defects) there are 
present in a piece of code, the higher the quality of that code. From this
perspective, code inspection boils down to low-level bug-chasing and we see 
this reflected in the tools which typically look for problems with pointer
arithmetic, memory (de)allocation, null references, array bounds errors,
etc.

In this paper, we will focus on a different aspect of code quality:
Inspired by the metaphor of "code smells" introduced in the refactoring
book [12], we review the code for problems that are generally associated
with bad program design and bad programming practicies.

Beck and Flower introduce this metaphore of "code smells" to describe the
patterns in code that indicate that refactoring can be
applied. "Refactoring is the process of changing a software system in such
a way that it does not alter the external behavior of the code yet improves 
its internal structure". It improves the design of the software system
after it was written by tidying up code and reducing its complexity. The
resulting software is easier to understand and maintain.

Code smells can be used to answer the question when and what to
refactor. .The idea is not necessarily that no code smells are permitted,
but rather that code smeslls are hints which tell us that refactoring may
be beneficial. Some examples of code smells are: duplicated code, methods
that are too long, classes that contain too much functionality, classes
that violate data hiding of encapsulation or classes that delegate the
majority of their functionality to other classes."

2.1 What smells are we going to detect?
"First, such a list of code smells can never be complete: there will always 
be a domains and projects where a diferent set of code smells should be
applied."

"Second, code smells are subjective: they are based on opinions and
experiences."

"Finally, code smells are not precise: "One thing we won't try to do here
is give precise criteria for when a refactoring is overdue. In our
experience no set of metrics reival informed human intuition [12,
p.75]. This is related to the subjectivity of code smells. 

"For these observations, we can only conclude that one of the main design
requirements for a code smell inspection tool is that smells should be
configurable by the user. As tool builders, we can predefine a number of
smells but configurability is need to allow for: (1) definititon of
additional smells, (2) removal of smells that shoudl be not be considered,
and (3) more precise definition of a smell sothat its parameters can be
tuned."

[3] 2.2 Howe are we going to detect these smells?
Static analysis of source code.

2.3 How are we going to present the results?
Graphs.


[aaron] code smells is an excellent idea. the problem with this paper seems 
to be the implementation of the tool. Especially the reporting aspect
associated with the presentation of the results. Overall the theoritical
aspects of code smells fall in line with PRI.

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
A critique of software defect prediction models
Norman E. Fenton, Martin Neil

[1] "Organizations are still asking how they can predict the quality of
their software before it is used despite the substantial research effort
spent attempting to find an answer to this question over the last 30
years. There are many papers advocating statistical models and metrics
which purport to answer the quailty question. Defects, like quality, can be 
defined in many different ways but are more commonly defined as deviations
from specifications or expectations which might lead to failures in
operations. 

[aaron] need to read this paper first. this paper is too hard.

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
When two eyes aren't enough
Karl E. Wiegers

[1] "This article describes several kinds of peer reviews that span a range 
of formaility and structure. It also provides guidance for selecting an
appropriate review technique for a given characterisitic."

[aaron] this is a good article to use for the related work, to explain the
different types of inspection (peer review as Wiegers calls it). Figure 1
and Table 1 would be great graphics to reference. 

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The seven deadly sins of software reviews
Karl E. Wiegers

[1] Sin 1: Participants Don't Understand the Review Process
"Team members may not know which of their work products should be
reviewed, when to review them, who should participate, and what review
approach is most appropriate in each situation."

"Training is the best way to ensure that your team members share a common
understanding of the review process. For most teams, four to eight hours of 
training should be sufficient, though you may wish to obtain additional
specialized training for those who will act as inspection moderators."

[2] "Your peer review processs should include procedures for both formal and
informal reviews. Not all work products require formal inspection (though
inspection is indisputably the most effective review method_, so a palette
of procedural options will let team members choose the most appropriate
tool for each situation."

Sin 3: Reviews Are Not Planned
"The benefits of well-executed software technical reviews are so great that 
project plans should explicitly show that key work products will be
examined at planned checkpoints."

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Systematic improvement of technical reviews in large-scale system
development 
Oliver Laitenberger, Marek Leszak, Werner Brunck, Dieter Stoll

"Today, reviews at Lucen/ONG usually consume around 9% to 12% of the total
development effort. These cost include gate (i.e. quality milestone)
reviews as well as technical reviews on documents, software sources, and
other artifacts of the development process. Still, somoe defect slip
through the review process. A retrospective root cause analysis [8]
revealed that a large amount of defects detected in late development
phases, that is, system integration and test, could have been found in
reviews. This finding provides the motivation for the improvement of the
existing review implementation to increase its effectiveness. "

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
A value-based framework for the cost-benefit evaluation of software
inspection processes
S. Biffl, M. Halling

[1] "As inspection is an effective but resource-intensive approach to find
defects, it is usually not cost-effective or may be even impossible to
inspect all working products extensively. Information from risk analysis
and the project value chain can be used to prioritieze which working
products to inspect to what extent."

"These benefits address different aspects of software projects and require
different measures for their quantification, while inspection costs are
usually measured in person hours [4] or their monetary equivalent. For
cost-benefit evaluation of inspection costs must be compared to benefits,
which gets more complicated, if they are measured in different units."

[2] "In general, inspection is useful if it is feasible and more
cost-effective than other quailty assurance approaches in the project to
find important potential defects in the working product."

"as quality assurance in general and inspection in particular is costly,
not all working products can usually be checked with the same care and
therefore a prioritization must be made. We suggest using information from
project risk analysis for this purpose. For risk management, e.g. with the
RiskIt approach [11], the chain of products and processes, which work
towards creating the final product, has some uncertainty of succeeding. The 
risk is to lose some of the expected level of value of the final system."

"Therefore the context of the risk exposure provides a basic guideline for
deciding on whether a product qualifies to be important enough for inspection."

[4] "The assessment of defect potential determines the documents to be
inspected influencing the inspection costs mainly though document size and
importance. Furthermore it has an impact on the selection of the inspection 
process as it provides information on target defects. The benefit from
inspecting a document with a large defect potential is the removal of
defects and a related information gain."

[aaron] similar to code smells

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
When is it cost effective to use formal software inspections?
Bob McCann

[1] "The purpose of this article is to present a way quantitatively to
determine the parametric limits to cost effectiveness of software
inspections based on a previously published model. This anlaysis leads to
the conclusion that it is cost effective to inspect both original code and
most modifications to the code after initial coding. Any exceptions should
be carefully considered based on quantitative analysis of the projected
impact of the exceptions."

"Deciding whether or not to inspect work products based on qualitative
understanding of various limiting parameters such as work product size,
preparation rate, or expected defect density."

"If sufficient metrics are collected during a software development project, 
it is possible to know the cost structure of the various development
processes. That in turn enables modeling of the cost impact of all
processes that discover software defects."

[aaron] this article presents many different formulas, which seem to no be
very useful. 

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Does every inspection need a meeting?
Lawernece G. Votta Jr.

"we have found that these inspection meetings are not as beneficial as
managers and developers think they are. Even worse, they cost much more in
terms of product development interval and developers' time than anyone
realizes. 

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Investigation the cost-effectiveness of reinspections in software
development
Stefan Biffl, Fernd Freimut, Oliver Laitengerger

"As a reinspection is often believed to be less efficient than an
inspection an important question is whether a reinspection justifies its
cost."

"In this paper we propose a cost-benefit model for inspection and
reinspection. We discuss the impact of cost and benefit parameters on the
net gain of a reinspection..."

[aaron] this article makes sense because the cost-benefit model is based on 
the previous inspection results. 

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
How to jump-start inspecting by outsourcing
Jasper Kamperman

"A novel approach to finding defects, an outsourced software inspection
service is easier to introduce and has successfully jump-started inspection 
in the software development organizations of major telecommunication and
industrial process companies."

"Perhaps even more than other changes to the development process, attempts
to introduce "traditional" inspections generally meet with a lot of
resistance. Most objections are along the lines of "it's difficult," it
costs too much," or we don't have time."

[2] 2. Compatibility 
"We have no time for this. This is a classic argument used against any kind 
of preventive work: scheduling time for these activities moves out all
remaining milestone, unless you're willing to beileve you will earn back
the investment later. Even if you believe you will make up for the time
"lost" in inspection, you haevv to make adjustments to the initial parts of 
your development schedules, which will have an impact on the resource
planning for testing. Given the importance that many higher-level managers
put on the question "When do we start testing?" moving out the start of
testing can be really hard to justify.

3. Complexity
"We don't have the resources. Even if the relative advantages are
understood, there is the practical complexity that you need people who are
profiecient in the programming language, have sufficient application
knowledge, have been trained on doing inspections, and last but not least,
have the right attitude. It may be far from simple to find these people and 
direct their efforts torwards inspection, away from what they're doing
now. Often, the attitude of test engineers is better, but they might not
have the coding skills that are needed to do an effective inspection."

4. Trialability
"the investment is too hhigh. Introducing inspection means training people
and setting up a structure to collect and process metrics."

[3]
"The first thing I'd like to clarify is that by nature, outsourced
inspection services are comoplementarty to other quality
initiatives. Outsourced inspections are generally poor at identifying
functional defects, which require knowledge of the domain the code is
dealing with.

Nonetheless, it is an effective way of finding structural defects such as
NULL pointre dereferences, menory and other resource leaks, uninitialized
variables, bad deallocations, etc.

Second, since it is impossible to simulate every single execution path with 
every possible value, and because tradeoffs have to be made to reach
acceptable false possitve and false negative rates, outsourced inspections
don't find every instance of these structural defects (just like manual
inspections won't find every instance).

[4] No use of in-house resources. This is the point of outsourcing: apart
from a little bit of effort to pacakge the source code and fill in the
survey, these services do not require any in-house resources. This addesses 
the compatibility by countering the objecting "we have no time for this,"
complexity by countering the object "we don't have the resources," and the
trialability object "The investment is too high."

"Finds defects that testing misses. A research group at a large European
corporation performed an independent validation of the effectiveness of
outsources inspections by outsourcing inspection of an embedded application 
that had already been inspected in-house and unit tested. The subsequent
outsourced inspection revealed thirteen heretofore undetected defects, of
which eleven needed to be fixed. This addresses all three objections under
observability: overconfidence in testing effectiveness, overconfidence in
testing tools, and misunderstood responsibilities. 

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
High Quality, Low Cost Software Inspections
Louis Poulin

"What do inspections, peer reviews, walk-throughs, and structured reviews
have in common? these are all terms that are used interchangeably in
software engineering. Yet, the activites that they entail are rarely
carried out consistently in the course of developing an application."

"Why is it that peer reviews in the publishing industry are widely
accpeted, while they are the first items to be dropped off the prioriy list 
in the software industry?

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Optimizing Software Inspections
Tom Gilb

[16] 1. Don't misues Inspection as a "clean-up" process. Use it to motivate, 
teach, measure, control quality, and to improve your process. Most people
seem to think INspection is for cleaning up bad work, bugs, and
defects. However, the greatest payback comes when INspection improves
future work, i.e., reduces defect injection. Ensure that your Inspection
process fully supports the aspects of teaching and continous process
improvement."

4. Measure your benefit from using Inspections. Inspection should always be 
highly profitable., e.g. 10-to-1 ROI. If not, it is time to adjust the
Inspection process or stop it. Benefits to be measured include rework
costs, preditability, productivity, document quality, and ROI."

5. Inspect upstream first. 

[17] 6. Check the significant portions of the material - avoid checking
commentary. 

7. Use sampling to understand the quality level of a document.


12. Use serious entry conditions, e.g., numerical quality of
sources.

You can acertain the state of a source document through inexpensive
sampling. A half day on a few pages is a small price to pay to know the
state of a document that coud destroy the quality of all your work and your 
project. 

Another serious entry condition is to do a cursory check on the product
document and return it to the author when it is anything less than a
quality piece of work busting at the seams to exit (based on a cursory
check that reveals few remaining defects). For example, if while planning
the Inspection, the team leader performs a 15 minute cursory check that
shows up a few major defects on a single page, it is time for a word with
the author in private. Pretend the document was never seriously
submitted. Certainly do not waste the time of your team to confirm shoddy
work. 

[19] 21. Give Inspection team leaders proper training, coaching after
initial training, formal certification, statistical follow-up, and if
necessary, remove their "license to inspect." Proper training of team
leaders takes about a week (half lecturs and half practice). Formal
Inspection team leader certification (an entry condiction to an Inspection) 
should be similar to that for pilots, drivers, and doctors - based on
demonstrated competence after training. 

Leaders who will not professionally carry out the job, even if it is
because their supervisor is pressuring them to cut corners, need to have
their license revoked. If you take professional leadership seriously, your
players will take Inspection seriously. Ensure ther is an adequate number
of trained people to support your Inspections - at least 20 percent of all
professionals. Some clients train up to all of their engineers. 

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Inspections - some surprising findings
Robert L. Glass

"I know inspections, if done correctly, are hard work. They require many
people to perform them, and in these days of schedule-driven projects, just 
finding those people is a hard task. Inspections require preparation, and
where does that time come from? They require rigorous thinking, the kind
that exhausts participants after only an hour or two of particiapation. And 
given the typical productivity figure of 100 lines of code per hour of
successful inspection, they are extremely costly, all claims that they are
cheaper than the alternatives notwithstanding. In other words, inspection
is a very bad form of error removal-but all the others are much worse."

"Because of all that "hard work" stuff in the previous paragraph, most
companies don't do many inspections, and some do none at all. At best, the
state of the practice is "we inspect our key components." At worse, it's
"we don't have the time to do inspections at all." And that's too bad,
because the value of inspections is probably the topic on which computing
reseawrch agrees more consistently than any other."

[18] "Are formal inspecctions, with assigned roles and pre-inspection
training in inspection process, the most effective way to go? No, say
several studies. 

The most interesting is a study by Rifkin and Deimel that presented a new
way of prparing for inspections. Instead of training the participants in
inspection process, they trained them in code reading comprehension
techniques, preparing them in a product-focused manner. And the findings
were spectacular."

"Are meetings the best way to perfom inspections?" ... "Several research
studies find the answer is either "no" or "probably not". .. "showing the
use of inspection meetings tend to slow projects progress by an average of
two weeks (because of coordination problems among inspection participants), 
and that meetings produced none of the expected synergy (wherein more
errors are found because of meeting participant interactions).

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Software inspections are not for quality but for engineering economics
Tom Gilb

"The bottom line is that I believe that it is more relevant to view
Inspection as a way to control the economic aspects of software
engineering, rather tahn a way to get 'quality' by early defect
removal. Quality needs to be multidemensional, specified in quantified
requirements and architected, engineered and designed into software
products. Inspection needs to be used to monitor the entire software
engineering process."

[3] "Sampling 1 to 4 representative logical pages is usually more than
enough to convince any project about the level of major defects present in
a document of any length. If there is any doubt or discussion about the
results, a further representative or random sample will always be
sufficient to convince any skeptic about the average level of major defects 
present." 



[end]















