%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 95-24.tex -- 
%% RCS:            : $Id: 95-24.tex,v 1.1 1996/01/15 20:46:05 johnson Exp johnson $
%% Author          : Philip Johnson
%% Created On      : Thu Feb 10 11:15:01 1994
%% Last Modified By: Philip Johnson
%% Last Modified On: Wed Jan 17 15:06:59 1996
%% Status          : Unknown
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1994 University of Hawaii
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% History
%% 10-Feb-1994          Philip Johnson  
%%    

\documentstyle[nftimes,11pt,/group/csdl/tex/definemargins,/group/csdl/tex/lmacros]{article}

\input{/group/csdl/tex/psfig/psfig}                
\begin{document}
\ls{1.3}
\title{{\bf Reengineering Inspection: \\
       The Future of Formal Technical Review}}

\author{Philip M. Johnson\\
        Department of Information and Computer Sciences\\
        University of Hawaii\\
        Honolulu, HI 96822}

% \date{CSDL/ICS Technical Report 95-24 \\ \today}

\maketitle

\begin{abstract}

  Formal technical review is acknowledged as a preeminant software
  quality improvement method. The ``inspection'' review method, first
  introduced by Michael Fagan twenty years ago, has led to dramatic
  improvements in software quality. It has also led to a myopia within
  the review community, which tends to view inspection-based methods as
  not just effective, but as the {\em optimal} approach to formal
  technical review.  This article challenges this view by presenting a
  taxonomy of software review that shows inspection to be just one among
  many valid approaches. The article then builds upon this framework to propose
  seven guidelines for the radical redesign and improvement of formal
  technical review during the next twenty years.

\end{abstract}

%\ls{1.5}
\section*{Introduction}

Despite many advances in automated verification and validation, human
review of software artifacts is still a uniquely important method for
software quality improvement.  First, unlike automated methods such as
testing, review does not require an executable or formally specified
artifact, enabling quality improvement on ``upstream'' work products such
as initial requirements documents. This has significant economic
implications, since studies show that defects can be one to two orders of
magnitude less costly to remove from initial requirements documents than
from implemented systems after distribution to the customer
\cite{Boehm81}. Second, review is effective for discovering certain
``soft'' but nevertheless costly defects, such as logically correct but
poorly structured code.  Third, review has a unique educational capability:
the process of analyzing and critiquing software artifacts produced by others
is a potent method for learning about languages, design techniques,
application domains, and so forth.


``Software review'' encompasses a broad spectrum of activities, from
informal, individual desk checking all the way to review carried out using
computer-mediated groupware environments with built-in process models.
This paper focusses primarily upon a subset of software review called {\em
formal technical review}.  We define formal technical review (FTR) as
follows:

\begin{quotation}
  {\em A method involving a structured encounter in which a group of technical
  personnel analyzes an artifact using a well-specified process.
  The outcome is another, structured artifact that assesses or improves
  the quality of the original artifact as well as the quality of the method.}
\end{quotation}

This definition excludes, for example, software review methods that do 
not involve a group of people working together, or quality improvement,
or the production of an outcome artifact, or process improvement.

Starting in 1976, when Michael Fagan published his seminal article
\cite{Fagan76}, the Code Inspection style of FTR (including such close
variants as Tom Gilb's Inspection \cite{Gilb93}) has come to dominate both
the theory and practice of formal technical review.  Indeed, many
researchers and practitioners equate the terms ``inspection'' and
``formal technical review''.  Inspection and its variants have not enjoyed
unwarranted popularity; in addition to Fagan's reports of cost-effective
quality improvement at IBM \cite{Fagan86}, inspection-based
techniques at Hewlett-Packard were calculated to yield a cost savings of
US\$21 million \cite{Grady94}, and others have reported favorable
cost-benefit analyses for inspection.

Despite a consistent stream of positive findings over twenty years,
industry adoption of inspection appears to remain quite low, although no
definitive data exists.  For example, an informal USENET survey we
conducted found that 80\% of 90 respondants practiced inspection
irregularly or not at all.  Some industry practitioners have found adoption
and practice of inspection-based review to be difficult, costly,
ineffective, and/or excessively time consuming, despite the prospect of
quality improvement \cite{Brykczynski94a}.

Although low usage and adoption difficulty provide only circumstantial
evidence of problems with inspection-based methods, more direct evidence
arises from recent research by Adam Porter \cite{Porter95} and
Larry Votta \cite{Votta93}. Among other findings, their research indicates
that meeting-based methods such as inspection can impose a hidden but
significant cost by increasing development interval time, and that
alternative approaches can overcome this problem without impacting upon
defect detection effectiveness.

This article does not attempt to negate twenty years of research and
practice showing that inspection-based methods can improve an
organization's ability to detect defects, particularly when inspection is
adopted by an organization that previously lacked {\em any} formal
technical review. Rather, this article combats a myopia that
has developed within the review community, where inspection is often viewed as
not only effective, but {\em optimal}. This myopia prevents those afflicted
from seeing the ``inspection glass ceiling'' that circumscribes the
potential application and effectiveness of FTR to software development.  It
can also stifle innovative experimentation in software review.

The glass ceiling over inspection may result from its historical origins:
inspection is a product of the mid-1970's, prior to the Internet,
computer-supported cooperative work technologies, the World Wide Web,
geographically distributed development groups, and virtual
organizations. Even the latest, most comprehensive material on Inspection
defines ``inspection tools'' as word processors and
spreadsheets---technologies from the 1970's \cite{Gilb93}. 

To shatter the ceiling restricting FTR use and effectiveness, we must {\em
  reengineer} inspection---in other words, {\em radically redesign} the
process of formal technical review.  This article supports such
reengineering by articulating various limitations of an inspection-centric
perspective, and by presenting seven guidelines for next generation,
breakthrough approaches to formal technical review.

\section*{A Taxonomy of FTR}

In order to understand where the future of formal technical review lies, it
is helpful to understand where it came from and where it is now.  To
illuminate key aspects of software review research and practice, Figure
\ref{fig:taxonomy} presents a taxonomic organization of important concepts,
methods, and systems.  The organization is phylogenetic, not chronological:
entities are grouped together based upon shared characteristics rather than
historical precedence.  The next several sections traverse the hierarchy,
discussing the divisions and issues revealed by them.


\begin{figure}[htpb]
\centerline{\psfig{figure=/group/csdl/techreports/95-24/future-ftr-fig.eps}}
\caption{{\em A taxonomy of software review methods,
with representative examples of methods.}}
\label{fig:taxonomy}
\nocite{Yourdon89,McConnell93,Johnson94,Gintell93,Mashayekhi94,Brothers90,Knight93,Parnas85,Fagan76,Fagan86,Martin90,Gilb93,Humphrey90,Dyer92}
\end{figure}


\subsection*{Development method dependence}

The top-level of the taxonomy descriminates review methods based upon 
their dependence upon the surrounding software development method.
Interestingly, this discriminant is only necessary to distinguish
the Verification-based Inspection method \cite{Dyer92} used in the
Cleanroom software development method from all other review methods, these
latter methods being ostensibly independent from the development method chosen.  

\paragraph{Verification-based Inspection.}
Cleanroom's formal technical review is tightly integrated with other
components of this development method, such as object-based box structure
specification and function-theoretic correctness verification.  This
integration is so complete that the Cleanroom development method would lose
its integrity if a non-Cleanroom review method were substituted, and 
Cleanroom's review method would lose its integrity if inserted into a
different development method. This level of interdependence is unique
within the field of FTR, and is the reason why Cleanroom, although also a
formal technical review method, occupies its own, distinct niche in the
taxonomy.

\subsection*{Informal technical review}

The second major discriminant separates formal from informal technical
review methods.  Formal is not used here to mean that the method is
amenable to theoretical analysis or truth-preserving transformation.  As
noted above, the term ``formal technical review'' delimits the wide variety
of group-based methods with a structured process and outcome, whose work
products improve the quality of the artifact and the process itself.
Formal technical review methods are not {\em a priori\/} ``better'' than
informal ones.  Two popular informal review methods are Structured
Walkthrough and Code Reading.

\paragraph{Structured Walkthrough.}
One example of an informal review process is the Structured Walkthrough
\cite{Yourdon89}.  Walkthroughs are deliberately loosely defined to refer
to short (30-60 minute) meetings between the author of a software artifact
and one or more other developers to discuss the artifact's quality.
Walkthroughs do not require anyone to document the results of the meeting,
nor to use any data collected from the meeting to improve the walkthrough
process itself.  Thus, walkthroughs do not qualify as a formal technical
review method.

\paragraph{Code Reading.}
Another example of an informal review method is Code Reading
\cite{McConnell93,Card87}.  In Code Reading, two or more developers read
code independently (typically 1,000 to 10,000 lines of code at a time) and
then meet with the author individually to discuss it.  Like walkthroughs,
Code Reading does not provide any information on the process used when
reading or discussing the code, nor does it specify any constraints on the
outcome.

\medskip

There are conflicting beliefs on the merits of informal review.  Proponents
of inspection-based methods and of empirically measured process improvement
such as the Capability Maturity Model argue that an unmeasured,
ill-defined process is intrinsically worse than a measured, well-defined
one.  However, to paraphrase an old cliche, ``trees still fall in the
forest, even if no one hears them.''  In other words, just because a
process is not measured doesn't mean it is not effective, it just means
that its effectiveness is not measured.  Proponents of informal techniques
argue a software version of the ``Heisenburg uncertainty principle'': you
cannot measure the walkthrough process without disturbing it---the
effectiveness of a walkthrough is substantially due to its lack of
structure and (management-imposed) empiricism.

\subsection*{Formal technical review (manual)}

The next level in the hierarchy discriminates formal technical review
methods depending upon whether or not the review method is intrinsically
computer-mediated or manual in nature.  This discriminant was chosen
because manual group processes have essential differences from
computer-mediated group processes. For example, one or more entire
spectrums of face-to-face, synchronous interaction (such as facial
gestures, vocal timbres, spacial orientiation, etc.) are typically lost in
computer-mediated environments.  On the other side, computer-mediated
environments offer spectrums of asynchronous, distributed, and/or anonymous
interaction not possible in manual review methods.  For these reasons, we
view computer-mediated review systems as having more in common with each
other than with any manual review process, and provide a separate branch in
the taxonomy for them. (Telephone and videoconference-based review
currently falls somewhere between ``purely'' manual and ``purely''
computer-mediated, but we do not consider those variants in this taxonomy.)
The next paragraphs briefly introduce several important manual formal
technical review methods and research experiments.


\paragraph{Code Inspection.}

The seminal formal technical review method is Michael Fagan's Code Inspection
\cite{Fagan76,Fagan86}. ``Classic'' Code Inspection consists of six phases
(Planning, Overview, Preparation, Inspection, Rework, and Followup), and
four roles (Moderator, Author, Coder, and Tester).  The central and most
precisely prescribed activity is the inspection meeting, in which a
reader paraphrases the source materials, statement by statement, and the
participants interrupt with questions that may eventually lead to discovery
of errors.

\paragraph{Inspection, 2-Person Inspection, and N-Fold Inspection.}

Classic Code Inspection has been modified in many ways.  Tom
Gilb's Inspection process \cite{Gilb93} is the most comprehensive
refinement, with a number of extensions and generalizations.
While Gilb's refinements still retain the general ``look and feel''
of Classic Code Inspection, others have experimented with more substantial
modifications. For example, David Bisant and James Lyle performed a
controlled experiment which provided evidence that a two-person
inspection team can still provide improved software quality over
programmers not using any inspection method. 
Johnny Martin, W. T. Tsai, and G. Michael
Schneider documented a pair of studies to evaluate the effectiveness of
inspecting the same document with several small inspection teams
\cite{Martin90}. They found that single inspections discovered less
errors than previously reported (only 35\% of faults present), but that 
nine-fold replication raised the the fault detection rate to 78\%.  


\paragraph{Software Review.}

Watts Humphrey's Software Review method \cite{Humphrey90} begins with a
group meeting where participants review background materials presented by
the producer. Next, reviewers analyze the review artifacts individually,
using an error checklist to guide their analysis.  Unlike inspection-based
methods, the producer then correlates and consolidates the errors found by
individual reviewers, and then presents the error list item by item in a
final group meeting. Humphrey's approach does not involve a reader or any
paraphrasing.

\paragraph{Active Design Review.}

Active Design Review departs even farther from inspection, and in fact was
designed to overcome difficulties perceived with
inspection-based methods \cite{Parnas85}.  In Active Design Reviews, each
reviewer must provide answers to a set of questionnaires designed by the
producer, thus increasing, focussing, and enabling evaluation of the
analysis efforts of the reviewers.  In addition, each reviewer is utilized
as a domain specialist, and asked to review only aspects of the artifact within
their expertise.  Finally, instead of a group meeting, the producer meets
with the reviewers individually to discuss the concerns raised in the
questionnaire.

\paragraph{Phased Inspection.}

Phased Inspection \cite{Knight93} elaborates the notion of specialization
introduced by Active Design Reviews.  Not only are reviewers specialists in
certain aspects of review, but the review process is divided into a linear
sequence of phases, where each specialist eliminates a class of defects at
each step.  A system called INSPEQ was developed to support this process,
and is discussed in the next section.


\subsection*{Formal technical review (computer-mediated)}


\paragraph{ITT's system.}
Probably the first computer-mediated review system was a prototype
workstation built by ITT in 1982 \cite{Jones95}. The workstation included
specialized hardware for review, including dual displays to support
separate display of the work product and review commentary.  The secondary
display communicated via flexible optical fiber, so that it could be picked
up and moved around.  Unfortunately, no documents concerning the use and
evaluation of this system were ever published.

\paragraph{ICICLE.}
The first published research on a computer-mediated review system
concerned ICICLE (Intelligent Code Inspection in a C Language Environment)
\cite{Brothers90}, a system developed at Bellcore.
ICICLE-based review consists of two phases: an asynchronous, independent
comment preparation phase followed by a synchronous meeting phase with
participants seated at adjacent workstations.  Although ICICLE was designed
to implement ``code inspection procedures without significantly changing
them'', the researchers found that ``the computer supported meeting format
appeared to cause substantial changes in the dynamics of the code
inspection meeting''.  For example, certain facilitation responsibilities
shifted from the Moderator to the Reader, and the Scribe's decision-making
power increased substantially.

\paragraph {Scrutiny.}
Scrutiny \cite{Gintell93} is a collaborative inspection
and review system built at Bull HN Information Systems. Scrutiny implements
a computer-mediated version of the standardized, manual inspection process
at Bull. Interestingly, this approach to review method design was chosen so
that users of the system would get credit for their work as being an
``official'' inspection.  However, as with ICICLE, actual Scrutiny usage
can depart radically from manual inspection-based approaches, such as its
use for geographically distributed reviews. In addition, Scrutiny provides
a petri-net based process modelling language that allows the system to
implement alternative review methods, such as a ``shared preparation''
phase in which reviewers have access to each other's preliminary findings.

\paragraph{CSI.}
CSI \cite{Mashayekhi94} (later named CAIS) was developed at
the University of Minnesota and used in a case study to compare
distributed, computer-mediated software review to face-to-face meetings.
As with other computer-mediated systems, although CSI implements the 
general process characteristics of Humphrey's Software Review method, 
its application to geographically distributed settings radically alters
the manner in which the group interacts. 


\paragraph {INSPEQ.}
INSPEQ \cite{Knight93} is a computer-mediated system
designed to support the Phased Inspection review method. While the INSPEQ system
is described as playing an important role in achieving the goals of
efficiency and rigor in the Phased Inspection process, it is not 
viewed as essential
to the Phased Inspection method.

\paragraph {CSRS.}
CSRS \cite{Johnson94} is a
computer-mediated system designed to support research and experimentation
on computer-mediated formal technical review. CSRS, like Scrutiny, provides
an internal process modelling mechanism to support a variety of review
methods. CSRS's primary method,  FTArm, is unique among methods by being
designed explicitly to support properties of computer-mediated
communication and review. It is not only not based upon any manual method,
but it could not even be enacted in a non-computer mediated setting. 


\section*{The future of formal technical review}

A major reason why the future of formal technical review will bear little
resemblance to its past is the increasing ubiquity of local and wide area
networks within the software development industry, and the organizational
transformations co-occurring with this technology.  Networks provide
essential infrastructure for ``wide area development'' (WAD), where
software development effort is geographically distributed across a town,
county, or the globe.  Such dispersion may be motivated by the desire to
obtain diverse cultural viewpoints and presence during the development of
software, by the economic advantages of so-called ``off-shore
development'', or by the desire to exploit the 24-hour clock present when
development groups are widely separated \cite{Dedene95,Yourdon93}.

Even if an individual software development organization does not itself
become geographically distributed, organizations are increasingly 
members of ``virtual'' enterprises: closely coordinated groups from separate
companies working on a common product.  Again, networks and on-line access
play a crucial role in enabling such organizational dynamics. 

Code Inspection and its allies, however, are firmly rooted within a
paradigm whose focal point is a manual, face-to-face group process.  They
cannot adapt easily to geographically and chronologically distributed
organizations, any more than the I/O paradigm embodied in punch cards can
adapt easily to modern graphical user interfaces.
Rather than port Code Inspection, FTR researchers and practitioners must
reengineer it.  The following sections present seven guidelines that provide
starting points for this process.

\subsection*{Software development method-specific FTR}

Except for Cleanroom, current FTR techniques are remarkably decoupled from
the software development method.  Most FTR methods have a one-size-fits-all
mentality: the same approach is assumed to work equally well regardless of
whether development follows a traditional waterfall lifecycle, or
evolutionary development, or rapid prototyping, or a risk-driven spiral
model.  ``Customizing'' methods to the development context is limited to
such peripheral issues as the contents of checklists.  The parochial nature
of such ``process improvement'' is revealed by the fact that metric data is
almost never applied to evaluating if some other FTR method might be preferable to
the one in place!

In the future, more effective forms of FTR can result from exploiting the
characteristics of the surrounding development method, just as Cleanroom's
Verification-based Inspection does today.  For example, a spiral model FTR
method may take into account the importance of risk assessment and the
number of previous turns around the spiral when designing not only
checklists, but also the roles of participants and the analysis techniques
chosen.

\subsection*{Minimal meeting, asynchronous FTR}

The current dominance of inspection-based approaches leads to the
prevailing notion that face-to-face meetings are central to formal
technical review.  Inspection training materials often spend more time on
the meeting phase then all other phases put together, and special
``moderator training classes'' are often required for leaders of these
meetings.  

However, as the taxonomy and accompaning research indicates, asynchronous
mechanisms for review artifact analysis and discussion is possible and, in
many cases, preferable.  First, asynchronous review accomodates the
divergent schedules of geographically and organizationally distributed
review groups: there is no longer a need for the group to meet in the same
place at the same time. Second, asynchronous review can support review of
larger artifacts, since the rate of review is impacted negatively by the
number of participants meeting simultaneously.  Third, asynchronous review
can ameliorate the interval time problem identified by Porter and Votta for
manual, meeting-based review methods.  Fourth, asynchronous review reduces
cost by eliminating ``air time fragmentation''---the idle time incurred by
individuals during most group meetings.  Finally, although meeting
proponents often justify their need based upon the presence of ``group
synergy'', such synergy can also be observed during asynchronous review,
and Porter and Votta's research questions the true contribution of this
synergy anyway.  Future FTR methods should view face-to-face meetings as the
``phase of last resort'', when other, lower cost alternatives have already
been employed.


\subsection*{Beyond defect removal to improved developer quality}

Although software quality improvement is the stated goal of all formal
technical review methods, most operationalize this improvement quite
narrowly via a single metric: the number of defects removed per unit time.
The focus leads to directly to a well-known heuristic for successful review
meetings: ``Raise issues, don't resolve them.''  In other words, never
attempt to discuss the solutions to a problem during review---simply note
the issue and move on.

Unfortunately, by focussing on this single metric, FTR methods suppress or
eliminate the ability of the group to improve software quality in other
ways. As one example, suppose that improving the software development
skills of the participants was a {\em first-class} goal of a review
method---in other words, it was measured and used to assess the
effectiveness of review just like defect removal.  A strong argument can be
made that overall software quality is affected far more profoundly by
improvements to developer skills, which reduces future defect {\em creation}, than
by simply removing defects from current individual documents.

If such ``developer quality improvement'' was a first-class, empirically
measured goal of review, then the ``Raise issues, don't resolve them''
heuristic would lose much of its vigor, because focussed issue resolution
discussions are a high quality, efficient, and effective means for learning
about alternative design/implementation strategies and their advantages and
disadvantages.

An even more striking change resulting from this new review goal is the
utility of reviewing documents of known {\em high} quality.  If defect
removal is the only measured goal of review, then review will tend to focus
on documents that are predicted to have large numbers of defects. It would
be pointless to review a document that is known to have only a small number
of defects (or perhaps none at all).  However, learning is enhanced by
analyzing positive examples as well as negative ones. If developer quality
improvement was a first class goal of review, then occasional review of
high quality documents would help produce the desired improvements in the
metrics associated with review. Future FTR methods should focus on the
producer as much as on the products of review.


\subsection*{Beyond defect removal to organizational guideline knowledge bases}

Another casualty of the current fixation on defect removal is the potential
of FTR to incrementally generate and maintain an organizational knowledge
base of indicators of high quality software development, as manifested
within review artifacts.  The development of such a knowledge base is
crucial for an organization that desires quality insights gained during one
review to be generalized and made available in a useful manner to other
reviewers and developers. Without such a repository, organizational
learning about software quality is greatly impaired. 

Support for organizational knowledge bases in current FTR methods consists
of checklists, or review guidelines, which are supposed to be developed and
maintained as a result of review. However, current methods treat these
guidelines as a second class citizen in two important ways, thus crippling
their effectiveness.  First, development and use of guidelines is not
measured as part of review effectiveness.  For example, consider a review
session that finds only one defect in the current document, but spends time
to generate ten new guidelines that are subsequently used to discover 100
defects in later review sessions.  While this review was certainly
effective in improving the organization's software quality, current FTR
metrics do not measure either the number of guidelines resulting from a
review meeting, nor the ``downstream'' effectiveness of these
guidelines. Based upon the metrics, this review session would be a failure,
since a very low number of defects were discovered in the original document
relative to the time invested.

The second way in which guideline development is discriminated against in
current FTR methods is by its placement as a separate, ``third hour''
activity. The simple fact that generation and maintenance of guidelines
occurs after the ``real'', measured work is accomplished virtually
guarantees that it will receive less effort than that devoted to defect
detection. Indeed, one could not fault an organization for ``optimizing
away'' the third hour meeting, since it does not appear to directly
contribute to the metrics bottom line.

In the future, FTR methods should accord to guideline knowledge base
generation the first class status it deserves.  They will measure effort
spent on guideline development and fold that into the calculation of 
review effectiveness. In addition, the development of guidelines will
no longer be an appendage to review, but an integral and integrated 
component.


\subsection*{Outsourcing review and insourcing review knowledge}

Another reengineering trend for the future of FTR is the outsourcing of
formal technical review.  There already exists at least one Internet-based
software review service, in which clients send review artifacts concerning
the Microsoft Windows User Interface to the company, who sends back a Word
Document containing review information.

In the future, outsourcing review should become a viable software quality
improvement technique.  For example, a company might hire an external
consultant with specialized knowledge not present in the company to
participate in formal technical review.  The goal of the consultant's
participation will be not only to discover defects in the current document,
but also to help educate company staff and to enhance the organization's
guideline knowledge base.

In addition to outsourcing review, companies could insource
review knowledge by ``buying, not building'' their review guideline databases.
For example, a guideline database providing high quality insights into Java
software development would be of great commercial value today.

\subsection*{Computer-mediation}

A clear trend for the future of FTR is computer-mediation of review process
and products.  Prior research demonstrates a spectrum of benefits possible
from computer-mediated formal technical review. First, a
computer-mediated review environment can reduce clerical overhead, increase the
accuracy of recorded review commentary, and allow automated collection of
metrics data and their subsequent analysis \cite{Johnson94}.  In contrast,
attempts to simply ``graft'' computer support onto a manual
inspection-based process have led to {\em decreased} data accuracy and {\em
  increased} clerical overhead \cite{Weller93}.  In addition,
computer-mediation supports the re-engineering trends identified above:
computer-mediation can tightly integrate the review method with other
components of the specific software development method, it supports
asynchronous review naturally, and it facilitates both alternative metrics
collection and knowledge base generation. Finally, computer-mediation
allows review consultants to telecommute to review, and supports
standardized structures for shrink-wrapped guideline databases.

Organizations will not adopt computer-mediated review simply because of
such potential advantages over manual techniques: they will move toward it
because WAD and virtual enterprises {\em require \/} computer-mediated
review, if the software quality assurance process is to adhere to the 
underlying distributed organizational model.



\subsection*{Review mega-groups}

Current FTR methods vary in their recommendations on the best group size,
but there is widespread consensus that the group should never exceed 6-9
members. This upward bound results from inspection-based methods, in which
the overhead and group process issues for meetings involving larger groups
tends to outweigh the potential benefits.

Sometimes, however, review by larger numbers of people may be useful.  In
fact, in certain circumstances, it may be {\em required}.  For example, at
Jet Propulsion Laboratory, certain software maintenance documents could
require formal review by 20 to 30 different people.  Simply
scheduling these review meetings, much less holding them effectively
presented enormous problems.  To resolve them, JPL instituted ``Electronic
Design Reviews'', a computer-mediated process involving distribution and
review of these documents using electronic mail  \cite{Kierk93}.

In the future, organizations should exploit computer-mediated, asynchronous
FTR methods to explore the effects of dramatically greater numbers of
participants in the review process.  For example, we are currently
designing a computer-mediated FTR environment that supports {\em review
mega-groups}, where dozens to hundreds of people can participate in a
single review.


\section*{Conclusions}

The software development community owes a great debt to the pioneering work
of Michael Fagan on software quality improvement through formal technical
review. For the past twenty years, this paradigm has led to improvements in
quality unmatched by any other single technique.  It is an tribute to its
effectiveness that many view inspection as the ultimate way to perform
formal technical review. For organizations without any prior experience
with formal technical review, and who desire a well-documented, well-proven
method for software quality improvement, inspection-based approaches will
undoubtably continue to be an excellent choice for years to come.

However, software development organizations and their underlying
technologies are far different than they were twenty years ago, and with
these changes come new opportunities.  It is time to rethink some of our
implicit assumptions about software improvement through formal technical
review, and explore radical redesigns of formal technical review. 

In the future, such reengineering will qualitatively broaden the scope and
impact of formal technical review.  Rather than meeting in a room, review
groups will meet across cities, states, and countries.  Rather than the
quality relying on the efforts of three or four reviewers, dozens or
hundreds of people may participate directly, effectively, and efficiently
in the improvement of a single artifact. Rather than simply removing
defects, review will be used and evaluated with respect to its
effectiveness in improving the quality of the producers as well as the
products.  

\section*{Acknowledgments}

This article results from five years of research by the Collaborative
Software Development Laboratory on software quality improvement through
formal technical review.  I would like to acknowledge the
insights and efforts of CSDL members throughout these years, but especially
those of Danu Tjahjono, Dadong Wan, Cam Moore, Rosemary Andrada, and Robert
Brewer.  Funding for this research has been provided in part by grants
from the Pacific International Center for High Technology Research,
Tektronix, Inc., and the National Science Foundation under award
CCR-9403475.


\newpage
\ls{1.0}
\bibliography{/group/csdl/bib/csdl-trs,/group/csdl/bib/ftr,95-24}
\bibliographystyle{plain}
\end{document}
     



