%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% lab-kit-design.tex -- 
%% RCS:            : $Id: nsf93-project-research.tex,v 1.5 93/10/08 11:35:52 johnson Exp Locker: johnson $
%% Author          : Philip Johnson
%% Created On      : Thu Aug 12 16:30:04 1993
%% Last Modified By: Philip Johnson
%% Last Modified On: Tue Jan 17 11:16:07 1995
%% Status          : Unknown
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1993 University of Hawaii
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% History
%% 12-Aug-1993		Philip Johnson	
%%    

\section{Experimental Design}
\subsection{Motivation}
\label{sec:lab-investigations}

From its initial conception, formal technical review has typically been a
meeting-centered activity.  Fagan's code inspection \cite{Fagan76}
and its descendents \cite{Gilb93} devote most of the their ``methodological
energy'' to the meeting. For example, out of the five roles defined for the
process (producer, reviewer, scribe, reader, moderator), the duties of the
last three take place primarily or exclusively during the meeting. Although
non-meeting centered FTR variants exist \cite{Parnas87,Knight93,csdl-93-17},
industry use of FTR is almost exclusively inspection-based and
meeting-centered.

Meeting-based FTR has several intrinsic problems. First, meetings are
always expensive, consuming the salaries of 4-8 technical staff positions
per hour.  Second, meetings require scheduling, which may significantly
increase development interval time \cite{Votta93}.  Third, review
meetings are difficult to run properly and are frequently inadequately
prepared for.

Despite these problems, meetings are typically accepted as an unavoidable
part of FTR because they provide an opportunity for ``group synergy'', the
ability of the group to collectively improve quality better than the sum of
the individual members' efforts.  Recent studies by Votta \cite{Votta93}
and Porter \cite{Porter94}, however, cast doubt on this claim, showing
evidence that meeting gain rate (number of new defects discovered during
the meeting) is offset by meeting loss rate (number of defects not reported
during the meeting).  Based upon this data, Votta and Porter recommend that
meetings be strictly limited or eliminated entirely from FTR.

If meetings can be shown to be unequivocally inappropriate for FTR, it
would precipitate the equivalent of a Copernican Revolution in the practice
of review.  However, a great deal more information about the advantages and
disadvantages of meetings must be known before such a change in world view
is mandated.

For example, Votta and Porter's studies appear to contradict Fagan's
assertion that: ``Sometimes flagrant errors are found during... [individual
preparation], but in general, the number of errors found is not nearly as
high as in the... [group meeting] \cite{Fagan76}.''  However, this
difference in outcome might be explained by observing that the FTR method
used for Votta and Porter's studies differs significantly from that used by
Fagan.  In Fagan's method, the primary goal of individual, pre-meeting work
is to understand the artifact, and the primary goal of the meeting is to
collectively review the document and generate defects.  In Votta and
Porter's studies, the primary goal of individual, pre-meeting work is to
generate defects, and the primary goal of the meeting is to collect these
defects and generate any additional defects possible. Given the difference
in methods, it is not surprising that Fagan's method generates more defects
during the meeting, while Votta and Porter's method generates more during
pre-meeting preparation.

Another reason why eliminating meetings from FTR might be ill-advised is
because discovering new defects is but one of several motivations for a
group meeting in an FTR method.  In most traditional FTR methods, the
entire artifact is paraphrased, line by line, during the meeting. This
helps provide two benefits that contribute significantly to the overall
effectiveness of the method.  First, it enhances {\em evenness of review
coverage} by ensuring that all portions of the artifact have been examined
to a satisfactory level of detail.  Second, it enhances {\em evenness of
reviewer domain knowledge}: paraphrasing the entire document tends to
ensure that every reviewer leaves the meeting with a sound understanding of
the reviewed artifact.  In methods without such a group meeting format,
both reviewer coverage and reviewer domain knowledge would be expected to
be less even. In other words, some portions of the artifact might not be
reviewed in detail, and some reviewers might not obtain a sound
understanding of the document.

To better understand the appropriate role of group meetings in FTR, we
therefore propose to perform a multi-factor, repeated-measures experimental
study to investigate differences between meeting-based and non-meeting-based
FTR.  The general nature of the experiment is as follows. Using CSRS, we
will implement two review methods: one method in which a group
synchronously reviews and records defects in a meeting, and a second method
in which a (nominal) group asynchronously reviews and records defects
independently.  We will collect performance measures from each treatment
condition that will enable us to assess the impact of each review method on
review cost, defect rate, review coverage evenness, and reviewer domain
knowledge evenness.  The experiment is designed to provide partial
replication of Votta and Porter's studies, as well as to provide new data
on the cost and effectiveness of meeting-based FTR.  We next discuss the
experiment in more detail.


\paragraph {Hypothesis.}

We believe that a formal technical review method using a group review
meeting with paraphrasing for the purpose of artifact comprehension and
defect discovery (a.k.a. {\em synchronous review}) will differ
significantly from a formal technical review method using individual review
for the purpose of artifact comprehension and defect discovery (a.k.a. {\em
asynchronous review}).  We hypothesize that: (a) synchronous review will cost
significantly more than asynchronous review; (b) synchronous review will produce
significantly more even review coverage; and (c) synchronous review will produce
significantly more even review domain knowledge. Finally, we hypothesize
that (d) synchronous review will {\em not} detect significantly more defects than
asynchronous review (i.e. we expect this study to support the data collected
by Votta and Porter).

\paragraph {Independent Variables.}

This experiment will manipulate the following independent variables:
\begin{enumerate}

\item {\em Review Method.} Two levels: Synchronous and Asynchronous.  We
  will implement two methods in CSRS that differ only in their degree of
  synchronicity.  Both methods involve groups of three reviewers, but in
  the synchronous method, the group works together in a meeting, while in
  the asynchronous method, the reviewers form a nominal group and do not
  directly interact.

\item {\em Experimental Replication.}  Two levels: the experiment will be
  performed in Fall of 1995 and Fall of 1996.

\item {\em Review Artifact.}  Two levels: two artifacts will be reviewed.
  This variable is counterbalanced such that each subject reviews both
  artifacts, and each artifact is reviewed by both methods.

\item {\em Review Method Order.} Two levels: for each subject, either a
  synchronous review will be followed by an asynchronous review, or
  vice-versa. Review Method Order is also counterbalanced, such that each
  method and each artifact appears equally in first and second positions.

\end{enumerate}

The Review Method is the actual treatment variable.  The other independent
variables are manipulated in order to assess various threats to the
experiment's internal validity.

\paragraph {Dependent Variables.}

During each experimental trial, we will measure the following dependent
variables:

\begin{enumerate}

\item {\em Individual effort, per artifact unit}. CSRS represents a C++ 
  review artifact as a set of individual program units, such as
  functions, declarations, and so forth, which are reviewed
  independently.  Effort is measured in seconds of active review
  time.

\item {\em Individual effort, total}. Sum of Dependent Variable 1 over the 
  entire artifact.

\item {\em Group effort, per artifact unit}.  This is the sum of
  Dependent Variable 1 over all members of the group for a single unit.

\item {\em Total group effort}.  Sum of Dependent Variable 3 over the
  entire artifact.

\item {\em Individual defect detection rate}.  For each individual, measured
  as the number of defects detected divided by the total number of defects
  known to be in the artifact.

\item {\em Group defect detection rate}. Measured analogously to the
  individual defect detection rate, but using the sum of all individually
  detected defects in the group.

\item {\em Individual post-review domain knowledge.}  Measured by
  performance on a test of artifact comprehension administered after the
  review.

\end{enumerate}

\paragraph {Subjects.}

The subjects used for this experiment will be students in ICS 313,
Programming Language Theory, in the Fall semesters of 1995 and 1996. (The
Spring semester enrollment in ICS 313 is not traditionally large enough
for the purposes of this experiment.)  Fall semester enrollment in ICS 313 
is typically around 40 students.  ICS 313 students are typically juniors or
seniors who are majoring in computer science and who have had 3-4 previous
computer science courses.

\paragraph {Design.}

The purpose of this experiment is to evaluate differences in cost and
effectiveness of a synchronous (group meeting-based) review method with an
asynchronous (nominal group-based) review method.  Our basic experimental
design is thus a replicated, three-factor, combined (both within and
between subjects) design.  


%% The original, 4 person design
%%% 
%%% \begin{figure}[htbp]
%%% \begin{center}
%%% \begin{tabular} {|p{0.60in}|p{0.50in}|p{1in}|p{1in}|p{1in}|p{1in}|} \cline{3-6}
%%% \multicolumn{2}{c}{~}   & \multicolumn{4}{|c|}{Replication/Artifact} \\ \cline{3-6}
%%% \multicolumn{2}{c|}{~} & \multicolumn{2}{|c|}{Fall, 1995}   & \multicolumn{2}{|c|}{Fall, 1996}    \\ \cline{3-6}
%%% \multicolumn{2}{c|}{~} & Artifact1          & Artifact2          & Artifact1          & Artifact2      \\ \hline
%%% Review        & Sync.   & A1,~B1,~C2,~D2 & E2,~F2,~G1,~H1 & I1,~J1,~K2,~L2 & M2,~N2,~O1,~P1   \\ \cline{2-6}
%%% Method        & Async.  & E1,~F1,~G2,~H2 & A2,~B2,~C1,~D1 & M1,~N1,~O2,~P2 & I2,~J2,~K1,~L1  \\ \hline  
%%% \end{tabular}
%%% \end{center}
%%% \caption{The proposed experimental design. The letters A-P each represent
%%%          distinct, four person teams, and the numbers 1 and 2 indicate the review
%%%          method ordering.}
%%% \label{fig:design}
%%% \end{figure}


\begin{figure}[t]
\small
\begin{center}
\begin{tabular} {|p{0.60in}|p{0.50in}|p{1in}|p{1in}|p{1in}|p{1in}|} \cline{3-6}
\multicolumn{2}{c}{~}  & \multicolumn{4}{|c|}{Replication/Artifact} \\ \cline{3-6}
\multicolumn{2}{c|}{~} & \multicolumn{2}{|c|}{Fall, 1995}   & \multicolumn{2}{|c|}{Fall, 1996}    \\ \cline{3-6}
\multicolumn{2}{c|}{~} & Artifact1  & Artifact2  & Artifact1  & Artifact2  \\ \hline
Review        & Sync.  & A1,~B1,~C1 & G2,~H2,~I2 & M1,~N1,~O1 & S2,~T2,~U2 \\
              &        & D2,~E2,~F2 & J1,~K1,~L1 & P2,~Q2,~R2 & V1,~W1,~X1  \\ \cline{2-6}
Method        & Async. & G1,~H1,~I1 & A2,~B2,~C2 & S1,~T1,~U1 & M2,~N2,~O2  \\  
              &        & J2,~K2,~L2 & D1,~E1,~F1 & V2,~W2,~X2 & P1,~Q1,~R1  \\ \hline  
\end{tabular}
\end{center}
\normalsize
\caption{The proposed experimental design. The letters A-X represent
         24 distinct, three person teams, and the numbers 1 and 2 indicate the review
         method ordering.}
\label{fig:design}
\end{figure}

%% Here's a rough ASCII sketch of what the table above looks like:
%                    -----------------------Replication/Artifact-------------
%                    ------Fall,1995-----------     -------Fall, 1996--------
%                    Artifact1       Artifact2      Artifact1      Artifact2
% Review|    Sync:   A1,B1,C2,D2     E2,F2,G1,H1    I1,J1,K2,L2    M2,N2,O1,P1
% Method|    Async:  E1,F1,G2,H2     A2,B2,C1,D1    M1,N1,O2,P2    I2,J2,K1,L1


Figure \ref{fig:design} illustrates the proposed experimental design.  For
each replication, 36 subjects will be selected from ICS 313 and
randomly divided into twelve teams of three reviewers each. The letters A-X
represent the resulting 24 teams.  Each team participates in two
experimental trials, one using the synchronous review method and one using
the asynchronous review method.  In Figure \ref{fig:design}, each letter
appears twice, once in the synchronous method row and once in the
asynchronous method row.  Each team reviews a different artifact during
each experimental trial.  Finally, the numbers 1 and 2 indicate review
method ordering.  For example, in Figure \ref{fig:design}, the placements
of ``A1'' and ``A2'' indicate that Group A reviews Artifact 1 with the
Synchronous method first, and the Artifact 2 with the Asynchronous method
second.  

\par{Procedure.}

For each replication, the experiment starts with a series of lectures on
software quality assurance through formal technical review, which is a
standard part of ICS 313.  These lectures are part of a tutorial on
formal technical review developed by the Principal Investigator
\cite{Johnson95}.  Students in the class are then randomly divided into
four person teams (although only eight teams will actually participate in
the experiment).

Each team is then trained in the use of CSRS and the two review methods.
Training consists of two sessions, one which introduces CSRS and the
asynchronous review method, and a second one which introduces the
synchronous method.  During each session, a sample C++ program is reviewed
using the method.  Each training session lasts approximately one hour.  The
training session is followed by two review rounds for each team, one using
the asynchronous review method and one using the synchronous review
method.  Each review round is a separately scheduled, approximately two
hour long session. Teams will meet in a single workstation lab at the University of
Hawaii for the training session and the review rounds.  Both training and
review rounds will be videotaped. 

For the asynchronous review method, subjects work independently, navigating
the hypertext network containing one of the two review artifacts and
generating issue nodes describing defects as they are discovered.  Both
review artifacts will be C++ programs closely related to previous ICS 313
assignments, so subjects will be generally familiar with the artifact's
requirements and design. There is no time limit placed on the subjects,
although the artifacts are small enough that two hours is typically more
than sufficient for review.  (We determined this from a pilot study of a
similar experiment.)  All members of a given team perform asynchronous
review at the same time, although they do not interact with each other
during review.

For the synchronous review method, members of a team employ CSRS to step
through either Artifact1 or Artifact2 together.  One member of the team is
randomly chosen for the role of ``Leader'', which means that they
paraphrase the review artifact and record any issues generated by the team.
For each synchronous review trial, and a single trained graduate student will
participate in the role of ``Moderator'', which both standardizes moderator
effects on the dependent variables and eliminates the additional training
required in an experimental design in which a subject assumes the role of
Moderator.  Once again, there is no set time limit placed on the subjects,
although two hours is typically sufficient.

At the conclusion of each experimental trial, each subject completes an
on-line questionnaire that evaluates their current level of comprehension
of the artifact under review, and also obtains certain demographic
information.

As incentive, each student's participation in this experiment will will be
graded for credit in ICS 313.  However, students will have the option to
decline participation, in which case an alternative homework assignment
will be assigned.

\paragraph {Instrumentation.}

The principal instrument used in this study is CSRS.  CSRS provides several
experimental advantages.  First, its method definition language allows us
to easily generate review methods that differ only in well-specified ways.
Second, it provides a standardized mechanism for review that reduces
variation in the experimental procedure.  Third, it provides measurement
mechanisms that allow us to record effort more precisely and accurately
than is possible with any manual review method. 

Other instruments include the training materials on FTR, the training
materials on CSRS, the two C++ review artifacts, and the post-round
questionnaire.  All instruments will be made available as part of a
experimental lab kit for replication by other researchers. 

\paragraph {Threats to Internal Validity.}

Threats to internal validity are factors other than the independent
variables that may affect the dependent variables and thus confound the
results.  The proposed experiment is designed to account for several
classes of threats to internal validity:

\begin{itemizenoindent}

\item {\em Selection effects} due to natural variation in human performance
  are limited by: (a) using a subject population that is relatively
  homogeneous in FTR skills; (b) assigning subjects randomly to teams; (c)
  using a within-subjects design for the treatment variable.

\item {\em Carryover effects} due to learning from the first experimental
  trial are controlled by counterbalancing the artifact, method, and
  review order independent variables, thus distributing any carryover
  effects equally across treatments.

\item {\em Replication effects} due to differences in materials,
  participants, or execution across replications are minimized by using the
  same class, review methods, and artifacts in each replication. In
  addition, CSRS will provide a highly standardized environment for FTR.

\end{itemizenoindent}

\paragraph {Threats to External Validity.}

Threats to external validity jeopardize the generalization of findings from
this experiment to industry practice. Three such threats are apparent: (1)
The reviewers may not be representative of industry professionals; (2) The
artifacts may not be representative of industry artifacts; and (3) The
review methods may not be representative of industry methods.

All three threats are real, although that does not mean the results
from this study will be meaningless or irrelevant to industry.  It simply 
means that external validation of the results from this study will be
necessary, and at least some of this external validation will performed as 
part of the industry-based research proposed later in this section.

\paragraph {Analysis.}

Analysis of the data collected from this study will proceed in two
phases. 

First, an analysis of variance will be performed in order to assess the
extent to which the treatment variable (review method) accounts for
differences in the dependent variables, and not the other independent
variables.

Second, assuming that the treatment variable does account for differences
in the dependent variables, non-parametric tests such as the Wilcoxon
matched pairs Sign test will be used to assess if the values of dependent
variables obtained for the two methods differ significantly from each
other. Additional statistical analyses will be made as indicated by the
actual data obtained. 

\paragraph {Anticipated Results.}

We anticipate that this experiment will provide useful new information
about the costs and benefits of meeting-based FTR, by showing whether or
not meeting-based review leads to more {\em thorough} review: are reviewers
more knowledgeable about the artifact and is the artifact reviewed more
evenly?  In addition, the experiment will provide new insight into the
cost associated with this thoroughness.  Finally, the experiment will
provide a partial replication of Porter and Votta's research, by testing
whether or not group-based review discovers more defects than individual
review.   

Either confirmation or rejection of our hypothesis will be important to the
practice of FTR.  If our hypothesis is confirmed, then it indicates that
meeting-based FTR methods have certain benefits (albeit with certain costs)
that should be taken into account when an organization designs an FTR
method.  If our hypothesis is rejected, then it contributes new support to
the claims made by Porter, Votta, and others that non-meeting-based FTR 
methods can be as effective as traditional, meeting-based methods, but
with lower cost. 
















 













