\chapter{Related Work}

This chapter will discuss some of the research about the PSP, the idea of
measurement dysfunction, and human error.  The definitive source for PSP
information is {\it A Discipline for Software Engineering}
\cite{Humphrey95}.  Chapter 2 discussed this work extensively, so it will
not be covered again here.

\section{The PSP}

As Armour and Humphrey point out in their technical report on software
product liability, more and more common products are controlled by computer
chips \cite{Armour93}. As the instructions used become increasingly
extensive and complex, more and more opportunities arise for software
failure.  This is especially important in the area of safety-critical
systems, where a software error can cause harm or even death. Although
preventing defects is difficult, it is easier and cheaper to prevent them
than to fix problems after a product has been released.  In fact, it can be
up to 100 times more expensive to fix a bug than to prevent it from
occurring in the first place.  Software engineers need to move beyond the
``build, test, and fix quality technology'' to a development process that
focuses first on quality and defect prevention.

In a 1996 article, Humphrey explains how the PSP is intended to work
hand-in-hand with the CMM \cite{Humphrey96}. ``While the CMM enables and
facilitates good work, it does not guarantee it... This is where the PSP
comes in, with its bottom-up approach to improvement.  PSP demonstrates
process improvement principles for {\it individual engineers} so they see
how to efficiently produce quality products.''  He then describes the
development of the PSP.  Initially, it had no levels to allow gradual
introduction of the complex methods. It was so difficult to learn that, in
its complete form, he was unable to get any industrial engineers to use it
for actual development.  Eventually he broke the PSP down into separate
components that could be introduced one by one during a training course.
He then reports results from some of the first PSP classes, given in both
academic and industrial settings.  The results are based on 104 engineers.
There were more students in the classes, but their data was not used
because they, ``reported either incomplete or obviously incorrect
results.'' Results included an increase in time estimation accuracy:  For
assignment 1, 32.7\% of the subjects estimated the time required within
20\% of the correct time.  By assignment 10, this number had increased to
49.0\%.  Reported defects fell from an average of 116.4 defects per KLOC
for assignment 1 to 48.9 defects per KLOC by assignment 10.

In his master's thesis, Dellien describes his attempt to introduce a
tailored version of the PSP into an existing industrial organization
\cite{Dellien97}. To do this he used a ``box-of-tools'' approach to the
PSP.  He analyzed the PSP, broke it down into components such as
measurement and quality management, compared these components with the
existing processes used by the organization, and rebuilt a modified version
of the PSP that addressed felt development needs but did not result in
overlapping organizational processes.  The he used the three steps of
prestudy, education, and enactment to help software developers to begin
using the PSP.

He states that using PSP in an industrial setting is different than using
it in an academic setting for several reasons.  First, if PSP data is made
at all public, there is a temptation to alter that data, thus threatening
personal integrity. Second, the PSP must somehow be made to fit in with
multi-person projects and existing development processes.  Third, too many
forms combined with a lack of automation make training and acceptance
difficult.  Finally, some people may resist using the PSP because it
requires a change in their work habits.  In his conclusion, he observes
that it is difficult to objectively evaluate whether a PSP introduction has
been successful or not, even when success is measured purely as
cost-effectiveness.

Sherdil and Madhavji used the PSP as the basis of their research on
human-oriented improvement in the software process \cite{sherdil96}.  The
first goal of their study was to measure the rate of the subjects'
progress, using such variables as productivity, defect rate, and estimation
error.  Their second goal was to determine how much of the progress was due
to task repetition (first-order learning) and how much was due to
``technology injection'' (second-order learning).  They used the standard
PSP measures to track progress, and treated the PSP size estimation method
and code reviews as the technologies injected.  Interestingly, they
attempted to verify the subjects' PSP data by checking ``... for
consistency, accuracy and logical validity.  Automatic tools were also used
to verify the program size values.  We also checked if two subjects were
illegally exchanging code, but never found such an occurrence.'' Using this
data, their study (replicated twice) showed an average progress rate of
about 20\%.  Code reviews, introduced after project 6, helped subjects to
have about 13\% fewer defects over the next three projects than would have
been expected had the first-order learning curve continued.  Similarly, the
size estimation method, introduced after project 3, reduced size estimation
error by about 7\% beyond what would have been expected.

Probably the most extensive study on the PSP to date was reported by Hayes
and Over in 1997 \cite{CMU97}.  It involved 298 engineers who spent more
than 15,000 hours writing over 300,000 LOC and removing about 22,000
defects, during the course of 23 separate PSP training programs in both
academic and industrial settings.  The results reported by Hayes and Over
provide very impressive evidence to support the effectiveness of the PSP.
Over the projects completed, the median improvement in size estimation was
by a factor of 2.5, meaning that 50\% of the engineers reduced error in
their size estimates by a factor of 2.5 or more.  The median improvement in
time estimation was by a factor of 1.75.  The median reduction in overall
defect density was by a factor of 1.5.  This included a substantial
reduction in the percentage of defects surviving to both the compile and
test phases, indicating that the engineers' habits had changed in such a
way that defects were caught much earlier in the development process,
rather than being ``tested out''.  Changes in productivity were very minor,
and were statistically insignificant between PSP0 and PSP2.

Although the PSP classes were taught by SEI-trained instructors, the course
material was not necessarily ``standard'' PSP, since an unspecified number
of instructors taught tailored versions of the PSP. It was not stated how
widely these versions of the PSP differed from ``standard'' PSP. In the
area of design review, for example, it was clear that at least one class
used design review from the first assignment on, while other instructors
never introduced it at all.  In another area, size measurement, there were
24 - 89 projects per assignment with no size reported, presumably because
of a tailored version of the PSP or because of incomplete data.  This kind
of variation naturally leads to unknown effects on the data. Many
instructors chose to exclude project 10, so the number of projects
available per assignment varied widely.  For example, when measuring
defects, there were 277 projects available for assignment 1 and only 150
projects for assignment 10.  Additionally, the drop-out rate was high, so
that even for assignment 9 there were only 202 projects available, about
73\% of the number for assignment 1.  It is not clear how and when the data
for project 10 or for the dropouts was used. It was specifically excluded
for some results.

The instructors all had spreadsheets of differing types to perform the
analysis stage calculations using data taken from the subjects' paper PSP
forms.  This presumably helped the instructors to obtain analysis-stage
data that was more accurate than that produced by the subjects' manual PSP
activities.  However, the report did not mention any steps taken by the
instructors to verify the accuracy of the subjects' primary PSP data. The
authors appear to make the assumption that quality as measured by number of
recorded defects corresponds to the actual quality of completed projects.

\newpage
The authors do acknowledge some of these problems, stating, ``The data
collected during PSP training was designed to help engineers monitor and
improve their processes, not to help in writing this report.''

Humphrey and others reported similar results in 1997 {\it IEEE Computer}
\cite{Ferguson97}.  They reported on the results of three case studies
involving groups of software engineers from Advanced Information Services,
Motorola Paging Products Group, and Union Switch \& Signal. In these
studies, quality was evaluated at least partially by such measures as
acceptance test defects and use defects.

Turning to the experiences of an individual developer, Worsley reported on
his impressions of the PSP after completing the 10 recommended assignment
in {\it A Discipline for Software Engineering} \cite{Worsley96}.  Overall,
the quality of his work products appeared to improve when measured as the
number of discovered and recorded defects per KLOC: 62.5, 19.2, and 45.9
for the first three projects versus 5.9, 26.7, and 8.9 for the last three
projects.  However, this improvement was at the expense of productivity:
the LOC per hour for the last three projects was less than half that of the
first three projects.  He states that the PSP can be difficult to learn and
that it ``takes a great deal of effort to follow it,'' but concludes that
``the knowledge and insight into your programming is very rewarding.''

A review of PSP principles and a possible future development for the PSP is
covered in a series of three articles by Humphrey in CrossTalk
\cite{Humphrey98a} \cite{Humphrey98b} \cite{Humphrey98c}.  He lists four
criteria for successful introduction of the PSP: The engineers must have
proper training, the training must be done for teams or groups, there must
be strong management support, and the team members must learn how to use
PSP as a team as well as individually.  To meet this last requirement, he
has been developing the Team Software Process (TSP).  Some of its
objectives are to help in team-building; to help software teams produce
high-quality, on-time work; to aid managers in coaching and motivating
software teams; and to help organizations in fostering CMM Level 5
behavior.  The TSP will guide teams through the four phases of requirement
definition, high-level design, implementation and integration/test.
Currently, it involves 23 scripts defining 173 launch and development
steps, 14 forms, and three standards.  Sample results are given for one
group of engineers, showing improved time estimation and improved quality
(measured as number of acceptance test defects).


\section{Measurement Dysfunction}

The term ``measurement dysfunction'' seems to have been introduced by
Austin in his book {\it Measuring and Managing Performance in
  Organizations} \cite{Austin96}.  He states that, ``Dysfunction's defining
characteristic is that the actions leading to it fulfill the letter but not
the spirit of stated intentions.''  Measurement dysfunction describes a
situation where people try, consciously or unconsciously, to change a
measure used for evaluation, without trying to change the actual underlying
behavior or result that is being measured.

As a classic example of measurement dysfunction, Austin cites the
apocryphal Soviet boot factory, evaluated by party leaders based on the
measurement of ``number of boots produced''.  In order to meet their
quotas, factory managers produced only left boots, size 7.  He reviews in
more detail the case of an employment office reported by Peter Blau in
1963.  The goal of the office was, of course, to find jobs for their
unemployed clients.  Initially, the employment office employees were
evaluated primarily by the measurement of ``number of interviews
conducted''.  They responded by focusing as much time as possible on doing
interviews, and very little time in actually finding jobs for their
clients, resulting in fewer clients receiving job referrals.  Management
then changed the evaluation measure to one comprised of eight different
indicators.  Employees then changed their behavior in a variety of ways to
improve their standing against various indicators.  These changes included
destroying records of interviews that did not result in job referrals and
in making referrals that represented poor job-client matches.

\newpage
In both these examples of measurement dysfunction, the true performance of
the organization decreased over time, even while the measurement indicators
of the level of performance improved.

Austin then proceeds to divide measurements into {\it motivational
  measurements} which are ``explicitly intended to affect the people who
are being measured'', and {\it informational measurements}, which ``are
valued primarily for the logistical, status, and research information they
convey''.  Examples of using measurements for motivation include the use of
sales figures for bonuses or rewarding strong performance with increased
changes for promotion.  Examples of using measurements for information
include evaluating measurements about an existing process to design a more
efficient one and measuring inventories in order to keep the optimal amount
of needed supplies on hand.

He argues that the only way to ensure accurate informational measurements
is to strictly segregate the two kinds of measurement based on their
intended use.  Unfortunately, this is difficult to do.  Very often there
is, intentionally or unintentionally, an actual or perceived use of an
informational measure for evaluation purposes.  This will almost always
result in a manipulation of the measurement, making it less than useful
both as a piece of information and as motivation of truly productive
behavior.

\section{Human Error}

The research reported by Hayes and Over shows a high rate of error during
software development, even when measured as programmer-reported defects
\cite{CMU97}.  At the level of PSP0 quality is not yet the primary focus of
the PSP.  At this initial level, engineers reported 94.3 defects per KLOC.
Even after quality was emphasized in PSP2 through design reviews, code
reviews, and various quality measures; engineers reported 25.5 defects per
KLOC (most of which were reported to be removed before testing).

\newpage
In research done by Panko and Halverson, subjects were asked to develop a
pro forma income statement using a spreadsheet, based on a word problem
\cite{Panko96}.  The subjects made relatively few errors at the cell level
(0.9\% to 2.4\%), but these errors tended to have a ``ripple effect'', so
that the bottom line figures contained a large number of values (53\% to
80\%).  The paper cites many other studies showing similar quality problems
in a wide variety of areas.

These two examples help to illustrate that human error is a problem in
computer-related tasks ranging from software development to spreadsheets.
Errors in the PSP are similar to spreadsheet errors in the sense that a
single error can end out affecting many other data fields.  Additionally,
spreadsheets are often used to perform analysis stage PSP calculations. 

The PSP is intended to reduce the effects of human error in software
development tasks.  However, it is interesting to ask, ``To what extent can
human error affect the PSP itself?''  The following chapter describes the
case study I performed to explore this question.

