%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% case-study.tex -- 
%% Author          : Philip Johnson
%% Created On      : Wed Apr  8 14:24:28 1998
%% Last Modified By: Philip Johnson
%% Last Modified On: Mon Aug 10 14:26:51 1998
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1998 Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 3 pages


\section{CASE STUDY}

To gain insight into the occurrence and significance of collection and
analysis data quality problems, we conducted a case study. The case study
began by teaching a modified version of the PSP designed to improve data
quality.  Next, we entered the data into a database that automated most
analysis calculations and revealed the presence of a subset of the possible
errors in student data.  We then performed additional analyses on these
errors to understand their cause and potential significance to PSP data
values and the method itself.

  \subsection{The Modified PSP Curriculum}
  
  The projects used for this study were obtained from a software
  engineering class taught by Philip Johnson, in which the PSP was taught
  over the course of a semester using nine project assignments. There were
  ten students in the class, and 89 completed projects.
   
  Because of the concern with data quality from prior experience teaching
  PSP, the instructor made four principal modifications to the standard PSP
  curriculum: increased process repetition,  increased process description,
  technical reviews, and tool support.
  
  {\bf Increased process repetition.} In the standard PSP curriculum,
  students are assigned 10 programs during the semester (in addition to
  several midterm and final reports). Over the course of these ten
  programs, students practice seven different PSP processes, which means
  that the development process used by the students changes for seven out
  of ten programs.  From our initial experience with the PSP, we found
  that the overhead of this almost constant ``process acquisition'' led to
  data errors and had a significant impact upon the overall data values.
  To ameliorate this problem, the modified curriculum included only five
  PSP processes, enabling students to practice most
  processes at least twice before moving on to a new one. The modified
  curriculum also included only nine programs instead of ten, providing
  additional time in each program for data collection and analysis.
  
  {\bf Increased process description.} In our initial experiences teaching
  the PSP, the instructor found that students had a great deal of trouble
  learning to do size and time estimation correctly.  For example, PSP time
  estimation requires choosing between three alternative methods for
  estimation depending upon the types of correlations that exist in the
  historical process data from prior programs.  To help resolve this and
  other problems, the instructor added four additional worksheets: (1) a
  Time Estimating Worksheet to provide a guide through the various methods
  of time estimation; (2) a Conceptual Design Worksheet to help in
  developing class names, method names, method parameters, and method
  return values; (3) an Object Size Category Worksheet to help in size
  estimation; and (4) a Size Estimating Template Appendix to provide a
  place to record planned and actual size for prior projects.
  
  {\bf Technical reviews.} At the completion of each project, students
  divided into pairs and carried out a technical review of each other's
  work.  A two-page checklist facilitated this process.  It included such
  questions as ``Did the author follow the PSP Development Phases
  correctly?'' and ``Is the Projected LOC calculated correctly?''  A second
  ``Technical Review Defect Recording Log'' form included columns for
  number, document, severity, location, and description. Students were
  given approximately 60 minutes to do the review.  The technical review
  forms were submitted with the completed projects.  The instructor
  reviewed the projects a second time for grading purposes, using the
  Technical Review Defect Recording Log to record any additional mistakes.
  
  {\bf Tool support.} Finally, the instructor provided four spreadsheets to
  support records of planned and actual data values. In addition, students
%PJ
  were provided with well-tested tools to count non-comment source lines of
  code for Java programs, to compare two versions of a Java program and
  report non-comment lines of code added and deleted, and to perform
  certain statistical analyses.  (In the textbook PSP curriculum, students
  ``bootstrap'' their environment by implementing these tools themselves.
  While elegant pedagogically, this approach unfortunately introduces a
  potentially significant source of data quality problems, since these
  freshly developed tools with no usage history are used to generate many
  of the measures used in later data analysis.)


%PJ more changes (08/06 Added AD)
  In addition to these curriculum modifications, the instructor
  emphasized data quality throughout the course, as recommended in the
  textbook.  For example, he augmented the lecture notes in the
  Instructor's Guide with fully worked out examples of the PSP process data
  for a fictitious student to show how data is collected and analyzed for
  each assignment and accumulated over the course of the semester.  He
  dedicated lectures to collection and analysis of data periodically
  throughout the semester. He regularly showed the class aggregate
  statistics on class performance.  He met with students individually and
  in groups throughout the semester to go over their assignments and PSP
  data while they were in the midst of planning, design, code, compile,
  test, and/or postmortem; but prior to project turn-in.  He uncovered and
  removed many, many PSP data errors through these meetings which are not
  counted in our results.  He did technical reviews of every assignment's
  PSP data, and circulated problem reports throughout the semester
  summarizing issues discovered from student data.

  \subsection{PSP Data Entry Tool}
  
  We developed a database application to support analysis of PSP data from
  PSP0 to PSP2, using the Progress 4GL/RDBMS \cite{Progress98}.  In order
  to reduce opportunities for making mistakes, this tool was designed to
  require a minimum amount of user input and to provide the user with
  default values whenever possible.  Apart from task and scheduling template 
  values, the application automated all calculations, from determining delta 
  times for Time Recording Log entries to performing linear regression for 
  size estimation. In addition, the application guides the user through the
  appropriate forms and fields in the order most appropriate for the
  current process and phase.
 
  \subsection{Error Recording Method}
  
  Once the database application was ready, we entered data from the student
  project PSP forms manually and compared each student value with the
  value computed by the application.  Although every discrepancy
  between the manually generated data and the application-generated data
  could be considered an error, we only counted an error at its insertion
  point.  For example, in a Time Recording Log entry for the Design phase,
  if {\it Stop} is incorrectly subtracted from {\it Start}, {\it Delta
    Time} will be incorrect.  Even if all other calculations are done
  correctly for the rest of the project, {\it Time in Phase, Design,
    Actual}; {\it Time in Phase, Total, Actual}; {\it Time in Phase,
    Design, To Date}; {\it Time in Phase, Total, To Date}; {\it Time in
    Phase, To Date \%}; and {\it Time in Phase, To Date} values for an
  indefinite number of future projects will all be inaccurate to some
  degree.  And this is just for the most simple process, PSP0!  In more
  advanced processes {\it LOC/Hour}, time estimation, {\it Cost-Performance
    Index}, and {\it Defect Removal Efficiency} values could all be
  affected for both the current project and future projects.  To eliminate
  this combinatorial explosion in the number of errors, we counted this as a
  single error in {\it Delta Time}. 
  
  Although we analyzed the project data quite carefully, we do not feel
  confident that we have uncovered all or even most of the errors in this
  case study.  While our database application does enable us to determine
  the correctness or incorrectness of values generated during the analysis
  stage of our data quality model, it provides only limited insight into
  collection stage errors.  For example, in the Time Recording Log, it was
  possible to check the {\it Delta Time} computation, but not the accuracy
  of {\it Date}, {\it Start}, {\it Stop}, or {\it Interruption Time}.  Of
  course, the tool could not, in general, detect the absence of entries for
  work that was done but not recorded.  Two other areas that created
  similar problems were the Defect Recording Log and the measured and
  counted {\it Program Size} fields for the Project Plan Summary.

  \subsection{PSP Error Data Analysis Tool}
  
  In order to analyze the 1539 errors uncovered by the PSP data entry tool,
  we developed a second database application, the PSP Error Data Analysis
  Tool.  For each error discovered, we tracked the person who made the
  error, the method by which the error was found (technical review,
  instructor review, or comparison with the PSP tool results), the
  assignment in which the error occurred, the PSP process used for that
  assignment, the PSP phase in which the student was working when the error
  occurred, the general error type, the specific error type, the severity
  of the error, the age of the error (number of assignments since the
  introduction of the PSP operation in which the error occurred), the
  incorrect and correct values (where applicable), and an optional comment
  for noting issues of interest in that error.

  \subsection{Error Correction}
  
  Although our initial analysis of our case study data revealed many
  errors, the sheer presence of errors might only lead to imprecision,
  rather than inaccuracy. In other words, it was possible that these errors
  were only ``noise'', similar in magnitude to naturally occurring random
  fluctuations in behavior, but not sufficient to actually change the
  trends or interpretations of PSP data.
 
  To test this hypothesis, we attempted, where possible, to fix errors so
  that original and corrected versions of the data could be compared.  It
  soon became clear that errors fell into three classes.  First, there were
  errors where the correct value could be determined.  This class included
  such values as {\it LOC/Hour} that were wrong simply because of an
  incorrect calculation. These errors were easily fixed by correctly
  performing the calculation in question.  Second, there were errors where
  the correct value could not be determined, such as a blank {\it Phase
    Injected} for a Defect Recording Log entry.  Fortunately, most errors
  in this class occurred in fields that didn't affect other fields, such as
  missing header data or missing dates in the Defect Recording Log.  Third,
  there were errors where the correct value could be guessed.  In a Time
  Recording Log entry with {\it Start} 10:00, {\it Stop} 10:30, {\it
    Interruption Time} 0, and {\it Delta Time} 40; it is clear that there
  is a problem, but not clear which field is incorrect and should be
  corrected.  However we can guess that there was a problem calculating
  {\it Delta Time} and assume that the other values are valid.  To correct
  this third class of errors in an explicit and consistent fashion, we
  developed a set of rules.  Underlying each of our rules is the assumption
  that primary data is more likely to be accurate than calculations
  performed upon it. The following lists each of the rules along with
  the number of times it was used in the case study.
      
  Rule 1 (used 53 times): Defects in Time Recording Log entries should be
  handled by assuming that the start/stop/interruption times are correct
  and that the delta time is wrong, unless two Time Recording Log entries
  overlap.  In that case, the preceding and following entries and the delta
  time for the current entry should be used to formulate plausible
  start/stop times.  Generally this will mean starting the second entry
  where the first one stops.

      
  Rule 2 (used 5 times): If a Time Recording Log is missing an entry for
  an entire phase, but the Project Plan Summary form contains a value for
  the phase under {\it Time in Phase (min.), Actual}, an appropriate Time
  Recording Log entry should be formulated with fabricated date and time
  values.
        
  Rule 3 (used 28 times): For conflicts between a Defect Recording Log and a Project Plan Summary
  it should be assumed that the number of defects and the phases recorded
  in the Defect Recording Log are correct and that the discrepancy occurred
  as a result of incorrectly adding up the numbers of defects
  injected/fixed per phase and/or incorrectly transferring these totals to
  the Project Plan Summary form.
      
  Rule 4 (used 10 times): If, for the Defect Recording Log, the total of
  all fix times for defects removed in a certain phase is more than the
  time recorded for that phase in the Time Recording Log, a Time Recording
  Log entry should be inserted with start and stop times that, combined
  with the existing Time Recording Log entries for the phase, will produce
  a delta time of the total fix times plus one minute for each defect.
  This will represent the minimum amount of time required to find and
  remove the recorded defects.
      
  Rule 5 (used 1 time): To provide a value for a blank {\it Time in Phase
    (min.), Plan} field on the Project Plan Summary form, the value for
  {\it Time in Phase (min.), Actual} for the same phase should be
  used. Note that this rule, if used widely, would itself introduce
  error into the correction process. However, we used it only once 
  on one project and it has negligible impact upon our results.
    
  Rule 6 (used 62 times): Conflicts in {\it Program Size (LOC)} fields on
  the Project Plan Summary form should be handled by assuming that {\it
    Base, Deleted, Modified, Added, and Reused} are correct and that errors
  are the result of incorrect calculations for {\it Total New and Changed}
  and {\it Total LOC}.  Actually, this is not a truly satisfactory
  assumption because {\it Total LOC, Actual} should be a measurement rather
  than a calculation and should therefore be relied upon.  However, given
  correct values for {\it Base, Deleted, Modified, Added,} and {\it
    Reused}, it is possible to calculate {\it Total LOC}, whereas it is
  impossible to even guess at the correct values for the other fields.
  Unfortunately, defects in the {\it Program Size (LOC)} fields were some
  of the most common defects.  


\subsection{Data Comparison}

After we partially corrected the project data according 
to the rule set, we investigated which values to compare to best reveal the
effects of errors.  Projects 8 and 9 had the most fields to
compare since they were completed using PSP2, and provided the best
opportunities for observing the cumulative effect of errors made in earlier
projects. Project 9 was the best project for comparison because students
had had the most practice in PSP by the time this project was completed and
because it provided more time for cumulative effects to exhibit their true
characteristics. Unfortunately one student did not complete this project,
resulting in fewer data points for the final project.
   
One of the more interesting areas for comparison would have been size and
time estimation.  This was not possible due to the difficulties in
adequately correcting the {\it Program Size (LOC)} fields. Instead, we
selected a few fields from each of the other major sections of the Project
Plan Summary, including some fields that resulted from fairly simple
calculations but represented to date values from all nine projects, and
other fields that were more local to the current project but were the
result of more difficult operations.



