%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- Mode: Latex -*- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% results.tex -- 
%% Author          : Philip Johnson
%% Created On      : Wed Apr  8 14:24:46 1998
%% Last Modified By: Philip Johnson
%% Last Modified On: Mon Aug 10 14:28:37 1998
%% RCS: $Id$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Copyright (C) 1998 Philip Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 2 pages

\section{RESULTS}

Despite the discovery of data quality problems to be reported below, we
still view the case study semester as an unqualified success from an
educational standpoint.  From a quantitative perspective, student data for
the course parallels the positive outcomes from other PSP
case studies, such as those reported by the Software
Engineering Institute \cite{CMU97}:

\begin{itemize}
\item Average defect density showed a downward trend from around 200
    defects/KLOC to around 50 defects/KLOC, a 75\% decrease.

\item Average productivity showed a very slight positive
    trend, from around 15 LOC/hour to around 20 LOC/hour.

\item Time and size estimation showed dramatic improvement. On the
    last program, both size and time estimation error
    dropped below 15\% for half the class, with several
    student estimates within 3-5\% of their actual values. For
    example, one size estimate of 507 LOC was
    off by only 11 LOC. One time estimate of 14.5 hours
    was off by only 25 minutes.

\item Two students out of ten during the case study achieved what we
    consider to be the ``Holy Grail" of PSP: 100\% yield,
    i.e. programs that compiled and ran correctly the first
    time without any syntax or run-time errors.
\end{itemize}

The qualitative outcomes were equally positive. Most students expressed a
very high degree of satisfaction with the course. The following
comments are typical:

\begin{itemize}

\item ``In September, I didn't know anything about software
engineering.  Now I know a great deal thanks to PSP.  I now
know the importance of why a process is used to finish a
task.  Software development is not easy and using a process
helps in development.'' 

\item ``I thought I was a good programmer, but after using
PSP I realize that I was nothing back then.  Now, I can
proudly say that I have gotten much much better than ever
before."

\item ``I must admit, when I started this course, I understood what we were
  supposed to do in good software engineering, but I never really did it.
  Now I understand the reasons behind these practices and the benefits of
  actually following a process instead of just jumping right into coding...
  Teachers who push doing planning and design might actually know what
  they're talking about."

\item ``At the beginning, I just coded to finish the project
or solve the problem. Now I take an in-depth look at the
problem and think about it for a while before trying to
develop a solution.  By executing and learning this process
I know way more about software engineering than when I
started this course."

\end{itemize}



Despite these excellent educational outcomes, our post-course analysis
discovered significant numbers of errors in the PSP data.  The following
sections provide a breakdown of these defects according to their type,
severity, age, the manner in which they were detected, whether they
occurred during the analysis or collection stage, their ``ripple
effect'', and the overall percentage error rate.


\subsection{Error Types}

We found that the errors naturally fell into one of seven general types.
We present each type in descending order of frequency, and include the
number of errors found of that type and the percentage of all errors
represented by this type.

{\bf Calculation Error.} (705 errors, 46\%). This error type applied
to data fields whose values were derived using any sort of calculation from
addition to linear regression.  If the calculation was not done correctly,
an error was counted.  This type was not used for values that were
incorrect because other fields used in the calculation contained bad
numbers.
        
{\bf Blank Field.} (275 errors, 18\%). This error type was used when a data field
required to contain a value, such as the {\it Start} field in a Time
Recording Log entry, was left blank.  This type was not used in fields
where a value was optional, such as comment fields.
        
{\bf Transfer of Data Between Projects Incorrect.} (212 errors, 14\%) This
error type was used for incorrect values in fields that involved data from
a prior project.  Typically these fields were ``to date'' fields that
involved adding a to date value from a prior project with a similar value
in the current project.  Unfortunately, it was often impossible to
determine if the error arose from bringing forward a bad number,
or incorrectly adding two good numbers, or bringing forward the correct number
and correctly adding it to the wrong number from the current form.
However, in two important areas, time and size estimation, the forms were
modified so that students were required to fill in the prior values to be
used in the estimation calculations. In these cases we could determine
when incorrect values originated in the transfer.
        
{\bf Entry Error.} (142 errors, 9\%). This error type applied when a
student clearly did not understand the purpose of a field or used an
incorrect method in selecting data.  Examples include the use of a phase
name in the {\it Fix Defect} field of the Defect Recording Log, or having
the {\it Defects Injected, To Date} values in the Project Plan Summary
originate from a different project than the {\it Program Size (LOC), To
  Date} values.
      
{\bf Transfer of Data Within Project Incorrect.} (99 errors, 6\%). This
error type is similar to the error type involving incorrect transfer of
data between projects, except that it applied to values transferred from
one form to another within the current project.  For example, filling in
172 for {\it Estimated New and Changed LOC} on the Size Estimating
Template, but using 290 for {\it Total New and Changed, Plan} on the
Project Plan Summary.
    
{\bf Impossible Values.} (90 errors, 6\%). This error type indicates that
two values were mutually exclusive.  Examples of this error type include
overlapping time log entries, defect fix times for a phase adding up to
more time than the time log entries for the phase, or phases occurring in
the Defect Recording Log in a different order than those in the Time
Recording Log.
      
{\bf Process Sequence not Followed} (16 errors, 1\%).  This error type was
used when the Time Recording Log showed a student moving back and forth
between phases such as Compile and Test instead of sequentially moving
through the phases appropriate for the process.\newline


\subsection{Error Severity}

Some PSP data errors have relatively little ``ripple effect'' upon other
data values, while others can have an enormous impact. To gain insight into
the distribution of the ripple effect, we classified the errors into one of
five ``severity'' levels.  We present the levels in increasing order
of ripple effect.  As before, we include the total number of errors
found for a given severity level and its percentage of the total.


{\bf Error has no impact on PSP data.} (104 errors, 7\%). This level
included errors such as missing header data, incorrect dates in the time
recording log, and filling in fields for a more advanced process.
        
{\bf Results in a single bad value, single form.} (674 errors, 44\%).  This
level was used if a significant field which affected no other fields, such
as {\it LOC/Hour, Actual}, was blank or incorrect.
        
{\bf Results in multiple bad values, single form.} (197 errors, 13\%).
This level indicates when an incorrect or blank value was used in the
calculation of values for one or more other fields on the same form, but
when none of these other values were used beyond the current form.  For
example, in PSP1 on the Size Estimating Template, incorrectly calculating a
prediction interval.  This results in a bad prediction interval and a bad
prediction range, but these values are not used anywhere else in the
process.
        
{\bf Results in multiple bad values, multiple forms, single project.} (41
errors, 3\%).  This level indicates when an incorrect or blank value was
used to determine the values for one or more other fields on one or more
different forms in the same project, but when none of these other values
were used beyond the current project.  For example, in PSP1, on the Size
Estimating Template, calculating an incorrect value for {\it Estimated
  Total New Reused (T)}.  This results in an incorrect value for {\it Total
  New Reused, Plan} on the Project Plan Summary form, but this value is not
referenced by future projects.
        
{\bf Results in multiple bad values, multiple forms, multiple pro\-jects.}
(523 errors, 34\%).  This level was used if an incorrect or blank value
affected future projects.  For example, when {\it Defects Injected,
  Planning, Actual} on the Project Plan Summary does not match the number
of defects entered for the planning phase in the Defect Recording Log.

\subsection {Age of Errors}

In any learning situation, a certain number of errors are to be expected.
We hypothesized that perhaps the errors we discovered were simply a natural
by-product of the learning process, and would ``go away'' as students
gained experience with the various techniques in the PSP.

To evaluate this hypothesis, we calculated the ``age'' of errors---in other
words, the number of projects since the introduction of the data field in
which the error could be observed. If the errors were simply a by-product
of the learning process, then we would expect a low average ``age'' for
errors.  In other words, people might make an error in a field initially,
but then stop making the error after gaining more experience with the data
field in question.

For example, the calculation of {\it Delta Time} for the Time Recording Log
was introduced in the first project.  If a student made an error in this
field during the first project the error would have an age of zero.  If a
similar error was made during the second project the error would have an
age of one.  By the ninth project this type of error would have an age of
eight.
      
We first analyzed the errors to determine the average error age in each
project.  Figure \ref{errorAgeAll} shows the average age for all errors
in each project.

\begin{figure}[htbp]
  \begin{center} 
  \begin{tabular}{|l|l|r|r|}\hline 
  Project \# & PSP Process & \# of Errors & Average Age \\ \hline\hline 
  1 & PSP0    &  51  &  0.00 \\ \hline
  2 & PSP0.1  &  59  &  0.73 \\ \hline    
  3 & PSP0.1  &  63  &  1.76 \\ \hline
  4 & PSP1    & 150  &  1.27 \\ \hline
  5 & PSP1    & 165  &  2.27 \\ \hline
  6 & PSP1    & 186  &  3.30 \\ \hline
  7 & PSP1.1  & 160  &  3.26 \\ \hline
  8 & PSP2    & 351  &  3.04 \\ \hline
  9 & PSP2    & 354  &  3.84 \\ \hline
  \end{tabular}
  \end{center}
  \caption{\label{errorAgeAll}Average Error Age by Project - All Errors} 
  \end{figure}
      
  We then filtered out the 309 errors with an age of zero.  This
  eliminated errors that could result from students being
  introduced to new fields and/or PSP operations for the first time.  
  Figure  \ref{errorAgeSome} shows the resulting data.

  \begin{figure} [htpb]
  \begin{center} 
  \begin{tabular}{|l|l|r|r|}\hline 
  Project \# & PSP Process & \# of Errors & Average Age \\ \hline\hline 
  1 & PSP0    &   0  &    NA \\ \hline 
  2 & PSP0.1  &  43  &  1.00 \\ \hline 
  3 & PSP0.1  &  63  &  1.76 \\ \hline
  4 & PSP1    &  70  &  2.71 \\ \hline
  5 & PSP1    & 165  &  2.27 \\ \hline
  6 & PSP1    & 186  &  3.30 \\ \hline
  7 & PSP1.1  & 135  &  3.86 \\ \hline
  8 & PSP2    & 214  &  4.99 \\ \hline
  9 & PSP2    & 354  &  3.84 \\ \hline
  \end{tabular}
  \end{center} 
  \caption{\label{errorAgeSome}Average Error Age Where Age is not Zero}
  \end{figure}
      
When combining the 1539 errors from all projects, the average error age
was 2.78 projects.  After removing the 309 errors with an age of zero,
the average error age rose to 3.48 projects.


\subsection{Error Detection Methods}

In this study, there were three ways an error could be detected: by another
student during technical review (40 errors), by the instructor during the
grading/evaluation process (32 errors), or through the use of the PSP data
entry tool (1467 errors).  Thus, students were made aware of about 5\% of
the mistakes in their completed projects during the course of the class.

\subsection{Analysis Stage Errors}

Our two stage model of PSP data quality indicates that errors can be
introduced during either collection or analysis. 
Most of the errors that we detected occurred during PSP
analysis activities, with 700 errors occurring in the Plan phase and 561
errors in the Postmortem phase. Some of the errors occurring in other
phases, such as errors in {\it Delta Time} calculations, were also analysis
errors.

\subsubsection{The Most Severe Errors} 

34\% of errors found were of the most serious type - persistent errors.
These were the errors resulting in multiple bad values on multiple forms
for multiple projects.  A defect of this type not only causes incorrect
values in the current project, but may still be causing flawed results ten
projects later, even if all subsequent calculations are done correctly.
Figure \ref{errorsCommon} shows the four most common errors of this type.

\begin{figure} [htpb]
 
    \begin{center} 
    \begin{tabular}{|l|r|}\hline 
    Description & \# \\ \hline\hline 
    Time Estimation: historical data  &    \\ 
    not transferred correctly         & 61 \\ \hline
    Size Estimation: historical data  &    \\
    not transferred correctly         & 56 \\ \hline 
    
    Time Log: delta time incorrect    & 48 \\ \hline 
    Project Plan Summary: Total LOC,  &    \\
    actual, not equal to B-D+A+R      & 45 \\ \hline 
    \end{tabular}
    \end{center}  
    \caption{\label{errorsCommon} Most Frequently Occurring Persistent Errors}   
\end{figure}
    
There were two main ways that the error in transferring time estimation
data appeared to occur: incorrectly transferring the value from the correct
field, or accidentally transferring the correct value from an incorrect
field.  For example, instead of transferring {\it Total New and Changed
  (N)} (Plan or Actual), students often transferred {\it Total LOC (T)}.
This could easily occur because the Project Plan Summary form has over 90
fields even at the level of PSP1, and these two values are vertically
adjacent on the form. It is particularly easy to make this mistake with the
Actual values because the fields are separated by one column from the
labels.  Additionally, it appeared that students made spreadsheets to avoid
thumbing through the entire stack of completed projects every time a time
or size estimation was needed for a new project.  We infer this because the
same incorrect value for a particular project would be transferred over and
over again for time and/or size estimation in new projects.
    
Similar factors surrounding the error in transferring data for size
estimation.  These transfer errors were not insignificant.  Over the 56
errors resulting from incorrect transfer of data used for size estimation,
the sum of the errors was 7753 LOC (lines of code), with an average error
of 138.4 LOC.  The sum of the LOC as they should have been transferred was
10,255, with an average of 183 LOC per field.  Thus, the average
incorrectly transferred number was in error by an amount equaling 75.6\% of
the number that should have been transferred.
    
The error in calculating {\it Delta Time} in the Time Recording Log was
notable in several respects.  First, the errors were not insignificant.
The average mistake was 37.8 minutes, which was an average of 39.9 percent
of the correct value. Secondly, of 48 occurrences, 16 were in error by one
hour and 4 were in error by two hours, indicating small errors in simple
arithmetic. Thirdly, the distribution of this error across projects is as
shown in Table \ref{deltaErrors}.

\begin{figure}
   \begin{center} 
   \begin{tabular}{|l|r|r|r|}\hline 
   Project \#  & Errors & Time Log Entries & \% in Error \\ \hline\hline 
   1  & 7 &  84 &  8.33 \\ \hline 
   2  & 2 &  88 &  2.27 \\ \hline 
   3  & 8 &  92 &  8.70 \\ \hline 
   4  & 8 & 108 &  7.41 \\ \hline  
   5  & 2 & 102 &  1.96 \\ \hline 
   6  & 9 & 121 &  7.44 \\ \hline 
   7  & 2 &  77 &  2.60 \\ \hline 
   8  & 5 & 122 &  4.10 \\ \hline 
   9  & 5 & 105 &  4.76 \\ \hline 
   \end{tabular} \newline \newline
   \end{center} 
   \caption{\label{deltaErrors}Delta Time Errors by Project}
\end{figure}
   
Despite nine projects worth of experience, this error never ``went away''.
However it did appear to occur less frequently after Project 6.
Interestingly, the assignment for this project was a Time Recording Log
applet, which at least some students seem to have used for subsequent
projects.

 
\subsection{Collection Stage Errors}

As noted previously, analysis stage errors are relatively easy to determine
and correct. However, the accuracy of recorded process measures from the
collection stage was much more difficult to examine because the time of
collection had already passed and, unlike the analysis operations, was
impossible to reproduce. However, we found both direct and indirect
evidence for collection errors during the case study.

\subsubsection{Direct Collection Error Evidence}

Direct evidence of collection problems appeared in the 90 errors of
type of ``Impossible Values''.  We classified these errors into
three major subtypes.
      
{\bf Internal Time Log Conflicts.} There were five time logs with
overlapping entries, indicating some sort of problem with accurately
collecting time-related data.

{\bf Internal Defect Log Conflicts.} 51 errors showed problems with
correctly collecting defect data.  48 of these errors were Defect Recording
Log entries showing defects injected during the Compile and Test phases,
but not as a result of correcting other defects found during Compile or
Test.
      
{\bf Discrepancies Between Time and Defect Logs.} In 22 cases, Defect
Recording Log entries were entered with dates that did not match any Time
Recording Log entries for the given date.  For example, a defect would be
recorded as injected during the Code phase on a Wednesday, but the time log
would show that all coding had been completed by Monday and that the
project was in the Test phase on Wednesday.  For 10 projects, the total
{\it Fix Time} for defects removed during a particular phase added up to
more time than was recorded for that phase in the Time Recording Log.
Finally, in two cases, the Defect Recording Log showed a different phase
order than the Time Recording Log.


\subsubsection{Indirect Collection Error Evidence}

Besides the recorded errors, there were other indicators that collection
problems had occurred. Some Time Recording Logs showed a suspicious number
of even-hour (e.g. 6:00 to 7:00, 10:00 to 12:00) entries.  Others showed
long stretches of consecutive entries with no breaks or interruptions.
Often, the total {\it Fix Time} for the defects in a phase was far less
than the time spent in the phase. For example, the Time Recording Log might
show three hours spent in the Test phase, but the Defect Recording Log
would show two defects that took eight minutes to fix.  Obviously, it is
not impossible that this would occur, but it is much more likely that not
all defects found in test were recorded.  

In a similar vein, some projects had suspiciously few defects overall, such
as seven defects for a project with 284 new lines of code and almost 11 hours 
of development time, (including 40 minutes in compile for two defects requiring 
6 minutes of fix time). Our analysis of the PSP data for that same project 
yielded 27 errors.

Finally, the instructor has anecdotally observed the following trend in
every PSP course he has taught so far: the students turning in the highest
quality projects also tend to record far higher numbers of defects than the
students who turn in average or lower quality projects.  If this trend is
real, then we can provide two possible explanations. It may be the case
that the students turning in lower quality projects tend to make far fewer
errors than those turning in the higher quality projects, although this
seems {\em extremely} unlikely.  What appears more likely is that the
students turning in the highest quality projects also exhibit the lowest
level of collection error, which indicates that substantial but
non-enumerable collection error exists in the PSP data we examined.


\subsection{Comparison of Original and Corrected Data}

% \begin{figure}
%    \begin{center} 
%    \begin{tabular}{|l|c|c|}\hline 
%    Student & Original & Corrected  \\ \hline\hline 
%    A & 0.32 &  0.99  \\ \hline 
%    B & 0.89 &  1.05  \\ \hline 
%    C & 1.17 &  1.05  \\ \hline 
%    D & 0.76 & 1.29  \\ \hline  
%    E & 0.67 & 0.78  \\ \hline 
%    F & 0.79 & 1.12  \\ \hline 
%    G & 0.27 &  1.29  \\ \hline 
%    H & 0.91 & 0.67  \\ \hline 
%    I & 0.87 & 1.39  \\ \hline 
%    J & 1.32 & 1.39  \\ \hline 
%    \end{tabular} \newline \newline
%    \end{center} 
%    \caption{\label{compareCPI} A comparison of original and corrected
%    cost-performance index values for the ten students in the case study.}
% \end{figure}

% \begin{figure}
%    \begin{center} 
%    \begin{tabular}{|l|c|c|}\hline 
%    Student & Original & Corrected  \\ \hline\hline 
%    A & 69 &  5.7  \\ \hline 
%    B & 36 &  27  \\ \hline 
%    C & 47 &  19  \\ \hline 
%    D & 48 & 23  \\ \hline  
%    E & 11 & 12  \\ \hline 
%    F & 11 & 13  \\ \hline 
%    G & 8 &  8  \\ \hline 
%    H & 9 & 9  \\ \hline 
%    I & 45 & 7  \\ \hline 
%    J & 57 & 26  \\ \hline 
%    \end{tabular} \newline \newline
%    \end{center} 
%    \caption{\label{compareYield} A comparison of original and corrected Yield values for the ten students in the case study.}
% \end{figure}

%PJ
When we compared the original and corrected data, we found significant
differences (p$<$.05) for the Cost-Performance Ratio (planned
time-to-date/actual time-to-date) and Yield (percentage of defects injected
before first compile that were also removed before first compile).  We used
the Wilcoxon Signed Rank Test \cite{Ferguson89}, a non-parametric test of
significance which does not make any assumptions regarding the underlying
distribution of the data.  Figure \ref{compareCPI} and Figure
\ref{compareYield} illustrate the differences between these two measures
graphically.  


  \begin{figure} [htbp]
    {\centerline{\psfig{figure=8cpi2.eps}}}
    \caption{\label{compareCPI}Effect of Correction on CPI}
  \end{figure}

  \begin{figure} [htbp]
    {\centerline{\psfig{figure=8yield2.eps}}}
    \caption{\label{compareYield}Effect of Correction on Yield}
  \end{figure}


A CPI value of 1 indicates that planned effort equals actual effort. CPI
values greater than 1 indicate overestimation of resource requirements,
while CPI values less than 1 indicate underestimation of resource
requirements.  In half of the subjects, correction of the CPI value
reversed its interpretation (from underplanning to overplanning, or
vice-versa).  In the remaining cases, several corrected CPI values differed
dramatically from original values.  For example Subject A's original CPI
was 0.32, indicating dramatic underplanning, while the corrected CPI was
0.99, indicating an average planned resource requirements virtually equal
the average actual resource requirements.

Correction of yield values tended to move their values downward, sometimes
dramatically.  In half of the subjects, the corrected yield was less than
half of the original yield values, indicating that subjects were removing a
far fewer proportion of defects from their programs prior to compiling than
indicated by the Yield measurement.
   


\subsection{Overall Percentage Error Rate}
 
Such a large number of data quality errors calls into question the 
quality of instruction. Perhaps these results are a simple artifact of
poor quality control on the part of the teacher? Unfortunately, 
the very large number of data values to check in the manual PSP suggests
otherwise. 

For example, a time recording log contains six fields (plus a comment
field, but this one is extraneous): Date, Start, Stop, Interrupt time,
Delta Time, and Phase. Students typically entered about 10 time log entries
for an assignment.  This results in 60 data values to check for one student
on one assignment, and 600 data values to check for a class of 10 students.
This is for one form and one assignment. Following this approach, one can
arrive at an estimate of almost 32,000 data values to be checked by hand
for this single case study, as illustrated in Figure \ref{overallErrors}.
The 1539 data errors uncovered during this study represents only 4.8\% of
the total possible, which means that the instructor obtained over 95\%
correctness (at least with respect to analysis-stage data quality).

\begin{figure}
   \begin{center} 
   \begin{tabular}{|l|r|r|r|}\hline 
   Process & Approx. Fields & Projects & Total Values \\ \hline\hline 
   PSP0    & 200 &  10 &  2000 \\ \hline 
   PSP0.1  & 220 &  20 &  4400 \\ \hline 
   PSP1.0  & 329 &  20 &  6580 \\ \hline 
   PSP1.1  & 437 &  20 &  8740 \\ \hline  
   PSP2.0  & 528 &  19 &  10,032 \\ \hline 
   \multicolumn{2}{|r|}{\bf Total} &  {\bf 89} &  {\bf 31,752} \\ \hline 
   \end{tabular} \newline \newline
   \end{center} 
   \caption{\label{overallErrors}Data values present in PSP}
\end{figure}





