<html>

<head>
<meta http-equiv="Content-Language" content="en-us">
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="GENERATOR" content="Microsoft FrontPage 4.0">
<meta name="ProgId" content="FrontPage.Editor.Document">
<title>NSF Review information for</title>
</head>

<body>

<h2>NSF Review information for: Supporting development of highly dependable
software through continuous, automated, in-process, and individualized software
measurement validation</h2>
<p>&nbsp;</p>
<h3>Context Statement</h3>
<p><br>
High Dependability Computer and Communications Systems Research Program<br>
Division of Computer-Communications Research<br>
Directorate for Computer and Information Science and Engineering<br>
National Science Foundation<br>
<br>
Program Review Panel for HDCSE Panel<br>
July 22, 2002<br>
<br>
<br>
A Review Panel was held on July 22 to evaluate 25 (24 distinct, one
collaborative) proposals submitted to the joint NSF-NASA High Dependability
Computer and Communications Systems Research (HDCCSR) Program for the July 4th
deadline. The Program Director was conflicted with the collaborative pair of
proposals (0234362 and 0234503); these proposals are not further considered in
this context statement. Each proposal received individual written reviews from
at least 3 panelists. In addition, the panelists classified each proposal into
one of three categories: Highly Competitive, Competitive, or Not Competitive.<br>
<br>
Of the total of 23 remaining proposals, 3 were rated as Highly Competitive, 6 as
Competitive, and 14 as Not Competitive. NASA now will consider the proposals and
the reviews. On the basis of available funds, HDCCSR expects to make
approximately 12 awards taken from proposals considered by this and one other
panel from this competition.</p>
<h3><b>PANEL SUMMARY:</b></h3>
<p><br>
The panel recommended a &quot;highly competitive&quot; rating.The bulk of the
research contribution will be evaluation a number of existing software metrics
approaches in this testbed. &nbsp;&nbsp;Selected software metrics are not the
innovative aspect; the focus is on how to use metrics effectively in a
high-dependability development project. The panel suggests giving greater
emphasis to defining new metrics, especially in light of the kinds of data that
the project will be collecting<br>
(more fine-grained data about developer activities). &nbsp;This project will
support risk identification and quality assurance objectives. Continual
monitoring during development will support incremental development.<br>
<br>
Comments on special criteria:<br>
1. Dependability: Software metrics studies for the MDS project are proposed for
the purpose of predicting bugs prior to operation.<br>
2. Tools/methodology: &nbsp;An existing tool HACKYSTAT will be leveraged and
extended to evaluate the value of software metrics in a high-dependability
project setting. Work includes integration of tools with the MDS development
environment.<br>
3. Measurement: &nbsp;Problem reports are the basis for dependability measures.<br>
4. Empirical study using testbed: Overall a very good empirical plan. The panel
pointed out that negative empirical results, if any, will be valuable to the
software engineering community. &nbsp;Thresholds and experimental evaluation
criteria should be clearly defined. &nbsp;If possible, the panel suggests making
the data collected (in terms of metrics, defects, etc.) available for other
researchers to study. The software process data collected in this study would be
valuable to others working in the field.<br>
<br>
<br>
PANEL RECOMMENDATION: highly competitive<br>
</p>
<h3>Review 1</h3>
PROPOSAL NO.: 0234568<br>
INSTITUTION: U of Hawaii<br>
NSF PROGRAM: HIGHLY DEPENDABLE COMPUTING<br>
PRINCIPAL INVESTIGATOR: Johnson, Philip<br>
TITLE: Supporting development of highly dependable software through continuous,
automated, in-process, and individualized software measurement validation<br>
RATING:Good
<p>REVIEW:<br>
What is the intellectual merit of the proposed activity?<br>
<br>
Build on JPL work to design, implement and validate software measures for highly
dependable software systems. Works with risk identification and quality
assurance objectives. COntinueal monitoring during development and incremental
development support<br>
<br>
What are the broader impacts of the proposed activity?<br>
<br>
make results avialble through the web<br>
Univesity is 75% minority and State is EPSCOR<br>
will support 16 students over course of research<br>
<br>
Summary Statement<br>
<br>
Much of this appears not to be &quot;new&quot; but is being applied with
specific objectives of high dependability computing, what are the thresholds
needed. THe continual application allows for mid-course correction, which is
also needed.</p>
<h3><br>
Review 2</h3>
PROPOSAL NO.: 0234568<br>
INSTITUTION: U of Hawaii<br>
NSF PROGRAM: HIGHLY DEPENDABLE COMPUTING<br>
PRINCIPAL INVESTIGATOR: Johnson, Philip<br>
TITLE: Supporting development of highly dependable software through continuous,
automated, in-process, and individualized software measurement validation<br>
RATING:Very Good
<p>REVIEW:<br>
What is the intellectual merit of the proposed activity?<br>
<br>
This proposal will attempt to validate software measures for highly dependable
software systems. The intent is to identify risky modules early and to support
early and incremental development. &nbsp;The thrust of the research is to
validate metrics.<br>
<br>
What are the broader impacts of the proposed activity?<br>
<br>
Hawaii is an EPSCOR state and the University is claimed to be 75% minority. The
PI has a record of making his tools and results available on the Internet and
offers to do so again.<br>
<br>
<br>
Summary Statement<br>
<br>
This is an impressive proposal. Well written and organized. The planned
experiments look sound and the experimental design is solid. &nbsp;The research
should help advance our knowledge in metrics for highly dependable software.<br>
<br>
The JPL connection will be very helpful, but in places it makes the proposed
research look a little narrow. Will the results just apply to their MDS or to
other software? I'm worried about papers that say &quot;we learned this about
MDS&quot; whereas I hope for papers that say &quot;we learned this about
software&quot;.<br>
<br>
This proposal is about how to use metrics, not how to generate metrics, which is
fine. On the other hand, the old methods of Chidamber and Kemerer failed to
adequately take essential OO language features like inheritance and polymorphism
into account. That might be something you want to put on your list of problems.<br>
<br>
The PI has good results from previous grants. His '95-'98 grant lists eight
papers, four of which are in journals and his current grant lists six, three in
journals.<br>
<br>
Four students may be a bit excessive.<br>
<br>
It's not clear what Port's contribution is. He has no funding allocated to him
and no explicit duties listed.<br>
</p>
<h3>Review 3</h3>
PROPOSAL NO.: 0234568<br>
INSTITUTION: U of Hawaii<br>
NSF PROGRAM: HIGHLY DEPENDABLE COMPUTING<br>
PRINCIPAL INVESTIGATOR: Johnson, Philip<br>
TITLE: Supporting development of highly dependable software through continuous,
automated, in-process, and individualized software measurement validation<br>
RATING:Good
<p>REVIEW:<br>
What is the intellectual merit of the proposed activity?<br>
<br>
Based on the prior work of the PI, it is clear that to make the approaches to
software measurement validation effective, a low-cost, non-obtrusive approach is
necessary. &nbsp;The PI has first-hand experience in this area, and this
proposal takes that previous work forward by applying it to the MDS software
architecture. &nbsp;This case study will provide a large-scale testbed for
understanding the value of the approach. &nbsp;The bulk of the research
contribution will be evaluation a number of existing approaches in this testbed.<br>
<br>
What are the broader impacts of the proposed activity?<br>
<br>
The use of software metrics in real development projects is mixed, even after
years of effort and some indications that such approaches are used commercially.
&nbsp;This project will provide solid feedback about whether such approaches are
effective and which of the many metrics proposed are effective in the testbed
environment.<br>
<br>
Summary Statement<br>
<br>
This project will build an infrastructure for capturing information from the
software development process in such a way that it can be used to first identify
what metrics are likely to clearly indicate problems (connections between
internal measures and external outcomes) and then those metrics can be used to
alert the developers so that they are aware of issues as soon as they arise.
&nbsp;The proposal extends the work of the PI, who with previous support built
the tool Hackystat. &nbsp;&nbsp;Because the PI has already implemented this tool
and it is being used at existing sites, the question arises what additional
support is really needed to take this work forward? &nbsp;&nbsp;A second
question revolves around the open question of whether software metrics can be
used effectively to identify defects early and reduce them from the shipped
project. &nbsp;As the PI mentions in his related work section, there is still
lack of consensus about this in the discipline.<br>
</p>
<h3>Review 4</h3>
PROPOSAL NO.: 0234568<br>
INSTITUTION: U of Hawaii<br>
NSF PROGRAM: HIGHLY DEPENDABLE COMPUTING<br>
PRINCIPAL INVESTIGATOR: Johnson, Philip<br>
TITLE: Supporting development of highly dependable software through continuous,
automated, in-process, and individualized software measurement validation<br>
RATING:Very Good
<p>REVIEW:<br>
What is the intellectual merit of the proposed activity?<br>
<br>
Intellectual merit<br>
<br>
--- Advancing knowledge: Significant advancement of empirical<br>
knowledge regarding software metrics for organizations<br>
requiring high dependability.<br>
<br>
--- Qualifications of investigators: Highly qualified.<br>
<br>
--- Creativity: A well balanced synthesis of the software<br>
metrics field, as applied to NASA's MDS project.<br>
<br>
--- Approach: Access to NASA MDS developers is the key to this<br>
project. &nbsp;A letter of collaboration assures MDS project<br>
cooperation.<br>
<br>
--- Resources: No problems.<br>
<br>
<br>
<br>
What are the broader impacts of the proposed activity?<br>
<br>
Broader impacts<br>
<br>
--- Integration of research and education: Courses and tools<br>
via Internet are proposed.<br>
<br>
--- Broaden participation of underrepresented groups:<br>
Excellent. 75% of students are an underserved minority<br>
(Pacific islanders, I assume) and it is located in an EPSCOR<br>
state.<br>
<br>
--- Enhance infrastructure: Significant tools are proposed,<br>
available via Internet.<br>
<br>
--- Dissemination of results: Publications and tools will<br>
disseminate results.<br>
<br>
--- Benefits to society: The letter of collaboration from the<br>
NASA MDS project shows that results will benefit NASA.<br>
<br>
<br>
<br>
Summary Statement<br>
<br>
Summary: A very good proposal, worthy of support. &nbsp;This work<br>
will be very attractive to NASA.<br>
<br>
Additional criteria for HDCCSR<br>
<br>
--- Fundamental issues in HDCCSR: Very good relevance.<br>
Software metrics studies for the MDS project are proposed for<br>
the purpose of predicting bugs prior to operation. The<br>
proposal is silent regarding the distinctive challenges of<br>
measuring MDS, which has several unusal types of software,<br>
such as goal-directed control.<br>
<br>
--- Research products: tools and methods: &nbsp;Tools for NASA's<br>
use are proposed.<br>
<br>
--- Measures for assessing the impact of products on<br>
dependability: Excellent. Problem reports are the basis for<br>
dependability measures.<br>
<br>
--- Empirical plan: Overall a very good empirical plan.<br>
However, specific research hypotheses are not stated.<br>
</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><br>
</p>
<p>&nbsp;</p>

</body>

</html>
